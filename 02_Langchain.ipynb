{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzohaibnasir/GenAI/blob/main/02_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT8aHyxK4zUR"
      },
      "source": [
        "# Langchain: a wrapper around openai, hugging face, etc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A8hI7N42X7L"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "OPENAIAPIKEY = userdata.get(\"OPENAIAPIKEY\")\n",
        "SERPAIAPIKEY = userdata.get(\"SERPAPI\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Nwi2OBc4etm"
      },
      "outputs": [],
      "source": [
        "! pip install langchain\n",
        "! pip install openai\n",
        "! pip install google-search-results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  1. A prompt could combine instructions, context, input, and output indicators to get improved results.\n",
        "```\n",
        "\n",
        "Answer the question based on the context below. Keep the answer short. Respond \"Unsure about answer\" if not sure about the answer.\n",
        "\n",
        "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
        "\n",
        "Question: What was OKT3 originally sourced from?\n",
        "\n",
        "Answer:\n",
        "```\n",
        "\n",
        "\n",
        "or\n",
        "```\n",
        "\n",
        "Classify the text into neutral, negative or positive.\n",
        "\n",
        "Text: I think the food was okay.\n",
        "Sentiment:\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "#### Code genreation\n",
        "\n",
        "```\n",
        "1. Table departments, columns = [DepartmentId, DepartmentName]\n",
        "2. Table students, columns = [DepartmentId, StudentId, StudentName]\n",
        "3. Create a MySQL query for all students in the Computer Science Department\n",
        "```\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Advanced Promptin\n",
        "#### 1. Zero/Few shot incontext learning\n",
        "\n",
        "#### 2. Chain-of-Thought Prompting\n",
        "\n",
        " chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.\n",
        "\n",
        "Prompt:\n",
        "\n",
        "```\n",
        "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\n",
        "```\n",
        "\n",
        "\n",
        "#### 3. Zero-Shot CoT\n",
        "```\n",
        "\n",
        "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "```\n",
        "\n",
        "\n",
        "The answer is incorrect! Now Let's try with the special prompt.\n",
        "\n",
        "Prompt:\n",
        "```\n",
        "\n",
        "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "\n",
        "Let's think step by step.\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "#### 4. Self-Consistency\n",
        "\n",
        " self-consistency aims \"to replace the naive greedy decoding used in chain-of-thought prompting\". The idea is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to select the most consistent answer. This helps to boost the performance of CoT prompting on tasks involving arithmetic and commonsense reasoning.\n",
        "\n",
        " Self-consistency in prompt engineering refers to the ability of a prompt to produce consistent and coherent output when given to a language model or other AI system. This means that the prompt is well-defined, unambiguous, and clear in its intent, allowing the AI to generate responses that align with the user's goals and expectations.\n",
        "\n",
        " Self-consistency addresses this by asking the LLM the same prompt or question multiple times and then analyzing the generated responses. The idea is to leverage the power of ensemble learning by considering different \"paths\" the LLM takes to arrive at an answer.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        " Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
        "there will be 21 trees. How many trees did the grove workers plant today?\n",
        "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
        "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
        "\n",
        "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
        "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
        "\n",
        "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
        "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
        "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
        "\n",
        "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
        "did Jason give to Denny?\n",
        "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
        "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
        "\n",
        "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
        "he have now?\n",
        "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
        "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
        "\n",
        "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
        "monday to thursday. How many computers are now in the server room?\n",
        "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
        "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
        "The answer is 29.\n",
        "\n",
        "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
        "golf balls did he have at the end of wednesday?\n",
        "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
        "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
        "\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: She bought 5 bagels for $3 each. This means she spent 5\n",
        "\n",
        "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
        "A:\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "output 1:\n",
        "\n",
        "`When I was 6 my sister was half my age, so she was 3. Now I am 70, so she is 70 - 3 = 67. The answer is 67.`\n",
        "\n",
        "output 2:\n",
        "`When the narrator was 6, his sister was half his age, which is 3. Now that the narrator is 70, his sister would be 70 - 3 = 67 years old. The answer is 67.\n",
        "`\n",
        "\n",
        "output 3:\n",
        "`When I was 6 my sister was half my age, so she was 3. Now I am 70, so she is 70/2 = 35. The answer is 35.\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "Computing for the final answer involves a few steps (check out the paper for the details) but for the sake of simplicity, we can see that there is already a majority answer emerging so that would essentially become the final answer.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### 5. Generated Knowledge Prompting\n",
        "\n",
        "What is Generated Knowledge Prompting?\n",
        "\n",
        "GKP breaks down the task into two stages:\n",
        "\n",
        "Knowledge Generation: In this stage, the prompt encourages the LLM to explicitly generate information relevant to the final answer. This knowledge might include facts, definitions, concepts, or background context.\n",
        "Answer Generation: With the generated knowledge in hand, the LLM proceeds to formulate the final answer or response to the prompt.\n",
        "\n",
        "Prompt\n",
        "```\n",
        "\n",
        "Input: Greece is larger than mexico.\n",
        "Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n",
        "\n",
        "Input: Glasses always fog up.\n",
        "Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\n",
        "\n",
        "Input: A fish is capable of thinking.\n",
        "Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of ’higher’ vertebrates including non-human primates. Fish’s long-term memories help them keep track of complex social relationships.\n",
        "\n",
        "Input: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.\n",
        "Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\n",
        "\n",
        "Input: A rock is the same size as a pebble.\n",
        "Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\n",
        "\n",
        "Input: Part of golf is trying to get a higher point total than others.\n",
        "Knowledge:\n",
        "```\n",
        "\n",
        "\n",
        "Knowledge 1:\n",
        "```\n",
        "The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.\n",
        "```\n",
        "\n",
        "\n",
        "Knowledge 2:\n",
        "```\n",
        "\n",
        "Golf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into a series of holes on a course using the fewest number of strokes. The goal is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.\n",
        "```\n",
        "\n",
        "\n",
        "The next step is to integrate the knowledge and get a prediction. I reformatted the question into QA format to guide the answer format.\n",
        "\n",
        "Prompt\n",
        "```\n",
        "\n",
        "Question: Part of golf is trying to get a higher point total than others. Yes or No?\n",
        "\n",
        "Knowledge: The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.\n",
        "\n",
        "Explain and Answer:\n",
        "```\n",
        "\n",
        "\n",
        "Answer 1 (confidence very high):\n",
        "```\n",
        "\n",
        "No, the objective of golf is not to get a higher point total than others. Rather, the objective is to play a set of holes in the least number of strokes. The total number of strokes is used to determine the winner of the game, not the total number of points.\n",
        "\n",
        "```\n",
        "\n",
        "Prompt\n",
        "```\n",
        "\n",
        " Question: Part of golf is trying to get a higher point total than others. Yes or No?\n",
        "\n",
        "Knowledge: Golf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into a series of holes on a course using the fewest number of strokes. The goal is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.\n",
        "\n",
        "Explain and Answer:\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Answer 2 (confidence is a lot lower):\n",
        "\n",
        "```\n",
        "\n",
        "Yes, part of golf is trying to get a higher point total than others. Each player tries to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.\n",
        "```\n",
        "\n",
        "Some really interesting things happened with this example. In the first answer, the model was very confident but in the second not so much. I simplify the process for demonstration purposes but there are a few more details to consider when arriving at the final answer`\n",
        "\n"
      ],
      "metadata": {
        "id": "FdWwUFdeD6-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3n7V2QDaK1e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automatic Prompt Engineer (APE) is a framework designed to automate the process of generating effective prompts for large language models (LLMs).  Here's a breakdown of how it works:\n",
        "\n",
        "**Challenges of Prompt Engineering**\n",
        "\n",
        "Crafting effective prompts for LLMs can be a complex task, requiring a deep understanding of the LLM's capabilities, the desired output, and the specific task at hand.  Finding the right balance between clarity and detail can be difficult, and manually creating prompts for each task can be time-consuming.\n",
        "\n",
        "**What is Automatic Prompt Engineer?**\n",
        "\n",
        "APE addresses these challenges by automating the process of prompt generation. It leverages techniques from classical program synthesis and the human approach to prompt engineering. Here's the core functionality:\n",
        "\n",
        "1. **Input and Goal:** APE takes two main inputs:\n",
        "    * **Output Demonstrations:** These are examples of the desired output for the task. They might be text summaries, translations, code snippets,  or other formats depending on the task at hand.\n",
        "    * **Task Description:** A brief description of the task can be provided to guide the prompt generation process.\n",
        "\n",
        "2. **Candidate Prompts:** Based on the input, APE generates several candidate prompts that it believes could lead to the desired output.\n",
        "\n",
        "3. **Evaluation and Selection:** It then evaluates each candidate prompt by running it with the LLM and analyzing the resulting output. This analysis might involve metrics like accuracy, coherence, or similarity to the provided demonstrations.\n",
        "\n",
        "4. **Final Prompt:** The best-performing candidate prompt, based on the evaluation criteria, is selected as the final prompt.\n",
        "\n",
        "**Benefits of Automatic Prompt Engineer**\n",
        "\n",
        "* **Reduced Effort:** APE saves time and effort by automating the prompt creation process.\n",
        "* **Improved Efficiency:** By exploring multiple candidate prompts, APE can identify high-performing prompts that might be missed through manual creation.\n",
        "* **Adaptability:** APE can be tailored to different tasks and LLM types by adjusting the evaluation criteria and prompt generation techniques.\n",
        "\n",
        "**Limitations of Automatic Prompt Engineer**\n",
        "\n",
        "* **Limited Scope:** While APE shows promise for various tasks, it might not be suitable for all scenarios, especially those requiring highly creative or nuanced prompts.\n",
        "* **Quality of Demonstrations:** The effectiveness of APE relies heavily on the quality and relevance of the provided output demonstrations.\n",
        "* **Computational Cost:** Generating and evaluating multiple candidate prompts can be computationally expensive for complex tasks or large LLMs.\n",
        "\n",
        "**Overall, Automatic Prompt Engineer is a valuable tool for streamlining and optimizing the prompt creation process for LLMs. While it might not replace human expertise entirely, it can significantly reduce the time and effort required to generate effective prompts, leading to improved LLM performance.**\n"
      ],
      "metadata": {
        "id": "wqiLCUrXKXPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Completion(OpenAI)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import openai\n",
        "\n",
        "openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI research assistant. You use a tone that is technical and scientific.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hello, who are you?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Greeting! I am an AI research assistant. How can I help you today?\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you tell me about the creation of black holes?\"}\n",
        "    ]\n",
        ")```\n",
        "\n"
      ],
      "metadata": {
        "id": "9efXjngQMEAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single-turn tasks\n",
        "The chat format enables multi-turn conversations but it also supports single-turn tasks similar to what we used with text-davinci-003. This means we can use ChatGPT to perform similar tasks as what we have demonstrated for the original GPT models. For example, let's try to perform the following question-answering task using ChatGPT:\n",
        "input\n",
        "\n",
        "```\n",
        "\n",
        "USER: Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
        "\n",
        "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
        "\n",
        "Question: What was OKT3 originally sourced from?\n",
        "\n",
        "Answer:\n",
        "\n",
        "```\n",
        "\n",
        "Output:\n",
        "```\n",
        "\n",
        "ASSISTANT: Mice.\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "lraodi4ENgB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More formally\n",
        "```\n",
        "\n",
        "CONTENT = \"\"\"Answer the question based on the context below. Keep the answer short and concise. Respond \\\"Unsure about answer\\\" if not sure about the answer.\n",
        "\n",
        "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
        "\n",
        "Question: What was OKT3 originally sourced from?\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": CONTENT},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "77qXdKGeNgFo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iHRcN4JDNy0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bvjJdxr9Ny4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "475nERGZNy7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rquSKyrDNy_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8a8R3xW2rZM"
      },
      "source": [
        "## `function calling` connects LLM to external tools(API):.\n",
        "\n",
        "By using argument in function, you will be calling a third party API. You can do this using ` chat.completions` method.\n",
        "\n",
        "but langchain is not only limited tot his.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XyY6Eu3CKV17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-CDf6OK2e7Y"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bZgnhWq2hOA",
        "outputId": "081741d4-96c3-408e-c78d-87ca1c76ee37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "client = OpenAI(api_key=OPENAIAPIKEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZuL8TpZH2hQG",
        "outputId": "318a4a87-8a9d-49e1-adf1-d62a586ffc6e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'There are 48 countries in Asia.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# zero shot prompting\n",
        "prompt = \"can yiou tell me total number of countries in asia?\"\n",
        "response = client.invoke(prompt).strip()\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVIOY9e72hSV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXh3YDqkGIpW"
      },
      "source": [
        "# Prompt templates\n",
        "\n",
        "A prompt template refers to a reproducible way to generate a prompt”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7o4jKHB2hYR"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2ExGmz_2haF"
      },
      "outputs": [],
      "source": [
        "# object of projecttemplate class\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables=[\"country\"],\n",
        "    template=\"can you tell me the capital of {country}?\",  # template: how may prompt will ve looking like\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Rr8aJmDo2hbl",
        "outputId": "3a008d5f-b803-4fcd-af87-8b028b745e63"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'can you tell me the capital of china?'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_template_name.format(country=\"china\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHcUBl-u19YH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ybKcX0f4OHsr",
        "outputId": "f437d661-4955-4c50-c184-fc416249ad77"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I is Zohaib.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = PromptTemplate(input_variables=[\"Pronoun\", \"Name\"], template=\"{Pronoun} is {Name}.\")\n",
        "\n",
        "x.format(Pronoun=\"I\", Name=\"Zohaib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh_Sm2-dOHvH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvQehZ0JYaOE"
      },
      "source": [
        "# with this you can create a prompt based on input variable. you also dont want use to give whole prompt. You may only want cityname from user. Langchain give us this functionality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MbXTs2r-OHxs",
        "outputId": "137d2569-ceee-4489-f994-d109cdc632a0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The capital of China is Beijing.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_template_name.format(country=\"china\")\n",
        "prompt1 = prompt_template_name.format(country=\"China\")\n",
        "response1 = client.invoke(prompt1).strip()\n",
        "response1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53Z-rRH9OH1q",
        "outputId": "aeda5fd9-c5ed-481e-bb65-e16a21c25b74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['product'], template='what is a good name for a country that makes a {product}?')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# another way\n",
        "prompt3 = PromptTemplate.from_template(\n",
        "    \"what is a good name for a country that makes a {product}?\"\n",
        ")\n",
        "prompt3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "unpO0d51XLry",
        "outputId": "7dc44eb0-00d9-4971-b8e9-25f088f9e423"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'what is a good name for a country that makes a toys?'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt3 = prompt3.format(product=\"toys\")\n",
        "prompt3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "abWa0B0qOH34",
        "outputId": "d7cfd16c-1158-4c32-d933-a9bba95ccd89"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Toylandia'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.invoke(prompt3).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAnwv4B3OH5Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08VTzoSYOH7s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck1aeW7xXqBR"
      },
      "source": [
        "## Agent\n",
        "\n",
        "is to call any third-party tool\n",
        "\n",
        "An LLM agent is an artificial intelligence system that utilizes a large language model (LLM) as its core computational engine to exhibit capabilities beyond text generation, including conducting conversations, completing tasks, reasoning, and can demonstrate some degree of autonomous behaviour.\n",
        "\n",
        "LLM agents are directed through carefully engineered prompts that encode personas, instructions, permissions, and context to shape the agent's responses and actions.\n",
        "\n",
        "A key advantage of LLM agents is their ability to varying degrees of autonomy. Based on the capabilities granted during the design phase, agents can exhibit self-directed behaviours ranging from purely reactive to highly proactive.\n",
        "\n",
        "With sufficient prompting and access to knowledge, LLM agents can work semi-autonomously to assist humans in a range of applications, from conversational chatbots to goal-driven automation of workflows and tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oYJFG_StXu-z",
        "outputId": "dc71b057-dda8-4f07-8e4e-162414750250"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The recent cricket World Cup was won by England in 2019.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt4 = \"who won the recent ccricket worldcup?\"\n",
        "response4 = client.invoke(prompt4).strip()\n",
        "response4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1STVpYJXvs0"
      },
      "outputs": [],
      "source": [
        "# for extracting realtime info\n",
        "\n",
        "\n",
        "# SERPAIAPIKEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZZit-K-XvvO"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentType, load_tools, initialize_agent\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaT4uMZNXvx_",
        "outputId": "debe25d3-68d5-46c4-99c8-dffb70c4f768"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='e77744bc185e55d699bd8780219369043f60c033c0c10219fffbd08c536173ac', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='e77744bc185e55d699bd8780219369043f60c033c0c10219fffbd08c536173ac', aiosession=None)>)]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# SERPAIAPIKEY\n",
        "\n",
        "# client = OpenAI(openai_api_key = OPENAIAPIKEY)\n",
        "\n",
        "# load a tool\n",
        "\n",
        "tool = load_tools([\"serpapi\"], serpapi_api_key=SERPAIAPIKEY, llm=client)\n",
        "tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCmqtYPoXv0L"
      },
      "outputs": [],
      "source": [
        "# agent_type\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tool, client, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        ")\n",
        "# agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "G1_bgXCWXv2X",
        "outputId": "cbb2f226-e4e7-4b23-9c90-c3239da4166b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m Cricket being a popular sport, I should be able to find the answer easily on a search engine.\n",
            "Action: Search\n",
            "Action Input: \"recent cricket worldcup winner\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m{'title': 'ICC Cricket World Cup', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cfc5901da78c6bd87d833f768ebd101429137beeb9a9c0674f.png', 'games': [{'tournament': 'ICC Cricket World Cup', 'stadium': 'Narendra Modi Stadium', 'date': 'Nov 19, 23', 'teams': [{'name': 'India', 'score': '240 (50)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fdb2d61f68c93d5c5d680d7c0fffb897fdb28b3e8650059349eb9734dde3775083.png'}, {'name': 'Australia', 'score': '241/4 (43)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fdb2d61f68c93d5c5de78dedffe8abcdfd464bb467402148d61eee27a3b6703e6d.png'}], 'status': 'AUS won by 6 wickets (42 balls left)'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Eden Gardens', 'date': 'Nov 16, 23', 'teams': [{'name': 'South Africa', 'score': '212 (49.4)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fd145692d1e4c364457f935d8d608026c0b073a7f05d3e653eea2c53707d55555b.png'}, {'name': 'Australia', 'score': '215/7 (47.2)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fd145692d1e4c3644599aa40bf498af342c0e35cc58ad84ec0de9fc585a97c01d6.png'}], 'status': 'AUS won by 3 wickets (16 balls left)'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Wankhede Stadium', 'date': 'Nov 15, 23', 'teams': [{'name': 'India', 'score': '397/4 (50)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fd89094923b2371fca8a1daf74b11ea96854b657272a927849ed05301dcdc4172a.png'}, {'name': 'New Zealand', 'score': '327 (48.5)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fd89094923b2371fca56bb0338b23923d086beb16d583cf2fcd536b3d6d57fbacd.png'}], 'status': 'IND won by 70 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'M. Chinnaswamy Stadium', 'date': 'Nov 12, 23', 'teams': [{'name': 'India', 'score': '410/4 (50)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fdba0e5b17b2d496711fa3bfbb4c7a349a4b50bbb32a99794d93dcb6c693f56812.png'}, {'name': 'Netherlands', 'score': '250 (47.5)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fdba0e5b17b2d49671916b8706bbc9954962fc70318918b588059157f83b15b830.png'}], 'status': 'IND won by 160 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Eden Gardens', 'date': 'Nov 11, 23', 'teams': [{'name': 'England', 'score': '337/9 (50)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fd5a8dc007ba300d2f88cdbea2f51efbb23346d489152bf8a134d7635f456c00ab.png'}, {'name': 'Pakistan', 'score': '244 (43.3)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fd5a8dc007ba300d2ff7d83a0338686b0b27b4c2b470bed706e2fb5fbbba9155e6.png'}], 'status': 'ENG won by 93 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Maharashtra Cricket Association Stadium', 'date': 'Nov 11, 23', 'teams': [{'name': 'Bangladesh', 'score': '306/8 (50)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fdd764f95c36c8c713869b01aae6d3cc26e91ce5285d3cb9b4a1b0f215ac9f0d08.png'}, {'name': 'Australia', 'score': '307/2 (44.4)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fdd764f95c36c8c713873bc6608699c9cd41512e2500f3cc124d3ab52145663853.png'}], 'status': 'AUS won by 8 wickets (32 balls left)'}]}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I see that Australia won the recent cricket world cup, but I should verify this information on a reliable source.\n",
            "Action: Search\n",
            "Action Input: \"recent cricket worldcup winner\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m{'title': 'ICC Cricket World Cup', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cfc5901da78c6bd87d833f768ebd101429137beeb9a9c0674f.png', 'games': [{'tournament': 'ICC Cricket World Cup', 'stadium': 'Narendra Modi Stadium', 'date': 'Nov 19, 23', 'teams': [{'name': 'India', 'score': '240 (50)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fdb2d61f68c93d5c5d680d7c0fffb897fdb28b3e8650059349eb9734dde3775083.png'}, {'name': 'Australia', 'score': '241/4 (43)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fdb2d61f68c93d5c5de78dedffe8abcdfd464bb467402148d61eee27a3b6703e6d.png'}], 'status': 'AUS won by 6 wickets (42 balls left)'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Eden Gardens', 'date': 'Nov 16, 23', 'teams': [{'name': 'South Africa', 'score': '212 (49.4)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fd145692d1e4c364457f935d8d608026c0b073a7f05d3e653eea2c53707d55555b.png'}, {'name': 'Australia', 'score': '215/7 (47.2)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fd145692d1e4c3644599aa40bf498af342c0e35cc58ad84ec0de9fc585a97c01d6.png'}], 'status': 'AUS won by 3 wickets (16 balls left)'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Wankhede Stadium', 'date': 'Nov 15, 23', 'teams': [{'name': 'India', 'score': '397/4 (50)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fd89094923b2371fca8a1daf74b11ea96854b657272a927849ed05301dcdc4172a.png'}, {'name': 'New Zealand', 'score': '327 (48.5)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fd89094923b2371fca56bb0338b23923d086beb16d583cf2fcd536b3d6d57fbacd.png'}], 'status': 'IND won by 70 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'M. Chinnaswamy Stadium', 'date': 'Nov 12, 23', 'teams': [{'name': 'India', 'score': '410/4 (50)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fdba0e5b17b2d496711fa3bfbb4c7a349a4b50bbb32a99794d93dcb6c693f56812.png'}, {'name': 'Netherlands', 'score': '250 (47.5)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fdba0e5b17b2d49671916b8706bbc9954962fc70318918b588059157f83b15b830.png'}], 'status': 'IND won by 160 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Eden Gardens', 'date': 'Nov 11, 23', 'teams': [{'name': 'England', 'score': '337/9 (50)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fd5a8dc007ba300d2f88cdbea2f51efbb23346d489152bf8a134d7635f456c00ab.png'}, {'name': 'Pakistan', 'score': '244 (43.3)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fd5a8dc007ba300d2ff7d83a0338686b0b27b4c2b470bed706e2fb5fbbba9155e6.png'}], 'status': 'ENG won by 93 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Maharashtra Cricket Association Stadium', 'date': 'Nov 11, 23', 'teams': [{'name': 'Bangladesh', 'score': '306/8 (50)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fdd764f95c36c8c713869b01aae6d3cc26e91ce5285d3cb9b4a1b0f215ac9f0d08.png'}, {'name': 'Australia', 'score': '307/2 (44.4)', 'thumbnail': 'https://serpapi.com/searches/65ec42690f9866d7e3a149e3/images/13e55d721619d9cf48d849f5e98006fdd764f95c36c8c713873bc6608699c9cd41512e2500f3cc124d3ab52145663853.png'}], 'status': 'AUS won by 8 wickets (32 balls left)'}]}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know that Australia is the winner of the recent cricket world cup.\n",
            "Final Answer: Australia is the winner of the recent cricket world cup.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Australia is the winner of the recent cricket world cup.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.run(\"Who won recent cricket worldcup?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-McYwibXv56"
      },
      "outputs": [],
      "source": [
        "! pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKi53hF2Xv84"
      },
      "outputs": [],
      "source": [
        "toolW = load_tools([\"wikipedia\"], llm=client)\n",
        "\n",
        "agentW = initialize_agent(\n",
        "    tool, client, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80hkv58wXv_m",
        "outputId": "84b7a697-6ae1-4c7a-e657-2e81683c6d12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use a search engine to find the answer\n",
            "Action: Search\n",
            "Action Input: recent cricket world cup winner\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m{'title': 'ICC Cricket World Cup', 'thumbnail': 'https://serpapi.com/searches/65ec45e32d5b12f331834683/images/2fdbb0c37f09e24f47c6b398225049d74afe799a01204331cc4330744248b6c6.png', 'games': [{'tournament': 'ICC Cricket World Cup', 'stadium': 'Narendra Modi Stadium', 'date': 'Nov 19, 23', 'teams': [{'name': 'India', 'score': '240 (50)', 'thumbnail': 'https://serpapi.com/searches/65ec45e32d5b12f331834683/images/2fdbb0c37f09e24f4e1e177856b55d267a25c24f6ecfd14d8eba614c1c1ffb24b005bfd6845353d62a11bbc2d21968dd.png'}, {'name': 'Australia', 'score': '241/4 (43)', 'thumbnail': 'https://serpapi.com/searches/65ec45e32d5b12f331834683/images/2fdbb0c37f09e24f4e1e177856b55d267a25c24f6ecfd14d120adf0c2b5cc309a16f55068e5b3a8b076511f8bda52596.png'}], 'status': 'AUS won by 6 wickets (42 balls left)'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Eden Gardens', 'date': 'Nov 16, 23', 'teams': [{'name': 'South Africa', 'score': '212 (49.4)', 'thumbnail': 'https://serpapi.com/searches/65ec45e32d5b12f331834683/images/2fdbb0c37f09e24f4e1e177856b55d26cedd35115b130680ee576312cfe07352166d731b0157e5d35ee7cb54debbd184.png'}, {'name': 'Australia', 'score': '215/7 (47.2)', 'thumbnail': 'https://serpapi.com/searches/65ec45e32d5b12f331834683/images/2fdbb0c37f09e24f4e1e177856b55d26cedd35115b130680816c3f13d5bba8613e16681e9c70851db5edc95473669ac3.png'}], 'status': 'AUS won by 3 wickets (16 balls left)'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Wankhede Stadium', 'date': 'Nov 15, 23', 'teams': [{'name': 'India', 'score': '397/4 (50)', 'thumbnail': 'https://serpapi.com/searches/65ec45e32d5b12f331834683/images/2fdbb0c37f09e24f4e1e177856b55d26d8785df224ca10390f8157006aa3115b84dea8a419269846c17f495ea117a1f5.png'}, {'name': 'New Zealand', 'score': '327 (48.5)', 'thumbnail': 'https://serpapi.com/searches/65ec45e32d5b12f331834683/images/2fdbb0c37f09e24f4e1e177856b55d26d8785df224ca103984071b49ac55efb6723569ad8cb09e074fdc4355c41557d8.png'}], 'status': 'IND won by 70 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'M. Chinnaswamy Stadium', 'date': 'Nov 12, 23', 'teams': [{'name': 'India', 'score': '410/4 (50)', 'thumbnail': 'https://serpapi.com/searches/65ec45e32d5b12f331834683/images/2fdbb0c37f09e24f4e1e177856b55d2696e25c13a9eaa4c538d3425b92730b39f88f028b11eb002a6a4108a97f83142c.png'}, {'name': 'Netherlands', 'score': '250 (47.5)', 'thumbnail': 'https://serpapi.com/searches/65ec45e32d5b12f331834683/images/2fdbb0c37f09e24f4e1e177856b55d2696e25c13a9eaa4c55de4974213f05d8d83826dc86b25b2d96085dd97336e1459.png'}], 'status': 'IND won by 160 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Eden Gardens', 'date': 'Nov 11, 23', 'teams': [{'name': 'England', 'score': '337/9 (50)', 'thumbnail': 'https://serpapi.com/searches/65ec45e32d5b12f331834683/images/2fdbb0c37f09e24f4e1e177856b55d261f8a238fc670709a45adeb22f132e87780086e0676d67fb434983fae093478b3.png'}, {'name': 'Pakistan', 'score': '244 (43.3)', 'thumbnail': 'https://serpapi.com/searches/65ec45e32d5b12f331834683/images/2fdbb0c37f09e24f4e1e177856b55d261f8a238fc670709a33460f88acdd60e885975d3fa235611eb5b9340dba37ccef.png'}], 'status': 'ENG won by 93 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Maharashtra Cricket Association Stadium', 'date': 'Nov 11, 23', 'teams': [{'name': 'Bangladesh', 'score': '306/8 (50)', 'thumbnail': 'https://serpapi.com/searches/65ec45e32d5b12f331834683/images/2fdbb0c37f09e24f4e1e177856b55d26b472cf22fa87ddd0da6a4df870fa888923ac98513caa7dd330e1da5b96268245.png'}, {'name': 'Australia', 'score': '307/2 (44.4)', 'thumbnail': 'https://serpapi.com/searches/65ec45e32d5b12f331834683/images/2fdbb0c37f09e24f4e1e177856b55d26b472cf22fa87ddd0529ddd99c91a37f6adc20541ba0a7c1370ec95a3308ce246.png'}], 'status': 'AUS won by 8 wickets (32 balls left)'}]}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Australia won the recent Cricket World Cup.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'who won recent criket worldcup?',\n",
              " 'output': 'Australia won the recent Cricket World Cup.'}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agentW.invoke(\"who won recent criket worldcup?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_8kyDeeXwCY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRLk8N66DhS-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av-3PxF3DjDd"
      },
      "source": [
        "# Chains\n",
        "\n",
        "Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.\n",
        "\n",
        "chain is just connecting several components.\n",
        "\n",
        "LCE:LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:\n",
        "\n",
        "# In summary, agents in LangChain add a layer of intelligence to applications by dynamically utilizing tools based on user input, while chains enable the combination of various components to create sophisticated applications that leverage language models and other tools effectively\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT3BFlPwDjKC",
        "outputId": "89e4237d-a3f4-4e77-f51b-2f95a3c05f2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['product'], template='what is a good name for a company that makes {product}')"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt5 = PromptTemplate.from_template(\n",
        "    \"what is a good name for a company that makes {product}\"\n",
        ")\n",
        "\n",
        "prompt5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "hzanRjQeDjMR",
        "outputId": "09763530-038b-4cd8-f05f-4168fe99f084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mwhat is a good name for a company that makes Chewing gum\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"1. Fresh Chew Co.\\n2. Bubble Bliss Inc.\\n3. Gumby's Gums\\n4. Chewlicious LLC\\n5. Minty Munchies\\n6. Simply Chewtastic\\n7. The Gum Factory\\n8. Happy Chew Co.\\n9. Pop n' Chew Industries\\n10. Bubble Buddies Inc.\""
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=client, prompt=prompt5, verbose=True)\n",
        "\n",
        "# chain\n",
        "\n",
        "chain.run(\"Chewing gum\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSx9pKf7DjPh",
        "outputId": "5706be03-0ad5-43b1-cc77-485bf02961f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['cuisine'], template='I want to open a restaurant for {cuisine} food, suggest a sophisticated name')"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# example 2\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"Cuisine\"],\n",
        "    template=\"I want to open a restaurant for {cuisine} food, suggest a sophisticated name\",\n",
        ")\n",
        "\n",
        "prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "d7ZJfzDbDjRn",
        "outputId": "8ec977c6-807d-401f-f47b-c53592652360"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"Taj Mahal Spice Co.\"'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain2 = LLMChain(llm=client, prompt=prompt_template)\n",
        "\n",
        "chain2.run(\"indian\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "FYJnyI4UDjVZ",
        "outputId": "a195b69a-21a6-4dc4-d051-265b37fce38c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI want to open a restaurant for american food, suggest a sophisticated name\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"Stateside Eats\"'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain3 = LLMChain(llm=client, prompt=prompt_template, verbose=True)\n",
        "chain3.run(\"american\").strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmIwC4MNI3m7"
      },
      "source": [
        "# if we want to combine multiple chain and set a seqence for that we use simplesequential chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvsmK2HTDjYX"
      },
      "outputs": [],
      "source": [
        "promptName = PromptTemplate.from_template(\n",
        "    \"I want to start a startup for {startup-name}, suggest me a crazy funny name for this\"\n",
        ")\n",
        "\n",
        "\n",
        "promptStrategy = PromptTemplate.from_template(\"Suggest me some strategies for {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X-W2ODYJZet"
      },
      "outputs": [],
      "source": [
        "nameChain = LLMChain(llm=client, prompt=promptName, verbose=True)\n",
        "\n",
        "strategyChain = LLMChain(\n",
        "    llm=client, prompt=promptStrategy, verbose=True  # we can use any other LLM here\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MtlmQ-A4LY6w",
        "outputId": "17506c16-5483-4511-d1f4-9620683114ca"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"AI-diculous Inc.\"'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nameChain.run(\"AI\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0Nna596JZg7"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "chain = SimpleSequentialChain(chains=[nameChain, strategyChain])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "19hgeUpxJZjC",
        "outputId": "2a806e16-c7fb-4948-8472-149aef7da8f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI want to start a startup for Machine Learning, suggest me a crazy funny name for this\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSuggest me some strategies for \n",
            "\n",
            "\"RoboNerds Inc.\"\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n1. Build a strong brand identity: Establishing a strong and recognizable brand is crucial for any business. Invest in creating a unique and memorable brand identity for RoboNerds Inc. This will help you stand out in the market and attract customers.\\n\\n2. Focus on innovation: As a company dealing with technology, it is important to constantly innovate and stay ahead of the competition. Invest in research and development to come up with new and improved products and services that will set RoboNerds Inc. apart from its competitors.\\n\\n3. Target niche markets: Instead of trying to cater to a wide range of customers, focus on targeting specific niche markets. This will help you tailor your products and services to meet the specific needs and preferences of your target audience, making your offerings more attractive to them.\\n\\n4. Leverage social media: In today's digital age, social media has become a powerful tool for businesses to reach and engage with their target audience. Use social media platforms like Twitter, Facebook, and LinkedIn to promote your brand, engage with customers, and showcase your products and services.\\n\\n5. Offer exceptional customer service: Providing excellent customer service can be a major differentiator for a business. Train your employees to be knowledgeable, helpful, and responsive to customer queries and\""
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run(\"Machine Learning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-5xlocONEv_"
      },
      "source": [
        "# we sort of didnt get answer of first chain\n",
        "\n",
        "# Now lets to understand sequentialChain\n",
        "\n",
        "(powerful than Simple sequential chain)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UyEu8qeJZk0",
        "outputId": "7a66083f-e096-437a-cd14-31aa44d5f50c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['cuisine'], template='I want to open a restaurant for {cuisine} food, suggest a sophisticated name')"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# example 2\n",
        "\n",
        "promptCuisine = PromptTemplate(\n",
        "    input_variables=[\"cuisine\"],\n",
        "    template=\"I want to open a restaurant for {cuisine} food, suggest a sophisticated name\",\n",
        ")\n",
        "\n",
        "promptCuisine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2I6xywAJZmy"
      },
      "outputs": [],
      "source": [
        "chainCuisine = LLMChain(\n",
        "    llm=client,\n",
        "    prompt=promptCuisine,\n",
        "    output_key=\"restaurant_name\",  # output willl be stored in output_key\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDAzq8GxJZoj"
      },
      "outputs": [],
      "source": [
        "promptitems = PromptTemplate.from_template(\"suggest some items for {restaurant_name}\")\n",
        "\n",
        "chainItems = LLMChain(\n",
        "    llm=client, prompt=promptitems, output_key=\"menu_items\", verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0fJyZ1MJZq1"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0dGD-CuDja3"
      },
      "outputs": [],
      "source": [
        "chain2 = SequentialChain(\n",
        "    chains=[chainCuisine, chainItems],\n",
        "    input_variables=[\"cuisine\"],\n",
        "    output_variables=[\"restaurant_name\", \"menu_items\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xtiQqesO5TG",
        "outputId": "3b5a6bb5-5841-48fc-e749-83840f9ccbdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI want to open a restaurant for indian food, suggest a sophisticated name\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3msuggest some items for \n",
            " \n",
            "\"Spice Symphony\"\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'cuisine': 'indian',\n",
              " 'restaurant_name': '\\n \\n\"Spice Symphony\"',\n",
              " 'menu_items': ' \\n\\n1. Garam Masala spice blend\\n2. Turmeric powder\\n3. Cumin seeds \\n4. Coriander seeds \\n5. Curry powder \\n6. Red chili powder \\n7. Paprika \\n8. Saffron \\n9. Cardamom pods \\n10. Fenugreek seeds \\n11. Cloves \\n12. Mustard seeds \\n13. Asafoetida (hing) \\n14. Tandoori masala \\n15. Chaat masala \\n16. Fennel seeds \\n17. Bay leaves \\n18. Black peppercorns \\n19. Ginger paste \\n20. Garlic paste \\n21. Onion powder \\n22. Mango powder (amchur) \\n23. Kasuri methi (dried fenugreek leaves) \\n24. Coconut milk \\n25. Tamarind paste \\n26. Rose water \\n27. Kewra water \\n28. Cinnamon sticks \\n29. Nutmeg \\n30. Curry leaves.'}"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain2.invoke(\"indian\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMMt7yMUO5qj",
        "outputId": "a1a3cf86-ddb3-4494-fccd-a25dac6cb15c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI want to open a restaurant for indian food, suggest a sophisticated name\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3msuggest some items for \n",
            "\"Maharaja's Palace: A Taste of India\"\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "responseC = chain2({\"cuisine\": \"indian\"})\n",
        "responseC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFLD4othQQUK",
        "outputId": "bbe0d8f2-dddc-463f-9746-cc5c4c297f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " themed dinner\n",
            "\n",
            "1. Chicken Tikka Masala\n",
            "2. Vegetable Biryani\n",
            "3. Lamb Vindaloo\n",
            "4. Palak Paneer\n",
            "5. Naan bread\n",
            "6. Samosas\n",
            "7. Mango Lassi\n",
            "8. Tandoori Chicken\n",
            "9. Chana Masala (chickpea curry)\n",
            "10. Malai Kofta (vegetable balls in creamy sauce)\n",
            "11. Gulab Jamun (Indian dessert)\n",
            "12. Raita (yogurt dip)\n",
            "13. Papadum (crispy lentil crackers)\n",
            "14. Mango chutney\n",
            "15. Rose or mango flavored ice cream\n",
            "16. Indian spiced rice pudding (kheer)\n",
            "17. Aloo Gobi (potato and cauliflower curry)\n",
            "18. Chicken Biryani\n",
            "19. Masala Chai (spiced tea)\n",
            "20. Gajar ka Halwa (carrot pudding)\n"
          ]
        }
      ],
      "source": [
        "print(responseC[\"menu_items\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP7avqFQPiOB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7lJ3GEUPiQL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rNmYfc7PkYV"
      },
      "source": [
        "# Document Loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPkjo8qoPiSz",
        "outputId": "3f9fe9a6-dd64-41ac-a8bf-13a31fed55d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.1.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install pypdf\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UocE13JBPiU_",
        "outputId": "04c5a697-f66d-4717-ea3f-563f44e0aba2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x7b1500792c50>"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loader = PyPDFLoader(\"/content/1409.0473.pdf\")\n",
        "loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seGYljWLPiXK",
        "outputId": "2a3390bb-6d4e-40d1-9c38-4e681eb1fa61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Published as a conference paper at ICLR 2015\n",
            "NEURAL MACHINE TRANSLATION\n",
            "BYJOINTLY LEARNING TO ALIGN AND TRANSLATE\n",
            "Dzmitry Bahdanau\n",
            "Jacobs University Bremen, Germany\n",
            "KyungHyun Cho Yoshua Bengio∗\n",
            "Universit ´e de Montr ´eal\n",
            "ABSTRACT\n",
            "Neural machine translation is a recently proposed approach to machine transla-\n",
            "tion. Unlike the traditional statistical machine translation, the neural machine\n",
            "translation aims at building a single neural network that can be jointly tuned to\n",
            "maximize the translation performance. The models proposed recently for neu-\n",
            "ral machine translation often belong to a family of encoder–decoders and encode\n",
            "a source sentence into a ﬁxed-length vector from which a decoder generates a\n",
            "translation. In this paper, we conjecture that the use of a ﬁxed-length vector is a\n",
            "bottleneck in improving the performance of this basic encoder–decoder architec-\n",
            "ture, and propose to extend this by allowing a model to automatically (soft-)search\n",
            "for parts of a source sentence that are relevant to predicting a target word, without\n",
            "having to form these parts as a hard segment explicitly. With this new approach,\n",
            "we achieve a translation performance comparable to the existing state-of-the-art\n",
            "phrase-based system on the task of English-to-French translation. Furthermore,\n",
            "qualitative analysis reveals that the (soft-)alignments found by the model agree\n",
            "well with our intuition.\n",
            "1 I NTRODUCTION\n",
            "Neural machine translation is a newly emerging approach to machine translation, recently proposed\n",
            "by Kalchbrenner and Blunsom (2013), Sutskever et al. (2014) and Cho et al. (2014b). Unlike the\n",
            "traditional phrase-based translation system (see, e.g., Koehn et al. , 2003) which consists of many\n",
            "small sub-components that are tuned separately, neural machine translation attempts to build and\n",
            "train a single, large neural network that reads a sentence and outputs a correct translation.\n",
            "Most of the proposed neural machine translation models belong to a family of encoder–\n",
            "decoders (Sutskever et al. , 2014; Cho et al. , 2014a), with an encoder and a decoder for each lan-\n",
            "guage, or involve a language-speciﬁc encoder applied to each sentence whose outputs are then com-\n",
            "pared (Hermann and Blunsom, 2014). An encoder neural network reads and encodes a source sen-\n",
            "tence into a ﬁxed-length vector. A decoder then outputs a translation from the encoded vector. The\n",
            "whole encoder–decoder system, which consists of the encoder and the decoder for a language pair,\n",
            "is jointly trained to maximize the probability of a correct translation given a source sentence.\n",
            "A potential issue with this encoder–decoder approach is that a neural network needs to be able to\n",
            "compress all the necessary information of a source sentence into a ﬁxed-length vector. This may\n",
            "make it difﬁcult for the neural network to cope with long sentences, especially those that are longer\n",
            "than the sentences in the training corpus. Cho et al. (2014b) showed that indeed the performance of\n",
            "a basic encoder–decoder deteriorates rapidly as the length of an input sentence increases.\n",
            "In order to address this issue, we introduce an extension to the encoder–decoder model which learns\n",
            "to align and translate jointly. Each time the proposed model generates a word in a translation, it\n",
            "(soft-)searches for a set of positions in a source sentence where the most relevant information is\n",
            "concentrated. The model then predicts a target word based on the context vectors associated with\n",
            "these source positions and all the previous generated target words.\n",
            "∗CIFAR Senior Fellow\n",
            "1arXiv:1409.0473v7  [cs.CL]  19 May 2016\n"
          ]
        }
      ],
      "source": [
        "pages = loader.load_and_split()\n",
        "print(pages[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5VoUGJ4Pia2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpvnZ-ZFPidj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tV9Ol9PwPigR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1IVu91uO52m"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
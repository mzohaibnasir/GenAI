{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkS6OwyA7MMsytz+UpURg6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzohaibnasir/GenAI/blob/main/06_vector_database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eCC232gMrD9q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# VECTOR DATABASE\n",
        "\n",
        "is a database for storing high dimensional vector such as word embeddings and image embeddings.\n",
        "A vector database stores pieces of information as vectors. Vector databases cluster related items together, enabling similarity searches and the construction of powerful AI models.\n",
        "\n",
        "# How do vector databases work?\n",
        "Each vector in a vector database corresponds to an object or item, whether that is a word, an image, a video, a movie, a document, or any other piece of data. These vectors are likely to be lengthy and complex, expressing the location of each object along dozens or even hundreds of dimensions.\n",
        "\n",
        "For example, a vector database of movies may locate movies along dimensions like running time, genre, year released, parental guidance rating, number of actors in common, number of viewers in common, and so on. If these vectors are created accurately, then similar movies are likely to end up clustered together in the vector database.\n",
        "\n",
        "# How are vector databases used?\n",
        "Similarity and semantic searches: Vector databases allow applications to connect pertinent items together. Vectors that are clustered together are similar and likely relevant to each other. This can help users search for relevant information (e.g. an image search), but it also helps applications:\n",
        "Recommend similar products\n",
        "Suggest songs, movies, or shows\n",
        "Suggest images or video\n",
        "Machine learning and deep learning: The ability to connect relevant items of information makes it possible to construct machine learning (and deep learning) models that can do complex cognitive tasks.\n",
        "Large language models (LLMs) and generative AI: LLMs, like that on which ChatGPT and Bard are built, rely on the contextual analysis of text made possible by vector databases. By associating words, sentences, and ideas with each other, LLMs can understand natural human language and even generate text.\n",
        "To summarize: Vector databases work at scale, work quickly, and are more cost-effective than querying machine learning models without them.\n",
        "\n",
        "\n",
        "\n",
        "# Embedding generation\n",
        "\n",
        "## non dl (frequency based)\n",
        "\n",
        "1. BOW(docmat)\n",
        "2. TF-IDF\n",
        "3. n-gram\n",
        "4. One hot encoding\n",
        "5. integer encoding\n",
        "\n",
        "## issues with non-dl\n",
        "\n",
        "### for One hot encoding & integer encoding\n",
        "\n",
        "1. sparse matrix(too many zeroes)\n",
        "2. no context\n",
        "\n",
        "### for BOW(docmat), TF-IDF & n-gram\n",
        "\n",
        "1. we create encoding using vocabularly\n",
        "2. still no context\n",
        "3. frequency based\n",
        "\n",
        "## with dl\n",
        "\n",
        "1. word2vec\n",
        "2. fast text\n",
        "3. ELMO\n",
        "4. BERT\n",
        "5. Glove(matrix factorization)\n",
        "\n",
        "### benefits\n",
        "\n",
        "1. creating dense vector\n",
        "2. context-full\n",
        "\n",
        "## WORD2VEC\n",
        "\n",
        "# `based on features i.e. king has features`\n",
        "\n",
        "we pass features into NN and we get embedding vector\n",
        "\n",
        "# Vector databases store embeddings. it indexes and store embeddings for faster retrieval and similarity search.\n",
        "\n",
        "1. are used in searching\n",
        "2. clustering where text strings are grouped by similarity\n",
        "3. Recommendation: related items are recommended\n",
        "4. classification\n"
      ],
      "metadata": {
        "id": "OivSdGFkxbXE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SzzRrQeGyqkb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ib_07YNvyuMR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Pinecone Vector DB"
      ],
      "metadata": {
        "id": "LrLVm4Q2yuUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain\n",
        "! pip install pinecon-client\n",
        "! pip install openai\n",
        "! pip install tiketoken\n",
        "! pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJaxwnhyy0tD",
        "outputId": "9caaffc7-0fd9-4273-fa9c-9425c4b4bd6a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.16)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.33)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.44)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.49)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pinecon-client (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pinecon-client\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.23.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tiketoken (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tiketoken\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os"
      ],
      "metadata": {
        "id": "Bz9GWKLJy1ss"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we'll collect data from pdfs and convert it into embeddings"
      ],
      "metadata": {
        "id": "AxWpZ7HsI2bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## perpare data"
      ],
      "metadata": {
        "id": "UQHd4PgRQ6uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aNVPAW9y1u3",
        "outputId": "7569fea8-e0c6-447a-e990-520a1c8f91e2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘pdfs’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
        "loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMwcCYHUIxAP",
        "outputId": "c7edc0ca-684d-467b-c030-04ded8cabccc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.document_loaders.pdf.PyPDFDirectoryLoader at 0x7eaf2d177910>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOeeDxu7y1xH",
        "outputId": "bd19303f-2c0c-4e7d-9d42-f884d30d49e8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 3293 0 (offset 2397)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Online edition (c)\\n2009 Cambridge UPAn\\nIntroduction\\nto\\nInformation\\nRetrieval\\nDraft of April 1, 2009', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 0}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 1}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPAn\\nIntroduction\\nto\\nInformation\\nRetrieval\\nChristopher D. Manning\\nPrabhakar Raghavan\\nHinrich Schütze\\nCambridge University Press\\nCambridge, England', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 2}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!\\nDONOT DISTRIBUTE WITHOUT PRIORPERMISSION\\n©2009 Cambridge University Press\\nByChristopher D. Manning, Prabhakar Raghavan &Hinrich Sch ütze\\nPrinted onApril 1,2009\\nWebsite: http://www.informationretrieval.org/\\nComments, corrections, andother feedback most welcome at:\\ninformationretrieval@yahoogroups.com', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 3}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . v\\nBrief Contents\\n1 Boolean retrieval 1\\n2 The term vocabulary and postings lists 19\\n3 Dictionaries and tolerant retrieval 49\\n4 Index construction 67\\n5 Index compression 85\\n6 Scoring, term weighting and the vector space model 109\\n7 Computing scores in a complete search system 135\\n8 Evaluation in information retrieval 151\\n9 Relevance feedback and query expansion 177\\n10 XML retrieval 195\\n11 Probabilistic information retrieval 219\\n12 Language models for information retrieval 237\\n13 Text classiﬁcation and Naive Bayes 253\\n14 Vector space classiﬁcation 289\\n15 Support vector machines and machine learning on document s 319\\n16 Flat clustering 349\\n17 Hierarchical clustering 377\\n18 Matrix decompositions and latent semantic indexing 403\\n19 Web search basics 421\\n20 Web crawling and indexes 443\\n21 Link analysis 461', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 4}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 5}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . vii\\nContents\\nList of Tables xv\\nList of Figures xix\\nTable of Notation xxvii\\nPreface xxxi\\n1Boolean retrieval 1\\n1.1 An example information retrieval problem 3\\n1.2 A ﬁrst take at building an inverted index 6\\n1.3 Processing Boolean queries 10\\n1.4 The extended Boolean model versus ranked retrieval 14\\n1.5 References and further reading 17\\n2The term vocabulary and postings lists 19\\n2.1 Document delineation and character sequence decoding 19\\n2.1.1 Obtaining the character sequence in a document 19\\n2.1.2 Choosing a document unit 20\\n2.2 Determining the vocabulary of terms 22\\n2.2.1 Tokenization 22\\n2.2.2 Dropping common terms: stop words 27\\n2.2.3 Normalization (equivalence classing of terms) 28\\n2.2.4 Stemming and lemmatization 32\\n2.3 Faster postings list intersection via skip pointers 36\\n2.4 Positional postings and phrase queries 39\\n2.4.1 Biword indexes 39\\n2.4.2 Positional indexes 41\\n2.4.3 Combination schemes 43\\n2.5 References and further reading 45', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 6}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPviii Contents\\n3Dictionaries and tolerant retrieval 49\\n3.1 Search structures for dictionaries 49\\n3.2 Wildcard queries 51\\n3.2.1 General wildcard queries 53\\n3.2.2 k-gram indexes for wildcard queries 54\\n3.3 Spelling correction 56\\n3.3.1 Implementing spelling correction 57\\n3.3.2 Forms of spelling correction 57\\n3.3.3 Edit distance 58\\n3.3.4 k-gram indexes for spelling correction 60\\n3.3.5 Context sensitive spelling correction 62\\n3.4 Phonetic correction 63\\n3.5 References and further reading 65\\n4Index construction 67\\n4.1 Hardware basics 68\\n4.2 Blocked sort-based indexing 69\\n4.3 Single-pass in-memory indexing 73\\n4.4 Distributed indexing 74\\n4.5 Dynamic indexing 78\\n4.6 Other types of indexes 80\\n4.7 References and further reading 83\\n5Index compression 85\\n5.1 Statistical properties of terms in information retriev al 86\\n5.1.1 Heaps’ law: Estimating the number of terms 88\\n5.1.2 Zipf’s law: Modeling the distribution of terms 89\\n5.2 Dictionary compression 90\\n5.2.1 Dictionary as a string 91\\n5.2.2 Blocked storage 92\\n5.3 Postings ﬁle compression 95\\n5.3.1 Variable byte codes 96\\n5.3.2 γcodes 98\\n5.4 References and further reading 105\\n6Scoring, term weighting and the vector space model 109\\n6.1 Parametric and zone indexes 110\\n6.1.1 Weighted zone scoring 112\\n6.1.2 Learning weights 113\\n6.1.3 The optimal weight g 115\\n6.2 Term frequency and weighting 117\\n6.2.1 Inverse document frequency 117\\n6.2.2 Tf-idf weighting 118', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 7}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPContents ix\\n6.3 The vector space model for scoring 120\\n6.3.1 Dot products 120\\n6.3.2 Queries as vectors 123\\n6.3.3 Computing vector scores 124\\n6.4 Variant tf-idf functions 126\\n6.4.1 Sublinear tf scaling 126\\n6.4.2 Maximum tf normalization 127\\n6.4.3 Document and query weighting schemes 128\\n6.4.4 Pivoted normalized document length 129\\n6.5 References and further reading 133\\n7Computing scores in a complete search system 135\\n7.1 Efﬁcient scoring and ranking 135\\n7.1.1 Inexact top Kdocument retrieval 137\\n7.1.2 Index elimination 137\\n7.1.3 Champion lists 138\\n7.1.4 Static quality scores and ordering 138\\n7.1.5 Impact ordering 140\\n7.1.6 Cluster pruning 141\\n7.2 Components of an information retrieval system 143\\n7.2.1 Tiered indexes 143\\n7.2.2 Query-term proximity 144\\n7.2.3 Designing parsing and scoring functions 145\\n7.2.4 Putting it all together 146\\n7.3 Vector space scoring and query operator interaction 147\\n7.4 References and further reading 149\\n8Evaluation in information retrieval 151\\n8.1 Information retrieval system evaluation 152\\n8.2 Standard test collections 153\\n8.3 Evaluation of unranked retrieval sets 154\\n8.4 Evaluation of ranked retrieval results 158\\n8.5 Assessing relevance 164\\n8.5.1 Critiques and justiﬁcations of the concept of\\nrelevance 166\\n8.6 A broader perspective: System quality and user utility 168\\n8.6.1 System issues 168\\n8.6.2 User utility 169\\n8.6.3 Reﬁning a deployed system 170\\n8.7 Results snippets 170\\n8.8 References and further reading 173\\n9Relevance feedback and query expansion 177', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 8}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPx Contents\\n9.1 Relevance feedback and pseudo relevance feedback 178\\n9.1.1 The Rocchio algorithm for relevance feedback 178\\n9.1.2 Probabilistic relevance feedback 183\\n9.1.3 When does relevance feedback work? 183\\n9.1.4 Relevance feedback on the web 185\\n9.1.5 Evaluation of relevance feedback strategies 186\\n9.1.6 Pseudo relevance feedback 187\\n9.1.7 Indirect relevance feedback 187\\n9.1.8 Summary 188\\n9.2 Global methods for query reformulation 189\\n9.2.1 Vocabulary tools for query reformulation 189\\n9.2.2 Query expansion 189\\n9.2.3 Automatic thesaurus generation 192\\n9.3 References and further reading 193\\n10XML retrieval 195\\n10.1 Basic XML concepts 197\\n10.2 Challenges in XML retrieval 201\\n10.3 A vector space model for XML retrieval 206\\n10.4 Evaluation of XML retrieval 210\\n10.5 Text-centric vs. data-centric XML retrieval 214\\n10.6 References and further reading 216\\n10.7 Exercises 217\\n11Probabilistic information retrieval 219\\n11.1 Review of basic probability theory 220\\n11.2 The Probability Ranking Principle 221\\n11.2.1 The 1/0 loss case 221\\n11.2.2 The PRP with retrieval costs 222\\n11.3 The Binary Independence Model 222\\n11.3.1 Deriving a ranking function for query terms 224\\n11.3.2 Probability estimates in theory 226\\n11.3.3 Probability estimates in practice 227\\n11.3.4 Probabilistic approaches to relevance feedback 228\\n11.4 An appraisal and some extensions 230\\n11.4.1 An appraisal of probabilistic models 230\\n11.4.2 Tree-structured dependencies between terms 231\\n11.4.3 Okapi BM25: a non-binary model 232\\n11.4.4 Bayesian network approaches to IR 234\\n11.5 References and further reading 235\\n12Language models for information retrieval 237\\n12.1 Language models 237', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 9}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPContents xi\\n12.1.1 Finite automata and language models 237\\n12.1.2 Types of language models 240\\n12.1.3 Multinomial distributions over words 241\\n12.2 The query likelihood model 242\\n12.2.1 Using query likelihood language models in IR 242\\n12.2.2 Estimating the query generation probability 243\\n12.2.3 Ponte and Croft’s Experiments 246\\n12.3 Language modeling versus other approaches in IR 248\\n12.4 Extended language modeling approaches 250\\n12.5 References and further reading 252\\n13Text classiﬁcation and Naive Bayes 253\\n13.1 The text classiﬁcation problem 256\\n13.2 Naive Bayes text classiﬁcation 258\\n13.2.1 Relation to multinomial unigram language model 262\\n13.3 The Bernoulli model 263\\n13.4 Properties of Naive Bayes 265\\n13.4.1 A variant of the multinomial model 270\\n13.5 Feature selection 271\\n13.5.1 Mutual information 272\\n13.5.2 χ2Feature selection 275\\n13.5.3 Frequency-based feature selection 277\\n13.5.4 Feature selection for multiple classiﬁers 278\\n13.5.5 Comparison of feature selection methods 278\\n13.6 Evaluation of text classiﬁcation 279\\n13.7 References and further reading 286\\n14Vector space classiﬁcation 289\\n14.1 Document representations and measures of relatedness in\\nvector spaces 291\\n14.2 Rocchio classiﬁcation 292\\n14.3 knearest neighbor 297\\n14.3.1 Time complexity and optimality of kNN 299\\n14.4 Linear versus nonlinear classiﬁers 301\\n14.5 Classiﬁcation with more than two classes 306\\n14.6 The bias-variance tradeoff 308\\n14.7 References and further reading 314\\n14.8 Exercises 315\\n15Support vector machines and machine learning on documents 319\\n15.1 Support vector machines: The linearly separable case 320\\n15.2 Extensions to the SVM model 327\\n15.2.1 Soft margin classiﬁcation 327', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 10}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPxii Contents\\n15.2.2 Multiclass SVMs 330\\n15.2.3 Nonlinear SVMs 330\\n15.2.4 Experimental results 333\\n15.3 Issues in the classiﬁcation of text documents 334\\n15.3.1 Choosing what kind of classiﬁer to use 335\\n15.3.2 Improving classiﬁer performance 337\\n15.4 Machine learning methods in ad hoc information retriev al 341\\n15.4.1 A simple example of machine-learned scoring 341\\n15.4.2 Result ranking by machine learning 344\\n15.5 References and further reading 346\\n16Flat clustering 349\\n16.1 Clustering in information retrieval 350\\n16.2 Problem statement 354\\n16.2.1 Cardinality – the number of clusters 355\\n16.3 Evaluation of clustering 356\\n16.4 K-means 360\\n16.4.1 Cluster cardinality in K-means 365\\n16.5 Model-based clustering 368\\n16.6 References and further reading 372\\n16.7 Exercises 374\\n17Hierarchical clustering 377\\n17.1 Hierarchical agglomerative clustering 378\\n17.2 Single-link and complete-link clustering 382\\n17.2.1 Time complexity of HAC 385\\n17.3 Group-average agglomerative clustering 388\\n17.4 Centroid clustering 391\\n17.5 Optimality of HAC 393\\n17.6 Divisive clustering 395\\n17.7 Cluster labeling 396\\n17.8 Implementation notes 398\\n17.9 References and further reading 399\\n17.10 Exercises 401\\n18Matrix decompositions and latent semantic indexing 403\\n18.1 Linear algebra review 403\\n18.1.1 Matrix decompositions 406\\n18.2 Term-document matrices and singular value\\ndecompositions 407\\n18.3 Low-rank approximations 410\\n18.4 Latent semantic indexing 412\\n18.5 References and further reading 417', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 11}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPContents xiii\\n19Web search basics 421\\n19.1 Background and history 421\\n19.2 Web characteristics 423\\n19.2.1 The web graph 425\\n19.2.2 Spam 427\\n19.3 Advertising as the economic model 429\\n19.4 The search user experience 432\\n19.4.1 User query needs 432\\n19.5 Index size and estimation 433\\n19.6 Near-duplicates and shingling 437\\n19.7 References and further reading 441\\n20Web crawling and indexes 443\\n20.1 Overview 443\\n20.1.1 Features a crawler must provide 443\\n20.1.2 Features a crawler should provide 444\\n20.2 Crawling 444\\n20.2.1 Crawler architecture 445\\n20.2.2 DNS resolution 449\\n20.2.3 The URL frontier 451\\n20.3 Distributing indexes 454\\n20.4 Connectivity servers 455\\n20.5 References and further reading 458\\n21Link analysis 461\\n21.1 The Web as a graph 462\\n21.1.1 Anchor text and the web graph 462\\n21.2 PageRank 464\\n21.2.1 Markov chains 465\\n21.2.2 The PageRank computation 468\\n21.2.3 Topic-speciﬁc PageRank 471\\n21.3 Hubs and Authorities 474\\n21.3.1 Choosing the subset of the Web 477\\n21.4 References and further reading 480\\nBibliography 483\\nAuthor Index 519', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 12}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 13}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . xv\\nList of Tables\\n4.1 Typical system parameters in 2007. The seek time is the ti me\\nneeded to position the disk head in a new position. The\\ntransfer time per byte is the rate of transfer from disk to\\nmemory when the head is in the right position. 68\\n4.2 Collection statistics for Reuters-RCV1. Values are rou nded for\\nthe computations in this book. The unrounded values are:\\n806,791 documents, 222 tokens per document, 391,523\\n(distinct) terms, 6.04 bytes per token with spaces and\\npunctuation, 4.5 bytes per token without spaces and\\npunctuation, 7.5 bytes per term, and 96,969,056 tokens. The\\nnumbers in this table correspond to the third line (“case\\nfolding”) in Table 5.1(page 87). 70\\n4.3 The ﬁve steps in constructing an index for Reuters-RCV1 i n\\nblocked sort-based indexing. Line numbers refer to Figure 4.2. 82\\n4.4 Collection statistics for a large collection. 82\\n5.1 The effect of preprocessing on the number of terms,\\nnonpositional postings, and tokens for Reuters-RCV1. “ ∆%”\\nindicates the reduction in size from the previous line, exce pt\\nthat “30 stop words” and “150 stop words” both use “case\\nfolding” as their reference line. “T%” is the cumulative\\n(“total”) reduction from unﬁltered. We performed stemming\\nwith the Porter stemmer (Chapter 2, page 33). 87\\n5.2 Dictionary compression for Reuters-RCV1. 95\\n5.3 Encoding gaps instead of document IDs. For example, we\\nstore gaps 107, 5, 43, . . . , instead of docIDs 283154, 283159,\\n283202, . . . for computer . The ﬁrst docID is left unchanged\\n(only shown for arachnocentric ). 96\\n5.4 VB encoding. 97', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 14}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPxvi List of Tables\\n5.5 Some examples of unary and γcodes. Unary codes are only\\nshown for the smaller numbers. Commas in γcodes are for\\nreadability only and are not part of the actual codes. 98\\n5.6 Index and dictionary compression for Reuters-RCV1. The\\ncompression ratio depends on the proportion of actual text i n\\nthe collection. Reuters-RCV1 contains a large amount of XML\\nmarkup. Using the two best compression schemes, γ\\nencoding and blocking with front coding, the ratio\\ncompressed index to collection size is therefore especiall y\\nsmall for Reuters-RCV1: (101+5.9)/3600≈0.03. 103\\n5.7 Two gap sequences to be merged in blocked sort-based\\nindexing 105\\n6.1 Cosine computation for Exercise 6.19. 132\\n8.1 Calculation of 11-point Interpolated Average Precisio n. 159\\n8.2 Calculating the kappa statistic. 165\\n10.1 RDB (relational database) search, unstructured infor mation\\nretrieval and structured information retrieval. 196\\n10.2 INEX 2002 collection statistics. 211\\n10.3 INEX 2002 results of the vector space model in Section 10.3 for\\ncontent-and-structure (CAS) queries and the quantization\\nfunction Q. 213\\n10.4 A comparison of content-only and full-structure searc h in\\nINEX 2003/2004. 214\\n13.1 Data for parameter estimation examples. 261\\n13.2 Training and test times for NB. 261\\n13.3 Multinomial versus Bernoulli model. 268\\n13.4 Correct estimation implies accurate prediction, but a ccurate\\nprediction does not imply correct estimation. 269\\n13.5 A set of documents for which the NB independence\\nassumptions are problematic. 270\\n13.6 Critical values of the χ2distribution with one degree of\\nfreedom. For example, if the two events are independent,\\nthen P(X2>6.63)<0.01. So for X2>6.63 the assumption of\\nindependence can be rejected with 99% conﬁdence. 277\\n13.7 The ten largest classes in the Reuters-21578 collectio n with\\nnumber of documents in training and test sets. 280', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 15}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPList of Tables xvii\\n13.8 Macro- and microaveraging. “Truth” is the true class an d\\n“call” the decision of the classiﬁer. In this example,\\nmacroaveraged precision is\\n[10/(10+10) +90/(10+90)]/2= (0.5+0.9)/2=0.7.\\nMicroaveraged precision is 100/ (100+20)≈0.83. 282\\n13.9 Text classiﬁcation effectiveness numbers on Reuters- 21578 for\\nF1(in percent). Results from Li and Yang (2003 ) (a), Joachims\\n(1998 ) (b: kNN) and Dumais et al. (1998 ) (b: NB, Rocchio,\\ntrees, SVM). 282\\n13.10 Data for parameter estimation exercise. 284\\n14.1 Vectors and class centroids for the data in Table 13.1. 294\\n14.2 Training and test times for Rocchio classiﬁcation. 296\\n14.3 Training and test times for kNN classiﬁcation. 299\\n14.4 A linear classiﬁer. 303\\n14.5 A confusion matrix for Reuters-21578. 308\\n15.1 Training and testing complexity of various classiﬁers\\nincluding SVMs. 329\\n15.2 SVM classiﬁer break-even F 1from ( Joachims 2002a , p. 114). 334\\n15.3 Training examples for machine-learned scoring. 342\\n16.1 Some applications of clustering in information retrie val. 351\\n16.2 The four external evaluation measures applied to the\\nclustering in Figure 16.4. 357\\n16.3 The EM clustering algorithm. 371\\n17.1 Comparison of HAC algorithms. 395\\n17.2 Automatically computed cluster labels. 397', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 16}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 17}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . xix\\nList of Figures\\n1.1 A term-document incidence matrix. 4\\n1.2 Results from Shakespeare for the query Brutus ANDCaesar\\nAND NOT Calpurnia . 5\\n1.3 The two parts of an inverted index. 7\\n1.4 Building an index by sorting and grouping. 8\\n1.5 Intersecting the postings lists for Brutus andCalpurnia from\\nFigure 1.3. 10\\n1.6 Algorithm for the intersection of two postings lists p1and p2. 11\\n1.7 Algorithm for conjunctive queries that returns the set o f\\ndocuments containing each term in the input list of terms. 12\\n2.1 An example of a vocalized Modern Standard Arabic word. 21\\n2.2 The conceptual linear order of characters is not necessa rily the\\norder that you see on the page. 21\\n2.3 The standard unsegmented form of Chinese text using the\\nsimpliﬁed characters of mainland China. 26\\n2.4 Ambiguities in Chinese word segmentation. 26\\n2.5 A stop list of 25 semantically non-selective words which are\\ncommon in Reuters-RCV1. 26\\n2.6 An example of how asymmetric expansion of query terms can\\nusefully model users’ expectations. 28\\n2.7 Japanese makes use of multiple intermingled writing sys tems\\nand, like Chinese, does not segment words. 31\\n2.8 A comparison of three stemming algorithms on a sample tex t. 34\\n2.9 Postings lists with skip pointers. 36\\n2.10 Postings lists intersection with skip pointers. 37\\n2.11 Positional index example. 41\\n2.12 An algorithm for proximity intersection of postings li stsp1\\nand p2. 42', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 18}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPxx List of Figures\\n3.1 A binary search tree. 51\\n3.2 A B-tree. 52\\n3.3 A portion of a permuterm index. 54\\n3.4 Example of a postings list in a 3-gram index. 55\\n3.5 Dynamic programming algorithm for computing the edit\\ndistance between strings s1and s2. 59\\n3.6 Example Levenshtein distance computation. 59\\n3.7 Matching at least two of the three 2-grams in the query bord. 61\\n4.1 Document from the Reuters newswire. 70\\n4.2 Blocked sort-based indexing. 71\\n4.3 Merging in blocked sort-based indexing. 72\\n4.4 Inversion of a block in single-pass in-memory indexing 73\\n4.5 An example of distributed indexing with MapReduce.\\nAdapted from Dean and Ghemawat (2004 ). 76\\n4.6 Map and reduce functions in MapReduce. 77\\n4.7 Logarithmic merging. Each token (termID,docID) is init ially\\nadded to in-memory index Z0by LM ERGE ADDTOKEN .\\nLOGARITHMIC MERGE initializes Z0and indexes . 79\\n4.8 A user-document matrix for access control lists. Elemen t(i,j)\\nis 1 if user ihas access to document jand 0 otherwise. During\\nquery processing, a user’s access postings list is intersec ted\\nwith the results list returned by the text part of the index. 81\\n5.1 Heaps’ law. 88\\n5.2 Zipf’s law for Reuters-RCV1. 90\\n5.3 Storing the dictionary as an array of ﬁxed-width entries . 91\\n5.4 Dictionary-as-a-string storage. 92\\n5.5 Blocked storage with four terms per block. 93\\n5.6 Search of the uncompressed dictionary (a) and a dictiona ry\\ncompressed by blocking with k=4 (b). 94\\n5.7 Front coding. 94\\n5.8 VB encoding and decoding. 97\\n5.9 Entropy H(P)as a function of P(x1)for a sample space with\\ntwo outcomes x1and x2. 100\\n5.10 Stratiﬁcation of terms for estimating the size of a γencoded\\ninverted index. 102\\n6.1 Parametric search. 111\\n6.2 Basic zone index 111\\n6.3 Zone index in which the zone is encoded in the postings\\nrather than the dictionary. 111', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 19}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPList of Figures xxi\\n6.4 Algorithm for computing the weighted zone score from two\\npostings lists. 113\\n6.5 An illustration of training examples. 115\\n6.6 The four possible combinations of sTand sB. 115\\n6.7 Collection frequency (cf) and document frequency (df) b ehave\\ndifferently, as in this example from the Reuters collection . 118\\n6.8 Example of idf values. 119\\n6.9 Table of tf values for Exercise 6.10. 120\\n6.10 Cosine similarity illustrated. 121\\n6.11 Euclidean normalized tf values for documents in Figure 6.9. 122\\n6.12 Term frequencies in three novels. 122\\n6.13 Term vectors for the three novels of Figure 6.12. 123\\n6.14 The basic algorithm for computing vector space scores. 125\\n6.15 SMART notation for tf-idf variants. 128\\n6.16 Pivoted document length normalization. 130\\n6.17 Implementing pivoted document length normalization b y\\nlinear scaling. 131\\n7.1 A faster algorithm for vector space scores. 136\\n7.2 A static quality-ordered index. 139\\n7.3 Cluster pruning. 142\\n7.4 Tiered indexes. 144\\n7.5 A complete search system. 147\\n8.1 Graph comparing the harmonic mean to other means. 157\\n8.2 Precision/recall graph. 158\\n8.3 Averaged 11-point precision/recall graph across 50 que ries\\nfor a representative TREC system. 160\\n8.4 The ROC curve corresponding to the precision-recall cur ve in\\nFigure 8.2. 162\\n8.5 An example of selecting text for a dynamic snippet. 172\\n9.1 Relevance feedback searching over images. 179\\n9.2 Example of relevance feedback on a text collection. 180\\n9.3 The Rocchio optimal query for separating relevant and\\nnonrelevant documents. 181\\n9.4 An application of Rocchio’s algorithm. 182\\n9.5 Results showing pseudo relevance feedback greatly\\nimproving performance. 187\\n9.6 An example of query expansion in the interface of the Yaho o!\\nweb search engine in 2006. 190\\n9.7 Examples of query expansion via the PubMed thesaurus. 191\\n9.8 An example of an automatically generated thesaurus. 192', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 20}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPxxii List of Figures\\n10.1 An XML document. 198\\n10.2 The XML document in Figure 10.1 as a simpliﬁed DOM object. 198\\n10.3 An XML query in NEXI format and its partial representati on\\nas a tree. 199\\n10.4 Tree representation of XML documents and queries. 200\\n10.5 Partitioning an XML document into non-overlapping\\nindexing units. 202\\n10.6 Schema heterogeneity: intervening nodes and mismatch ed\\nnames. 204\\n10.7 A structural mismatch between two queries and a documen t. 206\\n10.8 A mapping of an XML document (left) to a set of lexicalize d\\nsubtrees (right). 207\\n10.9 The algorithm for scoring documents with S IMNOMERGE . 209\\n10.10 Scoring of a query with one structural term in S IMNOMERGE . 209\\n10.11 Simpliﬁed schema of the documents in the INEX collecti on. 211\\n11.1 A tree of dependencies between terms. 232\\n12.1 A simple ﬁnite automaton and some of the strings in the\\nlanguage it generates. 238\\n12.2 A one-state ﬁnite automaton that acts as a unigram langu age\\nmodel. 238\\n12.3 Partial speciﬁcation of two unigram language models. 239\\n12.4 Results of a comparison of tf-idf with language modelin g\\n(LM) term weighting by Ponte and Croft (1998 ). 247\\n12.5 Three ways of developing the language modeling approac h:\\n(a) query likelihood, (b) document likelihood, and (c) mode l\\ncomparison. 250\\n13.1 Classes, training set, and test set in text classiﬁcati on . 257\\n13.2 Naive Bayes algorithm (multinomial model): Training a nd\\ntesting. 260\\n13.3 NB algorithm (Bernoulli model): Training and testing. 263\\n13.4 The multinomial NB model. 266\\n13.5 The Bernoulli NB model. 267\\n13.6 Basic feature selection algorithm for selecting the kbest features. 271\\n13.7 Features with high mutual information scores for six\\nReuters-RCV1 classes. 274\\n13.8 Effect of feature set size on accuracy for multinomial a nd\\nBernoulli models. 275\\n13.9 A sample document from the Reuters-21578 collection. 281\\n14.1 Vector space classiﬁcation into three classes. 290', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 21}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPList of Figures xxiii\\n14.2 Projections of small areas of the unit sphere preserve d istances. 291\\n14.3 Rocchio classiﬁcation. 293\\n14.4 Rocchio classiﬁcation: Training and testing. 295\\n14.5 The multimodal class “a” consists of two different clus ters\\n(small upper circles centered on X’s). 295\\n14.6 Voronoi tessellation and decision boundaries (double lines) in\\n1NN classiﬁcation. 297\\n14.7 kNN training (with preprocessing) and testing. 298\\n14.8 There are an inﬁnite number of hyperplanes that separat e two\\nlinearly separable classes. 301\\n14.9 Linear classiﬁcation algorithm. 302\\n14.10 A linear problem with noise. 304\\n14.11 A nonlinear problem. 305\\n14.12 Jhyperplanes do not divide space into Jdisjoint regions. 307\\n14.13 Arithmetic transformations for the bias-variance de composition. 310\\n14.14 Example for differences between Euclidean distance, dot\\nproduct similarity and cosine similarity. 316\\n14.15 A simple non-separable set of points. 317\\n15.1 The support vectors are the 5 points right up against the\\nmargin of the classiﬁer. 320\\n15.2 An intuition for large-margin classiﬁcation. 321\\n15.3 The geometric margin of a point ( r) and a decision boundary ( ρ).323\\n15.4 A tiny 3 data point training set for an SVM. 325\\n15.5 Large margin classiﬁcation with slack variables. 327\\n15.6 Projecting data that is not linearly separable into a hi gher\\ndimensional space can make it linearly separable. 331\\n15.7 A collection of training examples. 343\\n16.1 An example of a data set with a clear cluster structure. 349\\n16.2 Clustering of search results to improve recall. 352\\n16.3 An example of a user session in Scatter-Gather. 353\\n16.4 Purity as an external evaluation criterion for cluster quality. 357\\n16.5 The K-means algorithm. 361\\n16.6 A K-means example for K=2 inR2. 362\\n16.7 The outcome of clustering in K-means depends on the initial\\nseeds. 364\\n16.8 Estimated minimal residual sum of squares as a function of\\nthe number of clusters in K-means. 366\\n17.1 A dendrogram of a single-link clustering of 30 document s\\nfrom Reuters-RCV1. 379\\n17.2 A simple, but inefﬁcient HAC algorithm. 381', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 22}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPxxiv List of Figures\\n17.3 The different notions of cluster similarity used by the four\\nHAC algorithms. 381\\n17.4 A single-link (left) and complete-link (right) cluste ring of\\neight documents. 382\\n17.5 A dendrogram of a complete-link clustering. 383\\n17.6 Chaining in single-link clustering. 384\\n17.7 Outliers in complete-link clustering. 385\\n17.8 The priority-queue algorithm for HAC. 386\\n17.9 Single-link clustering algorithm using an NBM array. 387\\n17.10 Complete-link clustering is not best-merge persiste nt. 388\\n17.11 Three iterations of centroid clustering. 391\\n17.12 Centroid clustering is not monotonic. 392\\n18.1 Illustration of the singular-value decomposition. 409\\n18.2 Illustration of low rank approximation using the\\nsingular-value decomposition. 411\\n18.3 The documents of Example 18.4 reduced to two dimensions\\nin(V′)T. 416\\n18.4 Documents for Exercise 18.11 . 418\\n18.5 Glossary for Exercise 18.11 . 418\\n19.1 A dynamically generated web page. 425\\n19.2 Two nodes of the web graph joined by a link. 425\\n19.3 A sample small web graph. 426\\n19.4 The bowtie structure of the Web. 427\\n19.5 Cloaking as used by spammers. 428\\n19.6 Search advertising triggered by query keywords. 431\\n19.7 The various components of a web search engine. 434\\n19.8 Illustration of shingle sketches. 439\\n19.9 Two sets Sj1and Sj2; their Jaccard coefﬁcient is 2/5. 440\\n20.1 The basic crawler architecture. 446\\n20.2 Distributing the basic crawl architecture. 449\\n20.3 The URL frontier. 452\\n20.4 Example of an auxiliary hosts-to-back queues table. 453\\n20.5 A lexicographically ordered set of URLs. 456\\n20.6 A four-row segment of the table of links. 457\\n21.1 The random surfer at node A proceeds with probability 1/ 3 to\\neach of B, C and D. 464\\n21.2 A simple Markov chain with three states; the numbers on t he\\nlinks indicate the transition probabilities. 466\\n21.3 The sequence of probability vectors. 469', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 23}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPList of Figures xxv\\n21.4 A small web graph. 470\\n21.5 Topic-speciﬁc PageRank. 472\\n21.6 A sample run of HITS on the query japan elementaryschools . 479\\n21.7 Web graph for Exercise 21.22 . 480', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 24}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 25}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . xxvii\\nTable of Notation\\nSymbol Page Meaning\\nγ p.98 γcode\\nγ p.256 Classiﬁcation or clustering function: γ(d)isd’s class\\nor cluster\\nΓ p.256 Supervised learning method in Chapters 13and 14:\\nΓ(D)is the classiﬁcation function γlearned from\\ntraining set D\\nλ p.404 Eigenvalue\\n⃗µ(.) p.292 Centroid of a class (in Rocchio classiﬁcation) or a\\ncluster (in K-means and centroid clustering)\\nΦ p.114 Training example\\nσ p.408 Singular value\\nΘ(·) p.11 A tight bound on the complexity of an algorithm\\nω,ωk p.357 Cluster in clustering\\nΩ p.357 Clustering or set of clusters {ω1, . . . , ωK}\\narg maxxf(x)p.181 The value of xfor which freaches its maximum\\narg minxf(x)p.181 The value of xfor which freaches its minimum\\nc,cj p.256 Class or category in classiﬁcation\\ncft p.89 The collection frequency of term t(the total number\\nof times the term appears in the document collec-\\ntion)\\nC p.256 Set{c1, . . . , cJ}of all classes\\nC p.268 A random variable that takes as values members of\\nC', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 26}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPxxviii Table of Notation\\nC p.403 Term-document matrix\\nd p.4 Index of the dthdocument in the collection D\\nd p.71 A document\\n⃗d,⃗q p.181 Document vector, query vector\\nD p.354 Set{d1, . . . , dN}of all documents\\nDc p.292 Set of documents that is in class c\\nD p.256 Set{⟨d1,c1⟩, . . . ,⟨dN,cN⟩}of all labeled documents\\nin Chapters 13–15\\ndft p.118 The document frequency of term t(the total number\\nof documents in the collection the term appears in)\\nH p.99 Entropy\\nHM p.101 Mth harmonic number\\nI(X;Y) p.272 Mutual information of random variables Xand Y\\nidft p.118 Inverse document frequency of term t\\nJ p.256 Number of classes\\nk p.290 Top kitems from a set, e.g., knearest neighbors in\\nkNN, top kretrieved documents, top kselected fea-\\ntures from the vocabulary V\\nk p.54 Sequence of kcharacters\\nK p.354 Number of clusters\\nLd p.233 Length of document d(in tokens)\\nLa p.262 Length of the test document (or application docu-\\nment) in tokens\\nLave p.70 Average length of a document (in tokens)\\nM p.5 Size of the vocabulary ( |V|)\\nMa p.262 Size of the vocabulary of the test document (or ap-\\nplication document)\\nMave p.78 Average size of the vocabulary in a document in the\\ncollection\\nMd p.237 Language model for document d\\nN p.4 Number of documents in the retrieval or training\\ncollection\\nNc p.259 Number of documents in class c\\nN(ω) p.298 Number of times the event ωoccurred', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 27}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPTable of Notation xxix\\nO(·) p.11 A bound on the complexity of an algorithm\\nO(·) p.221 The odds of an event\\nP p.155 Precision\\nP(·) p.220 Probability\\nP p.465 Transition probability matrix\\nq p.59 A query\\nR p.155 Recall\\nsi p.58 A string\\nsi p.112 Boolean values for zone scoring\\nsim(d1,d2) p.121 Similarity score for documents d1,d2\\nT p.43 Total number of tokens in the document collection\\nTct p.259 Number of occurrences of word tin documents of\\nclass c\\nt p.4 Index of the tthterm in the vocabulary V\\nt p.61 A term in the vocabulary\\ntft,d p.117 The term frequency of term tin document d(the to-\\ntal number of occurrences of tind)\\nUt p.266 Random variable taking values 0 (term tis present)\\nand 1 ( tis not present)\\nV p.208 Vocabulary of terms {t1, . . . , tM}in a collection (a.k.a.\\nthe lexicon)\\n⃗v(d) p.122 Length-normalized document vector\\n⃗V(d) p.120 Vector of document d, not length-normalized\\nwft,d p.125 Weight of term tin document d\\nw p.112 A weight, for example for zones or terms\\n⃗wT⃗x=b p.293 Hyperplane; ⃗wis the normal vector of the hyper-\\nplane and wicomponent iof⃗w\\n⃗x p.222 Term incidence vector ⃗x= (x1, . . . , xM); more gen-\\nerally: document feature representation\\nX p.266 Random variable taking values in V, the vocabulary\\n(e.g., at a given position kin a document)\\nX p.256 Document space in text classiﬁcation\\n|A| p.61 Set cardinality: the number of members of set A\\n|S| p.404 Determinant of the square matrix S', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 28}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPxxx Table of Notation\\n|si| p.58 Length in characters of string si\\n|⃗x| p.139 Length of vector ⃗x\\n|⃗x−⃗y| p.131 Euclidean distance of ⃗xand⃗y(which is the length of\\n(⃗x−⃗y))', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 29}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . xxxi\\nPreface\\nAs recently as the 1990s, studies showed that most people pre ferred getting\\ninformation from other people rather than from information retrieval sys-\\ntems. Of course, in that time period, most people also used hu man travel\\nagents to book their travel. However, during the last decade , relentless opti-\\nmization of information retrieval effectiveness has drive n web search engines\\nto new quality levels where most people are satisﬁed most of t he time, and\\nweb search has become a standard and often preferred source o f information\\nﬁnding. For example, the 2004 Pew Internet Survey ( Fallows 2004 ) found\\nthat “92% of Internet users say the Internet is a good place to go for getting\\neveryday information.” To the surprise of many, the ﬁeld of i nformation re-\\ntrieval has moved from being a primarily academic disciplin e to being the\\nbasis underlying most people’s preferred means of informat ion access. This\\nbook presents the scientiﬁc underpinnings of this ﬁeld, at a level accessible\\nto graduate students as well as advanced undergraduates.\\nInformation retrieval did not begin with the Web. In respons e to various\\nchallenges of providing information access, the ﬁeld of inf ormation retrieval\\nevolved to give principled approaches to searching various forms of con-\\ntent. The ﬁeld began with scientiﬁc publications and librar y records, but\\nsoon spread to other forms of content, particularly those of information pro-\\nfessionals, such as journalists, lawyers, and doctors. Muc h of the scientiﬁc\\nresearch on information retrieval has occurred in these con texts, and much of\\nthe continued practice of information retrieval deals with providing access to\\nunstructured information in various corporate and governm ental domains,\\nand this work forms much of the foundation of our book.\\nNevertheless, in recent years, a principal driver of innova tion has been the\\nWorld Wide Web, unleashing publication at the scale of tens o f millions of\\ncontent creators. This explosion of published information would be moot\\nif the information could not be found, annotated and analyze d so that each\\nuser can quickly ﬁnd information that is both relevant and co mprehensive\\nfor their needs. By the late 1990s, many people felt that cont inuing to index', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 30}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPxxxii Preface\\nthe whole Web would rapidly become impossible, due to the Web ’s expo-\\nnential growth in size. But major scientiﬁc innovations, su perb engineering,\\nthe rapidly declining price of computer hardware, and the ri se of a commer-\\ncial underpinning for web search have all conspired to power today’s major\\nsearch engines, which are able to provide high-quality resu lts within subsec-\\nond response times for hundreds of millions of searches a day over billions\\nof web pages.\\nBook organization and course development\\nThis book is the result of a series of courses we have taught at Stanford Uni-\\nversity and at the University of Stuttgart, in a range of dura tions including\\na single quarter, one semester and two quarters. These cours es were aimed\\nat early-stage graduate students in computer science, but w e have also had\\nenrollment from upper-class computer science undergradua tes, as well as\\nstudents from law, medical informatics, statistics, lingu istics and various en-\\ngineering disciplines. The key design principle for this bo ok, therefore, was\\nto cover what we believe to be important in a one-term graduat e course on\\ninformation retrieval. An additional principle is to build each chapter around\\nmaterial that we believe can be covered in a single lecture of 75 to 90 minutes.\\nThe ﬁrst eight chapters of the book are devoted to the basics o f informa-\\ntion retrieval, and in particular the heart of search engine s; we consider this\\nmaterial to be core to any course on information retrieval. C hapter 1in-\\ntroduces inverted indexes, and shows how simple Boolean que ries can be\\nprocessed using such indexes. Chapter 2builds on this introduction by de-\\ntailing the manner in which documents are preprocessed befo re indexing\\nand by discussing how inverted indexes are augmented in vari ous ways for\\nfunctionality and speed. Chapter 3discusses search structures for dictionar-\\nies and how to process queries that have spelling errors and o ther imprecise\\nmatches to the vocabulary in the document collection being s earched. Chap-\\nter4describes a number of algorithms for constructing the inver ted index\\nfrom a text collection with particular attention to highly s calable and dis-\\ntributed algorithms that can be applied to very large collec tions. Chapter 5\\ncovers techniques for compressing dictionaries and invert ed indexes. These\\ntechniques are critical for achieving subsecond response t imes to user queries\\nin large search engines. The indexes and queries considered in Chapters 1–5\\nonly deal with Boolean retrieval , in which a document either matches a query,\\nor does not. A desire to measure the extent to which a document matches a\\nquery, or the score of a document for a query, motivates the de velopment of\\nterm weighting and the computation of scores in Chapters 6and 7, leading\\nto the idea of a list of documents that are rank-ordered for a q uery. Chapter 8\\nfocuses on the evaluation of an information retrieval syste m based on the', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 31}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPPreface xxxiii\\nrelevance of the documents it retrieves, allowing us to comp are the relative\\nperformances of different systems on benchmark document co llections and\\nqueries.\\nChapters 9–21build on the foundation of the ﬁrst eight chapters to cover\\na variety of more advanced topics. Chapter 9discusses methods by which\\nretrieval can be enhanced through the use of techniques like relevance feed-\\nback and query expansion, which aim at increasing the likeli hood of retriev-\\ning relevant documents. Chapter 10considers information retrieval from\\ndocuments that are structured with markup languages like XM L and HTML.\\nWe treat structured retrieval by reducing it to the vector sp ace scoring meth-\\nods developed in Chapter 6. Chapters 11and 12invoke probability theory to\\ncompute scores for documents on queries. Chapter 11develops traditional\\nprobabilistic information retrieval, which provides a fra mework for comput-\\ning the probability of relevance of a document, given a set of query terms.\\nThis probability may then be used as a score in ranking. Chapt er12illus-\\ntrates an alternative, wherein for each document in a collec tion, we build a\\nlanguage model from which one can estimate a probability tha t the language\\nmodel generates a given query. This probability is another q uantity with\\nwhich we can rank-order documents.\\nChapters 13–17give a treatment of various forms of machine learning and\\nnumerical methods in information retrieval. Chapters 13–15treat the prob-\\nlem of classifying documents into a set of known categories, given a set of\\ndocuments along with the classes they belong to. Chapter 13motivates sta-\\ntistical classiﬁcation as one of the key technologies neede d for a successful\\nsearch engine, introduces Naive Bayes, a conceptually simp le and efﬁcient\\ntext classiﬁcation method, and outlines the standard metho dology for evalu-\\nating text classiﬁers. Chapter 14employs the vector space model from Chap-\\nter6and introduces two classiﬁcation methods, Rocchio and kNN, that op-\\nerate on document vectors. It also presents the bias-varian ce tradeoff as an\\nimportant characterization of learning problems that prov ides criteria for se-\\nlecting an appropriate method for a text classiﬁcation prob lem. Chapter 15\\nintroduces support vector machines, which many researcher s currently view\\nas the most effective text classiﬁcation method. We also dev elop connections\\nin this chapter between the problem of classiﬁcation and see mingly disparate\\ntopics such as the induction of scoring functions from a set o f training exam-\\nples.\\nChapters 16–18consider the problem of inducing clusters of related doc-\\numents from a collection. In Chapter 16, we ﬁrst give an overview of a\\nnumber of important applications of clustering in informat ion retrieval. We\\nthen describe two ﬂat clustering algorithms: the K-means algorithm, an ef-\\nﬁcient and widely used document clustering method; and the E xpectation-\\nMaximization algorithm, which is computationally more exp ensive, but also\\nmore ﬂexible. Chapter 17motivates the need for hierarchically structured', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 32}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPxxxiv Preface\\nclusterings (instead of ﬂat clusterings) in many applicati ons in information\\nretrieval and introduces a number of clustering algorithms that produce a\\nhierarchy of clusters. The chapter also addresses the difﬁc ult problem of\\nautomatically computing labels for clusters. Chapter 18develops methods\\nfrom linear algebra that constitute an extension of cluster ing, and also offer\\nintriguing prospects for algebraic methods in information retrieval, which\\nhave been pursued in the approach of latent semantic indexin g.\\nChapters 19–21treat the problem of web search. We give in Chapter 19a\\nsummary of the basic challenges in web search, together with a set of tech-\\nniques that are pervasive in web information retrieval. Nex t, Chapter 20\\ndescribes the architecture and requirements of a basic web c rawler. Finally,\\nChapter 21considers the power of link analysis in web search, using in t he\\nprocess several methods from linear algebra and advanced pr obability the-\\nory.\\nThis book is not comprehensive in covering all topics relate d to informa-\\ntion retrieval. We have put aside a number of topics, which we deemed\\noutside the scope of what we wished to cover in an introductio n to infor-\\nmation retrieval class. Nevertheless, for people interest ed in these topics, we\\nprovide a few pointers to mainly textbook coverage here.\\nCross-language IR (Grossman and Frieder 2004 , ch. 4) and ( Oard and Dorr\\n1996 ).\\nImage and Multimedia IR (Grossman and Frieder 2004 , ch. 4), ( Baeza-Yates\\nand Ribeiro-Neto 1999 , ch. 6), ( Baeza-Yates and Ribeiro-Neto 1999 , ch. 11),\\n(Baeza-Yates and Ribeiro-Neto 1999 , ch. 12), ( del Bimbo 1999 ), (Lew 2001 ),\\nand ( Smeulders et al. 2000 ).\\nSpeech retrieval (Coden et al. 2002 ).\\nMusic Retrieval (Downie 2006 ) andhttp://www.ismir.net/ .\\nUser interfaces for IR (Baeza-Yates and Ribeiro-Neto 1999 , ch. 10).\\nParallel and Peer-to-Peer IR (Grossman and Frieder 2004 , ch. 7), ( Baeza-Yates\\nand Ribeiro-Neto 1999 , ch. 9), and ( Aberer 2001 ).\\nDigital libraries (Baeza-Yates and Ribeiro-Neto 1999 , ch. 15) and ( Lesk 2004 ).\\nInformation science perspective (Korfhage 1997 ), (Meadow et al. 1999 ), and\\n(Ingwersen and Järvelin 2005 ).\\nLogic-based approaches to IR (van Rijsbergen 1989 ).\\nNatural Language Processing techniques (Manning and Schütze 1999 ), (Ju-\\nrafsky and Martin 2008 ), and ( Lewis and Jones 1996 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 33}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPPreface xxxv\\nPrerequisites\\nIntroductory courses in data structures and algorithms, in linear algebra and\\nin probability theory sufﬁce as prerequisites for all 21 cha pters. We now give\\nmore detail for the beneﬁt of readers and instructors who wis h to tailor their\\nreading to some of the chapters.\\nChapters 1–5assume as prerequisite a basic course in algorithms and data\\nstructures. Chapters 6and 7require, in addition, a knowledge of basic lin-\\near algebra including vectors and dot products. No addition al prerequisites\\nare assumed until Chapter 11, where a basic course in probability theory is\\nrequired; Section 11.1 gives a quick review of the concepts necessary in Chap-\\nters 11–13. Chapter 15assumes that the reader is familiar with the notion of\\nnonlinear optimization, although the chapter may be read wi thout detailed\\nknowledge of algorithms for nonlinear optimization. Chapt er18demands a\\nﬁrst course in linear algebra including familiarity with th e notions of matrix\\nrank and eigenvectors; a brief review is given in Section 18.1. The knowledge\\nof eigenvalues and eigenvectors is also necessary in Chapte r21.\\nBook layout\\n✎Worked examples in the text appear with a pencil sign next to t hem in the left\\nmargin. Advanced or difﬁcult material appears in sections o r subsections\\nindicated with scissors in the margin. Exercises are marked in the margin✄with a question mark. The level of difﬁculty of exercises is i ndicated as easy\\n(⋆), medium ( ⋆⋆), or difﬁcult ( ⋆ ⋆ ⋆ ).?\\nAcknowledgments\\nWe would like to thank Cambridge University Press for allowi ng us to make\\nthe draft book available online, which facilitated much of t he feedback we\\nhave received while writing the book. We also thank Lauren Co wles, who\\nhas been an outstanding editor, providing several rounds of comments on\\neach chapter, on matters of style, organization, and covera ge, as well as de-\\ntailed comments on the subject matter of the book. To the exte nt that we\\nhave achieved our goals in writing this book, she deserves an important part\\nof the credit.\\nWe are very grateful to the many people who have given us comme nts,\\nsuggestions, and corrections based on draft versions of thi s book. We thank\\nfor providing various corrections and comments: Cheryl Aas heim, Josh At-\\ntenberg, Daniel Beck, Luc Bélanger, Georg Buscher, Tom Breu el, Daniel Bur-\\nckhardt, Fazli Can, Dinquan Chen, Stephen Clark, Ernest Dav is, Pedro Domin-\\ngos, Rodrigo Panchiniak Fernandes, Paolo Ferragina, Alex F raser, Norbert', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 34}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPxxxvi Preface\\nFuhr, Vignesh Ganapathy, Elmer Garduno, Xiubo Geng, David G ondek, Ser-\\ngio Govoni, Corinna Habets, Ben Handy, Donna Harman, Benjam in Haskell,\\nThomas Hühn, Deepak Jain, Ralf Jankowitsch, Dinakar Jayara jan, Vinay Kakade,\\nMei Kobayashi, Wessel Kraaij, Rick Laﬂeur, Florian Laws, Ha ng Li, David\\nLosada, David Mann, Ennio Masi, Sven Meyer zu Eissen, Alexan der Murzaku,\\nGonzalo Navarro, Frank McCown, Paul McNamee, Christoph Mül ler, Scott\\nOlsson, Tao Qin, Megha Raghavan, Michal Rosen-Zvi, Klaus Ro thenhäusler,\\nKenyu L. Runner, Alexander Salamanca, Grigory Sapunov, Evg eny Shad-\\nchnev, Tobias Scheffer, Nico Schlaefer, Ian Soboroff, Benn o Stein, Marcin\\nSydow, Andrew Turner, Jason Utt, Huey Vo, Travis Wade, Mike W alsh, Changliang\\nWang, Renjing Wang, and Thomas Zeume.\\nMany people gave us detailed feedback on individual chapter s, either at\\nour request or through their own initiative. For this, we’re particularly grate-\\nful to: James Allan, Omar Alonso, Ismail Sengor Altingovde, Vo Ngoc Anh,\\nRoi Blanco, Eric Breck, Eric Brown, Mark Carman, Carlos Cast illo, Junghoo\\nCho, Aron Culotta, Doug Cutting, Meghana Deodhar, Susan Dum ais, Jo-\\nhannes Fürnkranz, Andreas Heß, Djoerd Hiemstra, David Hull , Thorsten\\nJoachims, Siddharth Jonathan J. B., Jaap Kamps, Mounia Lalm as, Amy Langville,\\nNicholas Lester, Dave Lewis, Daniel Lowd, Yosi Mass, Jeff Mi chels, Alessan-\\ndro Moschitti, Amir Najmi, Marc Najork, Giorgio Maria Di Nun zio, Paul\\nOgilvie, Priyank Patel, Jan Pedersen, Kathryn Pedings, Vas silis Plachouras,\\nDaniel Ramage, Ghulam Raza, Stefan Riezler, Michael Schieh len, Helmut\\nSchmid, Falk Nicolas Scholer, Sabine Schulte im Walde, Fabr izio Sebastiani,\\nSarabjeet Singh, Valentin Spitkovsky, Alexander Strehl, J ohn Tait, Shivaku-\\nmar Vaithyanathan, Ellen Voorhees, Gerhard Weikum, Dawid W eiss, Yiming\\nYang, Yisong Yue, Jian Zhang, and Justin Zobel.\\nAnd ﬁnally there were a few reviewers who absolutely stood ou t in terms\\nof the quality and quantity of comments that they provided. W e thank them\\nfor their signiﬁcant impact on the content and structure of t he book. We\\nexpress our gratitude to Pavel Berkhin, Stefan Büttcher, Ja mie Callan, Byron\\nDom, Torsten Suel, and Andrew Trotman.\\nParts of the initial drafts of Chapters 13–15were based on slides that were\\ngenerously provided by Ray Mooney. While the material has go ne through\\nextensive revisions, we gratefully acknowledge Ray’s cont ribution to the\\nthree chapters in general and to the description of the time c omplexities of\\ntext classiﬁcation algorithms in particular.\\nThe above is unfortunately an incomplete list: we are still i n the process of\\nincorporating feedback we have received. And, like all opin ionated authors,\\nwe did not always heed the advice that was so freely given. The published\\nversions of the chapters remain solely the responsibility o f the authors.\\nThe authors thank Stanford University and the University of Stuttgart for\\nproviding a stimulating academic environment for discussi ng ideas and the\\nopportunity to teach courses from which this book arose and i n which its', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 35}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPPreface xxxvii\\ncontents were reﬁned. CM thanks his family for the many hours they’ve let\\nhim spend working on this book, and hopes he’ll have a bit more free time on\\nweekends next year. PR thanks his family for their patient su pport through\\nthe writing of this book and is also grateful to Yahoo! Inc. fo r providing a\\nfertile environment in which to work on this book. HS would li ke to thank\\nhis parents, family, and friends for their support while wri ting this book.\\nWeb and contact information\\nThis book has a companion website at http://informationretrieval.org . As well as\\nlinks to some more general resources, it is our intent to main tain on this web-\\nsite a set of slides for each chapter which may be used for the c orresponding\\nlecture. We gladly welcome further feedback, corrections, and suggestions\\non the book, which may be sent to all the authors at informationretrieval(at)yahoogroups (dot)com .', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 36}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 1\\n1 Boolean retrieval\\nThe meaning of the term information retrieval can be very broad. Just getting\\na credit card out of your wallet so that you can type in the card number\\nis a form of information retrieval. However, as an academic ﬁ eld of study,\\ninformation retrieval might be deﬁned thus: INFORMATION\\nRETRIEVAL\\nInformation retrieval (IR) is ﬁnding material (usually doc uments) of\\nan unstructured nature (usually text) that satisﬁes an info rmation need\\nfrom within large collections (usually stored on computers ).\\nAs deﬁned in this way, information retrieval used to be an act ivity that only\\na few people engaged in: reference librarians, paralegals, and similar pro-\\nfessional searchers. Now the world has changed, and hundred s of millions\\nof people engage in information retrieval every day when the y use a web\\nsearch engine or search their email.1Information retrieval is fast becoming\\nthe dominant form of information access, overtaking tradit ional database-\\nstyle searching (the sort that is going on when a clerk says to you: “I’m sorry,\\nI can only look up your order if you can give me your Order ID”).\\nIR can also cover other kinds of data and information problem s beyond\\nthat speciﬁed in the core deﬁnition above. The term “unstruc tured data”\\nrefers to data which does not have clear, semantically overt , easy-for-a-computer\\nstructure. It is the opposite of structured data, the canoni cal example of\\nwhich is a relational database, of the sort companies usuall y use to main-\\ntain product inventories and personnel records. In reality , almost no data\\nare truly “unstructured”. This is deﬁnitely true of all text data if you count\\nthe latent linguistic structure of human languages. But eve n accepting that\\nthe intended notion of structure is overt structure, most te xt has structure,\\nsuch as headings and paragraphs and footnotes, which is comm only repre-\\nsented in documents by explicit markup (such as the coding un derlying web\\n1. In modern parlance, the word “search” has tended to replac e “(information) retrieval”; the\\nterm “search” is quite ambiguous, but in context we use the tw o synonymously.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 37}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP2 1 Boolean retrieval\\npages). IR is also used to facilitate “semistructured” sear ch such as ﬁnding a\\ndocument where the title contains Java and the body contains threading .\\nThe ﬁeld of information retrieval also covers supporting us ers in browsing\\nor ﬁltering document collections or further processing a se t of retrieved doc-\\numents. Given a set of documents, clustering is the task of co ming up with a\\ngood grouping of the documents based on their contents. It is similar to ar-\\nranging books on a bookshelf according to their topic. Given a set of topics,\\nstanding information needs, or other categories (such as su itability of texts\\nfor different age groups), classiﬁcation is the task of deci ding which class(es),\\nif any, each of a set of documents belongs to. It is often appro ached by ﬁrst\\nmanually classifying some documents and then hoping to be ab le to classify\\nnew documents automatically.\\nInformation retrieval systems can also be distinguished by the scale at\\nwhich they operate, and it is useful to distinguish three pro minent scales.\\nInweb search , the system has to provide search over billions of documents\\nstored on millions of computers. Distinctive issues are nee ding to gather\\ndocuments for indexing, being able to build systems that wor k efﬁciently\\nat this enormous scale, and handling particular aspects of t he web, such as\\nthe exploitation of hypertext and not being fooled by site pr oviders manip-\\nulating page content in an attempt to boost their search engi ne rankings,\\ngiven the commercial importance of the web. We focus on all th ese issues\\nin Chapters 19–21. At the other extreme is personal information retrieval . In\\nthe last few years, consumer operating systems have integra ted information\\nretrieval (such as Apple’s Mac OS X Spotlight or Windows Vist a’s Instant\\nSearch). Email programs usually not only provide search but also text clas-\\nsiﬁcation: they at least provide a spam (junk mail) ﬁlter, an d commonly also\\nprovide either manual or automatic means for classifying ma il so that it can\\nbe placed directly into particular folders. Distinctive is sues here include han-\\ndling the broad range of document types on a typical personal computer,\\nand making the search system maintenance free and sufﬁcient ly lightweight\\nin terms of startup, processing, and disk space usage that it can run on one\\nmachine without annoying its owner. In between is the space o fenterprise,\\ninstitutional, and domain-speciﬁc search , where retrieval might be provided for\\ncollections such as a corporation’s internal documents, a d atabase of patents,\\nor research articles on biochemistry. In this case, the docu ments will typi-\\ncally be stored on centralized ﬁle systems and one or a handfu l of dedicated\\nmachines will provide search over the collection. This book contains tech-\\nniques of value over this whole spectrum, but our coverage of some aspects\\nof parallel and distributed search in web-scale search syst ems is compara-\\ntively light owing to the relatively small published litera ture on the details\\nof such systems. However, outside of a handful of web search c ompanies, a\\nsoftware developer is most likely to encounter the personal search and en-\\nterprise scenarios.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 38}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP1.1 An example information retrieval problem 3\\nIn this chapter we begin with a very simple example of an infor mation\\nretrieval problem, and introduce the idea of a term-documen t matrix (Sec-\\ntion 1.1) and the central inverted index data structure (Section 1.2). We will\\nthen examine the Boolean retrieval model and how Boolean que ries are pro-\\ncessed (Sections 1.3and 1.4).\\n1.1 An example information retrieval problem\\nA fat book which many people own is Shakespeare’s Collected W orks. Sup-\\npose you wanted to determine which plays of Shakespeare cont ain the words\\nBrutus ANDCaesar AND NOT Calpurnia . One way to do that is to start at the\\nbeginning and to read through all the text, noting for each pl ay whether\\nit contains Brutus andCaesar and excluding it from consideration if it con-\\ntainsCalpurnia . The simplest form of document retrieval is for a computer\\nto do this sort of linear scan through documents. This proces s is commonly\\nreferred to as grepping through text, after the Unix command grep , which GREP\\nperforms this process. Grepping through text can be a very ef fective process,\\nespecially given the speed of modern computers, and often al lows useful\\npossibilities for wildcard pattern matching through the us e of regular expres-\\nsions. With modern computers, for simple querying of modest collections\\n(the size of Shakespeare’s Collected Works is a bit under one million words\\nof text in total), you really need nothing more.\\nBut for many purposes, you do need more:\\n1. To process large document collections quickly. The amoun t of online data\\nhas grown at least as quickly as the speed of computers, and we would\\nnow like to be able to search collections that total in the ord er of billions\\nto trillions of words.\\n2. To allow more ﬂexible matching operations. For example, i t is impractical\\nto perform the query Romans NEARcountrymen withgrep , where NEAR\\nmight be deﬁned as “within 5 words” or “within the same senten ce”.\\n3. To allow ranked retrieval: in many cases you want the best a nswer to an\\ninformation need among many documents that contain certain words.\\nThe way to avoid linearly scanning the texts for each query is toindex the INDEX\\ndocuments in advance. Let us stick with Shakespeare’s Colle cted Works,\\nand use it to introduce the basics of the Boolean retrieval mo del. Suppose\\nwe record for each document – here a play of Shakespeare’s – wh ether it\\ncontains each word out of all the words Shakespeare used (Sha kespeare used\\nabout 32,000 different words). The result is a binary term-d ocument incidence INCIDENCE MATRIX\\nmatrix , as in Figure 1.1.Terms are the indexed units (further discussed in TERM\\nSection 2.2); they are usually words, and for the moment you can think of', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 39}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP4 1 Boolean retrieval\\nAntony Julius The Hamlet Othello Macbeth . . .\\nand Caesar Tempest\\nCleopatra\\nAntony 1 1 0 0 0 1\\nBrutus 1 1 0 1 0 0\\nCaesar 1 1 0 1 1 1\\nCalpurnia 0 1 0 0 0 0\\nCleopatra 1 0 0 0 0 0\\nmercy 1 0 1 1 1 1\\nworser 1 0 1 1 1 0\\n. . .\\n◮Figure 1.1 A term-document incidence matrix. Matrix element (t,d)is 1 if the\\nplay in column dcontains the word in row t, and is 0 otherwise.\\nthem as words, but the information retrieval literature nor mally speaks of\\nterms because some of them, such as perhaps I-9orHongKong are not usually\\nthought of as words. Now, depending on whether we look at the m atrix rows\\nor columns, we can have a vector for each term, which shows the documents\\nit appears in, or a vector for each document, showing the term s that occur in\\nit.2\\nTo answer the query Brutus ANDCaesar AND NOT Calpurnia , we take the\\nvectors for Brutus ,Caesar andCalpurnia , complement the last, and then do a\\nbitwise AND :\\n110100 AND 110111 AND 101111 = 100100\\nThe answers for this query are thus Antony and Cleopatra and Hamlet (Fig-\\nure1.2).\\nThe Boolean retrieval model is a model for information retrieval in which we BOOLEAN RETRIEVAL\\nMODEL can pose any query which is in the form of a Boolean expression of terms,\\nthat is, in which terms are combined with the operators AND ,OR, and NOT .\\nThe model views each document as just a set of words.\\nLet us now consider a more realistic scenario, simultaneous ly using the\\nopportunity to introduce some terminology and notation. Su ppose we have\\nN=1 million documents. By documents we mean whatever units we have DOCUMENT\\ndecided to build a retrieval system over. They might be indiv idual memos\\nor chapters of a book (see Section 2.1.2 (page 20) for further discussion). We\\nwill refer to the group of documents over which we perform ret rieval as the\\n(document) collection . It is sometimes also referred to as a corpus (abody of COLLECTION\\nCORPUS texts). Suppose each document is about 1000 words long (2–3 b ook pages). If\\n2. Formally, we take the transpose of the matrix to be able to g et the terms as column vectors.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 40}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP1.1 An example information retrieval problem 5\\nAntony and Cleopatra, Act III, Scene ii\\nAgrippa [Aside to Domitius Enobarbus]: Why, Enobarbus,\\nWhen Antony found Julius Caesar dead,\\nHe cried almost to roaring; and he wept\\nWhen at Philippi he found Brutus slain.\\nHamlet, Act III, Scene ii\\nLord Polonius: I did enact Julius Caesar: I was killed i’ the\\nCapitol; Brutus killed me.\\n◮Figure 1.2 Results from Shakespeare for the query Brutus ANDCaesar AND NOT\\nCalpurnia .\\nwe assume an average of 6 bytes per word including spaces and p unctuation,\\nthen this is a document collection about 6 GB in size. Typical ly, there might\\nbe about M=500,000 distinct terms in these documents. There is nothing\\nspecial about the numbers we have chosen, and they might vary by an order\\nof magnitude or more, but they give us some idea of the dimensi ons of the\\nkinds of problems we need to handle. We will discuss and model these size\\nassumptions in Section 5.1(page 86).\\nOur goal is to develop a system to address the ad hoc retrieval task. This is AD HOC RETRIEVAL\\nthe most standard IR task. In it, a system aims to provide docu ments from\\nwithin the collection that are relevant to an arbitrary user information need,\\ncommunicated to the system by means of a one-off, user-initi ated query. An\\ninformation need is the topic about which the user desires to know more, and INFORMATION NEED\\nis differentiated from a query , which is what the user conveys to the com- QUERY\\nputer in an attempt to communicate the information need. A do cument is\\nrelevant if it is one that the user perceives as containing informatio n of value RELEVANCE\\nwith respect to their personal information need. Our exampl e above was\\nrather artiﬁcial in that the information need was deﬁned in t erms of par-\\nticular words, whereas usually a user is interested in a topi c like “pipeline\\nleaks” and would like to ﬁnd relevant documents regardless o f whether they\\nprecisely use those words or express the concept with other w ords such as\\npipeline rupture . To assess the effectiveness of an IR system (i.e., the quality of EFFECTIVENESS\\nits search results), a user will usually want to know two key s tatistics about\\nthe system’s returned results for a query:\\nPrecision : What fraction of the returned results are relevant to the in forma- PRECISION\\ntion need?\\nRecall : What fraction of the relevant documents in the collection w ere re- RECALL\\nturned by the system?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 41}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP6 1 Boolean retrieval\\nDetailed discussion of relevance and evaluation measures i ncluding preci-\\nsion and recall is found in Chapter 8.\\nWe now cannot build a term-document matrix in a naive way. A 50 0K×\\n1M matrix has half-a-trillion 0’s and 1’s – too many to ﬁt in a c omputer’s\\nmemory. But the crucial observation is that the matrix is ext remely sparse,\\nthat is, it has few non-zero entries. Because each document i s 1000 words\\nlong, the matrix has no more than one billion 1’s, so a minimum of 99.8% of\\nthe cells are zero. A much better representation is to record only the things\\nthat do occur, that is, the 1 positions.\\nThis idea is central to the ﬁrst major concept in information retrieval, the\\ninverted index . The name is actually redundant: an index always maps back INVERTED INDEX\\nfrom terms to the parts of a document where they occur. Nevert heless, in-\\nverted index , or sometimes inverted ﬁle , has become the standard term in infor-\\nmation retrieval.3The basic idea of an inverted index is shown in Figure 1.3.\\nWe keep a dictionary of terms (sometimes also referred to as a vocabulary or DICTIONARY\\nVOCABULARY lexicon ; in this book, we use dictionary for the data structure and vocabulary\\nLEXICONfor the set of terms). Then for each term, we have a list that re cords which\\ndocuments the term occurs in. Each item in the list – which rec ords that a\\nterm appeared in a document (and, later, often, the position s in the docu-\\nment) – is conventionally called a posting .4The list is then called a postings POSTING\\nPOSTINGS LIST list(or inverted list), and all the postings lists taken togethe r are referred to as\\nthepostings . The dictionary in Figure 1.3has been sorted alphabetically and POSTINGS\\neach postings list is sorted by document ID. We will see why th is is useful in\\nSection 1.3, below, but later we will also consider alternatives to doin g this\\n(Section 7.1.5 ).\\n1.2 A ﬁrst take at building an inverted index\\nTo gain the speed beneﬁts of indexing at retrieval time, we ha ve to build the\\nindex in advance. The major steps in this are:\\n1. Collect the documents to be indexed:\\nFriends, Romans, countrymen. So let it be with Caesar . . .\\n2. Tokenize the text, turning each document into a list of tok ens:\\nFriends Romans countrymen So. . .\\n3. Some information retrieval researchers prefer the term inverted ﬁle , but expressions like in-\\ndex construction andindex compression are much more common than inverted ﬁle construction and\\ninvertedﬁlecompression . For consistency, we use (inverted)index throughout this book.\\n4. In a (non-positional) inverted index, a posting is just a d ocument ID, but it is inherently\\nassociated with a term, via the postings list it is placed on; sometimes we will also talk of a\\n(term, docID) pair as a posting.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 42}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP1.2 A ﬁrst take at building an inverted index 7\\nBrutus−→ 1 2 4 11 31 45 173 174\\nCaesar−→ 1 2 4 5 616 57 132 . . .\\nCalpurnia−→ 231 54 101\\n...\\n\\ued19\\ued18\\ued17\\ued1a \\ued19 \\ued18\\ued17 \\ued1a\\nDictionary Postings\\n◮Figure 1.3 The two parts of an inverted index. The dictionary is commonl y kept\\nin memory, with pointers to each postings list, which is stor ed on disk.\\n3. Do linguistic preprocessing, producing a list of normali zed tokens, which\\nare the indexing terms: friend roman countryman so. . .\\n4. Index the documents that each term occurs in by creating an inverted in-\\ndex, consisting of a dictionary and postings.\\nWe will deﬁne and discuss the earlier stages of processing, t hat is, steps 1–3,\\nin Section 2.2(page 22). Until then you can think of tokens and normalized\\ntokens as also loosely equivalent to words . Here, we assume that the ﬁrst\\n3 steps have already been done, and we examine building a basi c inverted\\nindex by sort-based indexing.\\nWithin a document collection, we assume that each document h as a unique\\nserial number, known as the document identiﬁer ( docID ). During index con- DOC ID\\nstruction, we can simply assign successive integers to each new document\\nwhen it is ﬁrst encountered. The input to indexing is a list of normalized\\ntokens for each document, which we can equally think of as a li st of pairs of\\nterm and docID, as in Figure 1.4. The core indexing step is sorting this list SORTING\\nso that the terms are alphabetical, giving us the representa tion in the middle\\ncolumn of Figure 1.4. Multiple occurrences of the same term from the same\\ndocument are then merged.5Instances of the same term are then grouped,\\nand the result is split into a dictionary and postings , as shown in the right\\ncolumn of Figure 1.4. Since a term generally occurs in a number of docu-\\nments, this data organization already reduces the storage r equirements of\\nthe index. The dictionary also records some statistics, suc h as the number of\\ndocuments which contain each term (the document frequency , which is here DOCUMENT\\nFREQUENCY also the length of each postings list). This information is n ot vital for a ba-\\nsic Boolean search engine, but it allows us to improve the efﬁ ciency of the\\n5. Unix users can note that these steps are similar to use of th esort and thenuniq commands.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 43}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP8 1 Boolean retrieval\\nDoc 1 Doc 2\\nI did enact Julius Caesar: I was killed\\ni’ the Capitol; Brutus killed me.So let it be with Caesar. The noble Brutus\\nhath told you Caesar was ambitious:\\nterm docID\\nI 1\\ndid 1\\nenact 1\\njulius 1\\ncaesar 1\\nI 1\\nwas 1\\nkilled 1\\ni’ 1\\nthe 1\\ncapitol 1\\nbrutus 1\\nkilled 1\\nme 1\\nso 2\\nlet 2\\nit 2\\nbe 2\\nwith 2\\ncaesar 2\\nthe 2\\nnoble 2\\nbrutus 2\\nhath 2\\ntold 2\\nyou 2\\ncaesar 2\\nwas 2\\nambitious 2=⇒term docID\\nambitious 2\\nbe 2\\nbrutus 1\\nbrutus 2\\ncapitol 1\\ncaesar 1\\ncaesar 2\\ncaesar 2\\ndid 1\\nenact 1\\nhath 1\\nI 1\\nI 1\\ni’ 1\\nit 2\\njulius 1\\nkilled 1\\nkilled 1\\nlet 2\\nme 1\\nnoble 2\\nso 2\\nthe 1\\nthe 2\\ntold 2\\nyou 2\\nwas 1\\nwas 2\\nwith 2=⇒term doc. freq. → postings lists\\nambitious 1→ 2\\nbe 1→ 2\\nbrutus 2→ 1→2\\ncapitol 1→ 1\\ncaesar 2→ 1→2\\ndid 1→ 1\\nenact 1→ 1\\nhath 1→ 2\\nI1→ 1\\ni’1→ 1\\nit1→ 2\\njulius 1→ 1\\nkilled 1→ 1\\nlet 1→ 2\\nme 1→ 1\\nnoble 1→ 2\\nso 1→ 2\\nthe 2→ 1→2\\ntold 1→ 2\\nyou 1→ 2\\nwas 2→ 1→2\\nwith 1→ 2\\n◮Figure 1.4 Building an index by sorting and grouping. The sequence of te rms\\nin each document, tagged by their documentID (left) is sorte d alphabetically (mid-\\ndle). Instances of the same term are then grouped by word and t hen by documentID.\\nThe terms and documentIDs are then separated out (right). Th e dictionary stores\\nthe terms, and has a pointer to the postings list for each term . It commonly also\\nstores other summary information such as, here, the documen t frequency of each\\nterm. We use this information for improving query time efﬁci ency and, later, for\\nweighting in ranked retrieval models. Each postings list st ores the list of documents\\nin which a term occurs, and may store other information such a s the term frequency\\n(the frequency of each term in each document) or the position (s) of the term in each\\ndocument.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 44}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP1.2 A ﬁrst take at building an inverted index 9\\nsearch engine at query time, and it is a statistic later used i n many ranked re-\\ntrieval models. The postings are secondarily sorted by docI D. This provides\\nthe basis for efﬁcient query processing. This inverted inde x structure is es-\\nsentially without rivals as the most efﬁcient structure for supporting ad hoc\\ntext search.\\nIn the resulting index, we pay for storage of both the diction ary and the\\npostings lists. The latter are much larger, but the dictiona ry is commonly\\nkept in memory, while postings lists are normally kept on dis k, so the size\\nof each is important, and in Chapter 5we will examine how each can be\\noptimized for storage and access efﬁciency. What data struc ture should be\\nused for a postings list? A ﬁxed length array would be wastefu l as some\\nwords occur in many documents, and others in very few. For an i n-memory\\npostings list, two good alternatives are singly linked list s or variable length\\narrays. Singly linked lists allow cheap insertion of docume nts into postings\\nlists (following updates, such as when recrawling the web fo r updated doc-\\numents), and naturally extend to more advanced indexing str ategies such as\\nskip lists (Section 2.3), which require additional pointers. Variable length ar-\\nrays win in space requirements by avoiding the overhead for p ointers and in\\ntime requirements because their use of contiguous memory in creases speed\\non modern processors with memory caches. Extra pointers can in practice be\\nencoded into the lists as offsets. If updates are relatively infrequent, variable\\nlength arrays will be more compact and faster to traverse. We can also use a\\nhybrid scheme with a linked list of ﬁxed length arrays for eac h term. When\\npostings lists are stored on disk, they are stored (perhaps c ompressed) as a\\ncontiguous run of postings without explicit pointers (as in Figure 1.3), so as\\nto minimize the size of the postings list and the number of dis k seeks to read\\na postings list into memory.\\n?Exercise 1.1 [⋆]\\nDraw the inverted index that would be built for the following document collection.\\n(See Figure 1.3for an example.)\\nDoc 1 new home sales top forecasts\\nDoc 2 home sales rise in july\\nDoc 3 increase in home sales in july\\nDoc 4 july new home sales rise\\nExercise 1.2 [⋆]\\nConsider these documents:\\nDoc 1 breakthrough drug for schizophrenia\\nDoc 2 new schizophrenia drug\\nDoc 3 new approach for treatment of schizophrenia\\nDoc 4 new hopes for schizophrenia patients\\na. Draw the term-document incidence matrix for this documen t collection.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 45}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP10 1 Boolean retrieval\\nBrutus−→ 1→2→4→11→31→45→173→174\\nCalpurnia−→ 2→31→54→101\\nIntersection =⇒ 2→31\\n◮Figure 1.5 Intersecting the postings lists for Brutus andCalpurnia from Figure 1.3.\\nb. Draw the inverted index representation for this collecti on, as in Figure 1.3(page 7).\\nExercise 1.3 [⋆]\\nFor the document collection shown in Exercise 1.2, what are the returned results for\\nthese queries:\\na.schizophrenia ANDdrug\\nb.forAND NOT(drugORapproach)\\n1.3 Processing Boolean queries\\nHow do we process a query using an inverted index and the basic Boolean\\nretrieval model? Consider processing the simple conjunctive query : SIMPLE CONJUNCTIVE\\nQUERIES\\n(1.1)Brutus ANDCalpurnia\\nover the inverted index partially shown in Figure 1.3(page 7). We:\\n1.LocateBrutus in the Dictionary\\n2.Retrieve its postings\\n3.LocateCalpurnia in the Dictionary\\n4.Retrieve its postings\\n5.Intersect the two postings lists, as shown in Figure 1.5.\\nThe intersection operation is the crucial one: we need to efﬁciently intersec t POSTINGS LIST\\nINTERSECTION postings lists so as to be able to quickly ﬁnd documents that c ontain both\\nterms. (This operation is sometimes referred to as merging postings lists: POSTINGS MERGE\\nthis slightly counterintuitive name reﬂects using the term merge algorithm for\\na general family of algorithms that combine multiple sorted lists by inter-\\nleaved advancing of pointers through each; here we are mergi ng the lists\\nwith a logical AND operation.)\\nThere is a simple and effective method of intersecting posti ngs lists using\\nthe merge algorithm (see Figure 1.6): we maintain pointers into both lists', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 46}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP1.3 Processing Boolean queries 11\\nINTERSECT (p1,p2)\\n1answer←⟨⟩\\n2while p1̸=NILand p2̸=NIL\\n3do if docID (p1) =docID (p2)\\n4 then ADD(answer ,docID (p1))\\n5 p1←next(p1)\\n6 p2←next(p2)\\n7 else if docID (p1)<docID (p2)\\n8 then p1←next(p1)\\n9 else p2←next(p2)\\n10 return answer\\n◮Figure 1.6 Algorithm for the intersection of two postings lists p1and p2.\\nand walk through the two postings lists simultaneously, in t ime linear in\\nthe total number of postings entries. At each step, we compar e the docID\\npointed to by both pointers. If they are the same, we put that d ocID in the\\nresults list, and advance both pointers. Otherwise we advan ce the pointer\\npointing to the smaller docID. If the lengths of the postings lists are xand\\ny, the intersection takes O(x+y)operations. Formally, the complexity of\\nquerying is Θ(N), where Nis the number of documents in the collection.6\\nOur indexing methods gain us just a constant, not a differenc e in Θtime\\ncomplexity compared to a linear scan, but in practice the con stant is huge.\\nTo use this algorithm, it is crucial that postings be sorted b y a single global\\nordering. Using a numeric sort by docID is one simple way to ac hieve this.\\nWe can extend the intersection operation to process more com plicated queries\\nlike:\\n(1.2) (Brutus ORCaesar )AND NOT Calpurnia\\nQuery optimization is the process of selecting how to organize the work of an- QUERY OPTIMIZATION\\nswering a query so that the least total amount of work needs to be done by\\nthe system. A major element of this for Boolean queries is the order in which\\npostings lists are accessed. What is the best order for query processing? Con-\\nsider a query that is an AND oftterms, for instance:\\n(1.3)Brutus ANDCaesar ANDCalpurnia\\nFor each of the tterms, we need to get its postings, then AND them together.\\nThe standard heuristic is to process terms in order of increa sing document\\n6. The notation Θ(·)is used to express an asymptotically tight bound on the compl exity of\\nan algorithm. Informally, this is often written as O(·), but this notation really expresses an\\nasymptotic upper bound, which need not be tight ( Cormen et al. 1990 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 47}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP12 1 Boolean retrieval\\nINTERSECT (⟨t1, . . . , tn⟩)\\n1terms←SORTBYINCREASING FREQUENCY (⟨t1, . . . , tn⟩)\\n2result←postings (f irst(terms))\\n3terms←rest(terms)\\n4while terms̸=NILand result̸=NIL\\n5doresult←INTERSECT (result ,postings (f irst(terms)))\\n6 terms←rest(terms)\\n7return result\\n◮Figure 1.7 Algorithm for conjunctive queries that returns the set of do cuments\\ncontaining each term in the input list of terms.\\nfrequency: if we start by intersecting the two smallest post ings lists, then all\\nintermediate results must be no bigger than the smallest pos tings list, and we\\nare therefore likely to do the least amount of total work. So, for the postings\\nlists in Figure 1.3(page 7), we execute the above query as:\\n(1.4) (Calpurnia ANDBrutus )ANDCaesar\\nThis is a ﬁrst justiﬁcation for keeping the frequency of term s in the dictionary:\\nit allows us to make this ordering decision based on in-memor y data before\\naccessing any postings list.\\nConsider now the optimization of more general queries, such as:\\n(1.5) (madding ORcrowd )AND (ignoble ORstrife )AND (killed ORslain)\\nAs before, we will get the frequencies for all terms, and we ca n then (con-\\nservatively) estimate the size of each ORby the sum of the frequencies of its\\ndisjuncts. We can then process the query in increasing order of the size of\\neach disjunctive term.\\nFor arbitrary Boolean queries, we have to evaluate and tempo rarily store\\nthe answers for intermediate expressions in a complex expre ssion. However,\\nin many circumstances, either because of the nature of the qu ery language,\\nor just because this is the most common type of query that user s submit, a\\nquery is purely conjunctive. In this case, rather than viewi ng merging post-\\nings lists as a function with two inputs and a distinct output , it is more ef-\\nﬁcient to intersect each retrieved postings list with the cu rrent intermediate\\nresult in memory, where we initialize the intermediate resu lt by loading the\\npostings list of the least frequent term. This algorithm is s hown in Figure 1.7.\\nThe intersection operation is then asymmetric: the interme diate results list\\nis in memory while the list it is being intersected with is bei ng read from\\ndisk. Moreover the intermediate results list is always at le ast as short as the\\nother list, and in many cases it is orders of magnitude shorte r. The postings', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 48}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP1.3 Processing Boolean queries 13\\nintersection can still be done by the algorithm in Figure 1.6, but when the\\ndifference between the list lengths is very large, opportun ities to use alter-\\nnative techniques open up. The intersection can be calculat ed in place by\\ndestructively modifying or marking invalid items in the int ermediate results\\nlist. Or the intersection can be done as a sequence of binary s earches in the\\nlong postings lists for each posting in the intermediate res ults list. Another\\npossibility is to store the long postings list as a hashtable , so that membership\\nof an intermediate result item can be calculated in constant rather than linear\\nor log time. However, such alternative techniques are difﬁc ult to combine\\nwith postings list compression of the sort discussed in Chap ter5. Moreover,\\nstandard postings list intersection operations remain nec essary when both\\nterms of a query are very common.\\n?Exercise 1.4 [⋆]\\nFor the queries below, can we still run through the intersect ion in time O(x+y),\\nwhere xand yare the lengths of the postings lists for Brutus andCaesar ? If not, what\\ncan we achieve?\\na.Brutus AND NOT Caesar\\nb.Brutus OR NOTCaesar\\nExercise 1.5 [⋆]\\nExtend the postings merge algorithm to arbitrary Boolean qu ery formulas. What is\\nits time complexity? For instance, consider:\\nc.(Brutus ORCaesar )AND NOT (Antony ORCleopatra )\\nCan we always merge in linear time? Linear in what? Can we do be tter than this?\\nExercise 1.6 [⋆⋆]\\nWe can use distributive laws for AND and ORto rewrite queries.\\na.Show how to rewrite the query in Exercise 1.5into disjunctive normal form using\\nthe distributive laws.\\nb.Would the resulting query be more or less efﬁciently evaluat ed than the original\\nform of this query?\\nc.Is this result true in general or does it depend on the words an d the contents of\\nthe document collection?\\nExercise 1.7 [⋆]\\nRecommend a query processing order for\\nd.(tangerine ORtrees )AND (marmalade ORskies )AND (kaleidoscope OReyes)\\ngiven the following postings list sizes:', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 49}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP14 1 Boolean retrieval\\nTerm Postings size\\neyes 213312\\nkaleidoscope 87009\\nmarmalade 107913\\nskies 271658\\ntangerine 46653\\ntrees 316812\\nExercise 1.8 [⋆]\\nIf the query is:\\ne.friends ANDromans AND (NOTcountrymen )\\nhow could we use the frequency of countrymen in evaluating the best query evaluation\\norder? In particular, propose a way of handling negation in d etermining the order of\\nquery processing.\\nExercise 1.9 [⋆⋆]\\nFor a conjunctive query, is processing postings lists in ord er of size guaranteed to be\\noptimal? Explain why it is, or give an example where it isn’t.\\nExercise 1.10 [⋆⋆]\\nWrite out a postings merge algorithm, in the style of Figure 1.6(page 11), for an xORy\\nquery.\\nExercise 1.11 [⋆⋆]\\nHow should the Boolean query xAND NOT ybe handled? Why is naive evaluation\\nof this query normally very expensive? Write out a postings m erge algorithm that\\nevaluates this query efﬁciently.\\n1.4 The extended Boolean model versus ranked retrieval\\nThe Boolean retrieval model contrasts with ranked retrieval models such as the RANKED RETRIEVAL\\nMODEL vector space model (Section 6.3), in which users largely use free text queries ,\\nFREE TEXT QUERIESthat is, just typing one or more words rather than using a prec ise language\\nwith operators for building up query expressions, and the sy stem decides\\nwhich documents best satisfy the query. Despite decades of a cademic re-\\nsearch on the advantages of ranked retrieval, systems imple menting the Boo-\\nlean retrieval model were the main or only search option prov ided by large\\ncommercial information providers for three decades until t he early 1990s (ap-\\nproximately the date of arrival of the World Wide Web). Howev er, these\\nsystems did not have just the basic Boolean operations ( AND ,OR, and NOT )\\nwhich we have presented so far. A strict Boolean expression o ver terms with\\nan unordered results set is too limited for many of the inform ation needs\\nthat people have, and these systems implemented extended Bo olean retrieval\\nmodels by incorporating additional operators such as term p roximity oper-\\nators. A proximity operator is a way of specifying that two terms in a query PROXIMITY OPERATOR', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 50}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP1.4 The extended Boolean model versus ranked retrieval 15\\nmust occur close to each other in a document, where closeness may be mea-\\nsured by limiting the allowed number of intervening words or by reference\\nto a structural unit such as a sentence or paragraph.\\n✎Example 1.1: Commercial Boolean searching: Westlaw. Westlaw ( http://www.westlaw.com/ )\\nis the largest commercial legal search service (in terms of t he number of paying sub-\\nscribers), with over half a million subscribers performing millions of searches a day\\nover tens of terabytes of text data. The service was started i n 1975. In 2005, Boolean\\nsearch (called “Terms and Connectors” by Westlaw) was still the default, and used\\nby a large percentage of users, although ranked free text que rying (called “Natural\\nLanguage” by Westlaw) was added in 1992. Here are some exampl e Boolean queries\\non Westlaw:\\nInformation need: Information on the legal theories involved in preventing th e\\ndisclosure of trade secrets by employees formerly employed by a competing\\ncompany. Query: \"trade secret\" /s disclos! /s prevent /s employe!\\nInformation need: Requirements for disabled people to be able to access a work-\\nplace.\\nQuery: disab! /p access! /s work-site work-place (employment /3 pl ace)\\nInformation need: Cases about a host’s responsibility for drunk guests.\\nQuery: host! /p (responsib! liab!) /p (intoxicat! drunk!) /p guest\\nNote the long, precise queries and the use of proximity opera tors, both uncommon\\nin web search. Submitted queries average about ten words in l ength. Unlike web\\nsearch conventions, a space between words represents disju nction (the tightest bind-\\ning operator), & is AND and /s, /p, and / kask for matches in the same sentence,\\nsame paragraph or within kwords respectively. Double quotes give a phrase search\\n(consecutive words); see Section 2.4(page 39). The exclamation mark (!) gives a trail-\\ning wildcard query (see Section 3.2, page 51); thusliab! matches all words starting\\nwithliab. Additionally work-site matches any of worksite ,work-site orwork site ; see\\nSection 2.2.1 (page 22). Typical expert queries are usually carefully deﬁned and i ncre-\\nmentally developed until they obtain what look to be good res ults to the user.\\nMany users, particularly professionals, prefer Boolean qu ery models. Boolean\\nqueries are precise: a document either matches the query or i t does not. This of-\\nfers the user greater control and transparency over what is r etrieved. And some do-\\nmains, such as legal materials, allow an effective means of d ocument ranking within a\\nBoolean model: Westlaw returns documents in reverse chrono logical order, which is\\nin practice quite effective. In 2007, the majority of law lib rarians still seem to rec-\\nommend terms and connectors for high recall searches, and th e majority of legal\\nusers think they are getting greater control by using them. H owever, this does not\\nmean that Boolean queries are more effective for profession al searchers. Indeed, ex-\\nperimenting on a Westlaw subcollection, Turtle (1994 ) found that free text queries\\nproduced better results than Boolean queries prepared by We stlaw’s own reference\\nlibrarians for the majority of the information needs in his e xperiments. A general\\nproblem with Boolean search is that using AND operators tends to produce high pre-\\ncision but low recall searches, while using ORoperators gives low precision but high\\nrecall searches, and it is difﬁcult or impossible to ﬁnd a sat isfactory middle ground.\\nIn this chapter, we have looked at the structure and construc tion of a basic', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 51}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP16 1 Boolean retrieval\\ninverted index, comprising a dictionary and postings lists . We introduced\\nthe Boolean retrieval model, and examined how to do efﬁcient retrieval via\\nlinear time merges and simple query optimization. In Chapte rs2–7we will\\nconsider in detail richer query models and the sort of augmen ted index struc-\\ntures that are needed to handle them efﬁciently. Here we just mention a few\\nof the main additional things we would like to be able to do:\\n1.We would like to better determine the set of terms in the dicti onary and\\nto provide retrieval that is tolerant to spelling mistakes a nd inconsistent\\nchoice of words.\\n2.It is often useful to search for compounds or phrases that den ote a concept\\nsuch as“operating system” . As the Westlaw examples show, we might also\\nwish to do proximity queries such as Gates NEARMicrosoft . To answer\\nsuch queries, the index has to be augmented to capture the pro ximities of\\nterms in documents.\\n3.A Boolean model only records term presence or absence, but of ten we\\nwould like to accumulate evidence, giving more weight to doc uments that\\nhave a term several times as opposed to ones that contain it on ly once. To\\nbe able to do this we need term frequency information (the number of times TERM FREQUENCY\\na term occurs in a document) in postings lists.\\n4.Boolean queries just retrieve a set of matching documents, b ut commonly\\nwe wish to have an effective method to order (or “rank”) the re turned\\nresults. This requires having a mechanism for determining a document\\nscore which encapsulates how good a match a document is for a q uery.\\nWith these additional ideas, we will have seen most of the bas ic technol-\\nogy that supports ad hoc searching over unstructured inform ation. Ad hoc\\nsearching over documents has recently conquered the world, powering not\\nonly web search engines but the kind of unstructured search t hat lies behind\\nthe large eCommerce websites. Although the main web search e ngines differ\\nby emphasizing free text querying, most of the basic issues a nd technologies\\nof indexing and querying remain the same, as we will see in lat er chapters.\\nMoreover, over time, web search engines have added at least p artial imple-\\nmentations of some of the most popular operators from extend ed Boolean\\nmodels: phrase search is especially popular and most have a v ery partial\\nimplementation of Boolean operators. Nevertheless, while these options are\\nliked by expert searchers, they are little used by most peopl e and are not the\\nmain focus in work on trying to improve web search engine perf ormance.\\n?Exercise 1.12 [⋆]\\nWrite a query using Westlaw syntax which would ﬁnd any of the w ordsprofessor ,\\nteacher , orlecturer in the same sentence as a form of the verb explain .', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 52}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP1.5 References and further reading 17\\nExercise 1.13 [⋆]\\nTry using the Boolean search features on a couple of major web search engines. For\\ninstance, choose a word, such as burglar , and submit the queries (i) burglar , (ii)burglar\\nANDburglar , and (iii) burglar ORburglar . Look at the estimated number of results and\\ntop hits. Do they make sense in terms of Boolean logic? Often t hey haven’t for major\\nsearch engines. Can you make sense of what is going on? What ab out if you try\\ndifferent words? For example, query for (i) knight , (ii)conquer , and then (iii) knight OR\\nconquer . What bound should the number of results from the ﬁrst two que ries place\\non the third query? Is this bound observed?\\n1.5 References and further reading\\nThe practical pursuit of computerized information retriev al began in the late\\n1940s ( Cleverdon 1991 ,Liddy 2005 ). A great increase in the production of\\nscientiﬁc literature, much in the form of less formal techni cal reports rather\\nthan traditional journal articles, coupled with the availa bility of computers,\\nled to interest in automatic document retrieval. However, i n those days, doc-\\nument retrieval was always based on author, title, and keywo rds; full-text\\nsearch came much later.\\nThe article of Bush (1945 ) provided lasting inspiration for the new ﬁeld:\\n“Consider a future device for individual use, which is a sort of mech-\\nanized private ﬁle and library. It needs a name, and, to coin o ne at\\nrandom, ‘memex’ will do. A memex is a device in which an indivi dual\\nstores all his books, records, and communications, and whic h is mech-\\nanized so that it may be consulted with exceeding speed and ﬂe xibility.\\nIt is an enlarged intimate supplement to his memory.”\\nThe term Information Retrieval was coined by Calvin Mooers in 1948/1950\\n(Mooers 1950 ).\\nIn 1958, much newspaper attention was paid to demonstration s at a con-\\nference (see Taube and Wooster 1958 ) of IBM “auto-indexing” machines, based\\nprimarily on the work of H. P . Luhn. Commercial interest quic kly gravitated\\ntowards Boolean retrieval systems, but the early years saw a heady debate\\nover various disparate technologies for retrieval systems . For example Moo-\\ners(1961 ) dissented:\\n“It is a common fallacy, underwritten at this date by the inve stment of\\nseveral million dollars in a variety of retrieval hardware, that the al-\\ngebra of George Boole (1847) is the appropriate formalism fo r retrieval\\nsystem design. This view is as widely and uncritically accep ted as it is\\nwrong.”\\nThe observation of AND vs. ORgiving you opposite extremes in a precision/\\nrecall tradeoff, but not the middle ground comes from ( Lee and Fox 1988 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 53}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP18 1 Boolean retrieval\\nThe book ( Witten et al. 1999 ) is the standard reference for an in-depth com-\\nparison of the space and time efﬁciency of the inverted index versus other\\npossible data structures; a more succinct and up-to-date pr esentation ap-\\npears in Zobel and Moffat (2006 ). We further discuss several approaches in\\nChapter 5.\\nFriedl (2006 ) covers the practical usage of regular expressions for searching. REGULAR EXPRESSIONS\\nThe underlying computer science appears in ( Hopcroft et al. 2000 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 54}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 19\\n2The term vocabulary and postings\\nlists\\nRecall the major steps in inverted index construction:\\n1.Collect the documents to be indexed.\\n2.Tokenize the text.\\n3.Do linguistic preprocessing of tokens.\\n4.Index the documents that each term occurs in.\\nIn this chapter we ﬁrst brieﬂy mention how the basic unit of a d ocument can\\nbe deﬁned and how the character sequence that it comprises is determined\\n(Section 2.1). We then examine in detail some of the substantive linguis-\\ntic issues of tokenization and linguistic preprocessing, w hich determine the\\nvocabulary of terms which a system uses (Section 2.2). Tokenization is the\\nprocess of chopping character streams into tokens, while li nguistic prepro-\\ncessing then deals with building equivalence classes of tok ens which are the\\nset of terms that are indexed. Indexing itself is covered in C hapters 1and 4.\\nThen we return to the implementation of postings lists. In Se ction 2.3, we\\nexamine an extended postings list data structure that suppo rts faster query-\\ning, while Section 2.4covers building postings data structures suitable for\\nhandling phrase and proximity queries, of the sort that comm only appear in\\nboth extended Boolean models and on the web.\\n2.1 Document delineation and character sequence decoding\\n2.1.1 Obtaining the character sequence in a document\\nDigital documents that are the input to an indexing process a re typically\\nbytes in a ﬁle or on a web server. The ﬁrst step of processing is to convert this\\nbyte sequence into a linear sequence of characters. For the c ase of plain En-\\nglish text in ASCII encoding, this is trivial. But often thin gs get much more', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 55}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP20 2 The term vocabulary and postings lists\\ncomplex. The sequence of characters may be encoded by one of v arious sin-\\ngle byte or multibyte encoding schemes, such as Unicode UTF- 8, or various\\nnational or vendor-speciﬁc standards. We need to determine the correct en-\\ncoding. This can be regarded as a machine learning classiﬁca tion problem,\\nas discussed in Chapter 13,1but is often handled by heuristic methods, user\\nselection, or by using provided document metadata. Once the encoding is\\ndetermined, we decode the byte sequence to a character seque nce. We might\\nsave the choice of encoding because it gives some evidence ab out what lan-\\nguage the document is written in.\\nThe characters may have to be decoded out of some binary repre sentation\\nlike Microsoft Word DOC ﬁles and/or a compressed format such as zip ﬁles.\\nAgain, we must determine the document format, and then an app ropriate\\ndecoder has to be used. Even for plain text documents, additi onal decoding\\nmay need to be done. In XML documents (Section 10.1, page 197), charac-\\nter entities, such as&amp; , need to be decoded to give the correct character,\\nnamely&for&amp; . Finally, the textual part of the document may need to\\nbe extracted out of other material that will not be processed . This might be\\nthe desired handling for XML ﬁles, if the markup is going to be ignored; we\\nwould almost certainly want to do this with postscript or PDF ﬁles. We will\\nnot deal further with these issues in this book, and will assu me henceforth\\nthat our documents are a list of characters. Commercial prod ucts usually\\nneed to support a broad range of document types and encodings , since users\\nwant things to just work with their data as is. Often, they jus t think of docu-\\nments as text inside applications and are not even aware of ho w it is encoded\\non disk. This problem is usually solved by licensing a softwa re library that\\nhandles decoding document formats and character encodings .\\nThe idea that text is a linear sequence of characters is also c alled into ques-\\ntion by some writing systems, such as Arabic, where text take s on some\\ntwo dimensional and mixed order characteristics, as shown i n Figures 2.1\\nand 2.2. But, despite some complicated writing system conventions , there\\nis an underlying sequence of sounds being represented and he nce an essen-\\ntially linear structure remains, and this is what is represe nted in the digital\\nrepresentation of Arabic, as shown in Figure 2.1.\\n2.1.2 Choosing a document unit\\nThe next phase is to determine what the document unit for indexing is. Thus DOCUMENT UNIT\\nfar we have assumed that documents are ﬁxed units for the purp oses of in-\\ndexing. For example, we take each ﬁle in a folder as a document . But there\\n1. A classiﬁer is a function that takes objects of some sort an d assigns them to one of a number\\nof distinct classes (see Chapter 13). Usually classiﬁcation is done by machine learning method s\\nsuch as probabilistic models, but it can also be done by hand- written rules.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 56}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP2.1 Document delineation and character sequence decoding 21\\nآِ    ⇐     ٌ    ب   ا  ت  ِ   ك     \\nun b ā  t  i k  \\n/kitābun/ ‘a book’  \\n \\n◮Figure 2.1 An example of a vocalized Modern Standard Arabic word. The wr iting\\nis from right to left and letters undergo complex mutations a s they are combined. The\\nrepresentation of short vowels (here, /i/ and /u/) and the ﬁn al /n/ (nunation) de-\\nparts from strict linearity by being represented as diacrit ics above and below letters.\\nNevertheless, the represented text is still clearly a linea r ordering of characters repre-\\nsenting sounds. Full vocalization, as here, normally appea rs only in the Koran and\\nchildren’s books. Day-to-day text is unvocalized (short vo wels are not represented\\nbut the letter for ¯a would still appear) or partially vocalized, with short vow els in-\\nserted in places where the writer perceives ambiguities. Th ese choices add further\\ncomplexities to indexing.\\nafii62825/afii62820 /afii62760/afii62820/afii62760/afii62802 .   \\n                               ← →   ← →                   ← START  \\n‘Algeria achieved its independence in 1962 after 132 years of French occupation.’ \\n \\n◮Figure 2.2 The conceptual linear order of characters is not necessaril y the order\\nthat you see on the page. In languages that are written right- to-left, such as Hebrew\\nand Arabic, it is quite common to also have left-to-right tex t interspersed, such as\\nnumbers and dollar amounts. With modern Unicode representa tion concepts, the\\norder of characters in ﬁles matches the conceptual order, an d the reversal of displayed\\ncharacters is handled by the rendering system, but this may n ot be true for documents\\nin older encodings.\\nare many cases in which you might want to do something differe nt. A tra-\\nditional Unix (mbox-format) email ﬁle stores a sequence of e mail messages\\n(an email folder) in one ﬁle, but you might wish to regard each email mes-\\nsage as a separate document. Many email messages now contain attached\\ndocuments, and you might then want to regard the email messag e and each\\ncontained attachment as separate documents. If an email mes sage has an\\nattached zip ﬁle, you might want to decode the zip ﬁle and rega rd each ﬁle\\nit contains as a separate document. Going in the opposite dir ection, various\\npieces of web software (such as latex2html ) take things that you might regard\\nas a single document (e.g., a Powerpoint ﬁle or a L ATEX document) and split\\nthem into separate HTML pages for each slide or subsection, s tored as sep-\\narate ﬁles. In these cases, you might want to combine multipl e ﬁles into a\\nsingle document.\\nMore generally, for very long documents, the issue of indexi nggranularity INDEXING\\nGRANULARITY arises. For a collection of books, it would usually be a bad id ea to index an', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 57}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP22 2 The term vocabulary and postings lists\\nentire book as a document. A search for Chinese toys might bring up a book\\nthat mentions China in the ﬁrst chapter and toys in the last chapter, but this\\ndoes not make it relevant to the query. Instead, we may well wi sh to index\\neach chapter or paragraph as a mini-document. Matches are th en more likely\\nto be relevant, and since the documents are smaller it will be much easier for\\nthe user to ﬁnd the relevant passages in the document. But why stop there?\\nWe could treat individual sentences as mini-documents. It b ecomes clear\\nthat there is a precision/recall tradeoff here. If the units get too small, we\\nare likely to miss important passages because terms were dis tributed over\\nseveral mini-documents, while if units are too large we tend to get spurious\\nmatches and the relevant information is hard for the user to ﬁ nd.\\nThe problems with large document units can be alleviated by u se of ex-\\nplicit or implicit proximity search (Sections 2.4.2 and 7.2.2 ), and the trade-\\noffs in resulting system performance that we are hinting at a re discussed\\nin Chapter 8. The issue of index granularity, and in particular a need to\\nsimultaneously index documents at multiple levels of granu larity, appears\\nprominently in XML retrieval, and is taken up again in Chapte r10. An IR\\nsystem should be designed to offer choices of granularity. F or this choice to\\nbe made well, the person who is deploying the system must have a good\\nunderstanding of the document collection, the users, and th eir likely infor-\\nmation needs and usage patterns. For now, we will henceforth assume that\\na suitable size document unit has been chosen, together with an appropriate\\nway of dividing or aggregating ﬁles, if needed.\\n2.2 Determining the vocabulary of terms\\n2.2.1 Tokenization\\nGiven a character sequence and a deﬁned document unit, token ization is the\\ntask of chopping it up into pieces, called tokens , perhaps at the same time\\nthrowing away certain characters, such as punctuation. Her e is an example\\nof tokenization:\\nInput: Friends, Romans, Countrymen, lend me your ears;\\nOutput: Friends Romans Countrymen lend me your ears\\nThese tokens are often loosely referred to as terms or words, but it is some-\\ntimes important to make a type/token distinction. A token is an instance TOKEN\\nof a sequence of characters in some particular document that are grouped\\ntogether as a useful semantic unit for processing. A type is the class of all TYPE\\ntokens containing the same character sequence. A term is a (perhaps nor- TERM\\nmalized) type that is included in the IR system’s dictionary . The set of index\\nterms could be entirely distinct from the tokens, for instan ce, they could be', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 58}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP2.2 Determining the vocabulary of terms 23\\nsemantic identiﬁers in a taxonomy, but in practice in modern IR systems they\\nare strongly related to the tokens in the document. However, rather than be-\\ning exactly the tokens that appear in the document, they are u sually derived\\nfrom them by various normalization processes which are disc ussed in Sec-\\ntion 2.2.3 .2For example, if the document to be indexed is to sleep perchance to\\ndream , then there are 5 tokens, but only 4 types (since there are 2 in stances of\\nto). However, if tois omitted from the index (as a stop word, see Section 2.2.2\\n(page 27)), then there will be only 3 terms: sleep ,perchance , and dream .\\nThe major question of the tokenization phase is what are the c orrect tokens\\nto use? In this example, it looks fairly trivial: you chop on w hitespace and\\nthrow away punctuation characters. This is a starting point , but even for\\nEnglish there are a number of tricky cases. For example, what do you do\\nabout the various uses of the apostrophe for possession and c ontractions?\\nMr. O’Neill thinks that the boys’ stories about Chile’s capi tal aren’t\\namusing.\\nForO’Neill , which of the following is the desired tokenization?\\nneill\\noneill\\no’neill\\no’neill\\noneill ?\\nAnd for aren’t , is it:\\naren’t\\narent\\nare n’t\\naren t?\\nA simple strategy is to just split on all non-alphanumeric ch aracters, but\\nwhile oneill looks okay, aren tlooks intuitively bad. For all of them,\\nthe choices determine which Boolean queries will match. A qu ery ofneill\\nANDcapital will match in three cases but not the other two. In how many\\ncases would a query of o’neill ANDcapital match? If no preprocessing of a\\nquery is done, then it would match in only one of the ﬁve cases. For either\\n2. That is, as deﬁned here, tokens that are not indexed (stop w ords) are not terms, and if mul-\\ntiple tokens are collapsed together via normalization, the y are indexed as one term, under the\\nnormalized form. However, we later relax this deﬁnition whe n discussing classiﬁcation and\\nclustering in Chapters 13–18, where there is no index. In these chapters, we drop the requi re-\\nment of inclusion in the dictionary. A term means a normalized word.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 59}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP24 2 The term vocabulary and postings lists\\nBoolean or free text queries, you always want to do the exact s ame tokeniza-\\ntion of document and query words, generally by processing qu eries with the\\nsame tokenizer. This guarantees that a sequence of characte rs in a text will\\nalways match the same sequence typed in a query.3\\nThese issues of tokenization are language-speciﬁc. It thus requires the lan-\\nguage of the document to be known. Language identiﬁcation based on clas- LANGUAGE\\nIDENTIFICATION siﬁers that use short character subsequences as features is highly effective;\\nmost languages have distinctive signature patterns (see pa ge46for refer-\\nences).\\nFor most languages and particular domains within them there are unusual\\nspeciﬁc tokens that we wish to recognize as terms, such as the programming\\nlanguages C++ andC#, aircraft names like B-52 , or a T.V . show name such\\nasM*A*S*H – which is sufﬁciently integrated into popular culture that you\\nﬁnd usages such as M*A*S*H-style hospitals . Computer technology has in-\\ntroduced new types of character sequences that a tokenizer s hould probably\\ntokenize as a single token, including email addresses ( jblack@mail.yahoo.com ),\\nweb URLs ( http://stuff.big.com/new/specials.html ),numeric IP addresses ( 142.32.48.231 ),\\npackage tracking numbers ( 1Z9999W99845399981 ), and more. One possible\\nsolution is to omit from indexing tokens such as monetary amo unts, num-\\nbers, and URLs, since their presence greatly expands the siz e of the vocab-\\nulary. However, this comes at a large cost in restricting wha t people can\\nsearch for. For instance, people might want to search in a bug database for\\nthe line number where an error occurs. Items such as the date o f an email,\\nwhich have a clear semantic type, are often indexed separate ly as document\\nmetadata (see Section 6.1, page 110).\\nIn English, hyphenation is used for various purposes ranging from split- HYPHENS\\nting up vowels in words ( co-education ) to joining nouns as names ( Hewlett-\\nPackard ) to a copyediting device to show word grouping ( the hold-him-back-\\nand-drag-him-away maneuver ). It is easy to feel that the ﬁrst example should be\\nregarded as one token (and is indeed more commonly written as just coedu-\\ncation ), the last should be separated into words, and that the middl e case is\\nunclear. Handling hyphens automatically can thus be comple x: it can either\\nbe done as a classiﬁcation problem, or more commonly by some h euristic\\nrules, such as allowing short hyphenated preﬁxes on words, b ut not longer\\nhyphenated forms.\\nConceptually, splitting on white space can also split what s hould be re-\\ngarded as a single token. This occurs most commonly with name s (San Fran-\\ncisco, Los Angeles ) but also with borrowed foreign phrases ( au fait ) and com-\\n3. For the free text case, this is straightforward. The Boole an case is more complex: this tok-\\nenization may produce multiple terms from one query word. Th is can be handled by combining\\nthe terms with an AND or as a phrase query (see Section 2.4, page 39). It is harder for a system\\nto handle the opposite case where the user entered as two term s something that was tokenized\\ntogether in the document processing.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 60}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP2.2 Determining the vocabulary of terms 25\\npounds that are sometimes written as a single word and someti mes space\\nseparated (such as white space vs.whitespace ). Other cases with internal spaces\\nthat we might wish to regard as a single token include phone nu mbers ((800)234-\\n2333 ) and dates ( Mar 11, 1983 ). Splitting tokens on spaces can cause bad\\nretrieval results, for example, if a search for York University mainly returns\\ndocuments containing New York University . The problems of hyphens and\\nnon-separating whitespace can even interact. Advertiseme nts for air fares\\nfrequently contain items like San Francisco-Los Angeles , where simply doing\\nwhitespace splitting would give unfortunate results. In su ch cases, issues of\\ntokenization interact with handling phrase queries (which we discuss in Sec-\\ntion 2.4(page 39)), particularly if we would like queries for all of lowercase ,\\nlower-case and lower case to return the same results. The last two can be han-\\ndled by splitting on hyphens and using a phrase index. Gettin g the ﬁrst case\\nright would depend on knowing that it is sometimes written as two words\\nand also indexing it in this way. One effective strategy in pr actice, which\\nis used by some Boolean retrieval systems such as Westlaw and Lexis-Nexis\\n(Example 1.1), is to encourage users to enter hyphens wherever they may be\\npossible, and whenever there is a hyphenated form, the syste m will general-\\nize the query to cover all three of the one word, hyphenated, a nd two word\\nforms, so that a query for over-eager will search for over-eager OR“over eager”\\nORovereager . However, this strategy depends on user training, since if y ou\\nquery using either of the other two forms, you get no generali zation.\\nEach new language presents some new issues. For instance, Fr ench has a\\nvariant use of the apostrophe for a reduced deﬁnite article ‘ the’ before a word\\nbeginning with a vowel (e.g., l’ensemble ) and has some uses of the hyphen\\nwith postposed clitic pronouns in imperatives and question s (e.g., donne-\\nmoi‘give me’). Getting the ﬁrst case correct will affect the cor rect indexing\\nof a fair percentage of nouns and adjectives: you would want d ocuments\\nmentioning both l’ensemble and un ensemble to be indexed under ensemble .\\nOther languages make the problem harder in new ways. German w rites\\ncompound nouns without spaces (e.g., Computerlinguistik ‘computational lin- COMPOUNDS\\nguistics’; Lebensversicherungsgesellschaftsangestellter ‘life insurance company\\nemployee’). Retrieval systems for German greatly beneﬁt fr om the use of a\\ncompound-splitter module, which is usually implemented by seeing if a word COMPOUND -SPLITTER\\ncan be subdivided into multiple words that appear in a vocabu lary. This phe-\\nnomenon reaches its limit case with major East Asian Languag es (e.g., Chi-\\nnese, Japanese, Korean, and Thai), where text is written wit hout any spaces\\nbetween words. An example is shown in Figure 2.3. One approach here is to\\nperform word segmentation as prior linguistic processing. Methods of word WORD SEGMENTATION\\nsegmentation vary from having a large vocabulary and taking the longest\\nvocabulary match with some heuristics for unknown words to t he use of\\nmachine learning sequence models, such as hidden Markov mod els or condi-\\ntional random ﬁelds, trained over hand-segmented words (se e the references', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 61}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP26 2 The term vocabulary and postings lists\\x00 \\x01 \\x02 \\x03 \\x04 \\x05 \\x06 \\x07 \\x05 \\x08 \\t \\n \\x0b \\x0c \\r \\x0e \\x0f \\x10 \\x11 \\x12 \\x13 \\x14 \\x15 \\x16\\x17 \\x18 \\x19\\n\\x00 \\x01 \\x02 \\x03 \\x05 \\x08 \\t \\x1a \\x1b \\x1c \\x1d \\x1e \\x1f  ! \" # $ % & \\'\\x18\\n\\x12 \\'\\x18 ( ) * \\x19\\n\\x00 \\x01 \\x02 \\x03 + , # - \\x08 \\r . / \\x12\\n◮Figure 2.3 The standard unsegmented form of Chinese text using the simp liﬁed\\ncharacters of mainland China. There is no whitespace betwee n words, not even be-\\ntween sentences – the apparent space after the Chinese perio d (◦) is just a typograph-\\nical illusion caused by placing the character on the left sid e of its square box. The\\nﬁrst sentence is just words in Chinese characters with no spa ces between them. The\\nsecond and third sentences include Arabic numerals and punc tuation breaking up\\nthe Chinese characters.\\n◮Figure 2.4 Ambiguities in Chinese word segmentation. The two characte rs can\\nbe treated as one word meaning ‘monk’ or as a sequence of two wo rds meaning ‘and’\\nand ‘still’.\\na an and are as at be by for from\\nhas he in is it its of on that the\\nto was were will with\\n◮Figure 2.5 A stop list of 25 semantically non-selective words which are common\\nin Reuters-RCV1.\\nin Section 2.5). Since there are multiple possible segmentations of chara cter\\nsequences (see Figure 2.4), all such methods make mistakes sometimes, and\\nso you are never guaranteed a consistent unique tokenizatio n. The other ap-\\nproach is to abandon word-based indexing and to do all indexi ng via just\\nshort subsequences of characters (character k-grams), regardless of whether\\nparticular sequences cross word boundaries or not. Three re asons why this\\napproach is appealing are that an individual Chinese charac ter is more like a\\nsyllable than a letter and usually has some semantic content , that most words\\nare short (the commonest length is 2 characters), and that, g iven the lack of\\nstandardization of word breaking in the writing system, it i s not always clear\\nwhere word boundaries should be placed anyway. Even in Engli sh, some\\ncases of where to put word boundaries are just orthographic c onventions –\\nthink of notwithstanding vs.not to mention orinto vs.on to – but people are\\neducated to write the words with consistent use of spaces.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 62}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP2.2 Determining the vocabulary of terms 27\\n2.2.2 Dropping common terms: stop words\\nSometimes, some extremely common words which would appear t o be of\\nlittle value in helping select documents matching a user nee d are excluded\\nfrom the vocabulary entirely. These words are called stop words . The general STOP WORDS\\nstrategy for determining a stop list is to sort the terms by collection frequency COLLECTION\\nFREQUENCY (the total number of times each term appears in the document c ollection),\\nand then to take the most frequent terms, often hand-ﬁltered for their se-\\nmantic content relative to the domain of the documents being indexed, as\\nastop list , the members of which are then discarded during indexing. An STOP LIST\\nexample of a stop list is shown in Figure 2.5. Using a stop list signiﬁcantly\\nreduces the number of postings that a system has to store; we w ill present\\nsome statistics on this in Chapter 5(see Table 5.1, page 87). And a lot of\\nthe time not indexing stop words does little harm: keyword se arches with\\nterms like theandbydon’t seem very useful. However, this is not true for\\nphrase searches. The phrase query “President of the United States” , which con-\\ntains two stop words, is more precise than President AND “United States” . The\\nmeaning of ﬂightstoLondon is likely to be lost if the word tois stopped out. A\\nsearch for Vannevar Bush’s article As we may think will be difﬁcult if the ﬁrst\\nthree words are stopped out, and the system searches simply f or documents\\ncontaining the word think . Some special query types are disproportionately\\naffected. Some song titles and well known pieces of verse con sist entirely of\\nwords that are commonly on stop lists ( To be or not to be ,Let It Be ,I don’t want\\nto be, . . . ).\\nThe general trend in IR systems over time has been from standa rd use of\\nquite large stop lists (200–300 terms) to very small stop lis ts (7–12 terms)\\nto no stop list whatsoever. Web search engines generally do n ot use stop\\nlists. Some of the design of modern IR systems has focused pre cisely on\\nhow we can exploit the statistics of language so as to be able t o cope with\\ncommon words in better ways. We will show in Section 5.3(page 95) how\\ngood compression techniques greatly reduce the cost of stor ing the postings\\nfor common words. Section 6.2.1 (page 117) then discusses how standard\\nterm weighting leads to very common words having little impa ct on doc-\\nument rankings. Finally, Section 7.1.5 (page 140) shows how an IR system\\nwith impact-sorted indexes can terminate scanning a postin gs list early when\\nweights get small, and hence common words do not cause a large additional\\nprocessing cost for the average query, even though postings lists for stop\\nwords are very long. So for most modern IR systems, the additi onal cost of\\nincluding stop words is not that big – neither in terms of inde x size nor in\\nterms of query processing time.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 63}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP28 2 The term vocabulary and postings lists\\nQuery term Terms in documents that should be matched\\nWindows Windows\\nwindows Windows, windows, window\\nwindow window, windows\\n◮Figure 2.6 An example of how asymmetric expansion of query terms can use fully\\nmodel users’ expectations.\\n2.2.3 Normalization (equivalence classing of terms)\\nHaving broken up our documents (and also our query) into toke ns, the easy\\ncase is if tokens in the query just match tokens in the token li st of the doc-\\nument. However, there are many cases when two character sequ ences are\\nnot quite the same but you would like a match to occur. For inst ance, if you\\nsearch for USA , you might hope to also match documents containing U.S.A .\\nToken normalization is the process of canonicalizing tokens so that matches TOKEN\\nNORMALIZATION occur despite superﬁcial differences in the character sequ ences of the to-\\nkens.4The most standard way to normalize is to implicitly create equivalence EQUIVALENCE CLASSES\\nclasses , which are normally named after one member of the set. For ins tance,\\nif the tokens anti-discriminatory and antidiscriminatory are both mapped onto\\nthe term antidiscriminatory , in both the document text and queries, then searches\\nfor one term will retrieve documents that contain either.\\nThe advantage of just using mapping rules that remove charac ters like hy-\\nphens is that the equivalence classing to be done is implicit , rather than being\\nfully calculated in advance: the terms that happen to become identical as the\\nresult of these rules are the equivalence classes. It is only easy to write rules\\nof this sort that remove characters. Since the equivalence c lasses are implicit,\\nit is not obvious when you might want to add characters. For in stance, it\\nwould be hard to know to turn antidiscriminatory into anti-discriminatory .\\nAn alternative to creating equivalence classes is to mainta in relations be-\\ntween unnormalized tokens. This method can be extended to ha nd-constructed\\nlists of synonyms such as carand automobile , a topic we discuss further in\\nChapter 9. These term relationships can be achieved in two ways. The us ual\\nway is to index unnormalized tokens and to maintain a query ex pansion list\\nof multiple vocabulary entries to consider for a certain que ry term. A query\\nterm is then effectively a disjunction of several postings l ists. The alterna-\\ntive is to perform the expansion during index construction. When the doc-\\nument contains automobile , we index it under caras well (and, usually, also\\nvice-versa). Use of either of these methods is considerably less efﬁcient than\\nequivalence classing, as there are more postings to store an d merge. The ﬁrst\\n4. It is also often referred to as term normalization , but we prefer to reserve the name term for the\\noutput of the normalization process.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 64}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP2.2 Determining the vocabulary of terms 29\\nmethod adds a query expansion dictionary and requires more p rocessing at\\nquery time, while the second method requires more space for s toring post-\\nings. Traditionally, expanding the space required for the p ostings lists was\\nseen as more disadvantageous, but with modern storage costs , the increased\\nﬂexibility that comes from distinct postings lists is appea ling.\\nThese approaches are more ﬂexible than equivalence classes because the\\nexpansion lists can overlap while not being identical. This means there can\\nbe an asymmetry in expansion. An example of how such an asymme try can\\nbe exploited is shown in Figure 2.6: if the user enters windows , we wish to\\nallow matches with the capitalized Windows operating system, but this is not\\nplausible if the user enters window , even though it is plausible for this query\\nto also match lowercase windows .\\nThe best amount of equivalence classing or query expansion t o do is a\\nfairly open question. Doing some deﬁnitely seems a good idea . But doing a\\nlot can easily have unexpected consequences of broadening q ueries in unin-\\ntended ways. For instance, equivalence-classing U.S.A. and USA to the latter\\nby deleting periods from tokens might at ﬁrst seem very reaso nable, given\\nthe prevalent pattern of optional use of periods in acronyms . However, if I\\nput in as my query term C.A.T. , I might be rather upset if it matches every\\nappearance of the word catin documents.5\\nBelow we present some of the forms of normalization that are c ommonly\\nemployed and how they are implemented. In many cases they see m helpful,\\nbut they can also do harm. In fact, you can worry about many det ails of\\nequivalence classing, but it often turns out that providing processing is done\\nconsistently to the query and to documents, the ﬁne details m ay not have\\nmuch aggregate effect on performance.\\nAccents and diacritics. Diacritics on characters in English have a fairly\\nmarginal status, and we might well want cliché and cliche to match, or naive\\nand naïve . This can be done by normalizing tokens to remove diacritics . In\\nmany other languages, diacritics are a regular part of the wr iting system and\\ndistinguish different sounds. Occasionally words are dist inguished only by\\ntheir accents. For instance, in Spanish, peña is ‘a cliff’, while pena is ‘sorrow’.\\nNevertheless, the important question is usually not prescr iptive or linguistic\\nbut is a question of how users are likely to write queries for t hese words. In\\nmany cases, users will enter queries for words without diacr itics, whether\\nfor reasons of speed, laziness, limited software, or habits born of the days\\nwhen it was hard to use non-ASCII text on many computer system s. In these\\ncases, it might be best to equate all words to a form without di acritics.\\n5. At the time we wrote this chapter (Aug. 2005), this was actu ally the case on Google: the top\\nresult for the query C.A.T. was a site about cats, the Cat Fanciers Web Site http://www.fanciers.com/ .', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 65}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP30 2 The term vocabulary and postings lists\\nCapitalization/case-folding. A common strategy is to do case-folding by re- CASE -FOLDING\\nducing all letters to lower case. Often this is a good idea: it will allow in-\\nstances of Automobile at the beginning of a sentence to match with a query of\\nautomobile . It will also help on a web search engine when most of your user s\\ntype in ferrari when they are interested in a Ferrari car. On the other hand,\\nsuch case folding can equate words that might better be kept a part. Many\\nproper nouns are derived from common nouns and so are disting uished only\\nby case, including companies ( General Motors ,The Associated Press ), govern-\\nment organizations ( the Fed vs.fed) and person names ( Bush ,Black ). We al-\\nready mentioned an example of unintended query expansion wi th acronyms,\\nwhich involved not only acronym normalization ( C.A.T.→CAT ) but also\\ncase-folding ( CAT→cat).\\nFor English, an alternative to making every token lowercase is to just make\\nsome tokens lowercase. The simplest heuristic is to convert to lowercase\\nwords at the beginning of a sentence and all words occurring i n a title that is\\nall uppercase or in which most or all words are capitalized. T hese words are\\nusually ordinary words that have been capitalized. Mid-sen tence capitalized\\nwords are left as capitalized (which is usually correct). Th is will mostly avoid\\ncase-folding in cases where distinctions should be kept apa rt. The same task\\ncan be done more accurately by a machine learning sequence mo del which\\nuses more features to make the decision of when to case-fold. This is known\\nastruecasing . However, trying to get capitalization right in this way pro bably TRUECASING\\ndoesn’t help if your users usually use lowercase regardless of the correct case\\nof words. Thus, lowercasing everything often remains the mo st practical\\nsolution.\\nOther issues in English. Other possible normalizations are quite idiosyn-\\ncratic and particular to English. For instance, you might wi sh to equate\\nne’er and never or the British spelling colour and the American spelling color .\\nDates, times and similar items come in multiple formats, pre senting addi-\\ntional challenges. You might wish to collapse together 3/12/91 and Mar. 12,\\n1991 . However, correct processing here is complicated by the fac t that in the\\nU.S., 3/12/91 isMar. 12, 1991 , whereas in Europe it is 3 Dec 1991 .\\nOther languages. English has maintained a dominant position on the WWW;\\napproximately 60% of web pages are in English ( Gerrand 2007 ). But that still\\nleaves 40% of the web, and the non-English portion might be ex pected to\\ngrow over time, since less than one third of Internet users an d less than 10%\\nof the world’s population primarily speak English. And ther e are signs of\\nchange: Sifry (2007 ) reports that only about one third of blog posts are in\\nEnglish.\\nOther languages again present distinctive issues in equiva lence classing.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 66}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP2.2 Determining the vocabulary of terms 31\\x00 \\x01 \\x02 \\x03 \\x04 \\x05 \\x06 \\x07 \\x08 \\x06 \\t \\n \\x0b \\x0c \\r \\x0e \\x0f \\x10 \\x01 \\x11 \\x12 \\x13 \\x14 \\x15 \\x16 \\x17 \\x18 \\x19 \\x07 \\x1a \\x1b\\x1c \\x1d \\x1e \\x1f \\x1f  ! \"  ! # $\\x0c % \\x01 \\x0c & \\' ( ) \\t * + , - . / 0 ) \\x10 \\r1\\x0c 2 3 4 5 6 7 & + 8 9 \\n : ; : < \\x07 = > \\t ? @ A B C \\x15 - D E6 8 9 \\n : ; : < ) F G * H I \\t * :\\n\\x1c J) K + L M N ? O P\\n\\x1c Q RS\\x01 T \\x07 U V V W X Y & Z [ N ? ) \\x1b + \\\\ ] ; ^ _ + \\x12 ` 4 a + b; c \\x07 d e * f V g h V - ? i N j k l m n \\x13 : A o \\x06 \\x08 \\x06 p N 5 +q\\nV r s t u & v w x )\\nQ y z {h | & } \\x06 \\x15 ~ \\x7f M ? @ A\\n◮Figure 2.7 Japanese makes use of multiple intermingled writing system s and,\\nlike Chinese, does not segment words. The text is mainly Chin ese characters with\\nthe hiragana syllabary for inﬂectional endings and functio n words. The part in latin\\nletters is actually a Japanese expression, but has been take n up as the name of an\\nenvironmental campaign by 2004 Nobel Peace Prize winner Wan gari Maathai. His\\nname is written using the katakana syllabary in the middle of the ﬁrst line. The ﬁrst\\nfour characters of the ﬁnal line express a monetary amount th at we would want to\\nmatch with ¥500,000 (500,000 Japanese yen).\\nThe French word for thehas distinctive forms based not only on the gender\\n(masculine or feminine) and number of the following noun, bu t also depend-\\ning on whether the following word begins with a vowel: le,la,l’,les. We may\\nwell wish to equivalence class these various forms of the. German has a con-\\nvention whereby vowels with an umlaut can be rendered instea d as a two\\nvowel digraph. We would want to treat Schütze and Schuetze as equivalent.\\nJapanese is a well-known difﬁcult writing system, as illust rated in Fig-\\nure2.7. Modern Japanese is standardly an intermingling of multipl e alpha-\\nbets, principally Chinese characters, two syllabaries (hi ragana and katakana)\\nand western characters (Latin letters, Arabic numerals, an d various sym-\\nbols). While there are strong conventions and standardizat ion through the\\neducation system over the choice of writing system, in many c ases the same\\nword can be written with multiple writing systems. For examp le, a word\\nmay be written in katakana for emphasis (somewhat like itali cs). Or a word\\nmay sometimes be written in hiragana and sometimes in Chines e charac-\\nters. Successful retrieval thus requires complex equivale nce classing across\\nthe writing systems. In particular, an end user might common ly present a\\nquery entirely in hiragana, because it is easier to type, jus t as Western end\\nusers commonly use all lowercase.\\nDocument collections being indexed can include documents f rom many\\ndifferent languages. Or a single document can easily contai n text from mul-\\ntiple languages. For instance, a French email might quote cl auses from a\\ncontract document written in English. Most commonly, the la nguage is de-\\ntected and language-particular tokenization and normaliz ation rules are ap-\\nplied at a predetermined granularity, such as whole documen ts or individual\\nparagraphs, but this still will not correctly deal with case s where language\\nchanges occur for brief quotations. When document collecti ons contain mul-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 67}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP32 2 The term vocabulary and postings lists\\ntiple languages, a single index may have to contain terms of s everal lan-\\nguages. One option is to run a language identiﬁcation classi ﬁer on docu-\\nments and then to tag terms in the vocabulary for their langua ge. Or this\\ntagging can simply be omitted, since it is relatively rare fo r the exact same\\ncharacter sequence to be a word in different languages.\\nWhen dealing with foreign or complex words, particularly fo reign names,\\nthe spelling may be unclear or there may be variant translite ration standards\\ngiving different spellings (for example, Chebyshev and Tchebycheff orBeijing\\nand Peking ). One way of dealing with this is to use heuristics to equiva-\\nlence class or expand terms with phonetic equivalents. The t raditional and\\nbest known such algorithm is the Soundex algorithm, which we cover in\\nSection 3.4(page 63).\\n2.2.4 Stemming and lemmatization\\nFor grammatical reasons, documents are going to use differe nt forms of a\\nword, such as organize ,organizes , and organizing . Additionally, there are fami-\\nlies of derivationally related words with similar meanings , such as democracy ,\\ndemocratic , and democratization . In many situations, it seems as if it would be\\nuseful for a search for one of these words to return documents that contain\\nanother word in the set.\\nThe goal of both stemming and lemmatization is to reduce inﬂe ctional\\nforms and sometimes derivationally related forms of a word t o a common\\nbase form. For instance:\\nam, are, is⇒be\\ncar, cars, car’s, cars’ ⇒car\\nThe result of this mapping of text will be something like:\\nthe boy’s cars are different colors ⇒\\nthe boy car be differ color\\nHowever, the two words differ in their ﬂavor. Stemming usually refers to STEMMING\\na crude heuristic process that chops off the ends of words in t he hope of\\nachieving this goal correctly most of the time, and often inc ludes the re-\\nmoval of derivational afﬁxes. Lemmatization usually refers to doing things LEMMATIZATION\\nproperly with the use of a vocabulary and morphological anal ysis of words,\\nnormally aiming to remove inﬂectional endings only and to re turn the base\\nor dictionary form of a word, which is known as the lemma . If confronted LEMMA\\nwith the token saw, stemming might return just s, whereas lemmatization\\nwould attempt to return either seeorsawdepending on whether the use of\\nthe token was as a verb or a noun. The two may also differ in that stemming\\nmost commonly collapses derivationally related words, whe reas lemmatiza-\\ntion commonly only collapses the different inﬂectional for ms of a lemma.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 68}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP2.2 Determining the vocabulary of terms 33\\nLinguistic processing for stemming or lemmatization is oft en done by an ad-\\nditional plug-in component to the indexing process, and a nu mber of such\\ncomponents exist, both commercial and open-source.\\nThe most common algorithm for stemming English, and one that has re-\\npeatedly been shown to be empirically very effective, is Porter’s algorithm PORTER STEMMER\\n(Porter 1980 ). The entire algorithm is too long and intricate to present h ere,\\nbut we will indicate its general nature. Porter’s algorithm consists of 5 phases\\nof word reductions, applied sequentially. Within each phas e there are var-\\nious conventions to select rules, such as selecting the rule from each rule\\ngroup that applies to the longest sufﬁx. In the ﬁrst phase, th is convention is\\nused with the following rule group:\\n(2.1) Rule Example\\nSSES→ SS caresses → caress\\nIES→ I ponies → poni\\nSS→ SS caress → caress\\nS→ cats→ cat\\nMany of the later rules use a concept of the measure of a word, which loosely\\nchecks the number of syllables to see whether a word is long en ough that it\\nis reasonable to regard the matching portion of a rule as a suf ﬁx rather than\\nas part of the stem of a word. For example, the rule:\\n(m>1) EMENT→\\nwould map replacement toreplac , but not cement toc. The ofﬁcial site for the\\nPorter Stemmer is:\\nhttp://www.tartarus.org/ ˜martin/PorterStemmer/\\nOther stemmers exist, including the older, one-pass Lovins stemmer ( Lovins\\n1968 ), and newer entrants like the Paice/Husk stemmer ( Paice 1990 ); see:\\nhttp://www.cs.waikato.ac.nz/ ˜eibe/stemmers/\\nhttp://www.comp.lancs.ac.uk/computing/research/stem ming/\\nFigure 2.8presents an informal comparison of the different behaviors of these\\nstemmers. Stemmers use language-speciﬁc rules, but they re quire less know-\\nledge than a lemmatizer, which needs a complete vocabulary a nd morpho-\\nlogical analysis to correctly lemmatize words. Particular domains may also\\nrequire special stemming rules. However, the exact stemmed form does not\\nmatter, only the equivalence classes it forms.\\nRather than using a stemmer, you can use a lemmatizer , a tool from Nat- LEMMATIZER\\nural Language Processing which does full morphological ana lysis to accu-\\nrately identify the lemma for each word. Doing full morpholo gical analysis\\nproduces at most very modest beneﬁts for retrieval. It is har d to say more,', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 69}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP34 2 The term vocabulary and postings lists\\nSample text: Such an analysis can reveal features that are not easily visi ble\\nfrom the variations in the individual genes and can lead to a p icture of\\nexpression that is more biologically transparent and acces sible to\\ninterpretation\\nLovins stemmer: such an analys can reve featur that ar not eas vis from th\\nvari in th individu gen and can lead to a pictur of expres that i s mor\\nbiolog transpar and acces to interpres\\nPorter stemmer: such an analysi can reveal featur that ar not easili visibl\\nfrom the variat in the individu gene and can lead to a pictur of express\\nthat is more biolog transpar and access to interpret\\nPaice stemmer: such an analys can rev feat that are not easy vis from the\\nvary in the individ gen and can lead to a pict of express that is mor\\nbiolog transp and access to interpret\\n◮Figure 2.8 A comparison of three stemming algorithms on a sample text.\\nbecause either form of normalization tends not to improve En glish informa-\\ntion retrieval performance in aggregate – at least not by ver y much. While\\nit helps a lot for some queries, it equally hurts performance a lot for others.\\nStemming increases recall while harming precision. As an ex ample of what\\ncan go wrong, note that the Porter stemmer stems all of the fol lowing words:\\noperate operating operates operation operative operative s operational\\ntooper. However, since operate in its various forms is a common verb, we\\nwould expect to lose considerable precision on queries such as the following\\nwith Porter stemming:\\noperational ANDresearch\\noperating ANDsystem\\noperative ANDdentistry\\nFor a case like this, moving to using a lemmatizer would not co mpletely ﬁx\\nthe problem because particular inﬂectional forms are used i n particular col-\\nlocations: a sentence with the words operate and system is not a good match\\nfor the query operating ANDsystem . Getting better value from term normaliza-\\ntion depends more on pragmatic issues of word use than on form al issues of\\nlinguistic morphology.\\nThe situation is different for languages with much more morp hology (such\\nas Spanish, German, and Finnish). Results in the European CL EF evaluations\\nhave repeatedly shown quite large gains from the use of stemm ers (and com-\\npound splitting for languages like German); see the referen ces in Section 2.5.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 70}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP2.2 Determining the vocabulary of terms 35\\n?Exercise 2.1 [⋆]\\nAre the following statements true or false?\\na.In a Boolean retrieval system, stemming never lowers precis ion.\\nb.In a Boolean retrieval system, stemming never lowers recall .\\nc.Stemming increases the size of the vocabulary.\\nd.Stemming should be invoked at indexing time but not while pro cessing a query.\\nExercise 2.2 [⋆]\\nSuggest what normalized form should be used for these words ( including the word\\nitself as a possibility):\\na.’Cos\\nb.Shi’ite\\nc.cont’d\\nd.Hawai’i\\ne.O’Rourke\\nExercise 2.3 [⋆]\\nThe following pairs of words are stemmed to the same form by th e Porter stemmer.\\nWhich pairs would you argue shouldn’t be conﬂated. Give your reasoning.\\na.abandon/abandonment\\nb.absorbency/absorbent\\nc.marketing/markets\\nd.university/universe\\ne.volume/volumes\\nExercise 2.4 [⋆]\\nFor the Porter stemmer rule group shown in ( 2.1):\\na.What is the purpose of including an identity rule such as SS →SS?\\nb.Applying just this rule group, what will the following words be stemmed to?\\ncircus canaries boss\\nc.What rule should be added to correctly stem pony ?\\nd.The stemming for ponies and pony might seem strange. Does it have a deleterious\\neffect on retrieval? Why or why not?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 71}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP36 2 The term vocabulary and postings lists\\n◮Figure 2.9 Postings lists with skip pointers. The postings intersecti on can use a\\nskip pointer when the end point is still less than the item on t he other list.\\n2.3 Faster postings list intersection via skip pointers\\nIn the remainder of this chapter, we will discuss extensions to postings list\\ndata structures and ways to increase the efﬁciency of using p ostings lists. Re-\\ncall the basic postings list intersection operation from Se ction 1.3(page 10):\\nwe walk through the two postings lists simultaneously, in ti me linear in the\\ntotal number of postings entries. If the list lengths are mand n, the intersec-\\ntion takes O(m+n)operations. Can we do better than this? That is, empiri-\\ncally, can we usually process postings list intersection in sublinear time? We\\ncan, if the index isn’t changing too fast.\\nOne way to do this is to use a skip list by augmenting postings lists with SKIP LIST\\nskip pointers (at indexing time), as shown in Figure 2.9. Skip pointers are\\neffectively shortcuts that allow us to avoid processing par ts of the postings\\nlist that will not ﬁgure in the search results. The two questi ons are then where\\nto place skip pointers and how to do efﬁcient merging using sk ip pointers.\\nConsider ﬁrst efﬁcient merging, with Figure 2.9as an example. Suppose\\nwe’ve stepped through the lists in the ﬁgure until we have mat ched 8on\\neach list and moved it to the results list. We advance both poi nters, giving us\\n16on the upper list and 41on the lower list. The smallest item is then the\\nelement 16on the top list. Rather than simply advancing the upper point er,\\nwe ﬁrst check the skip list pointer and note that 28 is also les s than 41. Hence\\nwe can follow the skip list pointer, and then we advance the up per pointer\\nto28. We thus avoid stepping to 19and 23on the upper list. A number\\nof variant versions of postings list intersection with skip pointers is possible\\ndepending on when exactly you check the skip pointer. One ver sion is shown', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 72}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP2.3 Faster postings list intersection via skip pointers 37\\nINTERSECT WITHSKIPS(p1,p2)\\n1answer←⟨⟩\\n2while p1̸=NILand p2̸=NIL\\n3do if docID (p1) =docID (p2)\\n4 then ADD(answer ,docID (p1))\\n5 p1←next(p1)\\n6 p2←next(p2)\\n7 else if docID (p1)<docID (p2)\\n8 then if hasSkip (p1)and(docID (skip(p1))≤docID (p2))\\n9 then while hasSkip (p1)and(docID (skip(p1))≤docID (p2))\\n10 dop1←skip(p1)\\n11 else p1←next(p1)\\n12 else if hasSkip (p2)and(docID (skip(p2))≤docID (p1))\\n13 then while hasSkip (p2)and(docID (skip(p2))≤docID (p1))\\n14 dop2←skip(p2)\\n15 else p2←next(p2)\\n16 return answer\\n◮Figure 2.10 Postings lists intersection with skip pointers.\\nin Figure 2.10. Skip pointers will only be available for the original posti ngs\\nlists. For an intermediate result in a complex query, the cal lhasSkip (p)will\\nalways return false. Finally, note that the presence of skip pointers only helps\\nfor AND queries, not for ORqueries.\\nWhere do we place skips? There is a tradeoff. More skips means shorter\\nskip spans, and that we are more likely to skip. But it also mea ns lots of\\ncomparisons to skip pointers, and lots of space storing skip pointers. Fewer\\nskips means few pointer comparisons, but then long skip span s which means\\nthat there will be fewer opportunities to skip. A simple heur istic for placing\\nskips, which has been found to work well in practice, is that f or a postings\\nlist of length P, use√\\nPevenly-spaced skip pointers. This heuristic can be\\nimproved upon; it ignores any details of the distribution of query terms.\\nBuilding effective skip pointers is easy if an index is relat ively static; it\\nis harder if a postings list keeps changing because of update s. A malicious\\ndeletion strategy can render skip lists ineffective.\\nChoosing the optimal encoding for an inverted index is an eve r-changing\\ngame for the system builder, because it is strongly dependen t on underly-\\ning computer technologies and their relative speeds and siz es. Traditionally,\\nCPUs were slow, and so highly compressed techniques were not optimal.\\nNow CPUs are fast and disk is slow, so reducing disk postings l ist size dom-\\ninates. However, if you’re running a search engine with ever ything in mem-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 73}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP38 2 The term vocabulary and postings lists\\nory then the equation changes again. We discuss the impact of hardware\\nparameters on index construction time in Section 4.1(page 68) and the im-\\npact of index size on system speed in Chapter 5.\\n?Exercise 2.5 [⋆]\\nWhy are skip pointers not useful for queries of the form xORy?\\nExercise 2.6 [⋆]\\nWe have a two-word query. For one term the postings list consi sts of the following 16\\nentries:\\n[4,6,10,12,14,16,18,20,22,32,47,81,120,122,157,180]\\nand for the other it is the one entry postings list:\\n[47].\\nWork out how many comparisons would be done to intersect the t wo postings lists\\nwith the following two strategies. Brieﬂy justify your answ ers:\\na.Using standard postings lists\\nb.Using postings lists stored with skip pointers, with a skip l ength of√\\nP, as sug-\\ngested in Section 2.3.\\nExercise 2.7 [⋆]\\nConsider a postings intersection between this postings lis t, with skip pointers:\\n3 5 9 15 24 39 60 68 75 81 84 89 92 96 97 100 115\\nand the following intermediate result postings list (which hence has no skip pointers):\\n3 5 89 95 97 99 100 101\\nTrace through the postings intersection algorithm in Figur e2.10 (page 37).\\na.How often is a skip pointer followed (i.e., p1is advanced to skip(p1))?\\nb.How many postings comparisons will be made by this algorithm while intersect-\\ning the two lists?\\nc.How many postings comparisons would be made if the postings l ists are inter-\\nsected without the use of skip pointers?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 74}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP2.4 Positional postings and phrase queries 39\\n2.4 Positional postings and phrase queries\\nMany complex or technical concepts and many organization an d product\\nnames are multiword compounds or phrases. We would like to be able to\\npose a query such as Stanford University by treating it as a phrase so that a\\nsentence in a document like The inventor Stanford Ovshinsky never went to uni-\\nversity. is not a match. Most recent search engines support a double qu otes\\nsyntax (“stanford university” ) for phrase queries , which has proven to be very PHRASE QUERIES\\neasily understood and successfully used by users. As many as 10% of web\\nqueries are phrase queries, and many more are implicit phras e queries (such\\nas person names), entered without use of double quotes. To be able to sup-\\nport such queries, it is no longer sufﬁcient for postings lis ts to be simply lists\\nof documents that contain individual terms. In this section we consider two\\napproaches to supporting phrase queries and their combinat ion. A search\\nengine should not only support phrase queries, but implemen t them efﬁ-\\nciently. A related but distinct concept is term proximity we ighting, where a\\ndocument is preferred to the extent that the query terms appe ar close to each\\nother in the text. This technique is covered in Section 7.2.2 (page 144) in the\\ncontext of ranked retrieval.\\n2.4.1 Biword indexes\\nOne approach to handling phrases is to consider every pair of consecutive\\nterms in a document as a phrase. For example, the text Friends, Romans,\\nCountrymen would generate the biwords : BIWORD INDEX\\nfriendsromans\\nromanscountrymen\\nIn this model, we treat each of these biwords as a vocabulary t erm. Being\\nable to process two-word phrase queries is immediate. Longe r phrases can\\nbe processed by breaking them down. The query stanford university palo alto\\ncan be broken into the Boolean query on biwords:\\n“stanforduniversity” AND“universitypalo” AND“paloalto”\\nThis query could be expected to work fairly well in practice, but there can\\nand will be occasional false positives. Without examining t he documents,\\nwe cannot verify that the documents matching the above Boole an query do\\nactually contain the original 4 word phrase.\\nAmong possible queries, nouns and noun phrases have a specia l status in\\ndescribing the concepts people are interested in searching for. But related\\nnouns can often be divided from each other by various functio n words, in\\nphrases such as the abolition of slavery orrenegotiation of the constitution . These\\nneeds can be incorporated into the biword indexing model in t he following', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 75}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP40 2 The term vocabulary and postings lists\\nway. First, we tokenize the text and perform part-of-speech -tagging.6We\\ncan then group terms into nouns, including proper nouns, (N) and function\\nwords, including articles and prepositions, (X), among oth er classes. Now\\ndeem any string of terms of the form NX*N to be an extended biwo rd. Each\\nsuch extended biword is made a term in the vocabulary. For exa mple:\\nrenegotiation of the constitution\\nN X X N\\nTo process a query using such an extended biword index, we nee d to also\\nparse it into N’s and X’s, and then segment the query into exte nded biwords,\\nwhich can be looked up in the index.\\nThis algorithm does not always work in an intuitively optima l manner\\nwhen parsing longer queries into Boolean queries. Using the above algo-\\nrithm, the query\\ncostoverruns ona power plant\\nis parsed into\\n“costoverruns” AND“overrunspower” AND“powerplant”\\nwhereas it might seem a better query to omit the middle biword . Better\\nresults can be obtained by using more precise part-of-speec h patterns that\\ndeﬁne which extended biwords should be indexed.\\nThe concept of a biword index can be extended to longer sequen ces of\\nwords, and if the index includes variable length word sequen ces, it is gen-\\nerally referred to as a phrase index . Indeed, searches for a single term are PHRASE INDEX\\nnot naturally handled in a biword index (you would need to sca n the dic-\\ntionary for all biwords containing the term), and so we also n eed to have an\\nindex of single-word terms. While there is always a chance of false positive\\nmatches, the chance of a false positive match on indexed phra ses of length 3\\nor more becomes very small indeed. But on the other hand, stor ing longer\\nphrases has the potential to greatly expand the vocabulary s ize. Maintain-\\ning exhaustive phrase indexes for phrases of length greater than two is a\\ndaunting prospect, and even use of an exhaustive biword dict ionary greatly\\nexpands the size of the vocabulary. However, towards the end of this sec-\\ntion we discuss the utility of the strategy of using a partial phrase index in a\\ncompound indexing scheme.\\n6. Part of speech taggers classify words as nouns, verbs, etc . – or, in practice, often as ﬁner-\\ngrained classes like “plural proper noun”. Many fairly accu rate (c. 96% per-tag accuracy) part-\\nof-speech taggers now exist, usually trained by machine lea rning methods on hand-tagged text.\\nSee, for instance, Manning and Schütze (1999 , ch. 10).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 76}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP2.4 Positional postings and phrase queries 41\\nto, 993427:\\n⟨1, 6:⟨7, 18, 33, 72, 86, 231 ⟩;\\n2, 5:⟨1, 17, 74, 222, 255⟩;\\n4, 5:⟨8, 16, 190, 429, 433 ⟩;\\n5, 2:⟨363, 367⟩;\\n7, 3:⟨13, 23, 191⟩; . . .⟩\\nbe, 178239:\\n⟨1, 2:⟨17, 25⟩;\\n4, 5:⟨17, 191, 291, 430, 434 ⟩;\\n5, 3:⟨14, 19, 101⟩; . . .⟩\\n◮Figure 2.11 Positional index example. The word tohas a document frequency\\n993,477, and occurs 6 times in document 1 at positions 7, 18, 3 3, etc.\\n2.4.2 Positional indexes\\nFor the reasons given, a biword index is not the standard solu tion. Rather,\\napositional index is most commonly employed. Here, for each term in the POSITIONAL INDEX\\nvocabulary, we store postings of the form docID: ⟨position1, position2, . . . ⟩,\\nas shown in Figure 2.11, where each position is a token index in the docu-\\nment. Each posting will also usually record the term frequen cy, for reasons\\ndiscussed in Chapter 6.\\nTo process a phrase query, you still need to access the invert ed index en-\\ntries for each distinct term. As before, you would start with the least frequent\\nterm and then work to further restrict the list of possible ca ndidates. In the\\nmerge operation, the same general technique is used as befor e, but rather\\nthan simply checking that both terms are in a document, you al so need to\\ncheck that their positions of appearance in the document are compatible with\\nthe phrase query being evaluated. This requires working out offsets between\\nthe words.\\n✎Example 2.1: Satisfying phrase queries. Suppose the postings lists for toand\\nbeare as in Figure 2.11, and the query is “tobeornotto be” . The postings lists to access\\nare:to,be,or,not. We will examine intersecting the postings lists for toandbe. We\\nﬁrst look for documents that contain both terms. Then, we loo k for places in the lists\\nwhere there is an occurrence of bewith a token index one higher than a position of to,\\nand then we look for another occurrence of each word with toke n index 4 higher than\\nthe ﬁrst occurrence. In the above lists, the pattern of occur rences that is a possible\\nmatch is:\\nto:⟨. . . ; 4:⟨. . . ,429,433⟩; . . .⟩\\nbe:⟨. . . ; 4:⟨. . . ,430,434⟩; . . .⟩', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 77}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP42 2 The term vocabulary and postings lists\\nPOSITIONAL INTERSECT (p1,p2,k)\\n1answer←⟨⟩\\n2while p1̸=NILand p2̸=NIL\\n3do if docID (p1) =docID (p2)\\n4 then l←⟨⟩\\n5 pp1←positions (p1)\\n6 pp2←positions (p2)\\n7 while pp1̸=NIL\\n8 do while pp2̸=NIL\\n9 do if|pos(pp1)−pos(pp2)|≤k\\n10 then ADD(l,pos(pp2))\\n11 else if pos(pp2)>pos(pp1)\\n12 then break\\n13 pp2←next(pp2)\\n14 while l̸=⟨⟩and|l[0]−pos(pp1)|>k\\n15 doDELETE (l[0])\\n16 for each ps∈l\\n17 doADD(answer ,⟨docID (p1),pos(pp1),ps⟩)\\n18 pp1←next(pp1)\\n19 p1←next(p1)\\n20 p2←next(p2)\\n21 else if docID (p1)<docID (p2)\\n22 then p1←next(p1)\\n23 else p2←next(p2)\\n24 return answer\\n◮Figure 2.12 An algorithm for proximity intersection of postings lists p1and p2.\\nThe algorithm ﬁnds places where the two terms appear within kwords of each other\\nand returns a list of triples giving docID and the term positi on in p1and p2.\\nThe same general method is applied for within kword proximity searches,\\nof the sort we saw in Example 1.1(page 15):\\nemployment/3place\\nHere, / kmeans “within kwords of (on either side)”. Clearly, positional in-\\ndexes can be used for such queries; biword indexes cannot. We show in\\nFigure 2.12 an algorithm for satisfying within kword proximity searches; it\\nis further discussed in Exercise 2.12.\\nPositional index size. Adopting a positional index expands required post-\\nings storage signiﬁcantly, even if we compress position val ues/offsets as we', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 78}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP2.4 Positional postings and phrase queries 43\\nwill discuss in Section 5.3(page 95). Indeed, moving to a positional index\\nalso changes the asymptotic complexity of a postings inters ection operation,\\nbecause the number of items to check is now bounded not by the n umber of\\ndocuments but by the total number of tokens in the document co llection T.\\nThat is, the complexity of a Boolean query is Θ(T)rather than Θ(N). How-\\never, most applications have little choice but to accept thi s, since most users\\nnow expect to have the functionality of phrase and proximity searches.\\nLet’s examine the space implications of having a positional index. A post-\\ning now needs an entry for each occurrence of a term. The index size thus\\ndepends on the average document size. The average web page ha s less than\\n1000 terms, but documents like SEC stock ﬁlings, books, and e ven some epic\\npoems easily reach 100,000 terms. Consider a term with frequ ency 1 in 1000\\nterms on average. The result is that large documents cause an increase of two\\norders of magnitude in the space required to store the postin gs list:\\nExpected Expected entries\\nDocument size postings in positional posting\\n1000 1 1\\n100,000 1 100\\nWhile the exact numbers depend on the type of documents and th e language\\nbeing indexed, some rough rules of thumb are to expect a posit ional index to\\nbe 2 to 4 times as large as a non-positional index, and to expec t a compressed\\npositional index to be about one third to one half the size of t he raw text\\n(after removal of markup, etc.) of the original uncompresse d documents.\\nSpeciﬁc numbers for an example collection are given in Table 5.1(page 87)\\nand Table 5.6(page 103).\\n2.4.3 Combination schemes\\nThe strategies of biword indexes and positional indexes can be fruitfully\\ncombined. If users commonly query on particular phrases, su ch asMichael\\nJackson , it is quite inefﬁcient to keep merging positional postings lists. A\\ncombination strategy uses a phrase index, or just a biword in dex, for certain\\nqueries and uses a positional index for other phrase queries . Good queries\\nto include in the phrase index are ones known to be common base d on re-\\ncent querying behavior. But this is not the only criterion: t he most expensive\\nphrase queries to evaluate are ones where the individual wor ds are com-\\nmon but the desired phrase is comparatively rare. Adding Britney Spears as\\na phrase index entry may only give a speedup factor to that que ry of about\\n3, since most documents that mention either word are valid re sults, whereas\\nadding The Who as a phrase index entry may speed up that query by a factor\\nof 1000. Hence, having the latter is more desirable, even if i t is a relatively\\nless common query.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 79}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP44 2 The term vocabulary and postings lists\\nWilliams et al. (2004 ) evaluate an even more sophisticated scheme which\\nemploys indexes of both these sorts and additionally a parti al next word\\nindex as a halfway house between the ﬁrst two strategies. For each term, a\\nnext word index records terms that follow it in a document. They conclude NEXT WORD INDEX\\nthat such a strategy allows a typical mixture of web phrase qu eries to be\\ncompleted in one quarter of the time taken by use of a position al index alone,\\nwhile taking up 26% more space than use of a positional index a lone.\\n?Exercise 2.8 [⋆]\\nAssume a biword index. Give an example of a document which wil l be returned\\nfor a query of New York University but is actually a false positive which should not be\\nreturned.\\nExercise 2.9 [⋆]\\nShown below is a portion of a positional index in the format: t erm: doc1:⟨position1,\\nposition2, . . .⟩; doc2:⟨position1, position2, . . . ⟩; etc.\\nangels : 2:⟨36,174,252,651⟩; 4:⟨12,22,102,432⟩; 7:⟨17⟩;\\nfools: 2:⟨1,17,74,222⟩; 4:⟨8,78,108,458⟩; 7:⟨3,13,23,193⟩;\\nfear: 2:⟨87,704,722,901⟩; 4:⟨13,43,113,433⟩; 7:⟨18,328,528⟩;\\nin: 2:⟨3,37,76,444,851⟩; 4:⟨10,20,110,470,500⟩; 7:⟨5,15,25,195⟩;\\nrush: 2:⟨2,66,194,321,702⟩; 4:⟨9,69,149,429,569⟩; 7:⟨4,14,404⟩;\\nto: 2:⟨47,86,234,999⟩; 4:⟨14,24,774,944⟩; 7:⟨199,319,599,709⟩;\\ntread : 2:⟨57,94,333⟩; 4:⟨15,35,155⟩; 7:⟨20,320⟩;\\nwhere : 2:⟨67,124,393,1001⟩; 4:⟨11,41,101,421,431⟩; 7:⟨16,36,736⟩;\\nWhich document(s) if any match each of the following queries , where each expression\\nwithin quotes is a phrase query?\\na.“fools rushin”\\nb.“fools rushin” AND“angels fear to tread”\\nExercise 2.10 [⋆]\\nConsider the following fragment of a positional index with t he format:\\nword: document: ⟨position, position, . . . ⟩; document:⟨position, . . .⟩\\n. . .\\nGates : 1:⟨3⟩; 2:⟨6⟩; 3:⟨2,17⟩; 4:⟨1⟩;\\nIBM: 4:⟨3⟩; 7:⟨14⟩;\\nMicrosoft : 1:⟨1⟩; 2:⟨1,21⟩; 3:⟨3⟩; 5:⟨16,22,51⟩;\\nThe/koperator, word1 / kword2 ﬁnds occurrences of word1 within kwords of word2 (on\\neither side), where kis a positive integer argument. Thus k=1 demands that word1\\nbe adjacent to word2 .\\na.Describe the set of documents that satisfy the query Gates /2 Microsoft .\\nb.Describe each set of values for kfor which the query Gates / kMicrosoft returns a\\ndifferent set of documents as the answer.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 80}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP2.5 References and further reading 45\\nExercise 2.11 [⋆⋆]\\nConsider the general procedure for merging two positional p ostings lists for a given\\ndocument, to determine the document positions where a docum ent satisﬁes a / k\\nclause (in general there can be multiple positions at which e ach term occurs in a sin-\\ngle document). We begin with a pointer to the position of occu rrence of each term\\nand move each pointer along the list of occurrences in the doc ument, checking as we\\ndo so whether we have a hit for / k. Each move of either pointer counts as a step. Let\\nLdenote the total number of occurrences of the two terms in the document. What is\\nthe big-O complexity of the merge procedure, if we wish to hav e postings including\\npositions in the result?\\nExercise 2.12 [⋆⋆]\\nConsider the adaptation of the basic algorithm for intersec tion of two postings lists\\n(Figure 1.6, page 11) to the one in Figure 2.12 (page 42), which handles proximity\\nqueries. A naive algorithm for this operation could be O(PLmax2), where Pis the\\nsum of the lengths of the postings lists (i.e., the sum of docu ment frequencies) and\\nLmaxis the maximum length of a document (in tokens).\\na.Go through this algorithm carefully and explain how it works .\\nb.What is the complexity of this algorithm? Justify your answe r carefully.\\nc.For certain queries and data distributions, would another a lgorithm be more efﬁ-\\ncient? What complexity does it have?\\nExercise 2.13 [⋆⋆]\\nSuppose we wish to use a postings intersection procedure to d etermine simply the\\nlist of documents that satisfy a / kclause, rather than returning the list of positions,\\nas in Figure 2.12 (page 42). For simplicity, assume k≥2. Let Ldenote the total\\nnumber of occurrences of the two terms in the document collec tion (i.e., the sum of\\ntheir collection frequencies). Which of the following is tr ue? Justify your answer.\\na.The merge can be accomplished in a number of steps linear in Land independent\\nofk, and we can ensure that each pointer moves only to the right.\\nb.The merge can be accomplished in a number of steps linear in Land independent\\nofk, but a pointer may be forced to move non-monotonically (i.e. , to sometimes\\nback up)\\nc.The merge can require kLsteps in some cases.\\nExercise 2.14 [⋆⋆]\\nHow could an IR system combine use of a positional index and us e of stop words?\\nWhat is the potential problem, and how could it be handled?\\n2.5 References and further reading\\nExhaustive discussion of the character-level processing o f East Asian lan- EAST ASIAN\\nLANGUAGES guages can be found in Lunde (1998 ). Character bigram indexes are perhaps\\nthe most standard approach to indexing Chinese, although so me systems use\\nword segmentation. Due to differences in the language and wr iting system,\\nword segmentation is most usual for Japanese ( Luk and Kwok 2002 ,Kishida', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 81}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP46 2 The term vocabulary and postings lists\\net al. 2005 ). The structure of a character k-gram index over unsegmented text\\ndiffers from that in Section 3.2.2 (page 54): there the k-gram dictionary points\\nto postings lists of entries in the regular dictionary, wher eas here it points\\ndirectly to document postings lists. For further discussio n of Chinese word\\nsegmentation, see Sproat et al. (1996 ),Sproat and Emerson (2003 ),Tseng et al.\\n(2005 ), and Gao et al. (2005 ).\\nLita et al. (2003 ) present a method for truecasing. Natural language pro-\\ncessing work on computational morphology is presented in ( Sproat 1992 ,\\nBeesley and Karttunen 2003 ).\\nLanguage identiﬁcation was perhaps ﬁrst explored in crypto graphy; for\\nexample, Konheim (1981 ) presents a character-level k-gram language identi-\\nﬁcation algorithm. While other methods such as looking for p articular dis-\\ntinctive function words and letter combinations have been u sed, with the\\nadvent of widespread digital text, many people have explore d the charac-\\ntern-gram technique, and found it to be highly successful ( Beesley 1998 ,\\nDunning 1994 ,Cavnar and Trenkle 1994 ). Written language identiﬁcation\\nis regarded as a fairly easy problem, while spoken language i dentiﬁcation\\nremains more difﬁcult; see Hughes et al. (2006 ) for a recent survey.\\nExperiments on and discussion of the positive and negative i mpact of\\nstemming in English can be found in the following works: Salton (1989 ),Har-\\nman (1991 ),Krovetz (1995 ),Hull (1996 ).Hollink et al. (2004 ) provide detailed\\nresults for the effectiveness of language-speciﬁc methods on 8 European lan-\\nguages. In terms of percent change in mean average precision (see page 159)\\nover a baseline system, diacritic removal gains up to 23% (be ing especially\\nhelpful for Finnish, French, and Swedish). Stemming helped markedly for\\nFinnish (30% improvement) and Spanish (10% improvement), b ut for most\\nlanguages, including English, the gain from stemming was in the range 0–\\n5%, and results from a lemmatizer were poorer still. Compoun d splitting\\ngained 25% for Swedish and 15% for German, but only 4% for Dutc h. Rather\\nthan language-particular methods, indexing character k-grams (as we sug-\\ngested for Chinese) could often give as good or better result s: using within-\\nword character 4-grams rather than words gave gains of 37% in Finnish, 27%\\nin Swedish, and 20% in German, while even being slightly posi tive for other\\nlanguages, such as Dutch, Spanish, and English. Tomlinson (2003 ) presents\\nbroadly similar results. Bar-Ilan and Gutman (2005 ) suggest that, at the\\ntime of their study (2003), the major commercial web search e ngines suffered\\nfrom lacking decent language-particular processing; for e xample, a query on\\nwww.google.fr forl’électricité did not separate off the article l’but only matched\\npages with precisely this string of article+noun.\\nThe classic presentation of skip pointers for IR can be found inMoffat and SKIP LIST\\nZobel (1996 ). Extended techniques are discussed in Boldi and Vigna (2005 ).\\nThe main paper in the algorithms literature is Pugh (1990 ), which uses mul-\\ntilevel skip pointers to give expected O(logP)list access (the same expected', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 82}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP2.5 References and further reading 47\\nefﬁciency as using a tree data structure) with less implemen tational complex-\\nity. In practice, the effectiveness of using skip pointers d epends on various\\nsystem parameters. Moffat and Zobel (1996 ) report conjunctive queries run-\\nning about ﬁve times faster with the use of skip pointers, but Bahle et al.\\n(2002 , p. 217) report that, with modern CPUs, using skip lists inst ead slows\\ndown search because it expands the size of the postings list ( i.e., disk I/O\\ndominates performance). In contrast, Strohman and Croft (2007 ) again show\\ngood performance gains from skipping, in a system architect ure designed to\\noptimize for the large memory spaces and multiple cores of re cent CPUs.\\nJohnson et al. (2006 ) report that 11.7% of all queries in two 2002 web query\\nlogs contained phrase queries, though Kammenhuber et al. (2006 ) report\\nonly 3% phrase queries for a different data set. Silverstein et al. (1999 ) note\\nthat many queries without explicit phrase operators are act ually implicit\\nphrase searches.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 83}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 84}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 49\\n3Dictionaries and tolerant\\nretrieval\\nIn Chapters 1and 2we developed the ideas underlying inverted indexes\\nfor handling Boolean and proximity queries. Here, we develo p techniques\\nthat are robust to typographical errors in the query, as well as alternative\\nspellings. In Section 3.1we develop data structures that help the search\\nfor terms in the vocabulary in an inverted index. In Section 3.2we study\\nthe idea of a wildcard query : a query such as *a*e*i*o*u* , which seeks doc- WILDCARD QUERY\\numents containing any term that includes all the ﬁve vowels i n sequence.\\nThe*symbol indicates any (possibly empty) string of characters . Users pose\\nsuch queries to a search engine when they are uncertain about how to spell\\na query term, or seek documents containing variants of a quer y term; for in-\\nstance, the query automat* would seek documents containing any of the terms\\nautomatic,automation andautomated .\\nWe then turn to other forms of imprecisely posed queries, foc using on\\nspelling errors in Section 3.3. Users make spelling errors either by accident,\\nor because the term they are searching for (e.g., Herman ) has no unambiguous\\nspelling in the collection. We detail a number of techniques for correcting\\nspelling errors in queries, one term at a time as well as for an entire string\\nof query terms. Finally, in Section 3.4we study a method for seeking vo-\\ncabulary terms that are phonetically close to the query term (s). This can be\\nespecially useful in cases like the Herman example, where the user may not\\nknow how a proper name is spelled in documents in the collecti on.\\nBecause we will develop many variants of inverted indexes in this chapter,\\nwe will use sometimes the phrase standard inverted index to mean the inverted\\nindex developed in Chapters 1and 2, in which each vocabulary term has a\\npostings list with the documents in the collection.\\n3.1 Search structures for dictionaries\\nGiven an inverted index and a query, our ﬁrst task is to determ ine whether\\neach query term exists in the vocabulary and if so, identify t he pointer to the', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 85}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP50 3 Dictionaries and tolerant retrieval\\ncorresponding postings. This vocabulary lookup operation uses a classical\\ndata structure called the dictionary and has two broad class es of solutions:\\nhashing, and search trees. In the literature of data structu res, the entries in\\nthe vocabulary (in our case, terms) are often referred to as keys. The choice\\nof solution (hashing, or search trees) is governed by a numbe r of questions:\\n(1) How many keys are we likely to have? (2) Is the number likel y to remain\\nstatic, or change a lot – and in the case of changes, are we like ly to only have\\nnew keys inserted, or to also have some keys in the dictionary be deleted? (3)\\nWhat are the relative frequencies with which various keys wi ll be accessed?\\nHashing has been used for dictionary lookup in some search en gines. Each\\nvocabulary term (key) is hashed into an integer over a large e nough space\\nthat hash collisions are unlikely; collisions if any are res olved by auxiliary\\nstructures that can demand care to maintain.1At query time, we hash each\\nquery term separately and following a pointer to the corresp onding post-\\nings, taking into account any logic for resolving hash colli sions. There is no\\neasy way to ﬁnd minor variants of a query term (such as the acce nted and\\nnon-accented versions of a word like resume ), since these could be hashed to\\nvery different integers. In particular, we cannot seek (for instance) all terms\\nbeginning with the preﬁx automat , an operation that we will require below\\nin Section 3.2. Finally, in a setting (such as the Web) where the size of the\\nvocabulary keeps growing, a hash function designed for curr ent needs may\\nnot sufﬁce in a few years’ time.\\nSearch trees overcome many of these issues – for instance, th ey permit us\\nto enumerate all vocabulary terms beginning with automat . The best-known\\nsearch tree is the binary tree , in which each internal node has two children. BINARY TREE\\nThe search for a term begins at the root of the tree. Each inter nal node (in-\\ncluding the root) represents a binary test, based on whose ou tcome the search\\nproceeds to one of the two sub-trees below that node. Figure 3.1gives an ex-\\nample of a binary search tree used for a dictionary. Efﬁcient search (with a\\nnumber of comparisons that is O(logM)) hinges on the tree being balanced:\\nthe numbers of terms under the two sub-trees of any node are ei ther equal\\nor differ by one. The principal issue here is that of rebalanc ing: as terms are\\ninserted into or deleted from the binary search tree, it need s to be rebalanced\\nso that the balance property is maintained.\\nTo mitigate rebalancing, one approach is to allow the number of sub-trees\\nunder an internal node to vary in a ﬁxed interval. A search tre e commonly\\nused for a dictionary is the B-tree – a search tree in which every internal node B-TREE\\nhas a number of children in the interval [a,b], where aand bare appropriate\\npositive integers; Figure 3.2shows an example with a=2 and b=4. Each\\nbranch under an internal node again represents a test for a ra nge of char-\\n1. So-called perfect hash functions are designed to preclud e collisions, but are rather more com-\\nplicated both to implement and to compute.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 86}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP3.2 Wildcard queries 51\\n◮Figure 3.1 A binary search tree. In this example the branch at the root pa rtitions\\nvocabulary terms into two subtrees, those whose ﬁrst letter is between aandm, and\\nthe rest.\\nacter sequences, as in the binary tree example of Figure 3.1. A B-tree may\\nbe viewed as “collapsing” multiple levels of the binary tree into one; this\\nis especially advantageous when some of the dictionary is di sk-resident, in\\nwhich case this collapsing serves the function of pre-fetch ing imminent bi-\\nnary tests. In such cases, the integers aand bare determined by the sizes of\\ndisk blocks. Section 3.5contains pointers to further background on search\\ntrees and B-trees.\\nIt should be noted that unlike hashing, search trees demand t hat the char-\\nacters used in the document collection have a prescribed ord ering; for in-\\nstance, the 26 letters of the English alphabet are always lis ted in the speciﬁc\\norderAthrough Z. Some Asian languages such as Chinese do not always\\nhave a unique ordering, although by now all languages (inclu ding Chinese\\nand Japanese) have adopted a standard ordering system for th eir character\\nsets.\\n3.2 Wildcard queries\\nWildcard queries are used in any of the following situations : (1) the user\\nis uncertain of the spelling of a query term (e.g., Sydney vs.Sidney , which', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 87}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP52 3 Dictionaries and tolerant retrieval\\n◮Figure 3.2 A B-tree. In this example every internal node has between 2 an d 4\\nchildren.\\nleads to the wildcard query S*dney ); (2) the user is aware of multiple vari-\\nants of spelling a term and (consciously) seeks documents co ntaining any of\\nthe variants (e.g., color vs.colour ); (3) the user seeks documents containing\\nvariants of a term that would be caught by stemming, but is uns ure whether\\nthe search engine performs stemming (e.g., judicial vs.judiciary , leading to the\\nwildcard query judicia* ); (4) the user is uncertain of the correct rendition of a\\nforeign word or phrase (e.g., the query Universit* Stuttgart ).\\nA query such as mon* is known as a trailing wildcard query , because the * WILDCARD QUERY\\nsymbol occurs only once, at the end of the search string. A sea rch tree on\\nthe dictionary is a convenient way of handling trailing wild card queries: we\\nwalk down the tree following the symbols m,o andnin turn, at which point\\nwe can enumerate the set Wof terms in the dictionary with the preﬁx mon.\\nFinally, we use|W|lookups on the standard inverted index to retrieve all\\ndocuments containing any term in W.\\nBut what about wildcard queries in which the *symbol is not constrained\\nto be at the end of the search string? Before handling this gen eral case, we\\nmention a slight generalization of trailing wildcard queri es. First, consider\\nleading wildcard queries , or queries of the form *mon . Consider a reverse B-tree\\non the dictionary – one in which each root-to-leaf path of the B-tree corre-\\nsponds to a term in the dictionary written backwards: thus, the term lemon\\nwould, in the B-tree, be represented by the path root-n-o-m-e-l . A walk down\\nthe reverse B-tree then enumerates all terms Rin the vocabulary with a given\\npreﬁx.\\nIn fact, using a regular B-tree together with a reverse B-tre e, we can handle\\nan even more general case: wildcard queries in which there is a single*sym-\\nbol, such as se*mon . To do this, we use the regular B-tree to enumerate the set\\nWof dictionary terms beginning with the preﬁx se, then the reverse B-tree to', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 88}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP3.2 Wildcard queries 53\\nenumerate the set Rof terms ending with the sufﬁx mon. Next, we take the\\nintersection W∩Rof these two sets, to arrive at the set of terms that begin\\nwith the preﬁx seand end with the sufﬁx mon. Finally, we use the standard\\ninverted index to retrieve all documents containing any ter ms in this inter-\\nsection. We can thus handle wildcard queries that contain a s ingle*symbol\\nusing two B-trees, the normal B-tree and a reverse B-tree.\\n3.2.1 General wildcard queries\\nWe now study two techniques for handling general wildcard qu eries. Both\\ntechniques share a common strategy: express the given wildc ard query qwas\\na Boolean query Qon a specially constructed index, such that the answer to\\nQis a superset of the set of vocabulary terms matching qw. Then, we check\\neach term in the answer to Qagainst qw, discarding those vocabulary terms\\nthat do not match qw. At this point we have the vocabulary terms matching\\nqwand can resort to the standard inverted index.\\nPermuterm indexes\\nOur ﬁrst special index for general wildcard queries is the permuterm index , PERMUTERM INDEX\\na form of inverted index. First, we introduce a special symbo l$into our\\ncharacter set, to mark the end of a term. Thus, the term hello is shown here as\\nthe augmented term hello$ . Next, we construct a permuterm index, in which\\nthe various rotations of each term (augmented with $) all link to the original\\nvocabulary term. Figure 3.3gives an example of such a permuterm index\\nentry for the term hello.\\nWe refer to the set of rotated terms in the permuterm index as t heper-\\nmuterm vocabulary .\\nHow does this index help us with wildcard queries? Consider t he wildcard\\nquerym*n. The key is to rotate such a wildcard query so that the *symbol\\nappears at the end of the string – thus the rotated wildcard qu ery becomes\\nn$m* . Next, we look up this string in the permuterm index, where se eking\\nn$m* (via a search tree) leads to rotations of (among others) the t ermsman\\nandmoron .\\nNow that the permuterm index enables us to identify the origi nal vocab-\\nulary terms matching a wildcard query, we look up these terms in the stan-\\ndard inverted index to retrieve matching documents. We can t hus handle\\nany wildcard query with a single *symbol. But what about a query such as\\nﬁ*mo*er ? In this case we ﬁrst enumerate the terms in the dictionary th at are\\nin the permuterm index of er$ﬁ* . Not all such dictionary terms will have\\nthe string moin the middle - we ﬁlter these out by exhaustive enumera-\\ntion, checking each candidate to see if it contains mo. In this example, the\\ntermﬁshmonger would survive this ﬁltering but ﬁlibuster would not. We then', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 89}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP54 3 Dictionaries and tolerant retrieval\\n◮Figure 3.3 A portion of a permuterm index.\\nrun the surviving terms through the standard inverted index for document\\nretrieval. One disadvantage of the permuterm index is that i ts dictionary\\nbecomes quite large, including as it does all rotations of ea ch term.\\nNotice the close interplay between the B-tree and the permut erm index\\nabove. Indeed, it suggests that the structure should perhap s be viewed as\\na permuterm B-tree. However, we follow traditional termino logy here in\\ndescribing the permuterm index as distinct from the B-tree t hat allows us to\\nselect the rotations with a given preﬁx.\\n3.2.2 k-gram indexes for wildcard queries\\nWhereas the permuterm index is simple, it can lead to a consid erable blowup\\nfrom the number of rotations per term; for a dictionary of Eng lish terms, this\\ncan represent an almost ten-fold space increase. We now pres ent a second\\ntechnique, known as the k-gram index, for processing wildcard queries. We\\nwill also use k-gram indexes in Section 3.3.4 . A k-gram is a sequence of k\\ncharacters. Thus cas,astandstlare all 3-grams occurring in the term castle .\\nWe use a special character $to denote the beginning or end of a term, so the\\nfull set of 3-grams generated for castle is:$ca,cas,ast,stl,tle,le$.\\nIn ak-gram index , the dictionary contains all k-grams that occur in any term k-GRAM INDEX\\nin the vocabulary. Each postings list points from a k-gram to all vocabulary', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 90}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP3.2 Wildcard queries 55\\netr beetroot metric petrify retrieval- - - -\\n◮Figure 3.4 Example of a postings list in a 3-gram index. Here the 3-gram etris\\nillustrated. Matching vocabulary terms are lexicographic ally ordered in the postings.\\nterms containing that k-gram. For instance, the 3-gram etrwould point to vo-\\ncabulary terms such as metric andretrieval . An example is given in Figure 3.4.\\nHow does such an index help us with wildcard queries? Conside r the\\nwildcard query re*ve . We are seeking documents containing any term that\\nbegins with reand ends with ve. Accordingly, we run the Boolean query $re\\nANDve$. This is looked up in the 3-gram index and yields a list of matc hing\\nterms such as relive ,remove andretrieve . Each of these matching terms is then\\nlooked up in the standard inverted index to yield documents m atching the\\nquery.\\nThere is however a difﬁculty with the use of k-gram indexes, that demands\\none further step of processing. Consider using the 3-gram in dex described\\nabove for the query red*. Following the process described above, we ﬁrst\\nissue the Boolean query $re AND red to the 3-gram index. This leads to a\\nmatch on terms such as retired , which contain the conjunction of the two 3-\\ngrams$reandred, yet do not match the original wildcard query red*.\\nTo cope with this, we introduce a post-ﬁltering step, in which the terms enu-\\nmerated by the Boolean query on the 3-gram index are checked i ndividually\\nagainst the original query red*. This is a simple string-matching operation\\nand weeds out terms such as retired that do not match the original query.\\nTerms that survive are then searched in the standard inverte d index as usual.\\nWe have seen that a wildcard query can result in multiple term s being\\nenumerated, each of which becomes a single-term query on the standard in-\\nverted index. Search engines do allow the combination of wil dcard queries\\nusing Boolean operators, for example, re*dAND fe*ri . What is the appropriate\\nsemantics for such a query? Since each wildcard query turns i nto a disjunc-\\ntion of single-term queries, the appropriate interpretati on of this example\\nis that we have a conjunction of disjunctions: we seek all doc uments that\\ncontain any term matching re*d andany term matching fe*ri.\\nEven without Boolean combinations of wildcard queries, the processing of\\na wildcard query can be quite expensive, because of the added lookup in the\\nspecial index, ﬁltering and ﬁnally the standard inverted in dex. A search en-\\ngine may support such rich functionality, but most commonly , the capability\\nis hidden behind an interface (say an “Advanced Query” inter face) that most', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 91}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP56 3 Dictionaries and tolerant retrieval\\nusers never use. Exposing such functionality in the search i nterface often en-\\ncourages users to invoke it even when they do not require it (s ay, by typing\\na preﬁx of their query followed by a *), increasing the proces sing load on the\\nsearch engine.\\n?Exercise 3.1\\nIn the permuterm index, each permuterm vocabulary term poin ts to the original vo-\\ncabulary term(s) from which it was derived. How many origina l vocabulary terms\\ncan there be in the postings list of a permuterm vocabulary te rm?\\nExercise 3.2\\nWrite down the entries in the permuterm index dictionary tha t are generated by the\\ntermmama .\\nExercise 3.3\\nIf you wanted to search for s*ng in a permuterm wildcard index , what key(s) would\\none do the lookup on?\\nExercise 3.4\\nRefer to Figure 3.4; it is pointed out in the caption that the vocabulary terms in the\\npostings are lexicographically ordered. Why is this orderi ng useful?\\nExercise 3.5\\nConsider again the query ﬁ*mo*er from Section 3.2.1 . What Boolean query on a bigram\\nindex would be generated for this query? Can you think of a ter m that matches the\\npermuterm query in Section 3.2.1 , but does not satisfy this Boolean query?\\nExercise 3.6\\nGive an example of a sentence that falsely matches the wildca rd query mon*h if the\\nsearch were to simply use a conjunction of bigrams.\\n3.3 Spelling correction\\nWe next look at the problem of correcting spelling errors in q ueries. For in-\\nstance, we may wish to retrieve documents containing the ter mcarrot when\\nthe user types the query carot . Google reports ( http://www.google.com/jobs/britney.html )\\nthat the following are all treated as misspellings of the que rybritney spears:\\nbritian spears, britney’s spears, brandy spears andprittany spears . We look at two\\nsteps to solving this problem: the ﬁrst based on edit distance and the second\\nbased on k-gram overlap . Before getting into the algorithmic details of these\\nmethods, we ﬁrst review how search engines provide spell-co rrection as part\\nof a user experience.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 92}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP3.3 Spelling correction 57\\n3.3.1 Implementing spelling correction\\nThere are two basic principles underlying most spelling cor rection algorithms.\\n1.Of various alternative correct spellings for a mis-spelled query, choose\\nthe “nearest” one. This demands that we have a notion of nearn ess or\\nproximity between a pair of queries. We will develop these pr oximity\\nmeasures in Section 3.3.3 .\\n2.When two correctly spelled queries are tied (or nearly tied) , select the one\\nthat is more common. For instance, grunt andgrant both seem equally\\nplausible as corrections for grnt. Then, the algorithm should choose the\\nmore common of grunt andgrant as the correction. The simplest notion\\nof more common is to consider the number of occurrences of the term\\nin the collection; thus if grunt occurs more often than grant , it would be\\nthe chosen correction. A different notion of more common is e mployed\\nin many search engines, especially on the web. The idea is to u se the\\ncorrection that is most common among queries typed in by othe r users.\\nThe idea here is that if grunt is typed as a query more often than grant , then\\nit is more likely that the user who typed grntintended to type the query\\ngrunt .\\nBeginning in Section 3.3.3 we describe notions of proximity between queries,\\nas well as their efﬁcient computation. Spelling correction algorithms build on\\nthese computations of proximity; their functionality is th en exposed to users\\nin one of several ways:\\n1.On the query carot always retrieve documents containing carot as well as\\nany “spell-corrected” version of carot , including carrot andtarot.\\n2.As in ( 1) above, but only when the query term carot is not in the dictionary.\\n3.As in ( 1) above, but only when the original query returned fewer than a\\npreset number of documents (say fewer than ﬁve documents).\\n4.When the original query returns fewer than a preset number of docu-\\nments, the search interface presents a spelling suggestion to the end user:\\nthis suggestion consists of the spell-corrected query term (s). Thus, the\\nsearch engine might respond to the user: “Did you mean carrot ?”\\n3.3.2 Forms of spelling correction\\nWe focus on two speciﬁc forms of spelling correction that we r efer to as\\nisolated-term correction and context-sensitive correction. In isolated-term cor-\\nrection, we attempt to correct a single query term at a time – e ven when we', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 93}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP58 3 Dictionaries and tolerant retrieval\\nhave a multiple-term query. The carot example demonstrates this type of cor-\\nrection. Such isolated-term correction would fail to detec t, for instance, that\\nthe query ﬂewformHeathrow contains a mis-spelling of the term from – because\\neach term in the query is correctly spelled in isolation.\\nWe begin by examining two techniques for addressing isolate d-term cor-\\nrection: edit distance, and k-gram overlap. We then proceed to context-\\nsensitive correction.\\n3.3.3 Edit distance\\nGiven two character strings s1and s2, the edit distance between them is the EDIT DISTANCE\\nminimum number of edit operations required to transform s1into s2. Most\\ncommonly, the edit operations allowed for this purpose are: (i) insert a char-\\nacter into a string; (ii) delete a character from a string and (iii) replace a char-\\nacter of a string by another character; for these operations , edit distance is\\nsometimes known as Levenshtein distance . For example, the edit distance be- LEVENSHTEIN\\nDISTANCE tweencatanddogis 3. In fact, the notion of edit distance can be generalized\\nto allowing different weights for different kinds of edit op erations, for in-\\nstance a higher weight may be placed on replacing the charact ersby the\\ncharacter p, than on replacing it by the character a(the latter being closer to s\\non the keyboard). Setting weights in this way depending on th e likelihood of\\nletters substituting for each other is very effective in pra ctice (see Section 3.4\\nfor the separate issue of phonetic similarity). However, th e remainder of our\\ntreatment here will focus on the case in which all edit operat ions have the\\nsame weight.\\nIt is well-known how to compute the (weighted) edit distance between\\ntwo strings in time O(|s1|×|s2|), where|si|denotes the length of a string si.\\nThe idea is to use the dynamic programming algorithm in Figur e3.5, where\\nthe characters in s1and s2are given in array form. The algorithm ﬁlls the\\n(integer) entries in a matrix mwhose two dimensions equal the lengths of\\nthe two strings whose edit distances is being computed; the (i,j)entry of the\\nmatrix will hold (after the algorithm is executed) the edit d istance between\\nthe strings consisting of the ﬁrst icharacters of s1and the ﬁrst jcharacters\\nofs2. The central dynamic programming step is depicted in Lines 8 -10 of\\nFigure 3.5, where the three quantities whose minimum is taken correspo nd\\nto substituting a character in s1, inserting a character in s1and inserting a\\ncharacter in s2.\\nFigure 3.6shows an example Levenshtein distance computation of Fig-\\nure3.5. The typical cell [i,j]has four entries formatted as a 2 ×2 cell. The\\nlower right entry in each cell is the min of the other three, co rresponding to\\nthe main dynamic programming step in Figure 3.5. The other three entries\\nare the three entries m[i−1,j−1] +0 or 1 depending on whether s1[i] =', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 94}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP3.3 Spelling correction 59\\nEDITDISTANCE (s1,s2)\\n1int m[i,j] =0\\n2fori←1to|s1|\\n3dom[i, 0] =i\\n4forj←1to|s2|\\n5dom[0,j] =j\\n6fori←1to|s1|\\n7do for j←1to|s2|\\n8 dom[i,j] =min{m[i−1,j−1] +if(s1[i] =s2[j])then 0 else 1ﬁ,\\n9 m[i−1,j] +1,\\n10 m[i,j−1] +1}\\n11 return m[|s1|,|s2|]\\n◮Figure 3.5 Dynamic programming algorithm for computing the edit dista nce be-\\ntween strings s1and s2.\\nf a s t\\n0 11 22 33 44\\nc1\\n112\\n2123\\n2234\\n3345\\n44\\na2\\n222\\n3213\\n3134\\n2245\\n33\\nt3\\n333\\n4332\\n4223\\n3224\\n32\\ns4\\n444\\n5443\\n5323\\n4233\\n33\\n◮Figure 3.6 Example Levenshtein distance computation. The 2 ×2 cell in the [i,j]\\nentry of the table shows the three numbers whose minimum yiel ds the fourth. The\\ncells in italics determine the edit distance in this example .\\ns2[j],m[i−1,j] +1 and m[i,j−1] +1. The cells with numbers in italics depict\\nthe path by which we determine the Levenshtein distance.\\nThe spelling correction problem however demands more than c omputing\\nedit distance: given a set Sof strings (corresponding to terms in the vocab-\\nulary) and a query string q, we seek the string(s) in Vof least edit distance\\nfrom q. We may view this as a decoding problem, in which the codeword s\\n(the strings in V) are prescribed in advance. The obvious way of doing this\\nis to compute the edit distance from qto each string in V, before selecting the', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 95}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP60 3 Dictionaries and tolerant retrieval\\nstring(s) of minimum edit distance. This exhaustive search is inordinately\\nexpensive. Accordingly, a number of heuristics are used in p ractice to efﬁ-\\nciently retrieve vocabulary terms likely to have low edit di stance to the query\\nterm(s).\\nThe simplest such heuristic is to restrict the search to dict ionary terms be-\\nginning with the same letter as the query string; the hope wou ld be that\\nspelling errors do not occur in the ﬁrst character of the quer y. A more sophis-\\nticated variant of this heuristic is to use a version of the pe rmuterm index,\\nin which we omit the end-of-word symbol $. Consider the set of all rota-\\ntions of the query string q. For each rotation rfrom this set, we traverse the\\nB-tree into the permuterm index, thereby retrieving all dic tionary terms that\\nhave a rotation beginning with r. For instance, if qismase and we consider\\nthe rotation r=sema , we would retrieve dictionary terms such as semantic\\nandsemaphore that do not have a small edit distance to q. Unfortunately, we\\nwould miss more pertinent dictionary terms such as mare andmane . To ad-\\ndress this, we reﬁne this rotation scheme: for each rotation , we omit a sufﬁx\\nofℓcharacters before performing the B-tree traversal. This en sures that each\\nterm in the set Rof terms retrieved from the dictionary includes a “long”\\nsubstring in common with q. The value of ℓcould depend on the length of q.\\nAlternatively, we may set it to a ﬁxed constant such as 2.\\n3.3.4 k-gram indexes for spelling correction\\nTo further limit the set of vocabulary terms for which we comp ute edit dis-\\ntances to the query term, we now show how to invoke the k-gram index of\\nSection 3.2.2 (page 54) to assist with retrieving vocabulary terms with low\\nedit distance to the query q. Once we retrieve such terms, we can then ﬁnd\\nthe ones of least edit distance from q.\\nIn fact, we will use the k-gram index to retrieve vocabulary terms that\\nhave many k-grams in common with the query. We will argue that for rea-\\nsonable deﬁnitions of “many k-grams in common,” the retrieval process is\\nessentially that of a single scan through the postings for th ek-grams in the\\nquery string q.\\nThe 2-gram (or bigram ) index in Figure 3.7shows (a portion of) the post-\\nings for the three bigrams in the query bord. Suppose we wanted to retrieve\\nvocabulary terms that contained at least two of these three b igrams. A single\\nscan of the postings (much as in Chapter 1) would let us enumerate all such\\nterms; in the example of Figure 3.7we would enumerate aboard, boardroom\\nandborder .\\nThis straightforward application of the linear scan inters ection of postings\\nimmediately reveals the shortcoming of simply requiring ma tched vocabu-\\nlary terms to contain a ﬁxed number of k-grams from the query q: terms\\nlikeboardroom , an implausible “correction” of bord, get enumerated. Conse-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 96}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP3.3 Spelling correction 61\\nrd aboard ardent boardroom borderor border lord morbid sordidbo aboard about boardroom border\\n- - - -- - - -- - - -\\n◮Figure 3.7 Matching at least two of the three 2-grams in the query bord.\\nquently, we require more nuanced measures of the overlap in k-grams be-\\ntween a vocabulary term and q. The linear scan intersection can be adapted\\nwhen the measure of overlap is the Jaccard coefﬁcient for measuring the over- JACCARD COEFFICIENT\\nlap between two sets Aand B, deﬁned to be|A∩B|/|A∪B|. The two sets we\\nconsider are the set of k-grams in the query q, and the set of k-grams in a vo-\\ncabulary term. As the scan proceeds, we proceed from one voca bulary term\\ntto the next, computing on the ﬂy the Jaccard coefﬁcient betwe enqand t. If\\nthe coefﬁcient exceeds a preset threshold, we add tto the output; if not, we\\nmove on to the next term in the postings. To compute the Jaccar d coefﬁcient,\\nwe need the set of k-grams in qand t.\\nSince we are scanning the postings for all k-grams in q, we immediately\\nhave these k-grams on hand. What about the k-grams of t? In principle,\\nwe could enumerate these on the ﬂy from t; in practice this is not only slow\\nbut potentially infeasible since, in all likelihood, the po stings entries them-\\nselves do not contain the complete string tbut rather some encoding of t. The\\ncrucial observation is that to compute the Jaccard coefﬁcie nt, we only need\\nthe length of the string t. To see this, recall the example of Figure 3.7and\\nconsider the point when the postings scan for query q=bord reaches term\\nt=boardroom . We know that two bigrams match. If the postings stored the\\n(pre-computed) number of bigrams in boardroom (namely, 8), we have all the\\ninformation we require to compute the Jaccard coefﬁcient to be 2/(8+3−2);\\nthe numerator is obtained from the number of postings hits (2 , fromboand\\nrd) while the denominator is the sum of the number of bigrams in bord and\\nboardroom , less the number of postings hits.\\nWe could replace the Jaccard coefﬁcient by other measures th at allow ef-\\nﬁcient on the ﬂy computation during postings scans. How do we use these', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 97}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP62 3 Dictionaries and tolerant retrieval\\nfor spelling correction? One method that has some empirical support is to\\nﬁrst use the k-gram index to enumerate a set of candidate vocabulary terms\\nthat are potential corrections of q. We then compute the edit distance from q\\nto each term in this set, selecting terms from the set with sma ll edit distance\\ntoq.\\n3.3.5 Context sensitive spelling correction\\nIsolated-term correction would fail to correct typographi cal errors such as\\nﬂew form Heathrow , where all three query terms are correctly spelled. When\\na phrase such as this retrieves few documents, a search engin e may like to\\noffer the corrected query ﬂewfromHeathrow . The simplest way to do this is to\\nenumerate corrections of each of the three query terms (usin g the methods\\nleading up to Section 3.3.4 ) even though each query term is correctly spelled,\\nthen try substitutions of each correction in the phrase. For the example ﬂew\\nform Heathrow , we enumerate such phrases as ﬂed form Heathrow andﬂew fore\\nHeathrow . For each such substitute phrase, the search engine runs the query\\nand determines the number of matching results.\\nThis enumeration can be expensive if we ﬁnd many corrections of the in-\\ndividual terms, since we could encounter a large number of co mbinations of\\nalternatives. Several heuristics are used to trim this spac e. In the example\\nabove, as we expand the alternatives for ﬂew andform, we retain only the\\nmost frequent combinations in the collection or in the query logs, which con-\\ntain previous queries by users. For instance, we would retai nﬂew from as an\\nalternative to try and extend to a three-term corrected quer y, but perhaps not\\nﬂed fore orﬂea form . In this example, the biword ﬂed fore is likely to be rare\\ncompared to the biword ﬂewfrom . Then, we only attempt to extend the list of\\ntop biwords (such as ﬂew from ), to corrections of Heathrow . As an alternative\\nto using the biword statistics in the collection, we may use t he logs of queries\\nissued by users; these could of course include queries with s pelling errors.\\n?Exercise 3.7\\nIf|si|denotes the length of string si, show that the edit distance between s1and s2is\\nnever more than max {|s1|,|s2|}.\\nExercise 3.8\\nCompute the edit distance between paris andalice. Write down the 5 ×5 array of\\ndistances between all preﬁxes as computed by the algorithm i n Figure 3.5.\\nExercise 3.9\\nWrite pseudocode showing the details of computing on the ﬂy t he Jaccard coefﬁcient\\nwhile scanning the postings of the k-gram index, as mentioned on page 61.\\nExercise 3.10\\nCompute the Jaccard coefﬁcients between the query bord and each of the terms in\\nFigure 3.7that contain the bigram or.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 98}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP3.4 Phonetic correction 63\\nExercise 3.11\\nConsider the four-term query catched in the rye and suppose that each of the query\\nterms has ﬁve alternative terms suggested by isolated-term correction. How many\\npossible corrected phrases must we consider if we do not trim the space of corrected\\nphrases, but instead try all six variants for each of the term s?\\nExercise 3.12\\nFor each of the preﬁxes of the query — catched ,catchedin andcatchedinthe — we have\\na number of substitute preﬁxes arising from each term and its alternatives. Suppose\\nthat we were to retain only the top 10 of these substitute preﬁ xes, as measured by\\nits number of occurrences in the collection. We eliminate th e rest from consideration\\nfor extension to longer preﬁxes: thus, if batched in is not one of the 10 most common\\n2-term queries in the collection, we do not consider any exte nsion of batchedin as pos-\\nsibly leading to a correction of catched intherye . How many of the possible substitute\\npreﬁxes are we eliminating at each phase?\\nExercise 3.13\\nAre we guaranteed that retaining and extending only the 10 co mmonest substitute\\npreﬁxes of catchedin will lead to one of the 10 commonest substitute preﬁxes of catched\\ninthe ?\\n3.4 Phonetic correction\\nOur ﬁnal technique for tolerant retrieval has to do with phonetic correction:\\nmisspellings that arise because the user types a query that s ounds like the tar-\\nget term. Such algorithms are especially applicable to sear ches on the names\\nof people. The main idea here is to generate, for each term, a “ phonetic hash”\\nso that similar-sounding terms hash to the same value. The id ea owes its\\norigins to work in international police departments from th e early 20th cen-\\ntury, seeking to match names for wanted criminals despite th e names being\\nspelled differently in different countries. It is mainly us ed to correct phonetic\\nmisspellings in proper nouns.\\nAlgorithms for such phonetic hashing are commonly collecti vely known as\\nsoundex algorithms. However, there is an original soundex algorith m, with SOUNDEX\\nvarious variants, built on the following scheme:\\n1.Turn every term to be indexed into a 4-character reduced form . Build an\\ninverted index from these reduced forms to the original term s; call this\\nthe soundex index.\\n2.Do the same with query terms.\\n3.When the query calls for a soundex match, search this soundex index.\\nThe variations in different soundex algorithms have to do wi th the conver-\\nsion of terms to 4-character forms. A commonly used conversi on results in\\na 4-character code, with the ﬁrst character being a letter of the alphabet and\\nthe other three being digits between 0 and 9.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 99}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP64 3 Dictionaries and tolerant retrieval\\n1.Retain the ﬁrst letter of the term.\\n2.Change all occurrences of the following letters to ’0’ (zero ): ’A’, E’, ’I’, ’O’,\\n’U’, ’H’, ’W’, ’Y’.\\n3.Change letters to digits as follows:\\nB, F, P , V to 1.\\nC, G, J, K, Q, S, X, Z to 2.\\nD,T to 3.\\nL to 4.\\nM, N to 5.\\nR to 6.\\n4.Repeatedly remove one out of each pair of consecutive identi cal digits.\\n5.Remove all zeros from the resulting string. Pad the resultin g string with\\ntrailing zeros and return the ﬁrst four positions, which wil l consist of a\\nletter followed by three digits.\\nFor an example of a soundex map, Hermann maps to H655. Given a query\\n(sayherman ), we compute its soundex code and then retrieve all vocabula ry\\nterms matching this soundex code from the soundex index, bef ore running\\nthe resulting query on the standard inverted index.\\nThis algorithm rests on a few observations: (1) vowels are vi ewed as inter-\\nchangeable, in transcribing names; (2) consonants with sim ilar sounds (e.g.,\\nD and T) are put in equivalence classes. This leads to related names often\\nhaving the same soundex codes. While these rules work for man y cases,\\nespecially European languages, such rules tend to be writin g system depen-\\ndent. For example, Chinese names can be written in Wade-Gile s or Pinyin\\ntranscription. While soundex works for some of the differen ces in the two\\ntranscriptions, for instance mapping both Wade-Giles hsand Pinyin xto 2,\\nit fails in other cases, for example Wade-Giles jand Pinyin rare mapped\\ndifferently.\\n?Exercise 3.14\\nFind two differently spelled proper nouns whose soundex cod es are the same.\\nExercise 3.15\\nFind two phonetically similar proper nouns whose soundex co des are different.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 100}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP3.5 References and further reading 65\\n3.5 References and further reading\\nKnuth (1997 ) is a comprehensive source for information on search trees, in-\\ncluding B-trees and their use in searching through dictiona ries.\\nGarﬁeld (1976 ) gives one of the ﬁrst complete descriptions of the permuter m\\nindex. Ferragina and Venturini (2007 ) give an approach to addressing the\\nspace blowup in permuterm indexes.\\nOne of the earliest formal treatments of spelling correctio n was due to\\nDamerau (1964 ). The notion of edit distance that we have used is due to Lev-\\nenshtein (1965 ) and the algorithm in Figure 3.5is due to Wagner and Fischer\\n(1974 ).Peterson (1980 ) and Kukich (1992 ) developed variants of methods\\nbased on edit distances, culminating in a detailed empirica l study of sev-\\neral methods by Zobel and Dart (1995 ), which shows that k-gram indexing\\nis very effective for ﬁnding candidate mismatches, but shou ld be combined\\nwith a more ﬁne-grained technique such as edit distance to de termine the\\nmost likely misspellings. Gusﬁeld (1997 ) is a standard reference on string\\nalgorithms such as edit distance.\\nProbabilistic models (“noisy channel” models) for spellin g correction were\\npioneered by Kernighan et al. (1990 ) and further developed by Brill and\\nMoore (2000 ) and Toutanova and Moore (2002 ). In these models, the mis-\\nspelled query is viewed as a probabilistic corruption of a co rrect query. They\\nhave a similar mathematical basis to the language model meth ods presented\\nin Chapter 12, and also provide ways of incorporating phonetic similarit y,\\ncloseness on the keyboard, and data from the actual spelling mistakes of\\nusers. Many would regard them as the state-of-the-art appro ach. Cucerzan\\nand Brill (2004 ) show how this work can be extended to learning spelling\\ncorrection models based on query reformulations in search e ngine logs.\\nThe soundex algorithm is attributed to Margaret K. Odell and Robert C.\\nRusselli (from U.S. patents granted in 1918 and 1922); the ve rsion described\\nhere draws on Bourne and Ford (1961 ).Zobel and Dart (1996 ) evaluate var-\\nious phonetic matching algorithms, ﬁnding that a variant of the soundex\\nalgorithm performs poorly for general spelling correction , but that other al-\\ngorithms based on the phonetic similarity of term pronuncia tions perform\\nwell.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 101}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 102}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 67\\n4 Index construction\\nIn this chapter, we look at how to construct an inverted index . We call this\\nprocess index construction orindexing ; the process or machine that performs it INDEXING\\ntheindexer . The design of indexing algorithms is governed by hardware c on- INDEXER\\nstraints. We therefore begin this chapter with a review of th e basics of com-\\nputer hardware that are relevant for indexing. We then intro duce blocked\\nsort-based indexing (Section 4.2), an efﬁcient single-machine algorithm de-\\nsigned for static collections that can be viewed as a more sca lable version of\\nthe basic sort-based indexing algorithm we introduced in Ch apter 1. Sec-\\ntion 4.3describes single-pass in-memory indexing, an algorithm th at has\\neven better scaling properties because it does not hold the v ocabulary in\\nmemory. For very large collections like the web, indexing ha s to be dis-\\ntributed over computer clusters with hundreds or thousands of machines.\\nWe discuss this in Section 4.4. Collections with frequent changes require dy-\\nnamic indexing introduced in Section 4.5so that changes in the collection are\\nimmediately reﬂected in the index. Finally, we cover some co mplicating is-\\nsues that can arise in indexing – such as security and indexes for ranked\\nretrieval – in Section 4.6.\\nIndex construction interacts with several topics covered i n other chapters.\\nThe indexer needs raw text, but documents are encoded in many ways (see\\nChapter 2). Indexers compress and decompress intermediate ﬁles and t he\\nﬁnal index (see Chapter 5). In web search, documents are not on a local\\nﬁle system, but have to be spidered or crawled (see Chapter 20). In enter-\\nprise search, most documents are encapsulated in varied con tent manage-\\nment systems, email applications, and databases. We give so me examples\\nin Section 4.7. Although most of these applications can be accessed via htt p,\\nnative Application Programming Interfaces (APIs) are usua lly more efﬁcient.\\nThe reader should be aware that building the subsystem that f eeds raw text\\nto the indexing process can in itself be a challenging proble m.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 103}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP68 4 Index construction\\n◮Table 4.1 Typical system parameters in 2007. The seek time is the time n eeded\\nto position the disk head in a new position. The transfer time per byte is the rate of\\ntransfer from disk to memory when the head is in the right posi tion.\\nSymbol Statistic Value\\ns average seek time 5 ms =5×10−3s\\nb transfer time per byte 0.02 µs=2×10−8s\\nprocessor’s clock rate 109s−1\\np lowlevel operation\\n(e.g., compare & swap a word) 0.01 µs=10−8s\\nsize of main memory several GB\\nsize of disk space 1 TB or more\\n4.1 Hardware basics\\nWhen building an information retrieval (IR) system, many de cisions are based\\non the characteristics of the computer hardware on which the system runs.\\nWe therefore begin this chapter with a brief review of comput er hardware.\\nPerformance characteristics typical of systems in 2007 are shown in Table 4.1.\\nA list of hardware basics that we need in this book to motivate IR system\\ndesign follows.\\n•Access to data in memory is much faster than access to data on d isk. It\\ntakes a few clock cycles (perhaps 5 ×10−9seconds) to access a byte in\\nmemory, but much longer to transfer it from disk (about 2 ×10−8sec-\\nonds). Consequently, we want to keep as much data as possible in mem-\\nory, especially those data that we need to access frequently . We call the\\ntechnique of keeping frequently used disk data in main memor ycaching . CACHING\\n•When doing a disk read or write, it takes a while for the disk he ad to\\nmove to the part of the disk where the data are located. This ti me is called\\ntheseek time and it averages 5 ms for typical disks. No data are being SEEK TIME\\ntransferred during the seek. To maximize data transfer rate s, chunks of\\ndata that will be read together should therefore be stored co ntiguously on\\ndisk. For example, using the numbers in Table 4.1it may take as little as\\n0.2 seconds to transfer 10 megabytes (MB) from disk to memory if it is\\nstored as one chunk, but up to 0.2 +100×(5×10−3) = 0.7 seconds if it\\nis stored in 100 noncontiguous chunks because we need to move the disk\\nhead up to 100 times.\\n•Operating systems generally read and write entire blocks. T hus, reading\\na single byte from disk can take as much time as reading the ent ire block.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 104}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP4.2 Blocked sort-based indexing 69\\nBlock sizes of 8, 16, 32, and 64 kilobytes (KB) are common. We c all the part\\nof main memory where a block being read or written is stored a buffer . BUFFER\\n•Data transfers from disk to memory are handled by the system b us, not by\\nthe processor. This means that the processor is available to process data\\nduring disk I/O. We can exploit this fact to speed up data tran sfers by\\nstoring compressed data on disk. Assuming an efﬁcient decom pression\\nalgorithm, the total time of reading and then decompressing compressed\\ndata is usually less than reading uncompressed data.\\n•Servers used in IR systems typically have several gigabytes (GB) of main\\nmemory, sometimes tens of GB. Available disk space is severa l orders of\\nmagnitude larger.\\n4.2 Blocked sort-based indexing\\nThe basic steps in constructing a nonpositional index are de picted in Fig-\\nure1.4(page 8). We ﬁrst make a pass through the collection assembling all\\nterm–docID pairs. We then sort the pairs with the term as the d ominant key\\nand docID as the secondary key. Finally, we organize the docI Ds for each\\nterm into a postings list and compute statistics like term an d document fre-\\nquency. For small collections, all this can be done in memory . In this chapter,\\nwe describe methods for large collections that require the u se of secondary\\nstorage.\\nTo make index construction more efﬁcient, we represent term s as termIDs\\n(instead of strings as we did in Figure 1.4), where each termID is a unique TERM ID\\nserial number. We can build the mapping from terms to termIDs on the ﬂy\\nwhile we are processing the collection; or, in a two-pass app roach, we com-\\npile the vocabulary in the ﬁrst pass and construct the invert ed index in the\\nsecond pass. The index construction algorithms described i n this chapter all\\ndo a single pass through the data. Section 4.7gives references to multipass\\nalgorithms that are preferable in certain applications, fo r example, when disk\\nspace is scarce.\\nWe work with the Reuters-RCV1 collection as our model collection in this REUTERS -RCV1\\nchapter, a collection with roughly 1 GB of text. It consists o f about 800,000\\ndocuments that were sent over the Reuters newswire during a 1 -year pe-\\nriod between August 20, 1996, and August 19, 1997. A typical d ocument is\\nshown in Figure 4.1, but note that we ignore multimedia information like\\nimages in this book and are only concerned with text. Reuters -RCV1 covers\\na wide range of international topics, including politics, b usiness, sports, and\\n(as in this example) science. Some key statistics of the coll ection are shown\\nin Table 4.2.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 105}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP70 4 Index construction\\n◮Table 4.2 Collection statistics for Reuters-RCV1. Values are rounde d for the com-\\nputations in this book. The unrounded values are: 806,791 do cuments, 222 tokens\\nper document, 391,523 (distinct) terms, 6.04 bytes per toke n with spaces and punc-\\ntuation, 4.5 bytes per token without spaces and punctuation , 7.5 bytes per term, and\\n96,969,056 tokens. The numbers in this table correspond to t he third line (“case fold-\\ning”) in Table 5.1(page 87).\\nSymbol Statistic Value\\nN documents 800,000\\nLave avg. # tokens per document 200\\nM terms 400,000\\navg. # bytes per token (incl. spaces/punct.) 6\\navg. # bytes per token (without spaces/punct.) 4.5\\navg. # bytes per term 7.5\\nT tokens 100,000,000\\nREUTERS\\nExtreme conditions create rare Antarctic cloudsYou are here:  Home > News > Science > Article\\nGo to a Section:      U.S.     International      Business      Markets Politics Entertainment Technology\\nTue Aug 1, 2006 3:20am ET\\nEmail This Article | Print This Article | Reprints\\nSYDNEY (Reuters) - Rare, mother-of-pearl colored clouds\\ncaused by extreme weather conditions above Antarctica are a\\npossible indication of global warming, Australian scientists said on\\nTuesday.\\nKnown as nacreous clouds, the spectacular formations showing delicate\\nwisps of colors were photographed in the sky over an Australian\\nmeteorological base at Mawson Station on July 25.Sports Oddly Enough\\n[-] Text [+]\\n◮Figure 4.1 Document from the Reuters newswire.\\nReuters-RCV1 has 100 million tokens. Collecting all termID –docID pairs of\\nthe collection using 4 bytes each for termID and docID theref ore requires 0.8\\nGB of storage. Typical collections today are often one or two orders of mag-\\nnitude larger than Reuters-RCV1. You can easily see how such collections\\noverwhelm even large computers if we try to sort their termID –docID pairs\\nin memory. If the size of the intermediate ﬁles during index c onstruction is\\nwithin a small factor of available memory, then the compress ion techniques\\nintroduced in Chapter 5can help; however, the postings ﬁle of many large\\ncollections cannot ﬁt into memory even after compression.\\nWith main memory insufﬁcient, we need to use an external sorting algo- EXTERNAL SORTING\\nALGORITHM rithm , that is, one that uses disk. For acceptable speed, the centr al require-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 106}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP4.2 Blocked sort-based indexing 71\\nBSBI NDEX CONSTRUCTION ()\\n1n←0\\n2while (all documents have not been processed)\\n3don←n+1\\n4 block←PARSE NEXTBLOCK()\\n5 BSBI-I NVERT (block)\\n6 W RITE BLOCK TODISK(block ,fn)\\n7 M ERGE BLOCKS (f1, . . . , fn;fmerged)\\n◮Figure 4.2 Blocked sort-based indexing. The algorithm stores inverte d blocks in\\nﬁles f1, . . . , fnand the merged index in fmerged .\\nment of such an algorithm is that it minimize the number of ran dom disk\\nseeks during sorting – sequential disk reads are far faster t han seeks as we\\nexplained in Section 4.1. One solution is the blocked sort-based indexing algo- BLOCKED SORT -BASED\\nINDEXING ALGORITHM rithm orBSBI in Figure 4.2. BSBI (i) segments the collection into parts of equal\\nsize, (ii) sorts the termID–docID pairs of each part in memor y, (iii) stores in-\\ntermediate sorted results on disk, and (iv) merges all inter mediate results\\ninto the ﬁnal index.\\nThe algorithm parses documents into termID–docID pairs and accumu-\\nlates the pairs in memory until a block of a ﬁxed size is full (P ARSE NEXTBLOCK\\nin Figure 4.2). We choose the block size to ﬁt comfortably into memory to\\npermit a fast in-memory sort. The block is then inverted and w ritten to disk.\\nInversion involves two steps. First, we sort the termID–docID pairs. N ext, INVERSION\\nwe collect all termID–docID pairs with the same termID into a postings list,\\nwhere a posting is simply a docID. The result, an inverted index for the block POSTING\\nwe have just read, is then written to disk. Applying this to Re uters-RCV1 and\\nassuming we can ﬁt 10 million termID–docID pairs into memory , we end up\\nwith ten blocks, each an inverted index of one part of the coll ection.\\nIn the ﬁnal step, the algorithm simultaneously merges the te n blocks into\\none large merged index. An example with two blocks is shown in Figure 4.3,\\nwhere we use dito denote the ithdocument of the collection. To do the merg-\\ning, we open all block ﬁles simultaneously, and maintain sma ll read buffers\\nfor the ten blocks we are reading and a write buffer for the ﬁna l merged in-\\ndex we are writing. In each iteration, we select the lowest te rmID that has\\nnot been processed yet using a priority queue or a similar dat a structure. All\\npostings lists for this termID are read and merged, and the me rged list is\\nwritten back to disk. Each read buffer is reﬁlled from its ﬁle when necessary.\\nHow expensive is BSBI? Its time complexity is Θ(TlogT)because the step\\nwith the highest time complexity is sorting and Tis an upper bound for the\\nnumber of items we must sort (i.e., the number of termID–docI D pairs). But', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 107}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP72 4 Index construction\\nbrutus d1,d3\\ncaesar d1,d2,d4\\nnoble d5\\nwith d1,d2,d3,d5brutus d6,d7\\ncaesar d8,d9\\njulius d10\\nkilled d8postings lists\\nto be merged\\nbrutus d1,d3,d6,d7\\ncaesar d1,d2,d4,d8,d9\\njulius d10\\nkilled d8\\nnoble d5\\nwith d1,d2,d3,d5merged\\npostings lists\\ndisk\\n◮Figure 4.3 Merging in blocked sort-based indexing. Two blocks (“posti ngs lists to\\nbe merged”) are loaded from disk into memory, merged in memor y (“merged post-\\nings lists”) and written back to disk. We show terms instead o f termIDs for better\\nreadability.\\nthe actual indexing time is usually dominated by the time it t akes to parse the\\ndocuments (P ARSE NEXTBLOCK ) and to do the ﬁnal merge (M ERGE BLOCKS ).\\nExercise 4.6asks you to compute the total index construction time for RCV 1\\nthat includes these steps as well as inverting the blocks and writing them to\\ndisk.\\nNotice that Reuters-RCV1 is not particularly large in an age when one or\\nmore GB of memory are standard on personal computers. With ap propriate\\ncompression (Chapter 5), we could have created an inverted index for RCV1\\nin memory on a not overly beefy server. The techniques we have described\\nare needed, however, for collections that are several order s of magnitude\\nlarger.\\n?Exercise 4.1\\nIf we need Tlog2Tcomparisons (where Tis the number of termID–docID pairs) and\\ntwo disk seeks for each comparison, how much time would index construction for\\nReuters-RCV1 take if we used disk instead of memory for stora ge and an unopti-\\nmized sorting algorithm (i.e., not an external sorting algo rithm)? Use the system\\nparameters in Table 4.1.\\nExercise 4.2 [⋆]\\nHow would you create the dictionary in blocked sort-based in dexing on the ﬂy to\\navoid an extra pass through the data?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 108}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP4.3 Single-pass in-memory indexing 73\\nSPIMI-I NVERT (token _stream )\\n1output _f ile=NEWFILE()\\n2dictionary =NEWHASH()\\n3while (free memory available)\\n4dotoken←next(token _stream )\\n5 ifterm(token)/∈dictionary\\n6 then postings _list=ADDTODICTIONARY (dictionary ,term(token))\\n7 else postings _list=GETPOSTINGS LIST(dictionary ,term(token))\\n8 iff ull(postings _list)\\n9 then postings _list=DOUBLE POSTINGS LIST(dictionary ,term(token))\\n10 A DDTOPOSTINGS LIST(postings _list,docID (token))\\n11 sorted _terms←SORTTERMS(dictionary )\\n12 W RITE BLOCK TODISK(sorted _terms ,dictionary ,output _f ile)\\n13 return output _f ile\\n◮Figure 4.4 Inversion of a block in single-pass in-memory indexing\\n4.3 Single-pass in-memory indexing\\nBlocked sort-based indexing has excellent scaling propert ies, but it needs\\na data structure for mapping terms to termIDs. For very large collections,\\nthis data structure does not ﬁt into memory. A more scalable a lternative is\\nsingle-pass in-memory indexing orSPIMI . SPIMI uses terms instead of termIDs, SINGLE -PASS\\nIN-MEMORY INDEXING writes each block’s dictionary to disk, and then starts a new dictionary for the\\nnext block. SPIMI can index collections of any size as long as there is enough\\ndisk space available.\\nThe SPIMI algorithm is shown in Figure 4.4. The part of the algorithm that\\nparses documents and turns them into a stream of term–docID p airs, which\\nwe call tokens here, has been omitted. SPIMI-I NVERT is called repeatedly on\\nthe token stream until the entire collection has been proces sed.\\nTokens are processed one by one (line 4) during each successi ve call of\\nSPIMI-I NVERT . When a term occurs for the ﬁrst time, it is added to the\\ndictionary (best implemented as a hash), and a new postings l ist is created\\n(line 6). The call in line 7 returns this postings list for sub sequent occurrences\\nof the term.\\nA difference between BSBI and SPIMI is that SPIMI adds a posti ng di-\\nrectly to its postings list (line 10). Instead of ﬁrst collec ting all termID–docID\\npairs and then sorting them (as we did in BSBI), each postings list is dynamic\\n(i.e., its size is adjusted as it grows) and it is immediately available to collect\\npostings. This has two advantages: It is faster because ther e is no sorting\\nrequired, and it saves memory because we keep track of the ter m a postings', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 109}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP74 4 Index construction\\nlist belongs to, so the termIDs of postings need not be stored . As a result, the\\nblocks that individual calls of SPIMI-I NVERT can process are much larger\\nand the index construction process as a whole is more efﬁcien t.\\nBecause we do not know how large the postings list of a term wil l be when\\nwe ﬁrst encounter it, we allocate space for a short postings l ist initially and\\ndouble the space each time it is full (lines 8–9). This means t hat some mem-\\nory is wasted, which counteracts the memory savings from the omission of\\ntermIDs in intermediate data structures. However, the over all memory re-\\nquirements for the dynamically constructed index of a block in SPIMI are\\nstill lower than in BSBI.\\nWhen memory has been exhausted, we write the index of the bloc k (which\\nconsists of the dictionary and the postings lists) to disk (l ine 12). We have to\\nsort the terms (line 11) before doing this because we want to w rite postings\\nlists in lexicographic order to facilitate the ﬁnal merging step. If each block’s\\npostings lists were written in unsorted order, merging bloc ks could not be\\naccomplished by a simple linear scan through each block.\\nEach call of SPIMI-I NVERT writes a block to disk, just as in BSBI. The last\\nstep of SPIMI (corresponding to line 7 in Figure 4.2; not shown in Figure 4.4)\\nis then to merge the blocks into the ﬁnal inverted index.\\nIn addition to constructing a new dictionary structure for e ach block and\\neliminating the expensive sorting step, SPIMI has a third im portant compo-\\nnent: compression. Both the postings and the dictionary ter ms can be stored\\ncompactly on disk if we employ compression. Compression inc reases the ef-\\nﬁciency of the algorithm further because we can process even larger blocks,\\nand because the individual blocks require less space on disk . We refer readers\\nto the literature for this aspect of the algorithm (Section 4.7).\\nThe time complexity of SPIMI is Θ(T)because no sorting of tokens is re-\\nquired and all operations are at most linear in the size of the collection.\\n4.4 Distributed indexing\\nCollections are often so large that we cannot perform index c onstruction efﬁ-\\nciently on a single machine. This is particularly true of the World Wide Web\\nfor which we need large computer clusters1to construct any reasonably sized\\nweb index. Web search engines, therefore, use distributed indexing algorithms\\nfor index construction. The result of the construction proc ess is a distributed\\nindex that is partitioned across several machines – either a ccording to term\\nor according to document. In this section, we describe distr ibuted indexing\\nfor a term-partitioned index. Most large search engines pre fer a document-\\n1. A cluster in this chapter is a group of tightly coupled comp uters that work together closely.\\nThis sense of the word is different from the use of cluster as a group of documents that are\\nsemantically similar in Chapters 16–18.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 110}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP4.4 Distributed indexing 75\\npartitioned index (which can be easily generated from a term -partitioned\\nindex). We discuss this topic further in Section 20.3 (page 454).\\nThe distributed index construction method we describe in th is section is an\\napplication of MapReduce , a general architecture for distributed computing. MAPREDUCE\\nMapReduce is designed for large computer clusters. The poin t of a cluster is\\nto solve large computing problems on cheap commodity machin es or nodes\\nthat are built from standard parts (processor, memory, disk ) as opposed to on\\na supercomputer with specialized hardware. Although hundr eds or thou-\\nsands of machines are available in such clusters, individua l machines can\\nfail at any time. One requirement for robust distributed ind exing is, there-\\nfore, that we divide the work up into chunks that we can easily assign and\\n– in case of failure – reassign. A master node directs the process of assigning MASTER NODE\\nand reassigning tasks to individual worker nodes.\\nThe map and reduce phases of MapReduce split up the computing job\\ninto chunks that standard machines can process in a short tim e. The various\\nsteps of MapReduce are shown in Figure 4.5and an example on a collection\\nconsisting of two documents is shown in Figure 4.6. First, the input data,\\nin our case a collection of web pages, are split into n splits where the size of SPLITS\\nthe split is chosen to ensure that the work can be distributed evenly (chunks\\nshould not be too large) and efﬁciently (the total number of c hunks we need\\nto manage should not be too large); 16 or 64 MB are good sizes in distributed\\nindexing. Splits are not preassigned to machines, but are in stead assigned\\nby the master node on an ongoing basis: As a machine ﬁnishes pr ocessing\\none split, it is assigned the next one. If a machine dies or bec omes a laggard\\ndue to hardware problems, the split it is working on is simply reassigned to\\nanother machine.\\nIn general, MapReduce breaks a large computing problem into smaller\\nparts by recasting it in terms of manipulation of key-value pairs . For index- KEY-VALUE PAIRS\\ning, a key-value pair has the form (termID,docID). In distri buted indexing,\\nthe mapping from terms to termIDs is also distributed and the refore more\\ncomplex than in single-machine indexing. A simple solution is to maintain\\na (perhaps precomputed) mapping for frequent terms that is c opied to all\\nnodes and to use terms directly (instead of termIDs) for infr equent terms.\\nWe do not address this problem here and assume that all nodes s hare a con-\\nsistent term→termID mapping.\\nThe map phase of MapReduce consists of mapping splits of the input data MAP PHASE\\nto key-value pairs. This is the same parsing task we also enco untered in BSBI\\nand SPIMI, and we therefore call the machines that execute th e map phase\\nparsers . Each parser writes its output to local intermediate ﬁles, t hesegment PARSER\\nSEGMENT FILEﬁles(shown as a-fg-p q-z in Figure 4.5).\\nFor the reduce phase , we want all values for a given key to be stored close REDUCE PHASE\\ntogether, so that they can be read and processed quickly. Thi s is achieved by', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 111}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP76 4 Index construction\\nmaster assign\\nmap\\nphasereduce\\nphaseassign\\nparsersplits\\nparser\\nparserinver terpostings\\ninver ter\\ninver tera-f\\ng-p\\nq-za-f g-p q-z\\na-f g-p q-z\\na-f\\nsegment\\nfilesg-p q-z\\n◮Figure 4.5 An example of distributed indexing with MapReduce. Adapted from\\nDean and Ghemawat (2004 ).\\npartitioning the keys into jterm partitions and having the parsers write key-\\nvalue pairs for each term partition into a separate segment ﬁ le. In Figure 4.5,\\nthe term partitions are according to ﬁrst letter: a–f, g–p, q –z, and j=3. (We\\nchose these key ranges for ease of exposition. In general, ke y ranges need not\\ncorrespond to contiguous terms or termIDs.) The term partit ions are deﬁned\\nby the person who operates the indexing system (Exercise 4.10). The parsers\\nthen write corresponding segment ﬁles, one for each term par tition. Each\\nterm partition thus corresponds to rsegments ﬁles, where ris the number\\nof parsers. For instance, Figure 4.5shows three a–f segment ﬁles of the a–f\\npartition, corresponding to the three parsers shown in the ﬁ gure.\\nCollecting all values (here: docIDs) for a given key (here: t ermID) into one\\nlist is the task of the inverters in the reduce phase. The master assigns each INVERTER\\nterm partition to a different inverter – and, as in the case of parsers, reas-\\nsigns term partitions in case of failing or slow inverters. E ach term partition\\n(corresponding to rsegment ﬁles, one on each parser) is processed by one in-\\nverter. We assume here that segment ﬁles are of a size that a si ngle machine\\ncan handle (Exercise 4.9). Finally, the list of values is sorted for each key and\\nwritten to the ﬁnal sorted postings list (“postings” in the ﬁ gure). (Note that\\npostings in Figure 4.6include term frequencies, whereas each posting in the\\nother sections of this chapter is simply a docID without term frequency in-\\nformation.) The data ﬂow is shown for a–f in Figure 4.5. This completes the\\nconstruction of the inverted index.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 112}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP4.4 Distributed indexing 77\\nSchema of map and reduce functions\\nmap: input list(k,v)\\nreduce: ( k,list(v)) output\\nInstantiation of the schema for index construction\\nmap: web collection list(termID ,docID)\\nreduce: ( \\xa0termID ,1list(docID) , \\xa0termID  ,2list(docID) , ...) (postings list list 1,postings 2,...)\\nExample for index construction\\nmap: d2:Cdied.d1:C came ,Cc’ed. (\\xa0C,d2,\\xa0died ,d2,\\xa0C,d1,\\xa0came ,d1,\\xa0C,d1,\\xa0〈c’ed ,d1〉)\\nreduce: ( \\xa0C,(d2,d1,d1) ,\\xa0died ,(d2) ,\\xa0came ,(d1) ,\\xa0c’ed,(d1) ) ( 〈C,(d1:2,d2:1)〉,\\xa0〈died ,(d2:1)〉,\\xa0〈came ,(d1:1)〉,\\xa0〈c’ed,(d1:1)〉\\xa0)〉 〉\\n〉 〉 〉 〉〉 〉 〉 〉 〉\\n〈〈 〈\\n〈 〈 〈〈 〈 〈 〈 〈→\\n→\\n→\\n→\\n→\\n→\\n◮Figure 4.6 Map and reduce functions in MapReduce. In general, the map fu nc-\\ntion produces a list of key-value pairs. All values for a key a re collected into one\\nlist in the reduce phase. This list is then processed further . The instantiations of the\\ntwo functions and an example are shown for index constructio n. Because the map\\nphase processes documents in a distributed fashion, termID –docID pairs need not be\\nordered correctly initially as in this example. The example shows terms instead of\\ntermIDs for better readability. We abbreviate Caesar asCandconquered asc’ed.\\nParsers and inverters are not separate sets of machines. The master iden-\\ntiﬁes idle machines and assigns tasks to them. The same machi ne can be a\\nparser in the map phase and an inverter in the reduce phase. An d there are\\noften other jobs that run in parallel with index constructio n, so in between\\nbeing a parser and an inverter a machine might do some crawlin g or another\\nunrelated task.\\nTo minimize write times before inverters reduce the data, ea ch parser writes\\nits segment ﬁles to its local disk . In the reduce phase, the master communi-\\ncates to an inverter the locations of the relevant segment ﬁl es (e.g., of the r\\nsegment ﬁles of the a–f partition). Each segment ﬁle only req uires one se-\\nquential read because all data relevant to a particular inve rter were written\\nto a single segment ﬁle by the parser. This setup minimizes th e amount of\\nnetwork trafﬁc needed during indexing.\\nFigure 4.6shows the general schema of the MapReduce functions. In-\\nput and output are often lists of key-value pairs themselves , so that several\\nMapReduce jobs can run in sequence. In fact, this was the desi gn of the\\nGoogle indexing system in 2004. What we describe in this sect ion corre-\\nsponds to only one of ﬁve to ten MapReduce operations in that i ndexing\\nsystem. Another MapReduce operation transforms the term-p artitioned in-\\ndex we just created into a document-partitioned one.\\nMapReduce offers a robust and conceptually simple framewor k for imple-\\nmenting index construction in a distributed environment. B y providing a\\nsemiautomatic method for splitting index construction int o smaller tasks, it\\ncan scale to almost arbitrarily large collections, given co mputer clusters of', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 113}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP78 4 Index construction\\nsufﬁcient size.\\n?Exercise 4.3\\nFor n=15 splits, r=10 segments, and j=3 term partitions, how long would\\ndistributed index creation take for Reuters-RCV1 in a MapRe duce architecture? Base\\nyour assumptions about cluster machines on Table 4.1.\\n4.5 Dynamic indexing\\nThus far, we have assumed that the document collection is sta tic. This is ﬁne\\nfor collections that change infrequently or never (e.g., th e Bible or Shake-\\nspeare). But most collections are modiﬁed frequently with d ocuments being\\nadded, deleted, and updated. This means that new terms need t o be added\\nto the dictionary, and postings lists need to be updated for e xisting terms.\\nThe simplest way to achieve this is to periodically reconstr uct the index\\nfrom scratch. This is a good solution if the number of changes over time is\\nsmall and a delay in making new documents searchable is accep table – and\\nif enough resources are available to construct a new index wh ile the old one\\nis still available for querying.\\nIf there is a requirement that new documents be included quic kly, one solu-\\ntion is to maintain two indexes: a large main index and a small auxiliary index AUXILIARY INDEX\\nthat stores new documents. The auxiliary index is kept in mem ory. Searches\\nare run across both indexes and results merged. Deletions ar e stored in an in-\\nvalidation bit vector. We can then ﬁlter out deleted documen ts before return-\\ning the search result. Documents are updated by deleting and reinserting\\nthem.\\nEach time the auxiliary index becomes too large, we merge it i nto the main\\nindex. The cost of this merging operation depends on how we st ore the index\\nin the ﬁle system. If we store each postings list as a separate ﬁle, then the\\nmerge simply consists of extending each postings list of the main index by\\nthe corresponding postings list of the auxiliary index. In t his scheme, the\\nreason for keeping the auxiliary index is to reduce the numbe r of disk seeks\\nrequired over time. Updating each document separately requ ires up to Mave\\ndisk seeks, where Maveis the average size of the vocabulary of documents in\\nthe collection. With an auxiliary index, we only put additio nal load on the\\ndisk when we merge auxiliary and main indexes.\\nUnfortunately, the one-ﬁle-per-postings-list scheme is i nfeasible because\\nmost ﬁle systems cannot efﬁciently handle very large number s of ﬁles. The\\nsimplest alternative is to store the index as one large ﬁle, t hat is, as a concate-\\nnation of all postings lists. In reality, we often choose a co mpromise between\\nthe two extremes (Section 4.7). To simplify the discussion, we choose the\\nsimple option of storing the index as one large ﬁle here.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 114}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP4.5 Dynamic indexing 79\\nLM ERGE ADDTOKEN(indexes ,Z0,token)\\n1Z0←MERGE(Z0,{token})\\n2if|Z0|=n\\n3 then for i←0to∞\\n4 do if Ii∈indexes\\n5 then Zi+1←MERGE(Ii,Zi)\\n6 (Zi+1is a temporary index on disk.)\\n7 indexes←indexes−{Ii}\\n8 else Ii←Zi(Zibecomes the permanent index I i.)\\n9 indexes←indexes∪{Ii}\\n10 B REAK\\n11 Z0←∅\\nLOGARITHMIC MERGE()\\n1Z0←∅ (Z0is the in-memory index.)\\n2indexes←∅\\n3while true\\n4doLM ERGE ADDTOKEN(indexes ,Z0,GETNEXTTOKEN())\\n◮Figure 4.7 Logarithmic merging. Each token (termID,docID) is initial ly added to\\nin-memory index Z0by LM ERGE ADDTOKEN . LOGARITHMIC MERGE initializes Z0\\nand indexes .\\nIn this scheme, we process each posting ⌊T/n⌋times because we touch it\\nduring each of⌊T/n⌋merges where nis the size of the auxiliary index and T\\nthe total number of postings. Thus, the overall time complex ity is Θ(T2/n).\\n(We neglect the representation of terms here and consider on ly the docIDs.\\nFor the purpose of time complexity, a postings list is simply a list of docIDs.)\\nWe can do better than Θ(T2/n)by introducing log2(T/n)indexes I0,I1,\\nI2, . . . of size 20×n, 21×n, 22×n. . . . Postings percolate up this sequence of\\nindexes and are processed only once on each level. This schem e is called log- LOGARITHMIC\\nMERGING arithmic merging (Figure 4.7). As before, up to npostings are accumulated in\\nan in-memory auxiliary index, which we call Z0. When the limit nis reached,\\nthe 20×npostings in Z0are transferred to a new index I0that is created on\\ndisk. The next time Z0is full, it is merged with I0to create an index Z1of size\\n21×n. Then Z1is either stored as I1(if there isn’t already an I1) or merged\\nwith I1into Z2(ifI1exists); and so on. We service search requests by query-\\ning in-memory Z0and all currently valid indexes Iion disk and merging the\\nresults. Readers familiar with the binomial heap data struc ture2will recog-\\n2. See, for example, ( Cormen et al. 1990 , Chapter 19).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 115}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP80 4 Index construction\\nnize its similarity with the structure of the inverted index es in logarithmic\\nmerging.\\nOverall index construction time is Θ(Tlog(T/n))because each posting\\nis processed only once on each of the log (T/n)levels. We trade this efﬁ-\\nciency gain for a slow down of query processing; we now need to merge\\nresults from log (T/n)indexes as opposed to just two (the main and auxil-\\niary indexes). As in the auxiliary index scheme, we still nee d to merge very\\nlarge indexes occasionally (which slows down the search sys tem during the\\nmerge), but this happens less frequently and the indexes inv olved in a merge\\non average are smaller.\\nHaving multiple indexes complicates the maintenance of col lection-wide\\nstatistics. For example, it affects the spelling correctio n algorithm in Sec-\\ntion 3.3(page 56) that selects the corrected alternative with the most hits.\\nWith multiple indexes and an invalidation bit vector, the co rrect number of\\nhits for a term is no longer a simple lookup. In fact, all aspec ts of an IR\\nsystem – index maintenance, query processing, distributio n, and so on – are\\nmore complex in logarithmic merging.\\nBecause of this complexity of dynamic indexing, some large s earch engines\\nadopt a reconstruction-from-scratch strategy. They do not construct indexes\\ndynamically. Instead, a new index is built from scratch peri odically. Query\\nprocessing is then switched from the new index and the old ind ex is deleted.\\n?Exercise 4.4\\nForn=2 and 1≤T≤30, perform a step-by-step simulation of the algorithm in\\nFigure 4.7. Create a table that shows, for each point in time at which T=2∗ktokens\\nhave been processed (1 ≤k≤15), which of the three indexes I0, . . . , I3are in use. The\\nﬁrst three lines of the table are given below.\\nI3I2I1I0\\n20 0 0 0\\n40 0 0 1\\n60 0 1 0\\n4.6 Other types of indexes\\nThis chapter only describes construction of nonpositional indexes. Except\\nfor the much larger data volume we need to accommodate, the ma in differ-\\nence for positional indexes is that (termID, docID, (positi on1, position2, . . . ))\\ntriples, instead of (termID, docID) pairs have to be process ed and that tokens\\nand postings contain positional information in addition to docIDs. With this\\nchange, the algorithms discussed here can all be applied to p ositional in-\\ndexes.\\nIn the indexes we have considered so far, postings lists are o rdered with\\nrespect to docID. As we see in Chapter 5, this is advantageous for compres-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 116}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP4.6 Other types of indexes 81\\nusersdocuments\\n0/1\\ndoc e., 1 otherwis0 if user can’t read\\n◮Figure 4.8 A user-document matrix for access control lists. Element (i,j)is 1 if\\nuser ihas access to document jand 0 otherwise. During query processing, a user’s\\naccess postings list is intersected with the results list re turned by the text part of the\\nindex.\\nsion – instead of docIDs we can compress smaller gaps between IDs, thus\\nreducing space requirements for the index. However, this st ructure for the\\nindex is not optimal when we build ranked (Chapters 6and 7) – as opposed to RANKED\\nBoolean – retrieval systems . In ranked retrieval, postings are often ordered ac- RETRIEVAL SYSTEMS\\ncording to weight or impact, with the highest-weighted post ings occurring\\nﬁrst. With this organization, scanning of long postings lis ts during query\\nprocessing can usually be terminated early when weights hav e become so\\nsmall that any further documents can be predicted to be of low similarity\\nto the query (see Chapter 6). In a docID-sorted index, new documents are\\nalways inserted at the end of postings lists. In an impact-so rted index (Sec-\\ntion 7.1.5 , page 140), the insertion can occur anywhere, thus complicating the\\nupdate of the inverted index.\\nSecurity is an important consideration for retrieval systems in corp orations. SECURITY\\nA low-level employee should not be able to ﬁnd the salary rost er of the cor-\\nporation, but authorized managers need to be able to search f or it. Users’\\nresults lists must not contain documents they are barred fro m opening; the\\nvery existence of a document can be sensitive information.\\nUser authorization is often mediated through access control lists or ACLs. ACCESS CONTROL LISTS\\nACLs can be dealt with in an information retrieval system by r epresenting\\neach document as the set of users that can access them (Figure 4.8) and then\\ninverting the resulting user-document matrix. The inverte d ACL index has,\\nfor each user, a “postings list” of documents they can access – the user’s ac-\\ncess list. Search results are then intersected with this lis t. However, such\\nan index is difﬁcult to maintain when access permissions cha nge – we dis-\\ncussed these difﬁculties in the context of incremental inde xing for regular\\npostings lists in Section 4.5. It also requires the processi ng of very long post-\\nings lists for users with access to large document subsets. U ser membership\\nis therefore often veriﬁed by retrieving access informatio n directly from the\\nﬁle system at query time – even though this slows down retriev al.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 117}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP82 4 Index construction\\n◮Table 4.3 The ﬁve steps in constructing an index for Reuters-RCV1 in bl ocked\\nsort-based indexing. Line numbers refer to Figure 4.2.\\nStep Time\\n1 reading of collection (line 4)\\n2 10 initial sorts of 107records each (line 5)\\n3 writing of 10 blocks (line 6)\\n4 total disk transfer time for merging (line 7)\\n5 time of actual merging (line 7)\\ntotal\\n◮Table 4.4 Collection statistics for a large collection.\\nSymbol Statistic Value\\nN # documents 1,000,000,000\\nLave # tokens per document 1000\\nM # distinct terms 44,000,000\\nWe discussed indexes for storing and retrieving terms (as op posed to doc-\\numents) in Chapter 3.\\n?Exercise 4.5\\nCan spelling correction compromise document-level securi ty? Consider the case where\\na spelling correction is based on documents to which the user does not have access.\\n?Exercise 4.6\\nTotal index construction time in blocked sort-based indexi ng is broken down in Ta-\\nble4.3. Fill out the time column of the table for Reuters-RCV1 assum ing a system\\nwith the parameters given in Table 4.1.\\nExercise 4.7\\nRepeat Exercise 4.6for the larger collection in Table 4.4. Choose a block size that is\\nrealistic for current technology (remember that a block sho uld easily ﬁt into main\\nmemory). How many blocks do you need?\\nExercise 4.8\\nAssume that we have a collection of modest size whose index ca n be constructed with\\nthe simple in-memory indexing algorithm in Figure 1.4(page 8). For this collection,\\ncompare memory, disk and time requirements of the simple alg orithm in Figure 1.4\\nand blocked sort-based indexing.\\nExercise 4.9\\nAssume that machines in MapReduce have 100 GB of disk space ea ch. Assume fur-\\nther that the postings list of the term thehas a size of 200 GB. Then the MapReduce\\nalgorithm as described cannot be run to construct the index. How would you modify\\nMapReduce so that it can handle this case?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 118}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP4.7 References and further reading 83\\nExercise 4.10\\nFor optimal load balancing, the inverters in MapReduce must get segmented postings\\nﬁles of similar sizes. For a new collection, the distributio n of key-value pairs may not\\nbe known in advance. How would you solve this problem?\\nExercise 4.11\\nApply MapReduce to the problem of counting how often each ter m occurs in a set of\\nﬁles. Specify map and reduce operations for this task. Write down an example along\\nthe lines of Figure 4.6.\\nExercise 4.12\\nWe claimed (on page 80) that an auxiliary index can impair the quality of collec-\\ntion statistics. An example is the term weighting method idf , which is deﬁned as\\nlog(N/df i)where Nis the total number of documents and df iis the number of docu-\\nments that term ioccurs in (Section 6.2.1 , page 117). Show that even a small auxiliary\\nindex can cause signiﬁcant error in idf when it is computed on the main index only.\\nConsider a rare term that suddenly occurs frequently (e.g., Flossie as inTropical Storm\\nFlossie ).\\n4.7 References and further reading\\nWitten et al. (1999 , Chapter 5) present an extensive treatment of the subject of\\nindex construction and additional indexing algorithms wit h different trade-\\noffs of memory, disk space, and time. In general, blocked sor t-based indexing\\ndoes well on all three counts. However, if conserving memory or disk space\\nis the main criterion, then other algorithms may be a better c hoice. See Wit-\\nten et al. (1999 ), Tables 5.4 and 5.5; BSBI is closest to “sort-based multiwa y\\nmerge,” but the two algorithms differ in dictionary structu re and use of com-\\npression.\\nMoffat and Bell (1995 ) show how to construct an index “in situ,” that\\nis, with disk space usage close to what is needed for the ﬁnal i ndex and\\nwith a minimum of additional temporary ﬁles (cf. also Harman and Candela\\n(1990 )). They give Lesk (1988 ) and Somogyi (1990 ) credit for being among\\nthe ﬁrst to employ sorting for index construction.\\nThe SPIMI method in Section 4.3is from ( Heinz and Zobel 2003 ). We have\\nsimpliﬁed several aspects of the algorithm, including comp ression and the\\nfact that each term’s data structure also contains, in addit ion to the postings\\nlist, its document frequency and house keeping information . We recommend\\nHeinz and Zobel (2003 ) and Zobel and Moffat (2006 ) as up-do-date, in-depth\\ntreatments of index construction. Other algorithms with go od scaling prop-\\nerties with respect to vocabulary size require several pass es through the data,\\ne.g., FAST-INV ( Fox and Lee 1991 ,Harman et al. 1992 ).\\nThe MapReduce architecture was introduced by Dean and Ghemawat (2004 ).\\nAn open source implementation of MapReduce is available at http://lucene.apache.org/hadoop/ .\\nRibeiro-Neto et al. (1999 ) and Melnik et al. (2001 ) describe other approaches', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 119}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP84 4 Index construction\\nto distributed indexing. Introductory chapters on distrib uted IR are ( Baeza-\\nYates and Ribeiro-Neto 1999 , Chapter 9) and ( Grossman and Frieder 2004 ,\\nChapter 8). See also Callan (2000 ).\\nLester et al. (2005 ) and Büttcher and Clarke (2005a ) analyze the proper-\\nties of logarithmic merging and compare it with other constr uction methods.\\nOne of the ﬁrst uses of this method was in Lucene ( http://lucene.apache.org ).\\nOther dynamic indexing methods are discussed by Büttcher et al. (2006 ) and\\nLester et al. (2006 ). The latter paper also discusses the strategy of replacing\\nthe old index by one built from scratch.\\nHeinz et al. (2002 ) compare data structures for accumulating the vocabu-\\nlary in memory. Büttcher and Clarke (2005b ) discuss security models for a\\ncommon inverted index for multiple users. A detailed charac terization of the\\nReuters-RCV1 collection can be found in ( Lewis et al. 2004 ). NIST distributes\\nthe collection (see http://trec.nist.gov/data/reuters/reuters.html ).\\nGarcia-Molina et al. (1999 , Chapter 2) review computer hardware relevant\\nto system design in depth.\\nAn effective indexer for enterprise search needs to be able t o communicate\\nefﬁciently with a number of applications that hold text data in corporations,\\nincluding Microsoft Outlook, IBM’s Lotus software, databa ses like Oracle\\nand MySQL, content management systems like Open Text, and en terprise\\nresource planning software like SAP .', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 120}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 85\\n5 Index compression\\nChapter 1introduced the dictionary and the inverted index as the cent ral\\ndata structures in information retrieval (IR). In this chap ter, we employ a\\nnumber of compression techniques for dictionary and invert ed index that\\nare essential for efﬁcient IR systems.\\nOne beneﬁt of compression is immediately clear. We need less disk space.\\nAs we will see, compression ratios of 1:4 are easy to achieve, potentially cut-\\nting the cost of storing the index by 75%.\\nThere are two more subtle beneﬁts of compression. The ﬁrst is increased\\nuse of caching. Search systems use some parts of the dictiona ry and the index\\nmuch more than others. For example, if we cache the postings l ist of a fre-\\nquently used query term t, then the computations necessary for responding\\nto the one-term query tcan be entirely done in memory. With compression,\\nwe can ﬁt a lot more information into main memory. Instead of h aving to\\nexpend a disk seek when processing a query with t, we instead access its\\npostings list in memory and decompress it. As we will see belo w, there are\\nsimple and efﬁcient decompression methods, so that the pena lty of having to\\ndecompress the postings list is small. As a result, we are abl e to decrease the\\nresponse time of the IR system substantially. Because memor y is a more ex-\\npensive resource than disk space, increased speed owing to c aching – rather\\nthan decreased space requirements – is often the prime motiv ator for com-\\npression.\\nThe second more subtle advantage of compression is faster tr ansfer of data\\nfrom disk to memory. Efﬁcient decompression algorithms run so fast on\\nmodern hardware that the total time of transferring a compre ssed chunk of\\ndata from disk and then decompressing it is usually less than transferring\\nthe same chunk of data in uncompressed form. For instance, we can reduce\\ninput/output (I/O) time by loading a much smaller compresse d postings\\nlist, even when you add on the cost of decompression. So, in mo st cases,\\nthe retrieval system runs faster on compressed postings lis ts than on uncom-\\npressed postings lists.\\nIf the main goal of compression is to conserve disk space, the n the speed', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 121}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP86 5 Index compression\\nof compression algorithms is of no concern. But for improved cache uti-\\nlization and faster disk-to-memory transfer, decompressi on speeds must be\\nhigh. The compression algorithms we discuss in this chapter are highly efﬁ-\\ncient and can therefore serve all three purposes of index com pression.\\nIn this chapter, we deﬁne a posting as a docID in a postings list. For exam- POSTING\\nple, the postings list (6; 20, 45, 100), where 6 is the termID o f the list’s term,\\ncontains three postings. As discussed in Section 2.4.2 (page 41), postings in\\nmost search systems also contain frequency and position inf ormation; but we\\nwill only consider simple docID postings here. See Section 5.4for references\\non compressing frequencies and positions.\\nThis chapter ﬁrst gives a statistical characterization of t he distribution of\\nthe entities we want to compress – terms and postings in large collections\\n(Section 5.1). We then look at compression of the dictionary, using the di ctionary-\\nas-a-string method and blocked storage (Section 5.2). Section 5.3describes\\ntwo techniques for compressing the postings ﬁle, variable b yte encoding and\\nγencoding.\\n5.1 Statistical properties of terms in information retriev al\\nAs in the last chapter, we use Reuters-RCV1 as our model colle ction (see Ta-\\nble4.2, page 70). We give some term and postings statistics for the collecti on\\nin Table 5.1. “∆%” indicates the reduction in size from the previous line.\\n“T%” is the cumulative reduction from unﬁltered.\\nThe table shows the number of terms for different levels of pr eprocessing\\n(column 2). The number of terms is the main factor in determin ing the size\\nof the dictionary. The number of nonpositional postings (co lumn 3) is an\\nindicator of the expected size of the nonpositional index of the collection.\\nThe expected size of a positional index is related to the numb er of positions\\nit must encode (column 4).\\nIn general, the statistics in Table 5.1show that preprocessing affects the size\\nof the dictionary and the number of nonpositional postings g reatly. Stem-\\nming and case folding reduce the number of (distinct) terms b y 17% each\\nand the number of nonpositional postings by 4% and 3%, respec tively. The\\ntreatment of the most frequent words is also important. The rule of 30 states RULE OF 30\\nthat the 30 most common words account for 30% of the tokens in w ritten text\\n(31% in the table). Eliminating the 150 most common words fro m indexing\\n(as stop words; cf. Section 2.2.2 , page 27) cuts 25% to 30% of the nonpositional\\npostings. But, although a stop list of 150 words reduces the n umber of post-\\nings by a quarter or more, this size reduction does not carry o ver to the size\\nof the compressed index. As we will see later in this chapter, the postings\\nlists of frequent words require only a few bits per posting af ter compression.\\nThe deltas in the table are in a range typical of large collect ions. Note,', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 122}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP5.1 Statistical properties of terms in information retriev al 87\\n◮Table 5.1 The effect of preprocessing on the number of terms, nonposit ional post-\\nings, and tokens for Reuters-RCV1. “ ∆%” indicates the reduction in size from the pre-\\nvious line, except that “30 stop words” and “150 stop words” b oth use “case folding”\\nas their reference line. “T%” is the cumulative (“total”) re duction from unﬁltered. We\\nperformed stemming with the Porter stemmer (Chapter 2, page 33).\\ntokens ( =number of position\\n(distinct) terms nonpositional postings entries in postin gs)\\nnumber ∆% T% number ∆% T% number ∆% T%\\nunﬁltered 484,494 109,971,179 197,879,290\\nno numbers 473,723 −2−2 100,680,242 −8−8 179,158,204 −9−9\\ncase folding 391,523 −17−19 96,969,056 −3−12 179,158,204 −0−9\\n30 stop words 391,493 −0−19 83,390,443 −14−24 121,857,825 −31−38\\n150 stop words 391,373 −0−19 67,001,847 −30−39 94,516,599 −47−52\\nstemming 322,383 −17−33 63,812,300 −4−42 94,516,599 −0−52\\nhowever, that the percentage reductions can be very differe nt for some text\\ncollections. For example, for a collection of web pages with a high proportion\\nof French text, a lemmatizer for French reduces vocabulary s ize much more\\nthan the Porter stemmer does for an English-only collection because French\\nis a morphologically richer language than English.\\nThe compression techniques we describe in the remainder of t his chapter\\narelossless , that is, all information is preserved. Better compression ratios LOSSLESS\\ncan be achieved with lossy compression , which discards some information. LOSSY COMPRESSION\\nCase folding, stemming, and stop word elimination are forms of lossy com-\\npression. Similarly, the vector space model (Chapter 6) and dimensionality\\nreduction techniques like latent semantic indexing (Chapt er18) create com-\\npact representations from which we cannot fully restore the original collec-\\ntion. Lossy compression makes sense when the “lost” informa tion is unlikely\\never to be used by the search system. For example, web search i s character-\\nized by a large number of documents, short queries, and users who only look\\nat the ﬁrst few pages of results. As a consequence, we can disc ard postings of\\ndocuments that would only be used for hits far down the list. T hus, there are\\nretrieval scenarios where lossy methods can be used for comp ression without\\nany reduction in effectiveness.\\nBefore introducing techniques for compressing the diction ary, we want to\\nestimate the number of distinct terms Min a collection. It is sometimes said\\nthat languages have a vocabulary of a certain size. The secon d edition of\\ntheOxford English Dictionary (OED) deﬁnes more than 600,000 words. But\\nthe vocabulary of most large collections is much larger than the OED. The\\nOED does not include most names of people, locations, produc ts, or scientiﬁc', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 123}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP88 5 Index compression\\n0246 80123456\\nlog10 Tlog10 M\\n◮Figure 5.1 Heaps’ law. Vocabulary size Mas a function of collection size T\\n(number of tokens) for Reuters-RCV1. For these data, the das hed line log10M=\\n0.49∗log10T+1.64 is the best least-squares ﬁt. Thus, k=101.64≈44 and b=0.49.\\nentities like genes. These names need to be included in the in verted index,\\nso our users can search for them.\\n5.1.1 Heaps’ law: Estimating the number of terms\\nA better way of getting a handle on MisHeaps’ law , which estimates vocab- HEAPS ’LAW\\nulary size as a function of collection size:\\nM=kTb(5.1)\\nwhere Tis the number of tokens in the collection. Typical values for the\\nparameters kand bare: 30≤k≤100 and b≈0.5. The motivation for\\nHeaps’ law is that the simplest possible relationship betwe en collection size\\nand vocabulary size is linear in log–log space and the assump tion of linearity\\nis usually born out in practice as shown in Figure 5.1for Reuters-RCV1. In\\nthis case, the ﬁt is excellent for T>105=100,000, for the parameter values\\nb=0.49 and k=44. For example, for the ﬁrst 1,000,020 tokens Heaps’ law', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 124}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP5.1 Statistical properties of terms in information retriev al 89\\npredicts 38,323 terms:\\n44×1,000,0200.49≈38,323.\\nThe actual number is 38,365 terms, very close to the predicti on.\\nThe parameter kis quite variable because vocabulary growth depends a\\nlot on the nature of the collection and how it is processed. Ca se-folding and\\nstemming reduce the growth rate of the vocabulary, whereas i ncluding num-\\nbers and spelling errors increase it. Regardless of the valu es of the param-\\neters for a particular collection, Heaps’ law suggests that (i) the dictionary\\nsize continues to increase with more documents in the collec tion, rather than\\na maximum vocabulary size being reached, and (ii) the size of the dictionary\\nis quite large for large collections. These two hypotheses h ave been empir-\\nically shown to be true of large text collections (Section 5.4). So dictionary\\ncompression is important for an effective information retr ieval system.\\n5.1.2 Zipf’s law: Modeling the distribution of terms\\nWe also want to understand how terms are distributed across d ocuments.\\nThis helps us to characterize the properties of the algorith ms for compressing\\npostings lists in Section 5.3.\\nA commonly used model of the distribution of terms in a collec tion is Zipf’s ZIPF’S LAW\\nlaw. It states that, if t1is the most common term in the collection, t2is the\\nnext most common, and so on, then the collection frequency cf iof the ith\\nmost common term is proportional to 1/ i:\\ncfi∝1\\ni. (5.2)\\nSo if the most frequent term occurs cf 1times, then the second most frequent\\nterm has half as many occurrences, the third most frequent te rm a third as\\nmany occurrences, and so on. The intuition is that frequency decreases very\\nrapidly with rank. Equation ( 5.2) is one of the simplest ways of formalizing\\nsuch a rapid decrease and it has been found to be a reasonably g ood model.\\nEquivalently, we can write Zipf’s law as cf i=cikor as log cf i=logc+\\nklogiwhere k=−1 and cis a constant to be deﬁned in Section 5.3.2 . It\\nis therefore a power law with exponent k=−1. See Chapter 19, page 426, POWER LAW\\nfor another power law, a law characterizing the distributio n of links on web\\npages.\\nThe log–log graph in Figure 5.2plots the collection frequency of a term as\\na function of its rank for Reuters-RCV1. A line with slope –1, corresponding\\nto the Zipf function log cf i=logc−logi, is also shown. The ﬁt of the data\\nto the law is not particularly good, but good enough to serve a s a model for\\nterm distributions in our calculations in Section 5.3.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 125}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP90 5 Index compression\\n01234560123456 7\\nlog10 rank7log10 cf\\n◮Figure 5.2 Zipf’s law for Reuters-RCV1. Frequency is plotted as a funct ion of\\nfrequency rank for the terms in the collection. The line is th e distribution predicted\\nby Zipf’s law (weighted least-squares ﬁt; intercept is 6.95 ).\\n?Exercise 5.1 [⋆]\\nAssuming one machine word per posting, what is the size of the uncompressed (non-\\npositional) index for different tokenizations based on Tab le5.1? How do these num-\\nbers compare with Table 5.6?\\n5.2 Dictionary compression\\nThis section presents a series of dictionary data structure s that achieve in-\\ncreasingly higher compression ratios. The dictionary is sm all compared with\\nthe postings ﬁle as suggested by Table 5.1. So why compress it if it is respon-\\nsible for only a small percentage of the overall space requir ements of the IR\\nsystem?\\nOne of the primary factors in determining the response time o f an IR sys-\\ntem is the number of disk seeks necessary to process a query. I f parts of the\\ndictionary are on disk, then many more disk seeks are necessa ry in query\\nevaluation. Thus, the main goal of compressing the dictiona ry is to ﬁt it in\\nmain memory, or at least a large portion of it, to support high query through-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 126}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP5.2 Dictionary compression 91\\nterm document\\nfrequencypointer to\\npostings list\\na 656,265 −→\\naachen 65 −→\\n. . . . . . . . .\\nzulu 221 −→\\nspace needed: 20 bytes 4 bytes 4 bytes\\n◮Figure 5.3 Storing the dictionary as an array of ﬁxed-width entries.\\nput. Although dictionaries of very large collections ﬁt int o the memory of a\\nstandard desktop machine, this is not true of many other appl ication scenar-\\nios. For example, an enterprise search server for a large cor poration may\\nhave to index a multiterabyte collection with a comparative ly large vocab-\\nulary because of the presence of documents in many different languages.\\nWe also want to be able to design search systems for limited ha rdware such\\nas mobile phones and onboard computers. Other reasons for wa nting to\\nconserve memory are fast startup time and having to share res ources with\\nother applications. The search system on your PC must get alo ng with the\\nmemory-hogging word processing suite you are using at the sa me time.\\n5.2.1 Dictionary as a string\\nThe simplest data structure for the dictionary is to sort the vocabulary lex-\\nicographically and store it in an array of ﬁxed-width entrie s as shown in\\nFigure 5.3. We allocate 20 bytes for the term itself (because few terms h ave\\nmore than twenty characters in English), 4 bytes for its docu ment frequency,\\nand 4 bytes for the pointer to its postings list. Four-byte po inters resolve a\\n4 gigabytes (GB) address space. For large collections like t he web, we need\\nto allocate more bytes per pointer. We look up terms in the arr ay by binary\\nsearch. For Reuters-RCV1, we need M×(20+4+4) = 400,000×28=\\n11.2megabytes (MB) for storing the dictionary in this schem e.\\nUsing ﬁxed-width entries for terms is clearly wasteful. The average length\\nof a term in English is about eight characters (Table 4.2, page 70), so on av-\\nerage we are wasting twelve characters in the ﬁxed-width sch eme. Also,\\nwe have no way of storing terms with more than twenty characte rs like\\nhydrochloroﬂuorocarbons andsupercalifragilisticexpialidocious . We can overcome\\nthese shortcomings by storing the dictionary terms as one lo ng string of char-\\nacters, as shown in Figure 5.4. The pointer to the next term is also used to\\ndemarcate the end of the current term. As before, we locate te rms in the data\\nstructure by way of binary search in the (now smaller) table. This scheme\\nsaves us 60% compared to ﬁxed-width storage – 12 bytes on aver age of the', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 127}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP92 5 Index compression\\n... sys tilesyzygeticsyzygialsyzygyszaibelyiteszecinszono .  .  .\\nfreq.\\n9\\n92\\n5\\n71\\n12\\n...\\n4bytespostings ptr.\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n...\\n4bytesterm ptr.\\n3bytes...→\\n→\\n→\\n→\\n→\\n◮Figure 5.4 Dictionary-as-a-string storage. Pointers mark the end of t he preceding\\nterm and the beginning of the next. For example, the ﬁrst thre e terms in this example\\naresystile ,syzygetic , andsyzygial .\\n20 bytes we allocated for terms before. However, we now also n eed to store\\nterm pointers. The term pointers resolve 400,000 ×8=3.2×106positions,\\nso they need to be log23.2×106≈22 bits or 3 bytes long.\\nIn this new scheme, we need 400,000 ×(4+4+3+8) = 7.6 MB for the\\nReuters-RCV1 dictionary: 4 bytes each for frequency and pos tings pointer, 3\\nbytes for the term pointer, and 8 bytes on average for the term . So we have\\nreduced the space requirements by one third from 11.2 to 7.6 M B.\\n5.2.2 Blocked storage\\nWe can further compress the dictionary by grouping terms in t he string into\\nblocks of size kand keeping a term pointer only for the ﬁrst term of each\\nblock (Figure 5.5). We store the length of the term in the string as an ad-\\nditional byte at the beginning of the term. We thus eliminate k−1 term\\npointers, but need an additional kbytes for storing the length of each term.\\nFork=4, we save (k−1)×3=9 bytes for term pointers, but need an ad-\\nditional k=4 bytes for term lengths. So the total space requirements for the\\ndictionary of Reuters-RCV1 are reduced by 5 bytes per four-t erm block, or a\\ntotal of 400,000×1/4×5=0.5 MB, bringing us down to 7.1 MB.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 128}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP5.2 Dictionary compression 93\\n... 7sys tile9syzygetic8syzygial6syzygy 11szaibelyite6szecin.  .  .\\nfreq.\\n9\\n92\\n5\\n71\\n12\\n...postings ptr.\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n...term ptr.\\n...→\\n→\\n→\\n→\\n→\\n◮Figure 5.5 Blocked storage with four terms per block. The ﬁrst block con sists of\\nsystile ,syzygetic ,syzygial , andsyzygy with lengths of seven, nine, eight, and six charac-\\nters, respectively. Each term is preceded by a byte encoding its length that indicates\\nhow many bytes to skip to reach subsequent terms.\\nBy increasing the block size k, we get better compression. However, there\\nis a tradeoff between compression and the speed of term looku p. For the\\neight-term dictionary in Figure 5.6, steps in binary search are shown as dou-\\nble lines and steps in list search as simple lines. We search f or terms in the un-\\ncompressed dictionary by binary search (a). In the compress ed dictionary, we\\nﬁrst locate the term’s block by binary search and then its pos ition within the\\nlist by linear search through the block (b). Searching the un compressed dic-\\ntionary in (a) takes on average (0+1+2+3+2+1+2+2)/8≈1.6 steps,\\nassuming each term is equally likely to come up in a query. For example,\\nﬁnding the two terms, aidandbox, takes three and two steps, respectively.\\nWith blocks of size k=4 in (b), we need (0+1+2+3+4+1+2+3)/8=2\\nsteps on average, ≈25% more. For example, ﬁnding dentakes one binary\\nsearch step and two steps through the block. By increasing k, we can get\\nthe size of the compressed dictionary arbitrarily close to t he minimum of\\n400,000×(4+4+1+8) = 6.8 MB, but term lookup becomes prohibitively\\nslow for large values of k.\\nOne source of redundancy in the dictionary we have not exploi ted yet is\\nthe fact that consecutive entries in an alphabetically sort ed list share common\\npreﬁxes. This observation leads to front coding (Figure 5.7). A common preﬁx FRONT CODING', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 129}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP94 5 Index compression\\n(a) aid\\nbox\\nden\\nex\\njob\\nox\\npit\\nwin\\n(b) aid box den ex\\njob ox pit win\\n◮Figure 5.6 Search of the uncompressed dictionary (a) and a dictionary c om-\\npressed by blocking with k=4 (b).\\nOne block in blocked compression ( k=4) . . .\\n8automata 8automate 9au tomatic 10automation\\n⇓\\n. . . further compressed with front coding.\\n8automat∗a1⋄e2⋄ic3⋄i on\\n◮Figure 5.7 Front coding. A sequence of terms with identical preﬁx (“aut omat”) is\\nencoded by marking the end of the preﬁx with ∗and replacing it with ⋄in subsequent\\nterms. As before, the ﬁrst byte of each entry encodes the numb er of characters.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 130}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP5.3 Postings ﬁle compression 95\\n◮Table 5.2 Dictionary compression for Reuters-RCV1.\\ndata structure size in MB\\ndictionary, ﬁxed-width 11.2\\ndictionary, term pointers into string 7.6\\n∼, with blocking, k=4 7.1\\n∼, with blocking & front coding 5.9\\nis identiﬁed for a subsequence of the term list and then refer red to with a\\nspecial character. In the case of Reuters, front coding save s another 1.2 MB,\\nas we found in an experiment.\\nOther schemes with even greater compression rely on minimal perfect\\nhashing, that is, a hash function that maps Mterms onto [1, . . . , M]without\\ncollisions. However, we cannot adapt perfect hashes increm entally because\\neach new term causes a collision and therefore requires the c reation of a new\\nperfect hash function. Therefore, they cannot be used in a dy namic environ-\\nment.\\nEven with the best compression scheme, it may not be feasible to store\\nthe entire dictionary in main memory for very large text coll ections and for\\nhardware with limited memory. If we have to partition the dic tionary onto\\npages that are stored on disk, then we can index the ﬁrst term o f each page\\nusing a B-tree. For processing most queries, the search syst em has to go to\\ndisk anyway to fetch the postings. One additional seek for re trieving the\\nterm’s dictionary page from disk is a signiﬁcant, but tolera ble increase in the\\ntime it takes to process a query.\\nTable 5.2summarizes the compression achieved by the four dictionary\\ndata structures.\\n?Exercise 5.2\\nEstimate the space usage of the Reuters-RCV1 dictionary wit h blocks of size k=8\\nand k=16 in blocked dictionary storage.\\nExercise 5.3\\nEstimate the time needed for term lookup in the compressed di ctionary of Reuters-\\nRCV1 with block sizes of k=4 (Figure 5.6, b), k=8, and k=16. What is the\\nslowdown compared with k=1 (Figure 5.6, a)?\\n5.3 Postings ﬁle compression\\nRecall from Table 4.2(page 70) that Reuters-RCV1 has 800,000 documents,\\n200 tokens per document, six characters per token, and 100,0 00,000 post-\\nings where we deﬁne a posting in this chapter as a docID in a pos tings\\nlist, that is, excluding frequency and position informatio n. These numbers', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 131}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP96 5 Index compression\\n◮Table 5.3 Encoding gaps instead of document IDs. For example, we store gaps\\n107, 5, 43, . . . , instead of docIDs 283154, 283159, 283202, . . . forcomputer . The ﬁrst\\ndocID is left unchanged (only shown for arachnocentric ).\\nencoding postings list\\nthe docIDs . . . 283042 283043 283044 283045\\ngaps 1 1 1\\ncomputer docIDs . . . 283047 283154 283159 283202\\ngaps 107 5 43\\narachnocentric docIDs 252000 500100\\ngaps 252000 248100\\ncorrespond to line 3 (“case folding”) in Table 5.1. Document identiﬁers are\\nlog2800,000≈20 bits long. Thus, the size of the collection is about 800,00 0×\\n200×6 bytes =960 MB and the size of the uncompressed postings ﬁle is\\n100,000,000×20/8=250 MB.\\nTo devise a more efﬁcient representation of the postings ﬁle , one that uses\\nfewer than 20 bits per document, we observe that the postings for frequent\\nterms are close together. Imagine going through the documen ts of a collec-\\ntion one by one and looking for a frequent term like computer . We will ﬁnd\\na document containing computer , then we skip a few documents that do not\\ncontain it, then there is again a document with the term and so on (see Ta-\\nble5.3). The key idea is that the gaps between postings are short, requiring a\\nlot less space than 20 bits to store. In fact, gaps for the most frequent terms\\nsuch astheandforare mostly equal to 1. But the gaps for a rare term that\\noccurs only once or twice in a collection (e.g., arachnocentric in Table 5.3) have\\nthe same order of magnitude as the docIDs and need 20 bits. For an econom-\\nical representation of this distribution of gaps, we need a variable encoding\\nmethod that uses fewer bits for short gaps.\\nTo encode small numbers in less space than large numbers, we l ook at two\\ntypes of methods: bytewise compression and bitwise compres sion. As the\\nnames suggest, these methods attempt to encode gaps with the minimum\\nnumber of bytes and bits, respectively.\\n5.3.1 Variable byte codes\\nVariable byte (VB) encoding uses an integral number of bytes to encode a gap. VARIABLE BYTE\\nENCODING The last 7 bits of a byte are “payload” and encode part of the ga p. The ﬁrst\\nbit of the byte is a continuation bit .It is set to 1 for the last byte of the encoded CONTINUATION BIT\\ngap and to 0 otherwise. To decode a variable byte code, we read a sequence\\nof bytes with continuation bit 0 terminated by a byte with con tinuation bit 1.\\nWe then extract and concatenate the 7-bit parts. Figure 5.8gives pseudocode', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 132}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP5.3 Postings ﬁle compression 97\\nVBE NCODE NUMBER (n)\\n1bytes←⟨⟩\\n2while true\\n3doPREPEND (bytes ,nmod 128 )\\n4 ifn<128\\n5 then BREAK\\n6 n←ndiv 128\\n7bytes[LENGTH (bytes)] += 128\\n8return bytes\\nVBE NCODE (numbers )\\n1bytestream←⟨⟩\\n2for each n∈numbers\\n3dobytes←VBE NCODE NUMBER (n)\\n4 bytestream←EXTEND (bytestream ,bytes)\\n5return bytestream\\nVBD ECODE (bytestream )\\n1numbers←⟨⟩\\n2n←0\\n3fori←1toLENGTH (bytestream )\\n4do if bytestream [i]<128\\n5 then n←128×n+bytestream [i]\\n6 else n←128×n+ (bytestream [i]−128)\\n7 A PPEND (numbers ,n)\\n8 n←0\\n9return numbers\\n◮Figure 5.8 VB encoding and decoding. The functions div and mod compute\\ninteger division and remainder after integer division, res pectively. P REPEND adds an\\nelement to the beginning of a list, for example, P REPEND (⟨1, 2⟩, 3) =⟨3, 1, 2⟩. EXTEND\\nextends a list, for example, E XTEND (⟨1,2⟩,⟨3, 4⟩) =⟨1, 2, 3, 4⟩.\\n◮Table 5.4 VB encoding. Gaps are encoded using an integral number of byt es.\\nThe ﬁrst bit, the continuation bit, of each byte indicates wh ether the code ends with\\nthis byte (1) or not (0).\\ndocIDs 824 829 215406\\ngaps 5 214577\\nVB code 00000110 10111000 10000101 00001101 00001100 10110 001', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 133}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP98 5 Index compression\\n◮Table 5.5 Some examples of unary and γcodes. Unary codes are only shown for\\nthe smaller numbers. Commas in γcodes are for readability only and are not part of\\nthe actual codes.\\nnumber unary code length offset γcode\\n0 0\\n1 10 0 0\\n2 110 10 0 10,0\\n3 1110 10 1 10,1\\n4 11110 110 00 110,00\\n9 1111111110 1110 001 1110,001\\n13 1110 101 1110,101\\n24 11110 1000 11110,1000\\n511 111111110 11111111 111111110,11111111\\n1025 11111111110 0000000001 11111111110,0000000001\\nfor VB encoding and decoding and Table 5.4an example of a VB-encoded\\npostings list.1\\nWith VB compression, the size of the compressed index for Reu ters-RCV1\\nis 116 MB as we veriﬁed in an experiment. This is a more than 50% reduction\\nof the size of the uncompressed index (see Table 5.6).\\nThe idea of VB encoding can also be applied to larger or smalle r units than\\nbytes: 32-bit words, 16-bit words, and 4-bit words or nibbles . Larger words NIBBLE\\nfurther decrease the amount of bit manipulation necessary a t the cost of less\\neffective (or no) compression. Word sizes smaller than byte s get even better\\ncompression ratios at the cost of more bit manipulation. In g eneral, bytes\\noffer a good compromise between compression ratio and speed of decom-\\npression.\\nFor most IR systems variable byte codes offer an excellent tr adeoff between\\ntime and space. They are also simple to implement – most of the alternatives\\nreferred to in Section 5.4are more complex. But if disk space is a scarce\\nresource, we can achieve better compression ratios by using bit-level encod-\\nings, in particular two closely related encodings: γcodes, which we will turn\\nto next, and δcodes (Exercise 5.9).\\n✄5.3.2 γcodes\\nVB codes use an adaptive number of bytes depending on the size of the gap.\\nBit-level codes adapt the length of the code on the ﬁner grain edbitlevel. The\\n1. Note that the origin is 0 in the table. Because we never need to encode a docID or a gap of\\n0, in practice the origin is usually 1, so that 10000000 encod es 1, 10000101 encodes 6 (not 5 as in\\nthe table), and so on.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 134}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP5.3 Postings ﬁle compression 99\\nsimplest bit-level code is unary code . The unary code of nis a string of n1s UNARY CODE\\nfollowed by a 0 (see the ﬁrst two columns of Table 5.5). Obviously, this is not\\na very efﬁcient code, but it will come in handy in a moment.\\nHow efﬁcient can a code be in principle? Assuming the 2ngaps Gwith\\n1≤G≤2nare all equally likely, the optimal encoding uses nbits for each\\nG. So some gaps ( G=2nin this case) cannot be encoded with fewer than\\nlog2Gbits. Our goal is to get as close to this lower bound as possibl e.\\nA method that is within a factor of optimal is γencoding .γcodes im- γENCODING\\nplement variable-length encoding by splitting the represe ntation of a gap G\\ninto a pair of length and offset .Offset isGin binary, but with the leading 1\\nremoved.2For example, for 13 (binary 1101) offset is 101. Length encodes the\\nlength of offset in unary code. For 13, the length of offset is 3 bits, which is 1110\\nin unary. The γcode of 13 is therefore 1110101, the concatenation of length\\n1110 and offset 101. The right hand column of Table 5.5gives additional\\nexamples of γcodes.\\nAγcode is decoded by ﬁrst reading the unary code up to the 0 that t er-\\nminates it, for example, the four bits 1110 when decoding 111 0101. Now we\\nknow how long the offset is: 3 bits. The offset 101 can then be r ead correctly\\nand the 1 that was chopped off in encoding is prepended: 101 →1101 = 13.\\nThe length of offset is⌊log2G⌋bits and the length of length is⌊log2G⌋+1\\nbits, so the length of the entire code is 2 ×⌊log2G⌋+1 bits. γcodes are\\nalways of odd length and they are within a factor of 2 of what we claimed\\nto be the optimal encoding length log2G. We derived this optimum from\\nthe assumption that the 2ngaps between 1 and 2nare equiprobable. But this\\nneed not be the case. In general, we do not know the probabilit y distribution\\nover gaps a priori.\\nThe characteristic of a discrete probability distribution3Pthat determines\\nits coding properties (including whether a code is optimal) is its entropy H (P), ENTROPY\\nwhich is deﬁned as follows:\\nH(P) =−∑\\nx∈XP(x)log2P(x)\\nwhere Xis the set of all possible numbers we need to be able to encode\\n(and therefore ∑x∈XP(x) = 1.0). Entropy is a measure of uncertainty as\\nshown in Figure 5.9for a probability distribution Pover two possible out-\\ncomes, namely, X={x1,x2}. Entropy is maximized ( H(P) =1) for P(x1) =\\nP(x2) =0.5 when uncertainty about which xiwill appear next is largest; and\\n2. We assume here that Ghas no leading 0s. If there are any, they are removed before de leting\\nthe leading 1.\\n3. Readers who want to review basic concepts of probability t heory may want to consult Rice\\n(2006 ) orRoss (2006 ). Note that we are interested in probability distributions over integers (gaps,\\nfrequencies, etc.), but that the coding properties of a prob ability distribution are independent of\\nwhether the outcomes are integers or something else.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 135}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP100 5 Index compression\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\\nP(x1)H(P)\\n◮Figure 5.9 Entropy H(P)as a function of P(x1)for a sample space with two\\noutcomes x1and x2.\\nminimized ( H(P) =0) for P(x1) =1,P(x2) =0 and for P(x1) =0,P(x2) =1\\nwhen there is absolute certainty.\\nIt can be shown that the lower bound for the expected length E(L)of a\\ncode LisH(P)if certain conditions hold (see the references). It can furt her\\nbe shown that for 1 <H(P)<∞,γencoding is within a factor of 3 of this\\noptimal encoding, approaching 2 for large H(P):\\nE(Lγ)\\nH(P)≤2+1\\nH(P)≤3.\\nWhat is remarkable about this result is that it holds for any p robability distri-\\nbution P. So without knowing anything about the properties of the dis tribu-\\ntion of gaps, we can apply γcodes and be certain that they are within a factor\\nof≈2 of the optimal code for distributions of large entropy. A co de like γ\\ncode with the property of being within a factor of optimal for an arbitrary\\ndistribution Pis called universal . UNIVERSAL CODE\\nIn addition to universality, γcodes have two other properties that are use-\\nful for index compression. First, they are preﬁx free , namely, no γcode is the PREFIX FREE\\npreﬁx of another. This means that there is always a unique dec oding of a\\nsequence of γcodes – and we do not need delimiters between them, which\\nwould decrease the efﬁciency of the code. The second propert y is that γ\\ncodes are parameter free . For many other efﬁcient codes, we have to ﬁt the PARAMETER FREE\\nparameters of a model (e.g., the binomial distribution) to t he distribution', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 136}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP5.3 Postings ﬁle compression 101\\nof gaps in the index. This complicates the implementation of compression\\nand decompression. For instance, the parameters need to be s tored and re-\\ntrieved. And in dynamic indexing, the distribution of gaps c an change, so\\nthat the original parameters are no longer appropriate. The se problems are\\navoided with a parameter-free code.\\nHow much compression of the inverted index do γcodes achieve? To\\nanswer this question we use Zipf’s law, the term distributio n model intro-\\nduced in Section 5.1.2 . According to Zipf’s law, the collection frequency cf i\\nis proportional to the inverse of the rank i, that is, there is a constant c′such\\nthat:\\ncfi=c′\\ni. (5.3)\\nWe can choose a different constant csuch that the fractions c/iare relative\\nfrequencies and sum to 1 (that is, c/i=cfi/T):\\n1=M\\n∑\\ni=1c\\ni=cM\\n∑\\ni=11\\ni=c H M (5.4)\\nc=1\\nHM(5.5)\\nwhere Mis the number of distinct terms and HMis the Mth harmonic num-\\nber.4Reuters-RCV1 has M=400,000 distinct terms and HM≈lnM, so we\\nhave\\nc=1\\nHM≈1\\nlnM=1\\nln 400,000≈1\\n13.\\nThus the ith term has a relative frequency of roughly 1/ (13i), and the ex-\\npected average number of occurrences of term iin a document of length L\\nis:\\nLc\\ni≈200×1\\n13\\ni≈15\\ni\\nwhere we interpret the relative frequency as a term occurren ce probability.\\nRecall that 200 is the average number of tokens per document i n Reuters-\\nRCV1 (Table 4.2).\\nNow we have derived term statistics that characterize the di stribution of\\nterms in the collection and, by extension, the distribution of gaps in the post-\\nings lists. From these statistics, we can calculate the spac e requirements for\\nan inverted index compressed with γencoding. We ﬁrst stratify the vocab-\\nulary into blocks of size Lc=15. On average, term ioccurs 15/ itimes per\\n4. Note that, unfortunately, the conventional symbol for bo th entropy and harmonic number is\\nH. Context should make clear which is meant in this chapter.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 137}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP102 5 Index compression\\nNdocuments\\nLcmost\\nfrequent Ngaps of 1 each\\nterms\\nLcnext most\\nfrequent N/2 gaps of 2 each\\nterms\\nLcnext most\\nfrequent N/3 gaps of 3 each\\nterms\\n. . . . . .\\n◮Figure 5.10 Stratiﬁcation of terms for estimating the size of a γencoded inverted\\nindex.\\ndocument. So the average number of occurrences fper document is 1 ≤ffor\\nterms in the ﬁrst block, corresponding to a total number of Ngaps per term.\\nThe average is1\\n2≤f<1 for terms in the second block, corresponding to\\nN/2 gaps per term, and1\\n3≤f<1\\n2for terms in the third block, correspond-\\ning to N/3 gaps per term, and so on. (We take the lower bound because it\\nsimpliﬁes subsequent calculations. As we will see, the ﬁnal estimate is too\\npessimistic, even with this assumption.) We will make the so mewhat unre-\\nalistic assumption that all gaps for a given term have the sam e size as shown\\nin Figure 5.10. Assuming such a uniform distribution of gaps, we then have\\ngaps of size 1 in block 1, gaps of size 2 in block 2, and so on.\\nEncoding the N/jgaps of size jwith γcodes, the number of bits needed\\nfor the postings list of a term in the jth block (corresponding to one row in\\nthe ﬁgure) is:\\nbits-per-row =N\\nj×(2×⌊log2j⌋+1)\\n≈2Nlog2j\\nj.\\nTo encode the entire block, we need (Lc)·(2Nlog2j)/jbits. There are M/(Lc)\\nblocks, so the postings ﬁle as a whole will take up:\\nM\\nLc\\n∑\\nj=12NLc log2j\\nj. (5.6)', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 138}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP5.3 Postings ﬁle compression 103\\n◮Table 5.6 Index and dictionary compression for Reuters-RCV1. The com pression\\nratio depends on the proportion of actual text in the collect ion. Reuters-RCV1 con-\\ntains a large amount of XML markup. Using the two best compres sion schemes, γ\\nencoding and blocking with front coding, the ratio compress ed index to collection\\nsize is therefore especially small for Reuters-RCV1: (101+5.9)/3600≈0.03.\\ndata structure size in MB\\ndictionary, ﬁxed-width 11.2\\ndictionary, term pointers into string 7.6\\n∼, with blocking, k=4 7.1\\n∼, with blocking & front coding 5.9\\ncollection (text, xml markup etc) 3600.0\\ncollection (text) 960.0\\nterm incidence matrix 40,000.0\\npostings, uncompressed (32-bit words) 400.0\\npostings, uncompressed (20 bits) 250.0\\npostings, variable byte encoded 116.0\\npostings, γencoded 101.0\\nFor Reuters-RCV1,M\\nLc≈400,000/15≈27,000 and\\n27,000\\n∑\\nj=12×106×15 log2j\\nj≈224 MB. (5.7)\\nSo the postings ﬁle of the compressed inverted index for our 9 60 MB collec-\\ntion has a size of 224 MB, one fourth the size of the original co llection.\\nWhen we run γcompression on Reuters-RCV1, the actual size of the com-\\npressed index is even lower: 101 MB, a bit more than one tenth o f the size of\\nthe collection. The reason for the discrepancy between pred icted and actual\\nvalue is that (i) Zipf’s law is not a very good approximation o f the actual dis-\\ntribution of term frequencies for Reuters-RCV1 and (ii) gap s are not uniform.\\nThe Zipf model predicts an index size of 251 MB for the unround ed numbers\\nfrom Table 4.2. If term frequencies are generated from the Zipf model and\\na compressed index is created for these artiﬁcial terms, the n the compressed\\nsize is 254 MB. So to the extent that the assumptions about the distribution\\nof term frequencies are accurate, the predictions of the mod el are correct.\\nTable 5.6summarizes the compression techniques covered in this chap ter.\\nThe term incidence matrix (Figure 1.1, page 4) for Reuters-RCV1 has size\\n400,000×800,000 =40×8×109bits or 40 GB.\\nγcodes achieve great compression ratios – about 15% better th an vari-\\nable byte codes for Reuters-RCV1. But they are expensive to d ecode. This is\\nbecause many bit-level operations – shifts and masks – are ne cessary to de-\\ncode a sequence of γcodes as the boundaries between codes will usually be', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 139}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP104 5 Index compression\\nsomewhere in the middle of a machine word. As a result, query p rocessing is\\nmore expensive for γcodes than for variable byte codes. Whether we choose\\nvariable byte or γencoding depends on the characteristics of an application,\\nfor example, on the relative weights we give to conserving di sk space versus\\nmaximizing query response time.\\nThe compression ratio for the index in Table 5.6is about 25%: 400 MB (un-\\ncompressed, each posting stored as a 32-bit word) versus 101 MB ( γ) and 116\\nMB (VB). This shows that both γand VB codes meet the objectives we stated\\nin the beginning of the chapter. Index compression substant ially improves\\ntime and space efﬁciency of indexes by reducing the amount of disk space\\nneeded, increasing the amount of information that can be kep t in the cache,\\nand speeding up data transfers from disk to memory.\\n?Exercise 5.4 [⋆]\\nCompute variable byte codes for the numbers in Tables 5.3and 5.5.\\nExercise 5.5 [⋆]\\nCompute variable byte and γcodes for the postings list ⟨777, 17743, 294068, 31251336 ⟩.\\nUse gaps instead of docIDs where possible. Write binary code s in 8-bit blocks.\\nExercise 5.6\\nConsider the postings list ⟨4, 10, 11, 12, 15, 62, 63, 265, 268, 270, 400 ⟩with a correspond-\\ning list of gaps⟨4, 6, 1, 1, 3, 47, 1, 202, 3, 2, 130 ⟩. Assume that the length of the postings\\nlist is stored separately, so the system knows when a posting s list is complete. Us-\\ning variable byte encoding: (i) What is the largest gap you ca n encode in 1 byte? (ii)\\nWhat is the largest gap you can encode in 2 bytes? (iii) How man y bytes will the\\nabove postings list require under this encoding? (Count onl y space for encoding the\\nsequence of numbers.)\\nExercise 5.7\\nA little trick is to notice that a gap cannot be of length 0 and t hat the stuff left to encode\\nafter shifting cannot be 0. Based on these observations: (i) Suggest a modiﬁcation to\\nvariable byte encoding that allows you to encode slightly la rger gaps in the same\\namount of space. (ii) What is the largest gap you can encode in 1 byte? (iii) What\\nis the largest gap you can encode in 2 bytes? (iv) How many byte s will the postings\\nlist in Exercise 5.6require under this encoding? (Count only space for encoding the\\nsequence of numbers.)\\nExercise 5.8 [⋆]\\nFrom the following sequence of γ-coded gaps, reconstruct ﬁrst the gap sequence and\\nthen the postings sequence: 111000111010101111110110111 1011.\\nExercise 5.9\\nγcodes are relatively inefﬁcient for large numbers (e.g., 10 25 in Table 5.5) as they\\nencode the length of the offset in inefﬁcient unary code. δcodes differ from γcodes δCODES\\nin that they encode the ﬁrst part of the code ( length ) inγcode instead of unary code.\\nThe encoding of offset is the same. For example, the δcode of 7 is 10,0,11 (again, we\\nadd commas for readability). 10,0 is the γcode for length (2 in this case) and the\\nencoding of offset (11) is unchanged. (i) Compute the δcodes for the other numbers', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 140}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP5.4 References and further reading 105\\n◮Table 5.7 Two gap sequences to be merged in blocked sort-based indexin g\\nγencoded gap sequence of run 1 111011011111100101111111111 0100011111001\\nγencoded gap sequence of run 2 111110100001111110001000111 11110010000011111010101\\nin Table 5.5. For what range of numbers is the δcode shorter than the γcode? (ii) γ\\ncode beats variable byte code in Table 5.6because the index contains stop words and\\nthus many small gaps. Show that variable byte code is more com pact if larger gaps\\ndominate. (iii) Compare the compression ratios of δcode and variable byte code for\\na distribution of gaps dominated by large gaps.\\nExercise 5.10\\nGo through the above calculation of index size and explicitl y state all the approxima-\\ntions that were made to arrive at Equation ( 5.6).\\nExercise 5.11\\nFor a collection of your choosing, determine the number of do cuments and terms and\\nthe average length of a document. (i) How large is the inverte d index predicted to be\\nby Equation ( 5.6)? (ii) Implement an indexer that creates a γ-compressed inverted\\nindex for the collection. How large is the actual index? (iii ) Implement an indexer\\nthat uses variable byte encoding. How large is the variable b yte encoded index?\\nExercise 5.12\\nTo be able to hold as many postings as possible in main memory, it is a good idea to\\ncompress intermediate index ﬁles during index constructio n. (i) This makes merging\\nruns in blocked sort-based indexing more complicated. As an example, work out the\\nγ-encoded merged sequence of the gaps in Table 5.7. (ii) Index construction is more\\nspace efﬁcient when using compression. Would you also expec t it to be faster?\\nExercise 5.13\\n(i) Show that the size of the vocabulary is ﬁnite according to Zipf’s law and inﬁnite\\naccording to Heaps’ law. (ii) Can we derive Heaps’ law from Zi pf’s law?\\n5.4 References and further reading\\nHeaps’ law was discovered by Heaps (1978 ). See also Baeza-Yates and Ribeiro-\\nNeto (1999 ). A detailed study of vocabulary growth in large collection s is\\n(Williams and Zobel 2005 ). Zipf’s law is due to Zipf (1949 ).Witten and Bell\\n(1990 ) investigate the quality of the ﬁt obtained by the law. Other term distri-\\nbution models, including K mixture and two-poisson model, a re discussed\\nbyManning and Schütze (1999 , Chapter 15). Carmel et al. (2001 ),Büttcher\\nand Clarke (2006 ),Blanco and Barreiro (2007 ), and Ntoulas and Cho (2007 )\\nshow that lossy compression can achieve good compression wi th no or no\\nsigniﬁcant decrease in retrieval effectiveness.\\nDictionary compression is covered in detail by Witten et al. (1999 , Chap-\\nter 4), which is recommended as additional reading.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 141}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP106 5 Index compression\\nSubsection 5.3.1 is based on ( Scholer et al. 2002 ). The authors ﬁnd that\\nvariable byte codes process queries two times faster than ei ther bit-level\\ncompressed indexes or uncompressed indexes with a 30% penal ty in com-\\npression ratio compared with the best bit-level compressio n method. They\\nalso show that compressed indexes can be superior to uncompr essed indexes\\nnot only in disk usage, but also in query processing speed. Co mpared with\\nVB codes, “variable nibble” codes showed 5% to 10% better com pression\\nand up to one third worse effectiveness in one experiment ( Anh and Moffat\\n2005 ).Trotman (2003 ) also recommends using VB codes unless disk space is\\nat a premium. In recent work, Anh and Moffat (2005 ;2006a ) and Zukowski\\net al. (2006 ) have constructed word-aligned binary codes that are both f aster\\nin decompression and at least as efﬁcient as VB codes. Zhang et al. (2007 ) in-\\nvestigate the increased effectiveness of caching when a num ber of different\\ncompression techniques for postings lists are used on moder n hardware.\\nδcodes (Exercise 5.9) and γcodes were introduced by Elias (1975 ), who\\nproved that both codes are universal. In addition, δcodes are asymptotically\\noptimal for H(P)→∞.δcodes perform better than γcodes if large num-\\nbers (greater than 15) dominate. A good introduction to info rmation theory,\\nincluding the concept of entropy, is ( Cover and Thomas 1991 ). While Elias\\ncodes are only asymptotically optimal, arithmetic codes ( Witten et al. 1999 ,\\nSection 2.4) can be constructed to be arbitrarily close to th e optimum H(P)\\nfor any P.\\nSeveral additional index compression techniques are cover ed by Witten et\\nal. (1999; Sections 3.3 and 3.4 and Chapter 5). They recommen d using param- PARAMETERIZED CODE\\neterized codes for index compression, codes that explicitly model the prob abil-\\nity distribution of gaps for each term. For example, they sho w that Golomb GOLOMB CODES\\ncodes achieve better compression ratios than γcodes for large collections.\\nMoffat and Zobel (1992 ) compare several parameterized methods, including\\nLLRUN ( Fraenkel and Klein 1985 ).\\nThe distribution of gaps in a postings list depends on the ass ignment of\\ndocIDs to documents. A number of researchers have looked int o assign-\\ning docIDs in a way that is conducive to the efﬁcient compress ion of gap\\nsequences ( Moffat and Stuiver 1996 ;Blandford and Blelloch 2002 ;Silvestri\\net al. 2004 ;Blanco and Barreiro 2006 ;Silvestri 2007 ). These techniques assign\\ndocIDs in a small range to documents in a cluster where a clust er can consist\\nof all documents in a given time period, on a particular web si te, or sharing\\nanother property. As a result, when a sequence of documents f rom a clus-\\nter occurs in a postings list, their gaps are small and can be m ore effectively\\ncompressed.\\nDifferent considerations apply to the compression of term f requencies and\\nword positions than to the compression of docIDs in postings lists. See Scho-\\nler et al. (2002 ) and Zobel and Moffat (2006 ).Zobel and Moffat (2006 ) is\\nrecommended in general as an in-depth and up-to-date tutori al on inverted', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 142}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP5.4 References and further reading 107\\nindexes, including index compression.\\nThis chapter only looks at index compression for Boolean ret rieval. For\\nranked retrieval (Chapter 6), it is advantageous to order postings according\\nto term frequency instead of docID. During query processing , the scanning\\nof many postings lists can then be terminated early because s maller weights\\ndo not change the ranking of the highest ranked kdocuments found so far. It\\nis not a good idea to precompute and store weights in the index (as opposed\\nto frequencies) because they cannot be compressed as well as integers (see\\nSection 7.1.5 , page 140).\\nDocument compression can also be important in an efﬁcient information re-\\ntrieval system. de Moura et al. (2000 ) and Brisaboa et al. (2007 ) describe\\ncompression schemes that allow direct searching of terms an d phrases in the\\ncompressed text, which is infeasible with standard text com pression utilities\\nlikegzip andcompress .\\n?Exercise 5.14 [⋆]\\nWe have deﬁned unary codes as being “10”: sequences of 1s term inated by a 0. In-\\nterchanging the roles of 0s and 1s yields an equivalent “01” u nary code. When this\\n01 unary code is used, the construction of a γcode can be stated as follows: (1) Write\\nGdown in binary using b=⌊log2j⌋+1 bits. (2) Prepend (b−1)0s. (i) Encode the\\nnumbers in Table 5.5in this alternative γcode. (ii) Show that this method produces\\na well-deﬁned alternative γcode in the sense that it has the same length and can be\\nuniquely decoded.\\nExercise 5.15 [⋆ ⋆ ⋆ ]\\nUnary code is not a universal code in the sense deﬁned above. H owever, there exists\\na distribution over gaps for which unary code is optimal. Whi ch distribution is this?\\nExercise 5.16\\nGive some examples of terms that violate the assumption that gaps all have the same\\nsize (which we made when estimating the space requirements o f aγ-encoded index).\\nWhat are general characteristics of these terms?\\nExercise 5.17\\nConsider a term whose postings list has size n, say, n=10,000. Compare the size of\\ntheγ-compressed gap-encoded postings list if the distribution of the term is uniform\\n(i.e., all gaps have the same size) versus its size when the di stribution is not uniform.\\nWhich compressed postings list is smaller?\\nExercise 5.18\\nWork out the sum in Equation ( 5.7) and show it adds up to about 251 MB. Use the\\nnumbers in Table 4.2, but do not round Lc,c, and the number of vocabulary blocks.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 143}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 144}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 109\\n6Scoring, term weighting and the\\nvector space model\\nThus far we have dealt with indexes that support Boolean quer ies: a docu-\\nment either matches or does not match a query. In the case of la rge document\\ncollections, the resulting number of matching documents ca n far exceed the\\nnumber a human user could possibly sift through. Accordingl y, it is essen-\\ntial for a search engine to rank-order the documents matchin g a query. To do\\nthis, the search engine computes, for each matching documen t, a score with\\nrespect to the query at hand. In this chapter we initiate the s tudy of assigning\\na score to a (query, document) pair. This chapter consists of three main ideas.\\n1.We introduce parametric and zone indexes in Section 6.1, which serve\\ntwo purposes. First, they allow us to index and retrieve docu ments by\\nmetadata such as the language in which a document is written. Second,\\nthey give us a simple means for scoring (and thereby ranking) documents\\nin response to a query.\\n2.Next, in Section 6.2we develop the idea of weighting the importance of a\\nterm in a document, based on the statistics of occurrence of t he term.\\n3.In Section 6.3we show that by viewing each document as a vector of such\\nweights, we can compute a score between a query and each docum ent.\\nThis view is known as vector space scoring.\\nSection 6.4develops several variants of term-weighting for the vector space\\nmodel. Chapter 7develops computational aspects of vector space scoring,\\nand related topics.\\nAs we develop these ideas, the notion of a query will assume mu ltiple\\nnuances. In Section 6.1we consider queries in which speciﬁc query terms\\noccur in speciﬁed regions of a matching document. Beginning Section 6.2we\\nwill in fact relax the requirement of matching speciﬁc regio ns of a document;\\ninstead, we will look at so-called free text queries that sim ply consist of query\\nterms with no speciﬁcation on their relative order, importa nce or where in a\\ndocument they should be found. The bulk of our study of scorin g will be in\\nthis latter notion of a query being such a set of terms.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 145}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP110 6 Scoring, term weighting and the vector space model\\n6.1 Parametric and zone indexes\\nWe have thus far viewed a document as a sequence of terms. In fa ct, most\\ndocuments have additional structure. Digital documents ge nerally encode,\\nin machine-recognizable form, certain metadata associated with each docu- METADATA\\nment. By metadata, we mean speciﬁc forms of data about a docum ent, such\\nas its author(s), title and date of publication. This metada ta would generally\\ninclude ﬁelds such as the date of creation and the format of the document, as FIELD\\nwell the author and possibly the title of the document. The po ssible values\\nof a ﬁeld should be thought of as ﬁnite – for instance, the set o f all dates of\\nauthorship.\\nConsider queries of the form “ﬁnd documents authored by Will iam Shake-\\nspeare in 1601, containing the phrase alaspoorYorick ”. Query processing then\\nconsists as usual of postings intersections, except that we may merge post-\\nings from standard inverted as well as parametric indexes . There is one para- PARAMETRIC INDEX\\nmetric index for each ﬁeld (say, date of creation); it allows us to select only\\nthe documents matching a date speciﬁed in the query. Figure 6.1illustrates\\nthe user’s view of such a parametric search. Some of the ﬁelds may assume\\nordered values, such as dates; in the example query above, th e year 1601 is\\none such ﬁeld value. The search engine may support querying r anges on\\nsuch ordered values; to this end, a structure like a B-tree ma y be used for the\\nﬁeld’s dictionary.\\nZones are similar to ﬁelds, except the contents of a zone can be arbi trary ZONE\\nfree text. Whereas a ﬁeld may take on a relatively small set of values, a zone\\ncan be thought of as an arbitrary, unbounded amount of text. F or instance,\\ndocument titles and abstracts are generally treated as zone s. We may build a\\nseparate inverted index for each zone of a document, to suppo rt queries such\\nas “ﬁnd documents with merchant in the title and william in the author list and\\nthe phrase gentle rain in the body”. This has the effect of building an index\\nthat looks like Figure 6.2. Whereas the dictionary for a parametric index\\ncomes from a ﬁxed vocabulary (the set of languages, or the set of dates), the\\ndictionary for a zone index must structure whatever vocabul ary stems from\\nthe text of that zone.\\nIn fact, we can reduce the size of the dictionary by encoding t he zone in\\nwhich a term occurs in the postings. In Figure 6.3for instance, we show how\\noccurrences of william in the title and author zones of various documents are\\nencoded. Such an encoding is useful when the size of the dicti onary is a\\nconcern (because we require the dictionary to ﬁt in main memo ry). But there\\nis another important reason why the encoding of Figure 6.3is useful: the\\nefﬁcient computation of scores using a technique we will cal lweighted zone WEIGHTED ZONE\\nSCORING scoring .', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 146}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP6.1 Parametric and zone indexes 111\\n◮Figure 6.1 Parametric search. In this example we have a collection with ﬁelds al-\\nlowing us to select publications by zones such as Author and ﬁ elds such as Language.\\nwilliam.author 2 3 5 8william.title 2 4 8 16william.abstract 11 121 1441 1729\\n- - - -- - - -- - - -\\n◮Figure 6.2 Basic zone index ; zones are encoded as extensions of diction ary en-\\ntries.\\nwilliam 2.author,2.title 3.author 4.title 5.author- - - -\\n◮Figure 6.3 Zone index in which the zone is encoded in the postings rather than\\nthe dictionary.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 147}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP112 6 Scoring, term weighting and the vector space model\\n6.1.1 Weighted zone scoring\\nThus far in Section 6.1we have focused on retrieving documents based on\\nBoolean queries on ﬁelds and zones. We now turn to a second app lication of\\nzones and ﬁelds.\\nGiven a Boolean query qand a document d, weighted zone scoring assigns\\nto the pair (q,d)a score in the interval [0, 1], by computing a linear combina-\\ntion of zone scores , where each zone of the document contributes a Boolean\\nvalue. More speciﬁcally, consider a set of documents each of which has ℓ\\nzones. Let g1, . . . , gℓ∈[0, 1]such that ∑ℓ\\ni=1gi=1. For 1≤i≤ℓ, letsibe the\\nBoolean score denoting a match (or absence thereof) between qand the ith\\nzone. For instance, the Boolean score from a zone could be 1 if all the query\\nterm(s) occur in that zone, and zero otherwise; indeed, it co uld be any Boo-\\nlean function that maps the presence of query terms in a zone t o 0, 1. Then,\\nthe weighted zone score is deﬁned to be\\nℓ\\n∑\\ni=1gisi. (6.1)\\nWeighted zone scoring is sometimes referred to also as ranked Boolean re- RANKED BOOLEAN\\nRETRIEVAL trieval .\\n✎Example 6.1: Consider the query shakespeare in a collection in which each doc-\\nument has three zones: author, title and body . The Boolean score function for a zone\\ntakes on the value 1 if the query term shakespeare is present in the zone, and zero\\notherwise. Weighted zone scoring in such a collection would require three weights\\ng1,g2and g3, respectively corresponding to the author, title and body zones. Suppose\\nwe set g1=0.2,g2=0.3 and g3=0.5 (so that the three weights add up to 1); this cor-\\nresponds to an application in which a match in the author zone is least important to\\nthe overall score, the titlezone somewhat more, and the body contributes even more.\\nThus if the term shakespeare were to appear in the titleand body zones but not the\\nauthor zone of a document, the score of this document would be 0.8.\\nHow do we implement the computation of weighted zone scores? A sim-\\nple approach would be to compute the score for each document i n turn,\\nadding in all the contributions from the various zones. Howe ver, we now\\nshow how we may compute weighted zone scores directly from in verted in-\\ndexes. The algorithm of Figure 6.4treats the case when the query qis a two-\\nterm query consisting of query terms q1and q2, and the Boolean function is\\nAND: 1 if both query terms are present in a zone and 0 otherwise . Following\\nthe description of the algorithm, we describe the extension to more complex\\nqueries and Boolean functions.\\nThe reader may have noticed the close similarity between thi s algorithm\\nand that in Figure 1.6. Indeed, they represent the same postings traversal,\\nexcept that instead of merely adding a document to the set of r esults for', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 148}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP6.1 Parametric and zone indexes 113\\nZONE SCORE(q1,q2)\\n1 ﬂoat scores[N] = [ 0]\\n2 constant g[ℓ]\\n3p1←postings (q1)\\n4p2←postings (q2)\\n5 // scores[] is an array with a score entry for each document, initialized to zero.\\n6 // p1and p2are initialized to point to the beginning of their respectiv e postings.\\n7 //Assume g[] is initialized to the respective zone weights .\\n8while p1̸=NILand p2̸=NIL\\n9do if docID (p1) =docID (p2)\\n10 then scores[docID (p1)]←WEIGHTED ZONE(p1,p2,g)\\n11 p1←next(p1)\\n12 p2←next(p2)\\n13 else if docID (p1)<docID (p2)\\n14 then p1←next(p1)\\n15 else p2←next(p2)\\n16 return scores\\n◮Figure 6.4 Algorithm for computing the weighted zone score from two pos tings\\nlists. Function W EIGHTED ZONE (not shown here) is assumed to compute the inner\\nloop of Equation 6.1.\\na Boolean AND query, we now compute a score for each such docum ent.\\nSome literature refers to the array scores[] above as a set of accumulators . The ACCUMULATOR\\nreason for this will be clear as we consider more complex Bool ean functions\\nthan the AND; thus we may assign a non-zero score to a document even if it\\ndoes not contain all query terms.\\n6.1.2 Learning weights\\nHow do we determine the weights gifor weighted zone scoring? These\\nweights could be speciﬁed by an expert (or, in principle, the user); but in-\\ncreasingly, these weights are “learned” using training exa mples that have\\nbeen judged editorially. This latter methodology falls und er a general class\\nof approaches to scoring and ranking in information retriev al, known as\\nmachine-learned relevance . We provide a brief introduction to this topic here MACHINE -LEARNED\\nRELEVANCE because weighted zone scoring presents a clean setting for i ntroducing it; a\\ncomplete development demands an understanding of machine l earning and\\nis deferred to Chapter 15.\\n1.We are provided with a set of training examples , each of which is a tu-\\nple consisting of a query qand a document d, together with a relevance', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 149}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP114 6 Scoring, term weighting and the vector space model\\njudgment for donq. In the simplest form, each relevance judgments is ei-\\nther Relevant orNon-relevant . More sophisticated implementations of the\\nmethodology make use of more nuanced judgments.\\n2.The weights giare then “learned” from these examples, in order that the\\nlearned scores approximate the relevance judgments in the t raining exam-\\nples.\\nFor weighted zone scoring, the process may be viewed as learn ing a lin-\\near function of the Boolean match scores contributed by the v arious zones.\\nThe expensive component of this methodology is the labor-in tensive assem-\\nbly of user-generated relevance judgments from which to lea rn the weights,\\nespecially in a collection that changes frequently (such as the Web). We now\\ndetail a simple example that illustrates how we can reduce th e problem of\\nlearning the weights gito a simple optimization problem.\\nWe now consider a simple case of weighted zone scoring, where each doc-\\nument has a titlezone and a body zone. Given a query qand a document d, we\\nuse the given Boolean match function to compute Boolean vari ables sT(d,q)\\nand sB(d,q), depending on whether the title (respectively, body) zone o fd\\nmatches query q. For instance, the algorithm in Figure 6.4uses an AND of\\nthe query terms for this Boolean function. We will compute a s core between\\n0 and 1 for each (document, query) pair using sT(d,q)and sB(d,q)by using\\na constant g∈[0, 1], as follows:\\nscore(d,q) =g·sT(d,q) + ( 1−g)sB(d,q). (6.2)\\nWe now describe how to determine the constant gfrom a set of training ex-\\namples , each of which is a triple of the form Φj= (dj,qj,r(dj,qj)). In each\\ntraining example, a given training document djand a given training query qj\\nare assessed by a human editor who delivers a relevance judgm entr(dj,qj)\\nthat is either Relevant orNon-relevant . This is illustrated in Figure 6.5, where\\nseven training examples are shown.\\nFor each training example Φjwe have Boolean values sT(dj,qj)and sB(dj,qj)\\nthat we use to compute a score from ( 6.2)\\nscore(dj,qj) =g·sT(dj,qj) + ( 1−g)sB(dj,qj). (6.3)\\nWe now compare this computed score to the human relevance jud gment for\\nthe same document-query pair (dj,qj); to this end, we will quantize each\\nRelevant judgment as a 1 and each Non-relevant judgment as a 0. Suppose\\nthat we deﬁne the error of the scoring function with weight gas\\nε(g,Φj) = ( r(dj,qj)−score(dj,qj))2,', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 150}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP6.1 Parametric and zone indexes 115\\nExample DocID Query sTsB Judgment\\nΦ1 37linux 1 1 Relevant\\nΦ2 37penguin 0 1 Non-relevant\\nΦ3 238system 0 1 Relevant\\nΦ4 238penguin 0 0 Non-relevant\\nΦ5 1741kernel 1 1 Relevant\\nΦ6 2094driver 0 1 Relevant\\nΦ7 3191driver 1 0 Non-relevant\\n◮Figure 6.5 An illustration of training examples.\\nsTsB Score\\n0 0 0\\n0 1 1−g\\n1 0 g\\n1 1 1\\n◮Figure 6.6 The four possible combinations of sTand sB.\\nwhere we have quantized the editorial relevance judgment r(dj,qj)to 0 or 1.\\nThen, the total error of a set of training examples is given by\\n∑\\njε(g,Φj). (6.4)\\nThe problem of learning the constant gfrom the given training examples\\nthen reduces to picking the value of gthat minimizes the total error in ( 6.4).\\nPicking the best value of gin (6.4) in the formulation of Section 6.1.3 re-\\nduces to the problem of minimizing a quadratic function of gover the inter-\\nval[0, 1]. This reduction is detailed in Section 6.1.3 .\\n✄6.1.3 The optimal weight g\\nWe begin by noting that for any training example Φjfor which sT(dj,qj) =0\\nand sB(dj,qj) = 1, the score computed by Equation ( 6.2) is 1−g. In similar\\nfashion, we may write down the score computed by Equation ( 6.2) for the\\nthree other possible combinations of sT(dj,qj)and sB(dj,qj); this is summa-\\nrized in Figure 6.6.\\nLetn01r(respectively, n01n) denote the number of training examples for\\nwhich sT(dj,qj) =0 and sB(dj,qj) =1 and the editorial judgment is Relevant\\n(respectively, Non-relevant ). Then the contribution to the total error in Equa-\\ntion ( 6.4) from training examples for which sT(dj,qj) = 0 and sB(dj,qj) = 1', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 151}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP116 6 Scoring, term weighting and the vector space model\\nis\\n[1−(1−g)]2n01r+ [0−(1−g)]2n01n.\\nBy writing in similar fashion the error contributions from t raining examples\\nof the other three combinations of values for sT(dj,qj)and sB(dj,qj)(and\\nextending the notation in the obvious manner), the total err or corresponding\\nto Equation ( 6.4) is\\n(n01r+n10n)g2+ (n10r+n01n)(1−g)2+n00r+n11n. (6.5)\\nBy differentiating Equation ( 6.5) with respect to gand setting the result to\\nzero, it follows that the optimal value of gis\\nn10r+n01n\\nn10r+n10n+n01r+n01n. (6.6)\\n?Exercise 6.1\\nWhen using weighted zone scoring, is it necessary for all zon es to use the same Boo-\\nlean match function?\\nExercise 6.2\\nIn Example 6.1above with weights g1=0.2,g2=0.31 and g3=0.49, what are all the\\ndistinct score values a document may get?\\nExercise 6.3\\nRewrite the algorithm in Figure 6.4to the case of more than two query terms.\\nExercise 6.4\\nWrite pseudocode for the function WeightedZone for the case of two postings lists in\\nFigure 6.4.\\nExercise 6.5\\nApply Equation 6.6to the sample training set in Figure 6.5to estimate the best value\\nofgfor this sample.\\nExercise 6.6\\nFor the value of gestimated in Exercise 6.5, compute the weighted zone score for each\\n(query, document) example. How do these scores relate to the relevance judgments\\nin Figure 6.5(quantized to 0/1)?\\nExercise 6.7\\nWhy does the expression for gin (6.6) not involve training examples in which sT(dt,qt)\\nand sB(dt,qt)have the same value?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 152}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP6.2 Term frequency and weighting 117\\n6.2 Term frequency and weighting\\nThus far, scoring has hinged on whether or not a query term is p resent in\\na zone within a document. We take the next logical step: a docu ment or\\nzone that mentions a query term more often has more to do with t hat query\\nand therefore should receive a higher score. To motivate thi s, we recall the\\nnotion of a free text query introduced in Section 1.4: a query in which the\\nterms of the query are typed freeform into the search interfa ce, without any\\nconnecting search operators (such as Boolean operators). T his query style,\\nwhich is extremely popular on the web, views the query as simp ly a set of\\nwords. A plausible scoring mechanism then is to compute a sco re that is the\\nsum, over the query terms, of the match scores between each qu ery term and\\nthe document.\\nTowards this end, we assign to each term in a document a weight for that\\nterm, that depends on the number of occurrences of the term in the doc-\\nument. We would like to compute a score between a query term tand a\\ndocument d, based on the weight of tind. The simplest approach is to assign\\nthe weight to be equal to the number of occurrences of term tin document d.\\nThis weighting scheme is referred to as term frequency and is denoted tf t,d, TERM FREQUENCY\\nwith the subscripts denoting the term and the document in ord er.\\nFor a document d, the set of weights determined by the tf weights above\\n(or indeed any weighting function that maps the number of occ urrences of t\\nindto a positive real value) may be viewed as a quantitative dige st of that\\ndocument. In this view of a document, known in the literature as the bag BAG OF WORDS\\nof words model , the exact ordering of the terms in a document is ignored but\\nthe number of occurrences of each term is material (in contra st to Boolean\\nretrieval). We only retain information on the number of occu rrences of each\\nterm. Thus, the document “Mary is quicker than John” is, in th is view, iden-\\ntical to the document “John is quicker than Mary”. Neverthel ess, it seems\\nintuitive that two documents with similar bag of words repre sentations are\\nsimilar in content. We will develop this intuition further i n Section 6.3.\\nBefore doing so we ﬁrst study the question: are all words in a d ocument\\nequally important? Clearly not; in Section 2.2.2 (page 27) we looked at the\\nidea of stop words – words that we decide not to index at all, and therefore do\\nnot contribute in any way to retrieval and scoring.\\n6.2.1 Inverse document frequency\\nRaw term frequency as above suffers from a critical problem: all terms are\\nconsidered equally important when it comes to assessing rel evancy on a\\nquery. In fact certain terms have little or no discriminatin g power in de-\\ntermining relevance. For instance, a collection of documen ts on the auto\\nindustry is likely to have the term auto in almost every document. To this', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 153}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP118 6 Scoring, term weighting and the vector space model\\nWord cf df\\ntry 10422 8760\\ninsurance 10440 3997\\n◮Figure 6.7 Collection frequency (cf) and document frequency (df) beha ve differ-\\nently, as in this example from the Reuters collection.\\nend, we introduce a mechanism for attenuating the effect of t erms that occur\\ntoo often in the collection to be meaningful for relevance de termination. An\\nimmediate idea is to scale down the term weights of terms with high collec-\\ntion frequency, deﬁned to be the total number of occurrences of a term in the\\ncollection. The idea would be to reduce the tf weight of a term by a factor\\nthat grows with its collection frequency.\\nInstead, it is more commonplace to use for this purpose the document fre- DOCUMENT\\nFREQUENCY quency dft, deﬁned to be the number of documents in the collection that c on-\\ntain a term t. This is because in trying to discriminate between document s\\nfor the purpose of scoring it is better to use a document-leve l statistic (such\\nas the number of documents containing a term) than to use a col lection-wide\\nstatistic for the term. The reason to prefer df to cf is illust rated in Figure 6.7,\\nwhere a simple example shows that collection frequency (cf) and document\\nfrequency (df) can behave rather differently. In particula r, the cf values for\\nbothtryandinsurance are roughly equal, but their df values differ signiﬁ-\\ncantly. Intuitively, we want the few documents that contain insurance to get\\na higher boost for a query on insurance than the many documents containing\\ntryget from a query on try.\\nHow is the document frequency df of a term used to scale its wei ght? De-\\nnoting as usual the total number of documents in a collection byN, we deﬁne\\ntheinverse document frequency (idf) of a term tas follows: INVERSE DOCUMENT\\nFREQUENCY\\nidft=logN\\ndft. (6.7)\\nThus the idf of a rare term is high, whereas the idf of a frequen t term is\\nlikely to be low. Figure 6.8gives an example of idf’s in the Reuters collection\\nof 806,791 documents; in this example logarithms are to the b ase 10. In fact,\\nas we will see in Exercise 6.12, the precise base of the logarithm is not material\\nto ranking. We will give on page 227a justiﬁcation of the particular form in\\nEquation ( 6.7).\\n6.2.2 Tf-idf weighting\\nWe now combine the deﬁnitions of term frequency and inverse d ocument\\nfrequency, to produce a composite weight for each term in eac h document.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 154}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP6.2 Term frequency and weighting 119\\nterm dftidft\\ncar 18,165 1.65\\nauto 6723 2.08\\ninsurance 19,241 1.62\\nbest 25,235 1.5\\n◮Figure 6.8 Example of idf values. Here we give the idf’s of terms with var ious\\nfrequencies in the Reuters collection of 806,791 documents .\\nThe tf-idf weighting scheme assigns to term ta weight in document dgiven TF-IDF\\nby\\ntf-idf t,d=tft,d×idft. (6.8)\\nIn other words, tf-idf t,dassigns to term ta weight in document dthat is\\n1.highest when toccurs many times within a small number of documents\\n(thus lending high discriminating power to those documents );\\n2.lower when the term occurs fewer times in a document, or occur s in many\\ndocuments (thus offering a less pronounced relevance signa l);\\n3.lowest when the term occurs in virtually all documents.\\nAt this point, we may view each document as a vector with one component DOCUMENT VECTOR\\ncorresponding to each term in the dictionary, together with a weight for each\\ncomponent that is given by ( 6.8). For dictionary terms that do not occur in\\na document, this weight is zero. This vector form will prove t o be crucial to\\nscoring and ranking; we will develop these ideas in Section 6.3. As a ﬁrst\\nstep, we introduce the overlap score measure : the score of a document dis the\\nsum, over all query terms, of the number of times each of the qu ery terms\\noccurs in d. We can reﬁne this idea so that we add up not the number of\\noccurrences of each query term tind, but instead the tf-idf weight of each\\nterm in d.\\nScore(q,d) =∑\\nt∈qtf-idf t,d. (6.9)\\nIn Section 6.3we will develop a more rigorous form of Equation ( 6.9).\\n?Exercise 6.8\\nWhy is the idf of a term always ﬁnite?\\nExercise 6.9\\nWhat is the idf of a term that occurs in every document? Compar e this with the use\\nof stop word lists.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 155}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP120 6 Scoring, term weighting and the vector space model\\nDoc1 Doc2 Doc3\\ncar 27 4 24\\nauto 3 33 0\\ninsurance 0 33 29\\nbest 14 0 17\\n◮Figure 6.9 Table of tf values for Exercise 6.10.\\nExercise 6.10\\nConsider the table of term frequencies for 3 documents denot ed Doc1, Doc2, Doc3 in\\nFigure 6.9. Compute the tf-idf weights for the terms car, auto, insurance, best , for each\\ndocument, using the idf values from Figure 6.8.\\nExercise 6.11\\nCan the tf-idf weight of a term in a document exceed 1?\\nExercise 6.12\\nHow does the base of the logarithm in ( 6.7) affect the score calculation in ( 6.9)? How\\ndoes the base of the logarithm affect the relative scores of t wo documents on a given\\nquery?\\nExercise 6.13\\nIf the logarithm in ( 6.7) is computed base 2, suggest a simple approximation to the id f\\nof a term.\\n6.3 The vector space model for scoring\\nIn Section 6.2(page 117) we developed the notion of a document vector that\\ncaptures the relative importance of the terms in a document. The representa-\\ntion of a set of documents as vectors in a common vector space i s known as\\nthevector space model and is fundamental to a host of information retrieval op- VECTOR SPACE MODEL\\nerations ranging from scoring documents on a query, documen t classiﬁcation\\nand document clustering. We ﬁrst develop the basic ideas und erlying vector\\nspace scoring; a pivotal step in this development is the view (Section 6.3.2 )\\nof queries as vectors in the same vector space as the document collection.\\n6.3.1 Dot products\\nWe denote by ⃗V(d)the vector derived from document d, with one com-\\nponent in the vector for each dictionary term. Unless otherw ise speciﬁed,\\nthe reader may assume that the components are computed using the tf-idf\\nweighting scheme, although the particular weighting schem e is immaterial\\nto the discussion that follows. The set of documents in a coll ection then may\\nbe viewed as a set of vectors in a vector space, in which there i s one axis for', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 156}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP6.3 The vector space model for scoring 121\\n0 101\\njealousgossip\\n⃗v(q)⃗v(d1)\\n⃗v(d2)\\n⃗v(d3)θ\\n◮Figure 6.10 Cosine similarity illustrated. sim (d1,d2) =cosθ.\\neach term. This representation loses the relative ordering of the terms in each\\ndocument; recall our example from Section 6.2(page 117), where we pointed\\nout that the documents Mary is quicker than John and John is quicker than Mary\\nare identical in such a bag of words representation.\\nHow do we quantify the similarity between two documents in th is vector\\nspace? A ﬁrst attempt might consider the magnitude of the vec tor difference\\nbetween two document vectors. This measure suffers from a dr awback: two\\ndocuments with very similar content can have a signiﬁcant ve ctor difference\\nsimply because one is much longer than the other. Thus the rel ative distribu-\\ntions of terms may be identical in the two documents, but the a bsolute term\\nfrequencies of one may be far larger.\\nTo compensate for the effect of document length, the standar d way of\\nquantifying the similarity between two documents d1and d2is to compute\\nthecosine similarity of their vector representations ⃗V(d1)and⃗V(d2) COSINE SIMILARITY\\nsim(d1,d2) =⃗V(d1)·⃗V(d2)\\n|⃗V(d1)||⃗V(d2)|, (6.10)\\nwhere the numerator represents the dot product (also known as the inner prod- DOT PRODUCT\\nuct) of the vectors ⃗V(d1)and⃗V(d2), while the denominator is the product of\\ntheir Euclidean lengths . The dot product ⃗x·⃗yof two vectors is deﬁned as EUCLIDEAN LENGTH\\n∑M\\ni=1xiyi. Let⃗V(d)denote the document vector for d, with Mcomponents\\n⃗V1(d). . .⃗VM(d). The Euclidean length of dis deﬁned to be√\\n∑M\\ni=1⃗V2\\ni(d).\\nThe effect of the denominator of Equation ( 6.10) is thus to length-normalize LENGTH -\\nNORMALIZATIONthe vectors ⃗V(d1)and⃗V(d2)to unit vectors ⃗v(d1) =⃗V(d1)/|⃗V(d1)|and', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 157}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP122 6 Scoring, term weighting and the vector space model\\nDoc1 Doc2 Doc3\\ncar 0.88 0.09 0.58\\nauto 0.10 0.71 0\\ninsurance 0 0.71 0.70\\nbest 0.46 0 0.41\\n◮Figure 6.11 Euclidean normalized tf values for documents in Figure 6.9.\\nterm SaS PaP WH\\naffection 115 58 20\\njealous 10 7 11\\ngossip 2 0 6\\n◮Figure 6.12 Term frequencies in three novels. The novels are Austen’s Sense and\\nSensibility, Pride and Prejudice and Brontë’s Wuthering Heights.\\n⃗v(d2) =⃗V(d2)/|⃗V(d2)|. We can then rewrite ( 6.10) as\\nsim(d1,d2) =⃗v(d1)·⃗v(d2). (6.11)\\n✎Example 6.2: Consider the documents in Figure 6.9. We now apply Euclidean\\nnormalization to the tf values from the table, for each of the three documents in the\\ntable. The quantity√\\n∑M\\ni=1⃗V2\\ni(d)has the values 30.56, 46.84 and 41.30 respectively\\nfor Doc1, Doc2 and Doc3. The resulting Euclidean normalized tf values for these\\ndocuments are shown in Figure 6.11.\\nThus, ( 6.11) can be viewed as the dot product of the normalized versions o f\\nthe two document vectors. This measure is the cosine of the an gleθbetween\\nthe two vectors, shown in Figure 6.10. What use is the similarity measure\\nsim(d1,d2)? Given a document d(potentially one of the diin the collection),\\nconsider searching for the documents in the collection most similar to d. Such\\na search is useful in a system where a user may identify a docum ent and\\nseek others like it – a feature available in the results lists of search engines\\nas a more like this feature. We reduce the problem of ﬁnding the document(s)\\nmost similar to dto that of ﬁnding the diwith the highest dot products (sim\\nvalues) ⃗v(d)·⃗v(di). We could do this by computing the dot products between\\n⃗v(d)and each of ⃗v(d1), . . . ,⃗v(dN), then picking off the highest resulting sim\\nvalues.\\n✎Example 6.3: Figure 6.12 shows the number of occurrences of three terms ( affection ,\\njealous andgossip ) in each of the following three novels: Jane Austen’s Sense and Sensi-\\nbility (SaS) and Pride and Prejudice (PaP) and Emily Brontë’s Wuthering Heights (WH).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 158}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP6.3 The vector space model for scoring 123\\nterm SaS PaP WH\\naffection 0.996 0.993 0.847\\njealous 0.087 0.120 0.466\\ngossip 0.017 0 0.254\\n◮Figure 6.13 Term vectors for the three novels of Figure 6.12. These are based on\\nraw term frequency only and are normalized as if these were th e only terms in the\\ncollection. (Since affection andjealous occur in all three documents, their tf-idf weight\\nwould be 0 in most formulations.)\\nOf course, there are many other terms occurring in each of the se novels. In this ex-\\nample we represent each of these novels as a unit vector in thr ee dimensions, corre-\\nsponding to these three terms (only); we use raw term frequen cies here, with no idf\\nmultiplier. The resulting weights are as shown in Figure 6.13.\\nNow consider the cosine similarities between pairs of the re sulting three-dimensional\\nvectors. A simple computation shows that sim( ⃗v(SAS), ⃗v(PAP)) is 0.999, whereas\\nsim(⃗v(SAS), ⃗v(WH)) is 0.888; thus, the two books authored by Austen (SaS an d PaP)\\nare considerably closer to each other than to Brontë’s Wuthering Heights . In fact, the\\nsimilarity between the ﬁrst two is almost perfect (when rest ricted to the three terms\\nwe consider). Here we have considered tf weights, but we coul d of course use other\\nterm weight functions.\\nViewing a collection of Ndocuments as a collection of vectors leads to a\\nnatural view of a collection as a term-document matrix : this is an M×Nmatrix TERM -DOCUMENT\\nMATRIX whose rows represent the Mterms (dimensions) of the Ncolumns, each of\\nwhich corresponds to a document. As always, the terms being i ndexed could\\nbe stemmed before indexing; for instance, jealous andjealousy would under\\nstemming be considered as a single dimension. This matrix vi ew will prove\\nto be useful in Chapter 18.\\n6.3.2 Queries as vectors\\nThere is a far more compelling reason to represent documents as vectors:\\nwe can also view a query as a vector. Consider the query q=jealous gossip .\\nThis query turns into the unit vector ⃗v(q) = ( 0, 0.707, 0.707 )on the three\\ncoordinates of Figures 6.12 and 6.13. The key idea now: to assign to each\\ndocument da score equal to the dot product\\n⃗v(q)·⃗v(d).\\nIn the example of Figure 6.13,Wuthering Heights is the top-scoring docu-\\nment for this query with a score of 0.509, with Pride and Prejudice a distant\\nsecond with a score of 0.085, and Sense and Sensibility last with a score of\\n0.074. This simple example is somewhat misleading: the numb er of dimen-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 159}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP124 6 Scoring, term weighting and the vector space model\\nsions in practice will be far larger than three: it will equal the vocabulary size\\nM.\\nTo summarize, by viewing a query as a “bag of words”, we are abl e to\\ntreat it as a very short document. As a consequence, we can use the cosine\\nsimilarity between the query vector and a document vector as a measure of\\nthe score of the document for that query. The resulting score s can then be\\nused to select the top-scoring documents for a query. Thus we have\\nscore(q,d) =⃗V(q)·⃗V(d)\\n|⃗V(q)||⃗V(d)|. (6.12)\\nA document may have a high cosine score for a query even if it do es not\\ncontain all query terms. Note that the preceding discussion does not hinge\\non any speciﬁc weighting of terms in the document vector, alt hough for the\\npresent we may think of them as either tf or tf-idf weights. In fact, a number\\nof weighting schemes are possible for query as well as docume nt vectors, as\\nillustrated in Example 6.4and developed further in Section 6.4.\\nComputing the cosine similarities between the query vector and each doc-\\nument vector in the collection, sorting the resulting score s and selecting the\\ntopKdocuments can be expensive — a single similarity computatio n can\\nentail a dot product in tens of thousands of dimensions, dema nding tens of\\nthousands of arithmetic operations. In Section 7.1we study how to use an in-\\nverted index for this purpose, followed by a series of heuris tics for improving\\non this.\\n✎Example 6.4: We now consider the query bestcarinsurance on a ﬁctitious collection\\nwith N=1,000,000 documents where the document frequencies of auto, best, car and\\ninsurance are respectively 5000, 50000, 10000 and 1000.\\nterm query document product\\ntf df idf w t,qtf wf w t,d\\nauto 0 5000 2.3 0 1 1 0.41 0\\nbest 1 50000 1.3 1.3 0 0 0 0\\ncar 1 10000 2.0 2.0 1 1 0.41 0.82\\ninsurance 1 1000 3.0 3.0 2 2 0.82 2.46\\nIn this example the weight of a term in the query is simply the i df (and zero for a\\nterm not in the query, such as auto); this is reﬂected in the column header w t,q(the en-\\ntry forauto is zero because the query does not contain the term auto). For documents,\\nwe use tf weighting with no use of idf but with Euclidean norma lization. The former\\nis shown under the column headed wf, while the latter is shown under the column\\nheaded w t,d. Invoking ( 6.9) now gives a net score of 0 +0+0.82+2.46=3.28.\\n6.3.3 Computing vector scores\\nIn a typical setting we have a collection of documents each re presented by a\\nvector, a free text query represented by a vector, and a posit ive integer K. We', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 160}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP6.3 The vector space model for scoring 125\\nCOSINE SCORE(q)\\n1 ﬂoat Scores [N] =0\\n2 Initialize Length [N]\\n3for each query term t\\n4docalculate w t,qand fetch postings list for t\\n5 for each pair(d, tft,d)in postings list\\n6 doScores [d] += wft,d×wt,q\\n7 Read the array Length [d]\\n8for each d\\n9doScores [d] =Scores [d]/Length [d]\\n10 return Top Kcomponents of Scores []\\n◮Figure 6.14 The basic algorithm for computing vector space scores.\\nseek the Kdocuments of the collection with the highest vector space sc ores on\\nthe given query. We now initiate the study of determining the Kdocuments\\nwith the highest vector space scores for a query. Typically, we seek these\\nKtop documents in ordered by decreasing score; for instance m any search\\nengines use K=10 to retrieve and rank-order the ﬁrst page of the ten best\\nresults. Here we give the basic algorithm for this computati on; we develop a\\nfuller treatment of efﬁcient techniques and approximation s in Chapter 7.\\nFigure 6.14 gives the basic algorithm for computing vector space scores .\\nThe array Length holds the lengths (normalization factors) for each of the N\\ndocuments, whereas the array Scores holds the scores for eac h of the docu-\\nments. When the scores are ﬁnally computed in Step 9, all that remains in\\nStep 10 is to pick off the Kdocuments with the highest scores.\\nThe outermost loop beginning Step 3 repeats the updating of S cores, iter-\\nating over each query term tin turn. In Step 5 we calculate the weight in\\nthe query vector for term t. Steps 6-8 update the score of each document by\\nadding in the contribution from term t. This process of adding in contribu-\\ntions one query term at a time is sometimes known as term-at-a-time scoring TERM -AT-A-TIME\\nor accumulation, and the Nelements of the array Scores are therefore known\\nasaccumulators . For this purpose, it would appear necessary to store, with ACCUMULATOR\\neach postings entry, the weight wf t,dof term tin document d(we have thus\\nfar used either tf or tf-idf for this weight, but leave open th e possibility of\\nother functions to be developed in Section 6.4). In fact this is wasteful, since\\nstoring this weight may require a ﬂoating point number. Two i deas help alle-\\nviate this space problem. First, if we are using inverse docu ment frequency,\\nwe need not precompute idf t; it sufﬁces to store N/df tat the head of the\\npostings for t. Second, we store the term frequency tf t,dfor each postings en-\\ntry. Finally, Step 12 extracts the top Kscores – this requires a priority queue', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 161}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP126 6 Scoring, term weighting and the vector space model\\ndata structure, often implemented using a heap. Such a heap t akes no more\\nthan 2 Ncomparisons to construct, following which each of the Ktop scores\\ncan be extracted from the heap at a cost of O(logN)comparisons.\\nNote that the general algorithm of Figure 6.14 does not prescribe a speciﬁc\\nimplementation of how we traverse the postings lists of the v arious query\\nterms; we may traverse them one term at a time as in the loop beg inning\\nat Step 3, or we could in fact traverse them concurrently as in Figure 1.6. In\\nsuch a concurrent postings traversal we compute the scores o f one document\\nat a time, so that it is sometimes called document-at-a-time scoring. We will DOCUMENT -AT-A-TIME\\nsay more about this in Section 7.1.5 .\\n?Exercise 6.14\\nIf we were to stem jealous andjealousy to a common stem before setting up the vector\\nspace, detail how the deﬁnitions of tf and idf should be modiﬁ ed.\\nExercise 6.15\\nRecall the tf-idf weights computed in Exercise 6.10. Compute the Euclidean nor-\\nmalized document vectors for each of the documents, where ea ch vector has four\\ncomponents, one for each of the four terms.\\nExercise 6.16\\nVerify that the sum of the squares of the components of each of the document vectors\\nin Exercise 6.15 is 1 (to within rounding error). Why is this the case?\\nExercise 6.17\\nWith term weights as computed in Exercise 6.15, rank the three documents by com-\\nputed score for the query carinsurance , for each of the following cases of term weight-\\ning in the query:\\n1.The weight of a term is 1 if present in the query, 0 otherwise.\\n2.Euclidean normalized idf.\\n6.4 Variant tf-idf functions\\nFor assigning a weight for each term in each document, a numbe r of alterna-\\ntives to tf and tf-idf have been considered. We discuss some o f the principal\\nones here; a more complete development is deferred to Chapte r11. We will\\nsummarize these alternatives in Section 6.4.3 (page 128).\\n6.4.1 Sublinear tf scaling\\nIt seems unlikely that twenty occurrences of a term in a docum ent truly carry\\ntwenty times the signiﬁcance of a single occurrence. Accord ingly, there has\\nbeen considerable research into variants of term frequency that go beyond\\ncounting the number of occurrences of a term. A common modiﬁc ation is', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 162}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP6.4 Variant tf-idf functions 127\\nto use instead the logarithm of the term frequency, which ass igns a weight\\ngiven by\\nwft,d={\\n1+log tf t,dif tf t,d>0\\n0 otherwise. (6.13)\\nIn this form, we may replace tf by some other function wf as in ( 6.13), to\\nobtain:\\nwf-idf t,d=wft,d×idft. (6.14)\\nEquation ( 6.9) can then be modiﬁed by replacing tf-idf by wf-idf as deﬁned\\nin (6.14).\\n6.4.2 Maximum tf normalization\\nOne well-studied technique is to normalize the tf weights of all terms occur-\\nring in a document by the maximum tf in that document. For each document\\nd, let tf max(d) = max τ∈dtfτ,d, where τranges over all terms in d. Then, we\\ncompute a normalized term frequency for each term tin document dby\\nntft,d=a+ (1−a)tft,d\\ntfmax(d), (6.15)\\nwhere ais a value between 0 and 1 and is generally set to 0.4, although some\\nearly work used the value 0.5. The term ain (6.15) is a smoothing term whose SMOOTHING\\nrole is to damp the contribution of the second term – which may be viewed as\\na scaling down of tf by the largest tf value in d. We will encounter smoothing\\nfurther in Chapter 13when discussing classiﬁcation; the basic idea is to avoid\\na large swing in ntf t,dfrom modest changes in tf t,d(say from 1 to 2). The main\\nidea of maximum tf normalization is to mitigate the followin g anomaly: we\\nobserve higher term frequencies in longer documents, merel y because longer\\ndocuments tend to repeat the same words over and over again. T o appreciate\\nthis, consider the following extreme example: supposed we w ere to take a\\ndocument dand create a new document d′by simply appending a copy of d\\nto itself. While d′should be no more relevant to any query than dis, the use\\nof (6.9) would assign it twice as high a score as d. Replacing tf-idf t,din (6.9) by\\nntf-idf t,deliminates the anomaly in this example. Maximum tf normaliz ation\\ndoes suffer from the following issues:\\n1.The method is unstable in the following sense: a change in the stop word\\nlist can dramatically alter term weightings (and therefore ranking). Thus,\\nit is hard to tune.\\n2.A document may contain an outlier term with an unusually larg e num-\\nber of occurrences of that term, not representative of the co ntent of that\\ndocument.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 163}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP128 6 Scoring, term weighting and the vector space model\\nTerm frequency Document frequency Normalization\\nn (natural) tf t,d n (no) 1 n (none) 1\\nl (logarithm) 1 +log(tft,d) t (idf) logN\\ndftc (cosine)1√\\nw2\\n1+w2\\n2+...+w2\\nM\\na (augmented) 0.5 +0.5×tft,d\\nmax t(tft,d)p (prob idf) max {0, logN−dft\\ndft}u (pivoted\\nunique)1/u(Section 6.4.4 )\\nb (boolean){1 if tf t,d>0\\n0 otherwiseb (byte size) 1/ CharLengthα,α<1\\nL (log ave)1+log(tft,d)\\n1+log(avet∈d(tft,d))\\n◮Figure 6.15 SMART notation for tf-idf variants. Here CharLength is the number\\nof characters in the document.\\n3.More generally, a document in which the most frequent term ap pears\\nroughly as often as many other terms should be treated differ ently from\\none with a more skewed distribution.\\n6.4.3 Document and query weighting schemes\\nEquation ( 6.12) is fundamental to information retrieval systems that use a ny\\nform of vector space scoring. Variations from one vector spa ce scoring method\\nto another hinge on the speciﬁc choices of weights in the vect ors⃗V(d)and\\n⃗V(q). Figure 6.15 lists some of the principal weighting schemes in use for\\neach of ⃗V(d)and⃗V(q), together with a mnemonic for representing a spe-\\nciﬁc combination of weights; this system of mnemonics is som etimes called\\nSMART notation, following the authors of an early text retri eval system. The\\nmnemonic for representing a combination of weights takes th e form ddd.qqq\\nwhere the ﬁrst triplet gives the term weighting of the docume nt vector, while\\nthe second triplet gives the weighting in the query vector. T he ﬁrst letter in\\neach triplet speciﬁes the term frequency component of the we ighting, the\\nsecond the document frequency component, and the third the f orm of nor-\\nmalization used. It is quite common to apply different norma lization func-\\ntions to ⃗V(d)and⃗V(q). For example, a very standard weighting scheme\\nislnc.ltc , where the document vector has log-weighted term frequency , no\\nidf (for both effectiveness and efﬁciency reasons), and cos ine normalization,\\nwhile the query vector uses log-weighted term frequency, id f weighting, and\\ncosine normalization.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 164}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP6.4 Variant tf-idf functions 129\\n✄6.4.4 Pivoted normalized document length\\nIn Section 6.3.1 we normalized each document vector by the Euclidean length\\nof the vector, so that all document vectors turned into unit v ectors. In doing\\nso, we eliminated all information on the length of the origin al document;\\nthis masks some subtleties about longer documents. First, l onger documents\\nwill – as a result of containing more terms – have higher tf val ues. Second,\\nlonger documents contain more distinct terms. These factor s can conspire to\\nraise the scores of longer documents, which (at least for som e information\\nneeds) is unnatural. Longer documents can broadly be lumped into two cat-\\negories: (1) verbose documents that essentially repeat the same content – in\\nthese, the length of the document does not alter the relative weights of dif-\\nferent terms; (2) documents covering multiple different to pics, in which the\\nsearch terms probably match small segments of the document b ut not all of\\nit – in this case, the relative weights of terms are quite diff erent from a single\\nshort document that matches the query terms. Compensating f or this phe-\\nnomenon is a form of document length normalization that is in dependent of\\nterm and document frequencies. To this end, we introduce a fo rm of normal-\\nizing the vector representations of documents in the collec tion, so that the\\nresulting “normalized” documents are not necessarily of un it length. Then,\\nwhen we compute the dot product score between a (unit) query v ector and\\nsuch a normalized document, the score is skewed to account fo r the effect\\nof document length on relevance. This form of compensation f or document\\nlength is known as pivoted document length normalization . PIVOTED DOCUMENT\\nLENGTH\\nNORMALIZATIONConsider a document collection together with an ensemble of queries for\\nthat collection. Suppose that we were given, for each query qand for each\\ndocument d, a Boolean judgment of whether or not dis relevant to the query\\nq; in Chapter 8we will see how to procure such a set of relevance judgments\\nfor a query ensemble and a document collection. Given this se t of relevance\\njudgments, we may compute a probability of relevance as a function of docu-\\nment length, averaged over all queries in the ensemble. The r esulting plot\\nmay look like the curve drawn in thick lines in Figure 6.16. To compute this\\ncurve, we bucket documents by length and compute the fractio n of relevant\\ndocuments in each bucket, then plot this fraction against th e median docu-\\nment length of each bucket. (Thus even though the “curve” in F igure 6.16\\nappears to be continuous, it is in fact a histogram of discret e buckets of doc-\\nument length.)\\nOn the other hand, the curve in thin lines shows what might hap pen with\\nthe same documents and query ensemble if we were to use releva nce as pre-\\nscribed by cosine normalization Equation ( 6.12) – thus, cosine normalization\\nhas a tendency to distort the computed relevance vis-à-vis t he true relevance,\\nat the expense of longer documents. The thin and thick curves crossover at a\\npoint pcorresponding to document length ℓp, which we refer to as the pivot', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 165}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP130 6 Scoring, term weighting and the vector space model\\nDocument lengthRelevance\\n\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\n\\x08\\x08\\x08\\x08\\x08\\x08\\x08\\x08\\nℓpp\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x08\\x08\\x08\\x08\\x08\\x08\\x08\\x08\\n-6\\n◮Figure 6.16 Pivoted document length normalization.\\nlength ; dashed lines mark this point on the x−and y−axes. The idea of\\npivoted document length normalization would then be to “rot ate” the co-\\nsine normalization curve counter-clockwise about pso that it more closely\\nmatches thick line representing the relevance vs. document length curve.\\nAs mentioned at the beginning of this section, we do so by usin g in Equa-\\ntion ( 6.12) a normalization factor for each document vector ⃗V(d)that is not\\nthe Euclidean length of that vector, but instead one that is l arger than the Eu-\\nclidean length for documents of length less than ℓp, and smaller for longer\\ndocuments.\\nTo this end, we ﬁrst note that the normalizing term for ⃗V(d)in the de-\\nnominator of Equation ( 6.12) is its Euclidean length, denoted |⃗V(d)|. In the\\nsimplest implementation of pivoted document length normal ization, we use\\na normalization factor in the denominator that is linear in |⃗V(d)|, but one\\nof slope <1 as in Figure 6.17. In this ﬁgure, the x−axis represents|⃗V(d)|,\\nwhile the y−axis represents possible normalization factors we can use. The\\nthin line y=xdepicts the use of cosine normalization. Notice the followi ng\\naspects of the thick line representing pivoted length norma lization:\\n1.It is linear in the document length and has the form\\na|⃗V(d)|+ (1−a)piv, (6.16)', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 166}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP6.4 Variant tf-idf functions 131\\n|⃗V(d)|Pivoted normalization\\ny=x; Cosine\\nPivoted\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\npiv\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\n-6\\n◮Figure 6.17 Implementing pivoted document length normalization by lin ear scal-\\ning.\\nwhere piv is the cosine normalization value at which the two c urves in-\\ntersect.\\n2.Its slope is a<1 and (3) it crosses the y=xline at piv.\\nIt has been argued that in practice, Equation ( 6.16) is well approximated by\\naud+ (1−a)piv,\\nwhere udis the number of unique terms in document d.\\nOf course, pivoted document length normalization is not app ropriate for\\nall applications. For instance, in a collection of answers t o frequently asked\\nquestions (say, at a customer service website), relevance m ay have little to\\ndo with document length. In other cases the dependency may be more com-\\nplex than can be accounted for by a simple linear pivoted norm alization. In\\nsuch cases, document length can be used as a feature in the mac hine learning\\nbased scoring approach of Section 6.1.2 .\\n?Exercise 6.18\\nOne measure of the similarity of two vectors is the Euclidean distance (orL2distance) EUCLIDEAN DISTANCE\\nbetween them:\\n|⃗x−⃗y|=\\ued6a\\ued6b\\ued6b√M\\n∑\\ni=1(xi−yi)2', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 167}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP132 6 Scoring, term weighting and the vector space model\\nquery document\\nword tf wf df idf qi=wf-idf tf wf di=normalized wf qi·di\\ndigital 10,000\\nvideo 100,000\\ncameras 50,000\\n◮Table 6.1 Cosine computation for Exercise 6.19.\\nGiven a query qand documents d1,d2, . . ., we may rank the documents diin order\\nof increasing Euclidean distance from q. Show that if qand the diare all normalized\\nto unit vectors, then the rank ordering produced by Euclidea n distance is identical to\\nthat produced by cosine similarities.\\nExercise 6.19\\nCompute the vector space similarity between the query “digi tal cameras” and the\\ndocument “digital cameras and video cameras” by ﬁlling out t he empty columns in\\nTable 6.1. Assume N=10,000,000, logarithmic term weighting (wf columns) for\\nquery and document, idf weighting for the query only and cosi ne normalization for\\nthe document only. Treat andas a stop word. Enter term counts in the tf columns.\\nWhat is the ﬁnal similarity score?\\nExercise 6.20\\nShow that for the query affection , the relative ordering of the scores of the three doc-\\numents in Figure 6.13 is the reverse of the ordering of the scores for the query jealous\\ngossip .\\nExercise 6.21\\nIn turning a query into a unit vector in Figure 6.13, we assigned equal weights to each\\nof the query terms. What other principled approaches are pla usible?\\nExercise 6.22\\nConsider the case of a query term that is not in the set of Mindexed terms; thus our\\nstandard construction of the query vector results in ⃗V(q)not being in the vector space\\ncreated from the collection. How would one adapt the vector s pace representation to\\nhandle this case?\\nExercise 6.23\\nRefer to the tf and idf values for four terms and three documen ts in Exercise 6.10.\\nCompute the two top scoring documents on the query best car insurance for each of\\nthe following weighing schemes: (i) nnn.atc ; (ii)ntc.atc .\\nExercise 6.24\\nSuppose that the word coyote does not occur in the collection used in Exercises 6.10\\nand 6.23. How would one compute ntc.atc scores for the query coyote insurance ?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 168}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP6.5 References and further reading 133\\n6.5 References and further reading\\nChapter 7develops the computational aspects of vector space scoring .Luhn\\n(1957 ;1958 ) describes some of the earliest reported applications of te rm weight-\\ning. His paper dwells on the importance of medium frequency t erms (terms\\nthat are neither too commonplace nor too rare) and may be thou ght of as an-\\nticipating tf-idf and related weighting schemes. Spärck Jones (1972 ) builds\\non this intuition through detailed experiments showing the use of inverse\\ndocument frequency in term weighting. A series of extension s and theoret-\\nical justiﬁcations of idf are due to Salton and Buckley (1987 )Robertson and\\nJones (1976 ),Croft and Harper (1979 ) and Papineni (2001 ). Robertson main-\\ntains a web page ( http://www.soi.city.ac.uk/ ˜ser/idf.html ) containing the history\\nof idf, including soft copies of early papers that predated e lectronic versions\\nof journal article. Singhal et al. (1996a ) develop pivoted document length\\nnormalization. Probabilistic language models (Chapter 11) develop weight-\\ning techniques that are more nuanced than tf-idf; the reader will ﬁnd this\\ndevelopment in Section 11.4.3 .\\nWe observed that by assigning a weight for each term in a docum ent, a\\ndocument may be viewed as a vector of term weights, one for eac h term in\\nthe collection. The SMART information retrieval system at C ornell ( Salton\\n1971b ) due to Salton and colleagues was perhaps the ﬁrst to view a do c-\\nument as a vector of weights. The basic computation of cosine scores as\\ndescribed in Section 6.3.3 is due to Zobel and Moffat (2006 ). The two query\\nevaluation strategies term-at-a-time and document-at-a- time are discussed\\nbyTurtle and Flood (1995 ).\\nThe SMART notation for tf-idf term weighting schemes in Figu re6.15 is\\npresented in ( Salton and Buckley 1988 ,Singhal et al. 1995 ;1996b ). Not all\\nversions of the notation are consistent; we most closely fol low ( Singhal et al.\\n1996b ). A more detailed and exhaustive notation was developed in Moffat\\nand Zobel (1998 ), considering a larger palette of schemes for term and doc-\\nument frequency weighting. Beyond the notation, Moffat and Zobel (1998 )\\nsought to set up a space of feasible weighting functions thro ugh which hill-\\nclimbing approaches could be used to begin with weighting sc hemes that\\nperformed well, then make local improvements to identify th e best combi-\\nnations. However, they report that such hill-climbing meth ods failed to lead\\nto any conclusions on the best weighting schemes.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 169}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 170}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 135\\n7Computing scores in a complete\\nsearch system\\nChapter 6developed the theory underlying term weighting in document s\\nfor the purposes of scoring, leading up to vector space model s and the basic\\ncosine scoring algorithm of Section 6.3.3 (page 124). In this chapter we be-\\ngin in Section 7.1with heuristics for speeding up this computation; many of\\nthese heuristics achieve their speed at the risk of not ﬁndin g quite the top K\\ndocuments matching the query. Some of these heuristics gene ralize beyond\\ncosine scoring. With Section 7.1in place, we have essentially all the compo-\\nnents needed for a complete search engine. We therefore take a step back\\nfrom cosine scoring, to the more general problem of computin g scores in a\\nsearch engine. In Section 7.2we outline a complete search engine, includ-\\ning indexes and structures to support not only cosine scorin g but also more\\ngeneral ranking factors such as query term proximity. We des cribe how all\\nof the various pieces ﬁt together in Section 7.2.4 . We conclude this chapter\\nwith Section 7.3, where we discuss how the vector space model for free text\\nqueries interacts with common query operators.\\n7.1 Efﬁcient scoring and ranking\\nWe begin by recapping the algorithm of Figure 6.14. For a query such as q=\\njealous gossip , two observations are immediate:\\n1.The unit vector ⃗v(q)has only two non-zero components.\\n2.In the absence of any weighting for query terms, these non-ze ro compo-\\nnents are equal – in this case, both equal 0.707.\\nFor the purpose of ranking the documents matching this query , we are\\nreally interested in the relative (rather than absolute) sc ores of the documents\\nin the collection. To this end, it sufﬁces to compute the cosi ne similarity from\\neach document unit vector ⃗v(d)to⃗V(q)(in which all non-zero components\\nof the query vector are set to 1), rather than to the unit vecto r⃗v(q). For any', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 171}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP136 7 Computing scores in a complete search system\\nFASTCOSINE SCORE(q)\\n1 ﬂoat Scores [N] =0\\n2for each d\\n3doInitialize Length [d]to the length of doc d\\n4for each query term t\\n5docalculate w t,qand fetch postings list for t\\n6 for each pair(d, tft,d)in postings list\\n7 doadd wf t,dtoScores [d]\\n8 Read the array Length [d]\\n9for each d\\n10 doDivide Scores [d]byLength [d]\\n11 return Top Kcomponents of Scores []\\n◮Figure 7.1 A faster algorithm for vector space scores.\\ntwo documents d1,d2\\n⃗V(q)·⃗v(d1)>⃗V(q)·⃗v(d2)⇔⃗v(q)·⃗v(d1)> ⃗v(q)·⃗v(d2). (7.1)\\nFor any document d, the cosine similarity ⃗V(q)·⃗v(d)is the weighted sum,\\nover all terms in the query q, of the weights of those terms in d. This in turn\\ncan be computed by a postings intersection exactly as in the a lgorithm of\\nFigure 6.14, with line 8 altered since we take wt,qto be 1 so that the multiply-\\nadd in that step becomes just an addition; the result is shown in Figure 7.1.\\nWe walk through the postings in the inverted index for the ter ms in q, accu-\\nmulating the total score for each document – very much as in pr ocessing a\\nBoolean query, except we assign a positive score to each docu ment that ap-\\npears in any of the postings being traversed. As mentioned in Section 6.3.3\\nwe maintain an idf value for each dictionary term and a tf valu e for each\\npostings entry. This scheme computes a score for every docum ent in the\\npostings of any of the query terms; the total number of such do cuments may\\nbe considerably smaller than N.\\nGiven these scores, the ﬁnal step before presenting results to a user is to\\npick out the Khighest-scoring documents. While one could sort the comple te\\nset of scores, a better approach is to use a heap to retrieve on ly the top K\\ndocuments in order. Where Jis the number of documents with non-zero\\ncosine scores, constructing such a heap can be performed in 2 Jcomparison\\nsteps, following which each of the Khighest scoring documents can be “read\\noff” the heap with log Jcomparison steps.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 172}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP7.1 Efﬁcient scoring and ranking 137\\n7.1.1 Inexact top Kdocument retrieval\\nThus far, we have focused on retrieving precisely the Khighest-scoring doc-\\numents for a query. We now consider schemes by which we produc eKdoc-\\numents that are likely to be among the Khighest scoring documents for a\\nquery. In doing so, we hope to dramatically lower the cost of c omputing\\ntheKdocuments we output, without materially altering the user’ s perceived\\nrelevance of the top Kresults. Consequently, in most applications it sufﬁces\\nto retrieve Kdocuments whose scores are very close to those of the Kbest.\\nIn the sections that follow we detail schemes that retrieve Ksuch documents\\nwhile potentially avoiding computing scores for most of the Ndocuments in\\nthe collection.\\nSuch inexact top- Kretrieval is not necessarily, from the user’s perspective,\\na bad thing. The top Kdocuments by the cosine measure are in any case not\\nnecessarily the Kbest for the query: cosine similarity is only a proxy for the\\nuser’s perceived relevance. In Sections 7.1.2 –7.1.6 below, we give heuristics\\nusing which we are likely to retrieve Kdocuments with cosine scores close\\nto those of the top Kdocuments. The principal cost in computing the out-\\nput stems from computing cosine similarities between the qu ery and a large\\nnumber of documents. Having a large number of documents in co ntention\\nalso increases the selection cost in the ﬁnal stage of cullin g the top Kdocu-\\nments from a heap. We now consider a series of ideas designed t o eliminate\\na large number of documents without computing their cosine s cores. The\\nheuristics have the following two-step scheme:\\n1.Find a set Aof documents that are contenders, where K<|A|≪ N.A\\ndoes not necessarily contain the Ktop-scoring documents for the query,\\nbut is likely to have many documents with scores near those of the top K.\\n2.Return the Ktop-scoring documents in A.\\nFrom the descriptions of these ideas it will be clear that man y of them require\\nparameters to be tuned to the collection and application at h and; pointers\\nto experience in setting these parameters may be found at the end of this\\nchapter. It should also be noted that most of these heuristic s are well-suited\\nto free text queries, but not for Boolean or phrase queries.\\n7.1.2 Index elimination\\nFor a multi-term query q, it is clear we only consider documents containing at\\nleast one of the query terms. We can take this a step further us ing additional\\nheuristics:\\n1.We only consider documents containing terms whose idf excee ds a preset\\nthreshold. Thus, in the postings traversal, we only travers e the postings', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 173}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP138 7 Computing scores in a complete search system\\nfor terms with high idf. This has a fairly signiﬁcant beneﬁt: the post-\\nings lists of low-idf terms are generally long; with these re moved from\\ncontention, the set of documents for which we compute cosine s is greatly\\nreduced. One way of viewing this heuristic: low-idf terms ar e treated as\\nstop words and do not contribute to scoring. For instance, on the query\\ncatcher in the rye , we only traverse the postings for catcher andrye. The\\ncutoff threshold can of course be adapted in a query-depende nt manner.\\n2.We only consider documents that contain many (and as a specia l case,\\nall) of the query terms. This can be accomplished during the p ostings\\ntraversal; we only compute scores for documents containing all (or many)\\nof the query terms. A danger of this scheme is that by requirin g all (or\\neven many) query terms to be present in a document before cons idering\\nit for cosine computation, we may end up with fewer than Kcandidate\\ndocuments in the output. This issue will discussed further i n Section 7.2.1 .\\n7.1.3 Champion lists\\nThe idea of champion lists (sometimes also called fancy lists ortop docs ) is to\\nprecompute, for each term tin the dictionary, the set of the rdocuments\\nwith the highest weights for t; the value of ris chosen in advance. For tf-\\nidf weighting, these would be the rdocuments with the highest tf values for\\nterm t. We call this set of rdocuments the champion list for term t.\\nNow, given a query qwe create a set Aas follows: we take the union\\nof the champion lists for each of the terms comprising q. We now restrict\\ncosine computation to only the documents in A. A critical parameter in this\\nscheme is the value r, which is highly application dependent. Intuitively, r\\nshould be large compared with K, especially if we use any form of the index\\nelimination described in Section 7.1.2 . One issue here is that the value ris set\\nat the time of index construction, whereas Kis application dependent and\\nmay not be available until the query is received; as a result w e may (as in the\\ncase of index elimination) ﬁnd ourselves with a set Athat has fewer than K\\ndocuments. There is no reason to have the same value of rfor all terms in the\\ndictionary; it could for instance be set to be higher for rare r terms.\\n7.1.4 Static quality scores and ordering\\nWe now further develop the idea of champion lists, in the some what more\\ngeneral setting of static quality scores . In many search engines, we have avail- STATIC QUALITY\\nSCORES able a measure of quality g(d)for each document dthat is query-independent\\nand thus static . This quality measure may be viewed as a number between\\nzero and one. For instance, in the context of news stories on t he web, g(d)\\nmay be derived from the number of favorable reviews of the sto ry by web', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 174}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP7.1 Efﬁcient scoring and ranking 139\\n◮Figure 7.2 A static quality-ordered index. In this example we assume th at Doc1,\\nDoc2 and Doc3 respectively have static quality scores g(1) =0.25, g(2) =0.5,g(3) =\\n1.\\nsurfers. Section 4.6(page 80) provides further discussion on this topic, as\\ndoes Chapter 21in the context of web search.\\nThe net score for a document dis some combination of g(d)together with\\nthe query-dependent score induced (say) by ( 6.12). The precise combination\\nmay be determined by the learning methods of Section 6.1.2 , to be developed\\nfurther in Section 15.4.1 ; but for the purposes of our exposition here, let us\\nconsider a simple sum:\\nnet-score (q,d) =g(d) +⃗V(q)·⃗V(d)\\n|⃗V(q)||⃗V(d)|. (7.2)\\nIn this simple form, the static quality g(d)and the query-dependent score\\nfrom ( 6.10) have equal contributions, assuming each is between 0 and 1.\\nOther relative weightings are possible; the effectiveness of our heuristics will\\ndepend on the speciﬁc relative weighting.\\nFirst, consider ordering the documents in the postings list for each term by\\ndecreasing value of g(d). This allows us to perform the postings intersection\\nalgorithm of Figure 1.6. In order to perform the intersection by a single pass\\nthrough the postings of each query term, the algorithm of Fig ure1.6relied on\\nthe postings being ordered by document IDs. But in fact, we on ly required\\nthat all postings be ordered by a single common ordering; her e we rely on the\\ng(d)values to provide this common ordering. This is illustrated in Figure 7.2,\\nwhere the postings are ordered in decreasing order of g(d).\\nThe ﬁrst idea is a direct extension of champion lists: for a we ll-chosen\\nvalue r, we maintain for each term taglobal champion list of the rdocuments', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 175}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP140 7 Computing scores in a complete search system\\nwith the highest values for g(d) +tf-idf t,d. The list itself is, like all the post-\\nings lists considered so far, sorted by a common order (eithe r by document\\nIDs or by static quality). Then at query time, we only compute the net scores\\n(7.2) for documents in the union of these global champion lists. I ntuitively,\\nthis has the effect of focusing on documents likely to have la rge net scores.\\nWe conclude the discussion of global champion lists with one further idea.\\nWe maintain for each term ttwo postings lists consisting of disjoint sets of\\ndocuments, each sorted by g(d)values. The ﬁrst list, which we call high,\\ncontains the mdocuments with the highest tf values for t. The second list,\\nwhich we call low, contains all other documents containing t. When process-\\ning a query, we ﬁrst scan only the high lists of the query terms , computing\\nnet scores for any document on the high lists of all (or more th an a certain\\nnumber of) query terms. If we obtain scores for Kdocuments in the process,\\nwe terminate. If not, we continue the scanning into the low li sts, scoring doc-\\numents in these postings lists. This idea is developed furth er in Section 7.2.1 .\\n7.1.5 Impact ordering\\nIn all the postings lists described thus far, we order the doc uments con-\\nsistently by some common ordering: typically by document ID but in Sec-\\ntion 7.1.4 by static quality scores. As noted at the end of Section 6.3.3 , such a\\ncommon ordering supports the concurrent traversal of all of the query terms’\\npostings lists, computing the score for each document as we e ncounter it.\\nComputing scores in this manner is sometimes referred to as d ocument-at-a-\\ntime scoring. We will now introduce a technique for inexact t op-Kretrieval\\nin which the postings are not all ordered by a common ordering , thereby\\nprecluding such a concurrent traversal. We will therefore r equire scores to\\nbe “accumulated” one term at a time as in the scheme of Figure 6.14, so that\\nwe have term-at-a-time scoring.\\nThe idea is to order the documents din the postings list of term tby\\ndecreasing order of tf t,d. Thus, the ordering of documents will vary from\\none postings list to another, and we cannot compute scores by a concurrent\\ntraversal of the postings lists of all query terms. Given pos tings lists ordered\\nby decreasing order of tf t,d, two ideas have been found to signiﬁcantly lower\\nthe number of documents for which we accumulate scores: (1) w hen travers-\\ning the postings list for a query term t, we stop after considering a preﬁx\\nof the postings list – either after a ﬁxed number of documents rhave been\\nseen, or after the value of tf t,dhas dropped below a threshold; (2) when ac-\\ncumulating scores in the outer loop of Figure 6.14, we consider the query\\nterms in decreasing order of idf, so that the query terms like ly to contribute\\nthe most to the ﬁnal scores are considered ﬁrst. This latter i dea too can be\\nadaptive at the time of processing a query: as we get to query t erms with\\nlower idf, we can determine whether to proceed based on the ch anges in', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 176}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP7.1 Efﬁcient scoring and ranking 141\\ndocument scores from processing the previous query term. If these changes\\nare minimal, we may omit accumulation from the remaining que ry terms, or\\nalternatively process shorter preﬁxes of their postings li sts.\\nThese ideas form a common generalization of the methods intr oduced in\\nSections 7.1.2 –7.1.4 . We may also implement a version of static ordering in\\nwhich each postings list is ordered by an additive combinati on of static and\\nquery-dependent scores. We would again lose the consistenc y of ordering\\nacross postings, thereby having to process query terms one a t time accumu-\\nlating scores for all documents as we go along. Depending on t he particular\\nscoring function, the postings list for a document may be ord ered by other\\nquantities than term frequency; under this more general set ting, this idea is\\nknown as impact ordering.\\n7.1.6 Cluster pruning\\nIncluster pruning we have a preprocessing step during which we cluster the\\ndocument vectors. Then at query time, we consider only docum ents in a\\nsmall number of clusters as candidates for which we compute c osine scores.\\nSpeciﬁcally, the preprocessing step is as follows:\\n1.Pick√\\nNdocuments at random from the collection. Call these leaders .\\n2.For each document that is not a leader, we compute its nearest leader.\\nWe refer to documents that are not leaders as followers . Intuitively, in the par-\\ntition of the followers induced by the use of√\\nNrandomly chosen leaders,\\nthe expected number of followers for each leader is ≈N/√\\nN=√\\nN. Next,\\nquery processing proceeds as follows:\\n1.Given a query q, ﬁnd the leader Lthat is closest to q. This entails comput-\\ning cosine similarities from qto each of the√\\nNleaders.\\n2.The candidate set Aconsists of Ltogether with its followers. We compute\\nthe cosine scores for all documents in this candidate set.\\nThe use of randomly chosen leaders for clustering is fast and likely to re-\\nﬂect the distribution of the document vectors in the vector s pace: a region\\nof the vector space that is dense in documents is likely to pro duce multi-\\nple leaders and thus a ﬁner partition into sub-regions. This illustrated in\\nFigure 7.3.\\nVariations of cluster pruning introduce additional parame ters b1and b2,\\nboth of which are positive integers. In the pre-processing s tep we attach\\neach follower to its b1closest leaders, rather than a single closest leader. At\\nquery time we consider the b2leaders closest to the query q. Clearly, the basic\\nscheme above corresponds to the case b1=b2=1. Further, increasing b1or', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 177}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP142 7 Computing scores in a complete search system\\n◮Figure 7.3 Cluster pruning.\\nb2increases the likelihood of ﬁnding Kdocuments that are more likely to be\\nin the set of true top-scoring Kdocuments, at the expense of more compu-\\ntation. We reiterate this approach when describing cluster ing in Chapter 16\\n(page 354).\\n?Exercise 7.1\\nWe suggested above (Figure 7.2) that the postings for static quality ordering be in\\ndecreasing order of g(d). Why do we use the decreasing rather than the increasing\\norder?\\nExercise 7.2\\nWhen discussing champion lists, we simply used the rdocuments with the largest tf\\nvalues to create the champion list for t. But when considering global champion lists,\\nwe used idf as well, identifying documents with the largest v alues of g(d) +tf-idf t,d.\\nWhy do we differentiate between these two cases?\\nExercise 7.3\\nIf we were to only have one-term queries, explain why the use o f global champion\\nlists with r=Ksufﬁces for identifying the Khighest scoring documents. What is a\\nsimple modiﬁcation to this idea if we were to only have s-term queries for any ﬁxed\\ninteger s>1?\\nExercise 7.4\\nExplain how the common global ordering by g(d)values in all high and low lists\\nhelps make the score computation efﬁcient.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 178}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP7.2 Components of an information retrieval system 143\\nExercise 7.5\\nConsider again the data of Exercise 6.23 withnnn.atc for the query-dependent scor-\\ning. Suppose that we were given static quality scores of 1 for Doc1 and 2 for Doc2.\\nDetermine under Equation ( 7.2) what ranges of static quality score for Doc3 result in\\nit being the ﬁrst, second or third result for the query best car insurance .\\nExercise 7.6\\nSketch the frequency-ordered postings for the data in Figur e6.9.\\nExercise 7.7\\nLet the static quality scores for Doc1, Doc2 and Doc3 in Figur e6.11 be respectively\\n0.25, 0.5 and 1. Sketch the postings for impact ordering when each postings list is\\nordered by the sum of the static quality score and the Euclide an normalized tf values\\nin Figure 6.11.\\nExercise 7.8\\nThe nearest-neighbor problem in the plane is the following: given a set of Ndata\\npoints on the plane, we preprocess them into some data struct ure such that, given\\na query point Q, we seek the point in Nthat is closest to Qin Euclidean distance.\\nClearly cluster pruning can be used as an approach to the near est-neighbor problem\\nin the plane, if we wished to avoid computing the distance fro mQto every one of\\nthe query points. Devise a simple example on the plane so that with two leaders, the\\nanswer returned by cluster pruning is incorrect (it is not th e data point closest to Q).\\n7.2 Components of an information retrieval system\\nIn this section we combine the ideas developed so far to descr ibe a rudimen-\\ntary search system that retrieves and scores documents. We ﬁ rst develop\\nfurther ideas for scoring, beyond vector spaces. Following this, we will put\\ntogether all of these elements to outline a complete system. Because we con-\\nsider a complete system, we do not restrict ourselves to vect or space retrieval\\nin this section. Indeed, our complete system will have provi sions for vector\\nspace as well as other query operators and forms of retrieval . In Section 7.3\\nwe will return to how vector space queries interact with othe r query opera-\\ntors.\\n7.2.1 Tiered indexes\\nWe mentioned in Section 7.1.2 that when using heuristics such as index elim-\\nination for inexact top- Kretrieval, we may occasionally ﬁnd ourselves with\\na set Aof contenders that has fewer than Kdocuments. A common solution\\nto this issue is the user of tiered indexes , which may be viewed as a gener- TIERED INDEXES\\nalization of champion lists. We illustrate this idea in Figu re7.4, where we\\nrepresent the documents and terms of Figure 6.9. In this example we set a tf\\nthreshold of 20 for tier 1 and 10 for tier 2, meaning that the ti er 1 index only\\nhas postings entries with tf values exceeding 20, while the t ier 2 index only', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 179}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP144 7 Computing scores in a complete search system\\n◮Figure 7.4 Tiered indexes. If we fail to get Kresults from tier 1, query processing\\n“falls back” to tier 2, and so on. Within each tier, postings a re ordered by document\\nID.\\nhas postings entries with tf values exceeding 10. In this exa mple we have\\nchosen to order the postings entries within a tier by documen t ID.\\n7.2.2 Query-term proximity\\nEspecially for free text queries on the web (Chapter 19), users prefer a doc-\\nument in which most or all of the query terms appear close to ea ch other,', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 180}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP7.2 Components of an information retrieval system 145\\nbecause this is evidence that the document has text focused o n their query\\nintent. Consider a query with two or more query terms, t1,t2, . . . , tk. Let ω\\nbe the width of the smallest window in a document dthat contains all the\\nquery terms, measured in the number of words in the window. Fo r instance,\\nif the document were to simply consist of the sentence The quality of mercy\\nis not strained , the smallest window for the query strained mercy would be 4.\\nIntuitively, the smaller that ωis, the better that dmatches the query. In cases\\nwhere the document does not contain all of the query terms, we can set ω\\nto be some enormous number. We could also consider variants i n which\\nonly words that are not stop words are considered in computin gω. Such\\nproximity-weighted scoring functions are a departure from pure cosine sim-\\nilarity and closer to the “soft conjunctive” semantics that Google and other\\nweb search engines evidently use.\\nHow can we design such a proximity-weighted scoring function to depend PROXIMITY WEIGHTING\\nonω? The simplest answer relies on a “hand coding” technique we i ntroduce\\nbelow in Section 7.2.3 . A more scalable approach goes back to Section 6.1.2 –\\nwe treat the integer ωas yet another feature in the scoring function, whose\\nimportance is assigned by machine learning, as will be devel oped further in\\nSection 15.4.1 .\\n7.2.3 Designing parsing and scoring functions\\nCommon search interfaces, particularly for consumer-faci ng search applica-\\ntions on the web, tend to mask query operators from the end use r. The intent\\nis to hide the complexity of these operators from the largely non-technical au-\\ndience for such applications, inviting free text queries . Given such interfaces,\\nhow should a search equipped with indexes for various retrie val operators\\ntreat a query such as risinginterestrates ? More generally, given the various fac-\\ntors we have studied that could affect the score of a document , how should\\nwe combine these features?\\nThe answer of course depends on the user population, the quer y distri-\\nbution and the collection of documents. Typically, a query parser is used to\\ntranslate the user-speciﬁed keywords into a query with vari ous operators\\nthat is executed against the underlying indexes. Sometimes , this execution\\ncan entail multiple queries against the underlying indexes ; for example, the\\nquery parser may issue a stream of queries:\\n1.Run the user-generated query string as a phrase query. Rank t hem by\\nvector space scoring using as query the vector consisting of the 3 terms\\nrising interestrates .\\n2.If fewer than ten documents contain the phrase rising interestrates , run the\\ntwo 2-term phrase queries rising interest andinterest rates ; rank these using\\nvector space scoring, as well.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 181}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP146 7 Computing scores in a complete search system\\n3.If we still have fewer than ten results, run the vector space q uery consist-\\ning of the three individual query terms.\\nEach of these steps (if invoked) may yield a list of scored doc uments, for\\neach of which we compute a score. This score must combine cont ributions\\nfrom vector space scoring, static quality, proximity weigh ting and potentially\\nother factors – particularly since a document may appear in t he lists from\\nmultiple steps. This demands an aggregate scoring function that accumulates EVIDENCE\\nACCUMULATION evidence of a document’s relevance from multiple sources. How do we de vise\\na query parser and how do we devise the aggregate scoring func tion?\\nThe answer depends on the setting. In many enterprise settin gs we have\\napplication builders who make use of a toolkit of available s coring opera-\\ntors, along with a query parsing layer, with which to manuall y conﬁgure\\nthe scoring function as well as the query parser. Such applic ation builders\\nmake use of the available zones, metadata and knowledge of ty pical doc-\\numents and queries to tune the parsing and scoring. In collec tions whose\\ncharacteristics change infrequently (in an enterprise app lication, signiﬁcant\\nchanges in collection and query characteristics typically happen with infre-\\nquent events such as the introduction of new document format s or document\\nmanagement systems, or a merger with another company). Web s earch on\\nthe other hand is faced with a constantly changing document c ollection with\\nnew characteristics being introduced all the time. It is als o a setting in which\\nthe number of scoring factors can run into the hundreds, maki ng hand-tuned\\nscoring a difﬁcult exercise. To address this, it is becoming increasingly com-\\nmon to use machine-learned scoring, extending the ideas we i ntroduced in\\nSection 6.1.2 , as will be discussed further in Section 15.4.1 .\\n7.2.4 Putting it all together\\nWe have now studied all the components necessary for a basic s earch system\\nthat supports free text queries as well as Boolean, zone and ﬁ eld queries. We\\nbrieﬂy review how the various pieces ﬁt together into an over all system; this\\nis depicted in Figure 7.5.\\nIn this ﬁgure, documents stream in from the left for parsing a nd linguis-\\ntic processing (language and format detection, tokenizati on and stemming).\\nThe resulting stream of tokens feeds into two modules. First , we retain a\\ncopy of each parsed document in a document cache. This will en able us\\nto generate results snippets: snippets of text accompanyin g each document\\nin the results list for a query. This snippet tries to give a su ccinct explana-\\ntion to the user of why the document matches the query. The aut omatic\\ngeneration of such snippets is the subject of Section 8.7. A second copy\\nof the tokens is fed to a bank of indexers that create a bank of i ndexes in-\\ncluding zone and ﬁeld indexes that store the metadata for eac h document,', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 182}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP7.3 Vector space scoring and query operator interaction 147\\n◮Figure 7.5 A complete search system. Data paths are shown primarily for a free\\ntext query.\\n(tiered) positional indexes, indexes for spelling correct ion and other tolerant\\nretrieval, and structures for accelerating inexact top- Kretrieval. A free text\\nuser query (top center) is sent down to the indexes both direc tly and through\\na module for generating spelling-correction candidates. A s noted in Chap-\\nter3the latter may optionally be invoked only when the original q uery fails\\nto retrieve enough results. Retrieved documents (dark arro w) are passed\\nto a scoring module that computes scores based on machine-le arned rank-\\ning (MLR), a technique that builds on Section 6.1.2 (to be further developed\\nin Section 15.4.1 ) for scoring and ranking documents. Finally, these ranked\\ndocuments are rendered as a results page.\\n?Exercise 7.9\\nExplain how the postings intersection algorithm ﬁrst intro duced in Section 1.3can be\\nadapted to ﬁnd the smallest integer ωthat contains all query terms.\\nExercise 7.10\\nAdapt this procedure to work when not all query terms are pres ent in a document.\\n7.3 Vector space scoring and query operator interaction\\nWe introduced the vector space model as a paradigm for free te xt queries.\\nWe conclude this chapter by discussing how the vector space s coring model', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 183}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP148 7 Computing scores in a complete search system\\nrelates to the query operators we have studied in earlier cha pters. The re-\\nlationship should be viewed at two levels: in terms of the exp ressiveness\\nof queries that a sophisticated user may pose, and in terms of the index that\\nsupports the evaluation of the various retrieval methods. I n building a search\\nengine, we may opt to support multiple query operators for an end user. In\\ndoing so we need to understand what components of the index ca n be shared\\nfor executing various query operators, as well as how to hand le user queries\\nthat mix various query operators.\\nVector space scoring supports so-called free text retrieva l, in which a query\\nis speciﬁed as a set of words without any query operators conn ecting them. It\\nallows documents matching the query to be scored and thus ran ked, unlike\\nthe Boolean, wildcard and phrase queries studied earlier. C lassically, the\\ninterpretation of such free text queries was that at least on e of the query terms\\nbe present in any retrieved document. However more recently , web search\\nengines such as Google have popularized the notion that a set of terms typed\\ninto their query boxes (thus on the face of it, a free text quer y) carries the\\nsemantics of a conjunctive query that only retrieves docume nts containing\\nall or most query terms.\\nBoolean retrieval\\nClearly a vector space index can be used to answer Boolean que ries, as long\\nas the weight of a term tin the document vector for dis non-zero when-\\never toccurs in d. The reverse is not true, since a Boolean index does not by\\ndefault maintain term weight information. There is no easy w ay of combin-\\ning vector space and Boolean queries from a user’s standpoin t: vector space\\nqueries are fundamentally a form of evidence accumulation , where the pres-\\nence of more query terms in a document adds to the score of a doc ument.\\nBoolean retrieval on the other hand, requires a user to speci fy a formula\\nforselecting documents through the presence (or absence) of speciﬁc com-\\nbinations of keywords, without inducing any relative order ing among them.\\nMathematically, it is in fact possible to invoke so-called p-norms to combine\\nBoolean and vector space queries, but we know of no system tha t makes use\\nof this fact.\\nWildcard queries\\nWildcard and vector space queries require different indexe s, except at the\\nbasic level that both can be implemented using postings and a dictionary\\n(e.g., a dictionary of trigrams for wildcard queries). If a s earch engine allows\\na user to specify a wildcard operator as part of a free text que ry (for instance,\\nthe query rom* restaurant ), we may interpret the wildcard component of the\\nquery as spawning multiple terms in the vector space (in this example, rome', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 184}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP7.4 References and further reading 149\\nandroman would be two such terms) all of which are added to the query\\nvector. The vector space query is then executed as usual, wit h matching\\ndocuments being scored and ranked; thus a document containi ng bothrome\\nandroma is likely to be scored higher than another containing only on e of\\nthem. The exact score ordering will of course depend on the re lative weights\\nof each term in matching documents.\\nPhrase queries\\nThe representation of documents as vectors is fundamentall y lossy: the rel-\\native order of terms in a document is lost in the encoding of a d ocument as\\na vector. Even if we were to try and somehow treat every biword as a term\\n(and thus an axis in the vector space), the weights on differe nt axes not in-\\ndependent: for instance the phrase German shepherd gets encoded in the axis\\ngerman shepherd , but immediately has a non-zero weight on the axes german\\nandshepherd . Further, notions such as idf would have to be extended to suc h\\nbiwords. Thus an index built for vector space retrieval cann ot, in general, be\\nused for phrase queries. Moreover, there is no way of demandi ng a vector\\nspace score for a phrase query — we only know the relative weig hts of each\\nterm in a document.\\nOn the query germanshepherd , we could use vector space retrieval to iden-\\ntify documents heavy in these two terms, with no way of prescr ibing that\\nthey occur consecutively. Phrase retrieval, on the other ha nd, tells us of the\\nexistence of the phrase german shepherd in a document, without any indi-\\ncation of the relative frequency or weight of this phrase. Wh ile these two\\nretrieval paradigms (phrase and vector space) consequentl y have different\\nimplementations in terms of indexes and retrieval algorith ms, they can in\\nsome cases be combined usefully, as in the three-step exampl e of query pars-\\ning in Section 7.2.3 .\\n7.4 References and further reading\\nHeuristics for fast query processing with early terminatio n are described by\\nAnh et al. (2001 ),Garcia et al. (2004 ),Anh and Moffat (2006b ),Persin et al.\\n(1996 ). Cluster pruning is investigated by Singitham et al. (2004 ) and by\\nChierichetti et al. (2007 ); see also Section 16.6 (page 372). Champion lists are\\ndescribed in Persin (1994 ) and (under the name top docs ) in Brown (1995 ), TOP DOCS\\nand further developed in Brin and Page (1998 ),Long and Suel (2003 ). While\\nthese heuristics are well-suited to free text queries that c an be viewed as vec-\\ntors, they complicate phrase queries; see Anh and Moffat (2006c ) for an index\\nstructure that supports both weighted and Boolean/phrase s earches. Carmel\\net al. (2001 )Clarke et al. (2000 ) and Song et al. (2005 ) treat the use of query', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 185}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP150 7 Computing scores in a complete search system\\nterm proximity in assessing relevance. Pioneering work on l earning of rank-\\ning functions was done by Fuhr (1989 ),Fuhr and Pfeifer (1994 ),Cooper et al.\\n(1994 ),Bartell (1994 ),Bartell et al. (1998 ) and by Cohen et al. (1998 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 186}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 151\\n8Evaluation in information\\nretrieval\\nWe have seen in the preceding chapters many alternatives in d esigning an IR\\nsystem. How do we know which of these techniques are effectiv e in which\\napplications? Should we use stop lists? Should we stem? Shou ld we use in-\\nverse document frequency weighting? Information retrieva l has developed\\nas a highly empirical discipline, requiring careful and tho rough evaluation to\\ndemonstrate the superior performance of novel techniques o n representative\\ndocument collections.\\nIn this chapter we begin with a discussion of measuring the ef fectiveness\\nof IR systems (Section 8.1) and the test collections that are most often used\\nfor this purpose (Section 8.2). We then present the straightforward notion of\\nrelevant and nonrelevant documents and the formal evaluati on methodol-\\nogy that has been developed for evaluating unranked retriev al results (Sec-\\ntion 8.3). This includes explaining the kinds of evaluation measure s that\\nare standardly used for document retrieval and related task s like text clas-\\nsiﬁcation and why they are appropriate. We then extend these notions and\\ndevelop further measures for evaluating ranked retrieval r esults (Section 8.4)\\nand discuss developing reliable and informative test colle ctions (Section 8.5).\\nWe then step back to introduce the notion of user utility, and how it is ap-\\nproximated by the use of document relevance (Section 8.6). The key utility\\nmeasure is user happiness. Speed of response and the size of t he index are\\nfactors in user happiness. It seems reasonable to assume tha t relevance of\\nresults is the most important factor: blindingly fast, usel ess answers do not\\nmake a user happy. However, user perceptions do not always co incide with\\nsystem designers’ notions of quality. For example, user hap piness commonly\\ndepends very strongly on user interface design issues, incl uding the layout,\\nclarity, and responsiveness of the user interface, which ar e independent of\\nthe quality of the results returned. We touch on other measur es of the qual-\\nity of a system, in particular the generation of high-qualit y result summary\\nsnippets, which strongly inﬂuence user utility, but are not measured in the\\nbasic relevance ranking paradigm (Section 8.7).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 187}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP152 8 Evaluation in information retrieval\\n8.1 Information retrieval system evaluation\\nTo measure ad hoc information retrieval effectiveness in th e standard way,\\nwe need a test collection consisting of three things:\\n1.A document collection\\n2.A test suite of information needs, expressible as queries\\n3.A set of relevance judgments, standardly a binary assessmen t of either\\nrelevant ornonrelevant for each query-document pair.\\nThe standard approach to information retrieval system eval uation revolves\\naround the notion of relevant and nonrelevant documents. With respect to a RELEVANCE\\nuser information need, a document in the test collection is g iven a binary\\nclassiﬁcation as either relevant or nonrelevant. This deci sion is referred to as\\nthegold standard orground truth judgment of relevance. The test document GOLD STANDARD\\nGROUND TRUTH collection and suite of information needs have to be of a reas onable size:\\nyou need to average performance over fairly large test sets, as results are\\nhighly variable over different documents and information n eeds. As a rule\\nof thumb, 50 information needs has usually been found to be a s ufﬁcient\\nminimum.\\nRelevance is assessed relative to an information need, nota query. For INFORMATION NEED\\nexample, an information need might be:\\nInformation on whether drinking red wine is more effective a t reduc-\\ning your risk of heart attacks than white wine.\\nThis might be translated into a query such as:\\nwine ANDred ANDwhite ANDheart ANDattack ANDeffective\\nA document is relevant if it addresses the stated informatio n need, not be-\\ncause it just happens to contain all the words in the query. Th is distinction is\\noften misunderstood in practice, because the information n eed is not overt.\\nBut, nevertheless, an information need is present. If a user typespython into a\\nweb search engine, they might be wanting to know where they ca n purchase\\na pet python. Or they might be wanting information on the prog ramming\\nlanguage Python. From a one word query, it is very difﬁcult fo r a system to\\nknow what the information need is. But, nevertheless, the us er has one, and\\ncan judge the returned results on the basis of their relevanc e to it. To evalu-\\nate a system, we require an overt expression of an informatio n need, which\\ncan be used for judging returned documents as relevant or non relevant. At\\nthis point, we make a simpliﬁcation: relevance can reasonab ly be thought\\nof as a scale, with some documents highly relevant and others marginally\\nso. But for the moment, we will use just a binary decision of re levance. We', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 188}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP8.2 Standard test collections 153\\ndiscuss the reasons for using binary relevance judgments an d alternatives in\\nSection 8.5.1 .\\nMany systems contain various weights (often known as parame ters) that\\ncan be adjusted to tune system performance. It is wrong to rep ort results on\\na test collection which were obtained by tuning these parame ters to maxi-\\nmize performance on that collection. That is because such tu ning overstates\\nthe expected performance of the system, because the weights will be set to\\nmaximize performance on one particular set of queries rathe r than for a ran-\\ndom sample of queries. In such cases, the correct procedure i s to have one\\nor more development test collections , and to tune the parameters on the devel- DEVELOPMENT TEST\\nCOLLECTION opment test collection. The tester then runs the system with those weights\\non the test collection and reports the results on that collec tion as an unbiased\\nestimate of performance.\\n8.2 Standard test collections\\nHere is a list of the most standard test collections and evalu ation series. We\\nfocus particularly on test collections for ad hoc informati on retrieval system\\nevaluation, but also mention a couple of similar test collec tions for text clas-\\nsiﬁcation.\\nThe Cranﬁeld collection. This was the pioneering test collection in allo wing CRANFIELD\\nprecise quantitative measures of information retrieval ef fectiveness, but\\nis nowadays too small for anything but the most elementary pi lot experi-\\nments. Collected in the United Kingdom starting in the late 1 950s, it con-\\ntains 1398 abstracts of aerodynamics journal articles, a se t of 225 queries,\\nand exhaustive relevance judgments of all (query, document ) pairs.\\nText Retrieval Conference (TREC) . The U.S. National Institute of Standards TREC\\nand Technology (NIST) has run a large IR test bed evaluation s eries since\\n1992. Within this framework, there have been many tracks ove r a range\\nof different test collections, but the best known test colle ctions are the\\nones used for the TREC Ad Hoc track during the ﬁrst 8 TREC evalu ations\\nbetween 1992 and 1999. In total, these test collections comp rise 6 CDs\\ncontaining 1.89 million documents (mainly, but not exclusi vely, newswire\\narticles) and relevance judgments for 450 information need s, which are\\ncalled topics and speciﬁed in detailed text passages. Individual test col -\\nlections are deﬁned over different subsets of this data. The early TRECs\\neach consisted of 50 information needs, evaluated over diff erent but over-\\nlapping sets of documents. TRECs 6–8 provide 150 informatio n needs\\nover about 528,000 newswire and Foreign Broadcast Informat ion Service\\narticles. This is probably the best subcollection to use in f uture work, be-\\ncause it is the largest and the topics are more consistent. Be cause the test', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 189}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP154 8 Evaluation in information retrieval\\ndocument collections are so large, there are no exhaustive r elevance judg-\\nments. Rather, NIST assessors’ relevance judgments are ava ilable only for\\nthe documents that were among the top kreturned for some system which\\nwas entered in the TREC evaluation for which the information need was\\ndeveloped.\\nIn more recent years, NIST has done evaluations on larger doc ument col-\\nlections, including the 25 million page GOV2 web page collection. From GOV2\\nthe beginning, the NIST test document collections were orde rs of magni-\\ntude larger than anything available to researchers previou sly and GOV2\\nis now the largest Web collection easily available for resea rch purposes.\\nNevertheless, the size of GOV2 is still more than 2 orders of m agnitude\\nsmaller than the current size of the document collections in dexed by the\\nlarge web search companies.\\nNII Test Collections for IR Systems ( NTCIR ). The NTCIR project has built NTCIR\\nvarious test collections of similar sizes to the TREC collec tions, focus-\\ning on East Asian language and cross-language information retrieval , where CROSS -LANGUAGE\\nINFORMATION\\nRETRIEVALqueries are made in one language over a document collection c ontaining\\ndocuments in one or more other languages. See: http://research.nii.ac.jp/ntcir/data/data-\\nen.html\\nCross Language Evaluation Forum ( CLEF ). This evaluation series has con- CLEF\\ncentrated on European languages and cross-language inform ation retrieval.\\nSee:http://www.clef-campaign.org/\\nReuters-21578 and Reuters-RCV1. For text classiﬁcation, t he most used test REUTERS\\ncollection has been the Reuters-21578 collection of 21578 n ewswire arti-\\ncles; see Chapter 13, page 279. More recently, Reuters released the much\\nlarger Reuters Corpus Volume 1 (RCV1), consisting of 806,79 1 documents;\\nsee Chapter 4, page 69. Its scale and rich annotation makes it a better basis\\nfor future research.\\n20 Newsgroups . This is another widely used text classiﬁcation collection , 20 N EWSGROUPS\\ncollected by Ken Lang. It consists of 1000 articles from each of 20 Usenet\\nnewsgroups (the newsgroup name being regarded as the catego ry). After\\nthe removal of duplicate articles, as it is usually used, it c ontains 18941\\narticles.\\n8.3 Evaluation of unranked retrieval sets\\nGiven these ingredients, how is system effectiveness measu red? The two\\nmost frequent and basic measures for information retrieval effectiveness are\\nprecision and recall. These are ﬁrst deﬁned for the simple ca se where an', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 190}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP8.3 Evaluation of unranked retrieval sets 155\\nIR system returns a set of documents for a query. We will see la ter how to\\nextend these notions to ranked retrieval situations.\\nPrecision (P) is the fraction of retrieved documents that are relevant PRECISION\\nPrecision =#(relevant items retrieved )\\n#(retrieved items )=P(relevant|retrieved ) (8.1)\\nRecall (R) is the fraction of relevant documents that are retrieved RECALL\\nRecall =#(relevant items retrieved )\\n#(relevant items )=P(retrieved|relevant ) (8.2)\\nThese notions can be made clear by examining the following co ntingency\\ntable:\\n(8.3)\\nRelevant Nonrelevant\\nRetrieved true positives (tp) false positives (fp)\\nNot retrieved false negatives (fn) true negatives (tn)\\nThen:\\nP=tp/(tp+f p) (8.4)\\nR=tp/(tp+f n)\\nAn obvious alternative that may occur to the reader is to judg e an infor-\\nmation retrieval system by its accuracy , that is, the fraction of its classiﬁca- ACCURACY\\ntions that are correct. In terms of the contingency table abo ve, accuracy =\\n(tp+tn)/(tp+f p+f n+tn). This seems plausible, since there are two ac-\\ntual classes, relevant and nonrelevant, and an information retrieval system\\ncan be thought of as a two-class classiﬁer which attempts to l abel them as\\nsuch (it retrieves the subset of documents which it believes to be relevant).\\nThis is precisely the effectiveness measure often used for e valuating machine\\nlearning classiﬁcation problems.\\nThere is a good reason why accuracy is not an appropriate meas ure for\\ninformation retrieval problems. In almost all circumstanc es, the data is ex-\\ntremely skewed: normally over 99.9% of the documents are in t he nonrele-\\nvant category. A system tuned to maximize accuracy can appea r to perform\\nwell by simply deeming all documents nonrelevant to all quer ies. Even if the\\nsystem is quite good, trying to label some documents as relev ant will almost\\nalways lead to a high rate of false positives. However, label ing all documents\\nas nonrelevant is completely unsatisfying to an informatio n retrieval system\\nuser. Users are always going to want to see some documents, an d can be', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 191}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP156 8 Evaluation in information retrieval\\nassumed to have a certain tolerance for seeing some false pos itives provid-\\ning that they get some useful information. The measures of pr ecision and\\nrecall concentrate the evaluation on the return of true posi tives, asking what\\npercentage of the relevant documents have been found and how many false\\npositives have also been returned.\\nThe advantage of having the two numbers for precision and rec all is that\\none is more important than the other in many circumstances. T ypical web\\nsurfers would like every result on the ﬁrst page to be relevan t (high preci-\\nsion) but have not the slightest interest in knowing let alon e looking at every\\ndocument that is relevant. In contrast, various profession al searchers such as\\nparalegals and intelligence analysts are very concerned wi th trying to get as\\nhigh recall as possible, and will tolerate fairly low precis ion results in order to\\nget it. Individuals searching their hard disks are also ofte n interested in high\\nrecall searches. Nevertheless, the two quantities clearly trade off against one\\nanother: you can always get a recall of 1 (but very low precisi on) by retriev-\\ning all documents for all queries! Recall is a non-decreasin g function of the\\nnumber of documents retrieved. On the other hand, in a good sy stem, preci-\\nsion usually decreases as the number of documents retrieved is increased. In\\ngeneral we want to get some amount of recall while tolerating only a certain\\npercentage of false positives.\\nA single measure that trades off precision versus recall is t heF measure , FMEASURE\\nwhich is the weighted harmonic mean of precision and recall:\\nF=1\\nα1\\nP+ (1−α)1\\nR=(β2+1)PR\\nβ2P+Rwhere β2=1−α\\nα(8.5)\\nwhere α∈[0, 1]and thus β2∈[0,∞]. The default balanced F measure equally\\nweights precision and recall, which means making α=1/2 or β=1. It is\\ncommonly written as F1, which is short for Fβ=1, even though the formula-\\ntion in terms of αmore transparently exhibits the F measure as a weighted\\nharmonic mean. When using β=1, the formula on the right simpliﬁes to:\\nFβ=1=2PR\\nP+R(8.6)\\nHowever, using an even weighting is not the only choice. Valu es of β<1\\nemphasize precision, while values of β>1 emphasize recall. For example, a\\nvalue of β=3 or β=5 might be used if recall is to be emphasized. Recall,\\nprecision, and the F measure are inherently measures betwee n 0 and 1, but\\nthey are also very commonly written as percentages, on a scal e between 0\\nand 100.\\nWhy do we use a harmonic mean rather than the simpler average ( arith-\\nmetic mean)? Recall that we can always get 100% recall by just returning all\\ndocuments, and therefore we can always get a 50% arithmetic m ean by the', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 192}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP8.3 Evaluation of unranked retrieval sets 1570\\n2 0\\n4 0\\n6 0\\n8 0\\n1 0 00 2 0 4 0 6 0 8 0 1 0 0P r e c i s i o n ( R e c a l l f i x e d a t 7 0 % )\\nM i n i m u mM a x i m u mA r i t h m e t i cGe o m e t r i cH a r m o n i c\\n◮Figure 8.1 Graph comparing the harmonic mean to other means. The graph\\nshows a slice through the calculation of various means of pre cision and recall for\\nthe ﬁxed recall value of 70%. The harmonic mean is always less than either the arith-\\nmetic or geometric mean, and often quite close to the minimum of the two numbers.\\nWhen the precision is also 70%, all the measures coincide.\\nsame process. This strongly suggests that the arithmetic me an is an unsuit-\\nable measure to use. In contrast, if we assume that 1 document in 10,000 is\\nrelevant to the query, the harmonic mean score of this strate gy is 0.02%. The\\nharmonic mean is always less than or equal to the arithmetic m ean and the\\ngeometric mean. When the values of two numbers differ greatl y, the har-\\nmonic mean is closer to their minimum than to their arithmeti c mean; see\\nFigure 8.1.\\n?Exercise 8.1 [⋆]\\nAn IR system returns 8 relevant documents, and 10 nonrelevan t documents. There\\nare a total of 20 relevant documents in the collection. What i s the precision of the\\nsystem on this search, and what is its recall?\\nExercise 8.2 [⋆]\\nThe balanced F measure (a.k.a. F 1) is deﬁned as the harmonic mean of precision and\\nrecall. What is the advantage of using the harmonic mean rath er than “averaging”\\n(using the arithmetic mean)?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 193}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP158 8 Evaluation in information retrieval\\n0.00.20.40.60.81.0\\n0.0 0.2 0.4 0.6 0.8 1.0\\nRecallPrecision \\n◮Figure 8.2 Precision/recall graph.\\nExercise 8.3 [⋆⋆]\\nDerive the equivalence between the two formulas for F measur e shown in Equa-\\ntion ( 8.5), given that α=1/(β2+1).\\n8.4 Evaluation of ranked retrieval results\\nPrecision, recall, and the F measure are set-based measures . They are com-\\nputed using unordered sets of documents. We need to extend th ese measures\\n(or to deﬁne new measures) if we are to evaluate the ranked ret rieval results\\nthat are now standard with search engines. In a ranked retrie val context,\\nappropriate sets of retrieved documents are naturally give n by the top kre-\\ntrieved documents. For each such set, precision and recall v alues can be\\nplotted to give a precision-recall curve , such as the one shown in Figure 8.2. PRECISION -RECALL\\nCURVE Precision-recall curves have a distinctive saw-tooth shap e: if the (k+1)th\\ndocument retrieved is nonrelevant then recall is the same as for the top k\\ndocuments, but precision has dropped. If it is relevant, the n both precision\\nand recall increase, and the curve jags up and to the right. It is often useful to\\nremove these jiggles and the standard way to do this is with an interpolated\\nprecision: the interpolated precision p interp at a certain recall level ris deﬁned INTERPOLATED\\nPRECISION', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 194}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP8.4 Evaluation of ranked retrieval results 159\\nRecall Interp.\\nPrecision\\n0.0 1.00\\n0.1 0.67\\n0.2 0.63\\n0.3 0.55\\n0.4 0.45\\n0.5 0.41\\n0.6 0.36\\n0.7 0.29\\n0.8 0.13\\n0.9 0.10\\n1.0 0.08\\n◮Table 8.1 Calculation of 11-point Interpolated Average Precision. T his is for the\\nprecision-recall curve shown in Figure 8.2.\\nas the highest precision found for any recall level r′≥r:\\npinterp(r) =max\\nr′≥rp(r′) (8.7)\\nThe justiﬁcation is that almost anyone would be prepared to l ook at a few\\nmore documents if it would increase the percentage of the vie wed set that\\nwere relevant (that is, if the precision of the larger set is h igher). Interpolated\\nprecision is shown by a thinner line in Figure 8.2. With this deﬁnition, the\\ninterpolated precision at a recall of 0 is well-deﬁned (Exer cise8.4).\\nExamining the entire precision-recall curve is very inform ative, but there\\nis often a desire to boil this information down to a few number s, or perhaps\\neven a single number. The traditional way of doing this (used for instance\\nin the ﬁrst 8 TREC Ad Hoc evaluations) is the 11-point interpolated average 11- POINT\\nINTERPOLATED\\nAVERAGE PRECISIONprecision . For each information need, the interpolated precision is m easured\\nat the 11 recall levels of 0.0, 0.1, 0.2, . . . , 1.0. For the prec ision-recall curve in\\nFigure 8.2, these 11 values are shown in Table 8.1. For each recall level, we\\nthen calculate the arithmetic mean of the interpolated prec ision at that recall\\nlevel for each information need in the test collection. A com posite precision-\\nrecall curve showing 11 points can then be graphed. Figure 8.3shows an\\nexample graph of such results from a representative good sys tem at TREC 8.\\nIn recent years, other measures have become more common. Mos t stan-\\ndard among the TREC community is Mean Average Precision (MAP), which MEAN AVERAGE\\nPRECISION provides a single-ﬁgure measure of quality across recall le vels. Among eval-\\nuation measures, MAP has been shown to have especially good d iscrimina-\\ntion and stability. For a single information need, Average P recision is the', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 195}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP160 8 Evaluation in information retrieval\\n00.20.40.60.81\\n0 0.2 0.4 0.6 0.8 1\\nRecallPrecision \\n◮Figure 8.3 Averaged 11-point precision/recall graph across 50 querie s for a rep-\\nresentative TREC system. The Mean Average Precision for thi s system is 0.2553.\\naverage of the precision value obtained for the set of top kdocuments exist-\\ning after each relevant document is retrieved, and this valu e is then averaged\\nover information needs. That is, if the set of relevant docum ents for an in-\\nformation need qj∈Qis{d1, . . .dmj}and Rjkis the set of ranked retrieval\\nresults from the top result until you get to document dk, then\\nMAP(Q) =1\\n|Q||Q|\\n∑\\nj=11\\nmjmj\\n∑\\nk=1Precision (Rjk) (8.8)\\nWhen a relevant document is not retrieved at all,1the precision value in the\\nabove equation is taken to be 0. For a single information need , the average\\nprecision approximates the area under the uninterpolated p recision-recall\\ncurve, and so the MAP is roughly the average area under the pre cision-recall\\ncurve for a set of queries.\\nUsing MAP , ﬁxed recall levels are not chosen, and there is no i nterpola-\\ntion. The MAP value for a test collection is the arithmetic me an of average\\n1. A system may not fully order all documents in the collectio n in response to a query or at\\nany rate an evaluation exercise may be based on submitting on ly the top kresults for each\\ninformation need.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 196}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP8.4 Evaluation of ranked retrieval results 161\\nprecision values for individual information needs. (This h as the effect of\\nweighting each information need equally in the ﬁnal reporte d number, even\\nif many documents are relevant to some queries whereas very f ew are rele-\\nvant to other queries.) Calculated MAP scores normally vary widely across\\ninformation needs when measured within a single system, for instance, be-\\ntween 0.1 and 0.7. Indeed, there is normally more agreement i n MAP for\\nan individual information need across systems than for MAP s cores for dif-\\nferent information needs for the same system. This means tha t a set of test\\ninformation needs must be large and diverse enough to be repr esentative of\\nsystem effectiveness across different queries.\\nThe above measures factor in precision at all recall levels. For many promi- PRECISION AT k\\nnent applications, particularly web search, this may not be germane to users.\\nWhat matters is rather how many good results there are on the ﬁ rst page or\\nthe ﬁrst three pages. This leads to measuring precision at ﬁx ed low levels of\\nretrieved results, such as 10 or 30 documents. This is referr ed to as “Precision\\natk”, for example “Precision at 10”. It has the advantage of not r equiring any\\nestimate of the size of the set of relevant documents but the d isadvantages\\nthat it is the least stable of the commonly used evaluation me asures and that\\nit does not average well, since the total number of relevant d ocuments for a\\nquery has a strong inﬂuence on precision at k.\\nAn alternative, which alleviates this problem, is R-precision . It requires R-PRECISION\\nhaving a set of known relevant documents Rel, from which we calculate the\\nprecision of the top Reldocuments returned. (The set Relmay be incomplete,\\nsuch as when Relis formed by creating relevance judgments for the pooled\\ntopkresults of particular systems in a set of experiments.) R-pr ecision ad-\\njusts for the size of the set of relevant documents: A perfect system could\\nscore 1 on this metric for each query, whereas, even a perfect system could\\nonly achieve a precision at 20 of 0.4 if there were only 8 docum ents in the\\ncollection relevant to an information need. Averaging this measure across\\nqueries thus makes more sense. This measure is harder to expl ain to naive\\nusers than Precision at kbut easier to explain than MAP. If there are |Rel|\\nrelevant documents for a query, we examine the top |Rel|results of a sys-\\ntem, and ﬁnd that rare relevant, then by deﬁnition, not only is the precision\\n(and hence R-precision) r/|Rel|, but the recall of this result set is also r/|Rel|.\\nThus, R-precision turns out to be identical to the break-even point , another BREAK -EVEN POINT\\nmeasure which is sometimes used, deﬁned in terms of this equa lity relation-\\nship holding. Like Precision at k, R-precision describes only one point on\\nthe precision-recall curve, rather than attempting to summ arize effectiveness\\nacross the curve, and it is somewhat unclear why you should be interested\\nin the break-even point rather than either the best point on t he curve (the\\npoint with maximal F-measure) or a retrieval level of intere st to a particular\\napplication (Precision at k). Nevertheless, R-precision turns out to be highly\\ncorrelated with MAP empirically, despite measuring only a s ingle point on', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 197}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP162 8 Evaluation in information retrieval\\n0.00.20.40.60.81.0\\n0 0.2 0.4 0.6 0.8 1\\n1 − specificitysensitivity ( = recall) \\n◮Figure 8.4 The ROC curve corresponding to the precision-recall curve i n Fig-\\nure8.2.\\n.\\nthe curve.\\nAnother concept sometimes used in evaluation is an ROC curve . (“ROC” ROC CURVE\\nstands for “Receiver Operating Characteristics”, but know ing that doesn’t\\nhelp most people.) An ROC curve plots the true positive rate o r sensitiv-\\nity against the false positive rate or (1 −speciﬁcity). Here, sensitivity is just SENSITIVITY\\nanother term for recall. The false positive rate is given by f p/(f p+tn). Fig-\\nure8.4shows the ROC curve corresponding to the precision-recall c urve in\\nFigure 8.2. An ROC curve always goes from the bottom left to the top right of\\nthe graph. For a good system, the graph climbs steeply on the l eft side. For\\nunranked result sets, speciﬁcity , given by tn/(f p+tn), was not seen as a very SPECIFICITY\\nuseful notion. Because the set of true negatives is always so large, its value\\nwould be almost 1 for all information needs (and, correspond ingly, the value\\nof the false positive rate would be almost 0). That is, the “in teresting” part of\\nFigure 8.2is 0<recall <0.4, a part which is compressed to a small corner\\nof Figure 8.4. But an ROC curve could make sense when looking over the\\nfull retrieval spectrum, and it provides another way of look ing at the data.\\nIn many ﬁelds, a common aggregate measure is to report the are a under the\\nROC curve, which is the ROC analog of MAP. Precision-recall c urves are\\nsometimes loosely referred to as ROC curves. This is underst andable, but\\nnot accurate.\\nA ﬁnal approach that has seen increasing adoption, especial ly when em-\\nployed with machine learning approaches to ranking (see Sec tion 15.4, page 341)\\nis measures of cumulative gain , and in particular normalized discounted cumu- CUMULATIVE GAIN\\nNORMALIZED\\nDISCOUNTED\\nCUMULATIVE GAIN', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 198}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP8.4 Evaluation of ranked retrieval results 163\\nlative gain (NDCG ). NDCG is designed for situations of non-binary notions NDCG\\nof relevance (cf. Section 8.5.1 ). Like precision at k, it is evaluated over some\\nnumber kof top search results. For a set of queries Q, let R(j,d)be the rele-\\nvance score assessors gave to document dfor query j. Then,\\nNDCG (Q,k) =1\\n|Q||Q|\\n∑\\nj=1Zkjk\\n∑\\nm=12R(j,m)−1\\nlog2(1+m), (8.9)\\nwhere Zkjis a normalization factor calculated to make it so that a perf ect\\nranking’s NDCG at kfor query jis 1. For queries for which k′<kdocuments\\nare retrieved, the last summation is done up to k′.\\n?Exercise 8.4 [⋆]\\nWhat are the possible values for interpolated precision at a recall level of 0?\\nExercise 8.5 [⋆⋆]\\nMust there always be a break-even point between precision an d recall? Either show\\nthere must be or give a counter-example.\\nExercise 8.6 [⋆⋆]\\nWhat is the relationship between the value of F1and the break-even point?\\nExercise 8.7 [⋆⋆]\\nThe Dice coefﬁcient of two sets is a measure of their intersection scaled by their size DICE COEFFICIENT\\n(giving a value in the range 0 to 1):\\nDice(X,Y) =2|X∩Y|\\n|X|+|Y|\\nShow that the balanced F-measure ( F1) is equal to the Dice coefﬁcient of the retrieved\\nand relevant document sets.\\nExercise 8.8 [⋆]\\nConsider an information need for which there are 4 relevant d ocuments in the collec-\\ntion. Contrast two systems run on this collection. Their top 10 results are judged for\\nrelevance as follows (the leftmost item is the top ranked sea rch result):\\nSystem 1 R N R N N N N N R R\\nSystem 2 N R N N R R R N N N\\na.What is the MAP of each system? Which has a higher MAP?\\nb.Does this result intuitively make sense? What does it say abo ut what is important\\nin getting a good MAP score?\\nc.What is the R-precision of each system? (Does it rank the syst ems the same as\\nMAP?)', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 199}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP164 8 Evaluation in information retrieval\\nExercise 8.9 [⋆⋆]\\nThe following list of Rs and Ns represents relevant (R) and no nrelevant (N) returned\\ndocuments in a ranked list of 20 documents retrieved in respo nse to a query from a\\ncollection of 10,000 documents. The top of the ranked list (t he document the system\\nthinks is most likely to be relevant) is on the left of the list . This list shows 6 relevant\\ndocuments. Assume that there are 8 relevant documents in tot al in the collection.\\nR R N N N N N N R N R N N N R N N N N R\\na.What is the precision of the system on the top 20?\\nb.What is the F 1on the top 20?\\nc.What is the uninterpolated precision of the system at 25% rec all?\\nd.What is the interpolated precision at 33% recall?\\ne.Assume that these 20 documents are the complete result set of the system. What\\nis the MAP for the query?\\nAssume, now, instead, that the system returned the entire 10 ,000 documents in a\\nranked list, and these are the ﬁrst 20 results returned.\\nf.What is the largest possible MAP that this system could have?\\ng.What is the smallest possible MAP that this system could have ?\\nh.In a set of experiments, only the top 20 results are evaluated by hand. The result\\nin (e) is used to approximate the range (f)–(g). For this exam ple, how large (in\\nabsolute terms) can the error for the MAP be by calculating (e ) instead of (f) and\\n(g) for this query?\\n8.5 Assessing relevance\\nTo properly evaluate a system, your test information needs m ust be germane\\nto the documents in the test document collection, and approp riate for pre-\\ndicted usage of the system. These information needs are best designed by\\ndomain experts. Using random combinations of query terms as an informa-\\ntion need is generally not a good idea because typically they will not resem-\\nble the actual distribution of information needs.\\nGiven information needs and documents, you need to collect r elevance\\nassessments. This is a time-consuming and expensive proces s involving hu-\\nman beings. For tiny collections like Cranﬁeld, exhaustive judgments of rel-\\nevance for each query and document pair were obtained. For la rge modern\\ncollections, it is usual for relevance to be assessed only fo r a subset of the\\ndocuments for each query. The most standard approach is pooling , where rel- POOLING\\nevance is assessed over a subset of the collection that is for med from the top\\nkdocuments returned by a number of different IR systems (usua lly the ones\\nto be evaluated), and perhaps other sources such as the resul ts of Boolean\\nkeyword searches or documents found by expert searchers in a n interactive\\nprocess.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 200}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP8.5 Assessing relevance 165\\nJudge 2 Relevance\\nYes No Total\\nJudge 1 Yes 300 20 320\\nRelevance No 10 70 80\\nTotal 310 90 400\\nObserved proportion of the times the judges agreed\\nP(A) = ( 300+70)/400=370/400 =0.925\\nPooled marginals\\nP(nonrelevant ) = ( 80+90)/(400+400) =170/800 =0.2125\\nP(relevant ) = ( 320+310)/(400+400) =630/800 =0.7878\\nProbability that the two judges agreed by chance\\nP(E) =P(nonrelevant )2+P(relevant )2=0.21252+0.78782=0.665\\nKappa statistic\\nκ= (P(A)−P(E))/(1−P(E)) = ( 0.925−0.665)/(1−0.665) =0.776\\n◮Table 8.2 Calculating the kappa statistic.\\nA human is not a device that reliably reports a gold standard j udgment\\nof relevance of a document to a query. Rather, humans and thei r relevance\\njudgments are quite idiosyncratic and variable. But this is not a problem\\nto be solved: in the ﬁnal analysis, the success of an IR system depends on\\nhow good it is at satisfying the needs of these idiosyncratic humans, one\\ninformation need at a time.\\nNevertheless, it is interesting to consider and measure how much agree-\\nment between judges there is on relevance judgments. In the s ocial sciences,\\na common measure for agreement between judges is the kappa statistic . It is KAPPA STATISTIC\\ndesigned for categorical judgments and corrects a simple ag reement rate for\\nthe rate of chance agreement.\\nkappa =P(A)−P(E)\\n1−P(E)(8.10)\\nwhere P(A)is the proportion of the times the judges agreed, and P(E)is the\\nproportion of the times they would be expected to agree by cha nce. There\\nare choices in how the latter is estimated: if we simply say we are making\\na two-class decision and assume nothing more, then the expec ted chance\\nagreement rate is 0.5. However, normally the class distribu tion assigned is\\nskewed, and it is usual to use marginal statistics to calculate expected agree- MARGINAL\\nment.2There are still two ways to do it depending on whether one pool s\\n2. For a contingency table, as in Table 8.2, a marginal statistic is formed by summing a row or\\ncolumn. The marginal ai.k=∑jaijk.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 201}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP166 8 Evaluation in information retrieval\\nthe marginal distribution across judges or uses the margina ls for each judge\\nseparately; both forms have been used, but we present the poo led version\\nbecause it is more conservative in the presence of systemati c differences in as-\\nsessments across judges. The calculations are shown in Tabl e8.2. The kappa\\nvalue will be 1 if two judges always agree, 0 if they agree only at the rate\\ngiven by chance, and negative if they are worse than random. I f there are\\nmore than two judges, it is normal to calculate an average pai rwise kappa\\nvalue. As a rule of thumb, a kappa value above 0.8 is taken as go od agree-\\nment, a kappa value between 0.67 and 0.8 is taken as fair agree ment, and\\nagreement below 0.67 is seen as data providing a dubious basi s for an evalu-\\nation, though the precise cutoffs depend on the purposes for which the data\\nwill be used.\\nInterjudge agreement of relevance has been measured within the TREC\\nevaluations and for medical IR collections. Using the above rules of thumb,\\nthe level of agreement normally falls in the range of “fair” ( 0.67–0.8). The fact\\nthat human agreement on a binary relevance judgment is quite modest is one\\nreason for not requiring more ﬁne-grained relevance labeli ng from the test\\nset creator. To answer the question of whether IR evaluation results are valid\\ndespite the variation of individual assessors’ judgments, people have exper-\\nimented with evaluations taking one or the other of two judge s’ opinions as\\nthe gold standard. The choice can make a considerable absolute difference to\\nreported scores, but has in general been found to have little impact on the rel-\\native effectiveness ranking of either different systems or varia nts of a single\\nsystem which are being compared for effectiveness.\\n8.5.1 Critiques and justiﬁcations of the concept of relevan ce\\nThe advantage of system evaluation, as enabled by the standa rd model of\\nrelevant and nonrelevant documents, is that we have a ﬁxed se tting in which\\nwe can vary IR systems and system parameters to carry out comp arative ex-\\nperiments. Such formal testing is much less expensive and al lows clearer\\ndiagnosis of the effect of changing system parameters than d oing user stud-\\nies of retrieval effectiveness. Indeed, once we have a forma l measure that\\nwe have conﬁdence in, we can proceed to optimize effectivene ss by machine\\nlearning methods, rather than tuning parameters by hand. Of course, if the\\nformal measure poorly describes what users actually want, d oing this will\\nnot be effective in improving user satisfaction. Our perspe ctive is that, in\\npractice, the standard formal measures for IR evaluation, a lthough a simpli-\\nﬁcation, are good enough, and recent work in optimizing form al evaluation\\nmeasures in IR has succeeded brilliantly. There are numerou s examples of\\ntechniques developed in formal evaluation settings, which improve effec-\\ntiveness in operational settings, such as the development o f document length\\nnormalization methods within the context of TREC (Sections 6.4.4 and 11.4.3 )', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 202}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP8.5 Assessing relevance 167\\nand machine learning methods for adjusting parameter weigh ts in scoring\\n(Section 6.1.2 ).\\nThat is not to say that there are not problems latent within th e abstrac-\\ntions used. The relevance of one document is treated as indep endent of the\\nrelevance of other documents in the collection. (This assum ption is actually\\nbuilt into most retrieval systems – documents are scored aga inst queries, not\\nagainst each other – as well as being assumed in the evaluatio n methods.)\\nAssessments are binary: there aren’t any nuanced assessmen ts of relevance.\\nRelevance of a document to an information need is treated as a n absolute,\\nobjective decision. But judgments of relevance are subject ive, varying across\\npeople, as we discussed above. In practice, human assessors are also imper-\\nfect measuring instruments, susceptible to failures of und erstanding and at-\\ntention. We also have to assume that users’ information need s do not change\\nas they start looking at retrieval results. Any results base d on one collection\\nare heavily skewed by the choice of collection, queries, and relevance judg-\\nment set: the results may not translate from one domain to ano ther or to a\\ndifferent user population.\\nSome of these problems may be ﬁxable. A number of recent evalu ations,\\nincluding INEX, some TREC tracks, and NTCIR have adopted an o rdinal\\nnotion of relevance with documents divided into 3 or 4 classe s, distinguish-\\ning slightly relevant documents from highly relevant docum ents. See Sec-\\ntion 10.4 (page 210) for a detailed discussion of how this is implemented in\\nthe INEX evaluations.\\nOne clear problem with the relevance-based assessment that we have pre-\\nsented is the distinction between relevance and marginal relevance : whether MARGINAL RELEVANCE\\na document still has distinctive usefulness after the user h as looked at cer-\\ntain other documents ( Carbonell and Goldstein 1998 ). Even if a document\\nis highly relevant, its information can be completely redun dant with other\\ndocuments which have already been examined. The most extrem e case of\\nthis is documents that are duplicates – a phenomenon that is a ctually very\\ncommon on the World Wide Web – but it can also easily occur when sev-\\neral documents provide a similar precis of an event. In such c ircumstances,\\nmarginal relevance is clearly a better measure of utility to the user. Maximiz-\\ning marginal relevance requires returning documents that e xhibit diversity\\nand novelty. One way to approach measuring this is by using di stinct facts\\nor entities as evaluation units. This perhaps more directly measures true\\nutility to the user but doing this makes it harder to create a t est collection.\\n?Exercise 8.10 [⋆⋆]\\nBelow is a table showing how two human judges rated the releva nce of a set of 12\\ndocuments to a particular information need (0 = nonrelevant , 1 = relevant). Let us as-\\nsume that you’ve written an IR system that for this query retu rns the set of documents\\n{4, 5, 6, 7, 8}.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 203}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP168 8 Evaluation in information retrieval\\ndocID Judge 1 Judge 2\\n1 0 0\\n2 0 0\\n3 1 1\\n4 1 1\\n5 1 0\\n6 1 0\\n7 1 0\\n8 1 0\\n9 0 1\\n10 0 1\\n11 0 1\\n12 0 1\\na.Calculate the kappa measure between the two judges.\\nb.Calculate precision, recall, and F1of your system if a document is considered rel-\\nevant only if the two judges agree.\\nc.Calculate precision, recall, and F1of your system if a document is considered rel-\\nevant if either judge thinks it is relevant.\\n8.6 A broader perspective: System quality and user utility\\nFormal evaluation measures are at some distance from our ult imate interest\\nin measures of human utility: how satisﬁed is each user with t he results the\\nsystem gives for each information need that they pose? The st andard way to\\nmeasure human satisfaction is by various kinds of user studi es. These might\\ninclude quantitative measures, both objective, such as tim e to complete a\\ntask, as well as subjective, such as a score for satisfaction with the search\\nengine, and qualitative measures, such as user comments on t he search in-\\nterface. In this section we will touch on other system aspect s that allow quan-\\ntitative evaluation and the issue of user utility.\\n8.6.1 System issues\\nThere are many practical benchmarks on which to rate an infor mation re-\\ntrieval system beyond its retrieval quality. These include :\\n•How fast does it index, that is, how many documents per hour do es it\\nindex for a certain distribution over document lengths? (cf . Chapter 4)\\n•How fast does it search, that is, what is its latency as a funct ion of index\\nsize?\\n•How expressive is its query language? How fast is it on comple x queries?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 204}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP8.6 A broader perspective: System quality and user utility 169\\n•How large is its document collection, in terms of the number o f doc-\\numents or the collection having information distributed ac ross a broad\\nrange of topics?\\nAll these criteria apart from query language expressivenes s are straightfor-\\nwardly measurable : we can quantify the speed or size. Various kinds of fea-\\nture checklists can make query language expressiveness sem i-precise.\\n8.6.2 User utility\\nWhat we would really like is a way of quantifying aggregate us er happiness,\\nbased on the relevance, speed, and user interface of a system . One part of\\nthis is understanding the distribution of people we wish to m ake happy, and\\nthis depends entirely on the setting. For a web search engine , happy search\\nusers are those who ﬁnd what they want. One indirect measure o f such users\\nis that they tend to return to the same engine. Measuring the r ate of return\\nof users is thus an effective metric, which would of course be more effective\\nif you could also measure how much these users used other sear ch engines.\\nBut advertisers are also users of modern web search engines. They are happy\\nif customers click through to their sites and then make purch ases. On an\\neCommerce web site, a user is likely to be wanting to purchase something.\\nThus, we can measure the time to purchase, or the fraction of s earchers who\\nbecome buyers. On a shopfront web site, perhaps both the user ’s and the\\nstore owner’s needs are satisﬁed if a purchase is made. Never theless, in\\ngeneral, we need to decide whether it is the end user’s or the e Commerce\\nsite owner’s happiness that we are trying to optimize. Usual ly, it is the store\\nowner who is paying us.\\nFor an “enterprise” (company, government, or academic) int ranet search\\nengine, the relevant metric is more likely to be user product ivity: how much\\ntime do users spend looking for information that they need. T here are also\\nmany other practical criteria concerning such matters as in formation secu-\\nrity, which we mentioned in Section 4.6(page 80).\\nUser happiness is elusive to measure, and this is part of why t he standard\\nmethodology uses the proxy of relevance of search results. T he standard\\ndirect way to get at user satisfaction is to run user studies, where people en-\\ngage in tasks, and usually various metrics are measured, the participants are\\nobserved, and ethnographic interview techniques are used t o get qualitative\\ninformation on satisfaction. User studies are very useful i n system design,\\nbut they are time consuming and expensive to do. They are also difﬁcult to\\ndo well, and expertise is required to design the studies and t o interpret the\\nresults. We will not discuss the details of human usability t esting here.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 205}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP170 8 Evaluation in information retrieval\\n8.6.3 Reﬁning a deployed system\\nIf an IR system has been built and is being used by a large numbe r of users,\\nthe system’s builders can evaluate possible changes by depl oying variant\\nversions of the system and recording measures that are indic ative of user\\nsatisfaction with one variant vs. others as they are being us ed. This method\\nis frequently used by web search engines.\\nThe most common version of this is A/B testing , a term borrowed from the A/B TEST\\nadvertising industry. For such a test, precisely one thing i s changed between\\nthe current system and a proposed system, and a small proport ion of traf-\\nﬁc (say, 1–10% of users) is randomly directed to the variant s ystem, while\\nmost users use the current system. For example, if we wish to i nvestigate a\\nchange to the ranking algorithm, we redirect a random sample of users to\\na variant system and evaluate measures such as the frequency with which\\npeople click on the top result, or any result on the ﬁrst page. (This particular\\nanalysis method is referred to as clickthrough log analysis orclickstream min- CLICKTHROUGH LOG\\nANALYSIS\\nCLICKSTREAM MININGing. It is further discussed as a method of implicit feedback in S ection 9.1.7\\n(page 187).)\\nThe basis of A/B testing is running a bunch of single variable tests (either\\nin sequence or in parallel): for each test only one parameter is varied from the\\ncontrol (the current live system). It is therefore easy to se e whether varying\\neach parameter has a positive or negative effect. Such testi ng of a live system\\ncan easily and cheaply gauge the effect of a change on users, a nd, with a\\nlarge enough user base, it is practical to measure even very s mall positive\\nand negative effects. In principle, more analytic power can be achieved by\\nvarying multiple things at once in an uncorrelated (random) way, and doing\\nstandard multivariate statistical analysis, such as multi ple linear regression.\\nIn practice, though, A/B testing is widely used, because A/B tests are easy\\nto deploy, easy to understand, and easy to explain to managem ent.\\n8.7 Results snippets\\nHaving chosen or ranked the documents matching a query, we wi sh to pre-\\nsent a results list that will be informative to the user. In ma ny cases the\\nuser will not want to examine all the returned documents and s o we want\\nto make the results list informative enough that the user can do a ﬁnal rank-\\ning of the documents for themselves based on relevance to the ir information\\nneed.3The standard way of doing this is to provide a snippet , a short sum- SNIPPET\\nmary of the document, which is designed so as to allow the user to decide\\nits relevance. Typically, the snippet consists of the docum ent title and a short\\n3. There are exceptions, in domains where recall is emphasiz ed. For instance, in many legal\\ndisclosure cases, a legal associate will review every document that matches a keyword search.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 206}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP8.7 Results snippets 171\\nsummary, which is automatically extracted. The question is how to design\\nthe summary so as to maximize its usefulness to the user.\\nThe two basic kinds of summaries are static , which are always the same STATIC SUMMARY\\nregardless of the query, and dynamic (or query-dependent), which are cus- DYNAMIC SUMMARY\\ntomized according to the user’s information need as deduced from a query.\\nDynamic summaries attempt to explain why a particular docum ent was re-\\ntrieved for the query at hand.\\nA static summary is generally comprised of either or both a su bset of the\\ndocument and metadata associated with the document. The sim plest form\\nof summary takes the ﬁrst two sentences or 50 words of a docume nt, or ex-\\ntracts particular zones of a document, such as the title and a uthor. Instead of\\nzones of a document, the summary can instead use metadata ass ociated with\\nthe document. This may be an alternative way to provide an aut hor or date,\\nor may include elements which are designed to give a summary, such as the\\ndescription metadata which can appear in the meta element of a web\\nHTML page. This summary is typically extracted and cached at indexing\\ntime, in such a way that it can be retrieved and presented quic kly when dis-\\nplaying search results, whereas having to access the actual document content\\nmight be a relatively expensive operation.\\nThere has been extensive work within natural language proce ssing (NLP)\\non better ways to do text summarization . Most such work still aims only to TEXT SUMMARIZATION\\nchoose sentences from the original document to present and c oncentrates on\\nhow to select good sentences. The models typically combine p ositional fac-\\ntors, favoring the ﬁrst and last paragraphs of documents and the ﬁrst and last\\nsentences of paragraphs, with content factors, emphasizin g sentences with\\nkey terms, which have low document frequency in the collecti on as a whole,\\nbut high frequency and good distribution across the particu lar document\\nbeing returned. In sophisticated NLP approaches, the syste m synthesizes\\nsentences for a summary, either by doing full text generatio n or by editing\\nand perhaps combining sentences used in the document. For ex ample, it\\nmight delete a relative clause or replace a pronoun with the n oun phrase\\nthat it refers to. This last class of methods remains in the re alm of research\\nand is seldom used for search results: it is easier, safer, an d often even better\\nto just use sentences from the original document.\\nDynamic summaries display one or more “windows” on the docum ent,\\naiming to present the pieces that have the most utility to the user in evalu-\\nating the document with respect to their information need. U sually these\\nwindows contain one or several of the query terms, and so are o ften re-\\nferred to as keyword-in-context (KWIC) snippets, though sometimes they may KEYWORD -IN-CONTEXT\\nstill be pieces of the text such as the title that are selected for their query-\\nindependent information value just as in the case of static s ummarization.\\nDynamic summaries are generated in conjunction with scorin g. If the query\\nis found as a phrase, occurrences of the phrase in the documen t will be', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 207}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP172 8 Evaluation in information retrieval\\n. . .In recent years, Papua New Guinea has faced severe economic\\ndifﬁculties and economic growth has slowed, partly as a result of weak\\ngovernance and civil war, and partly as a result of external f actors such as\\nthe Bougainville civil war which led to the closure in 1989 of the Panguna\\nmine (at that time the most important foreign exchange earne r and\\ncontributor to Government ﬁnances), the Asian ﬁnancial cri sis, a decline in\\nthe prices of gold and copper, and a fall in the production of o il.PNG’s\\neconomic development record over the past few years is evide nce that\\ngovernance issues underly many of the country’s problems. G ood\\ngovernance, which may be deﬁned as the transparent and accou ntable\\nmanagement of human, natural, economic and ﬁnancial resour ces for the\\npurposes of equitable and sustainable development, ﬂows fr om proper\\npublic sector management, efﬁcient ﬁscal and accounting me chanisms, and\\na willingness to make service delivery a priority in practic e. . . .\\n◮Figure 8.5 An example of selecting text for a dynamic snippet. This snip pet was\\ngenerated for a document in response to the query new guinea economic development .\\nThe ﬁgure shows in bold italic where the selected snippet tex t occurred in the original\\ndocument.\\nshown as the summary. If not, windows within the document tha t contain\\nmultiple query terms will be selected. Commonly these windo ws may just\\nstretch some number of words to the left and right of the query terms. This is\\na place where NLP techniques can usefully be employed: users prefer snip-\\npets that read well because they contain complete phrases.\\nDynamic summaries are generally regarded as greatly improv ing the us-\\nability of IR systems, but they present a complication for IR system design. A\\ndynamic summary cannot be precomputed, but, on the other han d, if a sys-\\ntem has only a positional index, then it cannot easily recons truct the context\\nsurrounding search engine hits in order to generate such a dy namic sum-\\nmary. This is one reason for using static summaries. The stan dard solution\\nto this in a world of large and cheap disk drives is to locally c ache all the\\ndocuments at index time (notwithstanding that this approac h raises various\\nlegal, information security and control issues that are far from resolved) as\\nshown in Figure 7.5(page 147). Then, a system can simply scan a document\\nwhich is about to appear in a displayed results list to ﬁnd sni ppets containing\\nthe query words. Beyond simply access to the text, producing a good KWIC\\nsnippet requires some care. Given a variety of keyword occur rences in a\\ndocument, the goal is to choose fragments which are: (i) maxi mally informa-\\ntive about the discussion of those terms in the document, (ii ) self-contained\\nenough to be easy to read, and (iii) short enough to ﬁt within t he normally\\nstrict constraints on the space available for summaries.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 208}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP8.8 References and further reading 173\\nGenerating snippets must be fast since the system is typical ly generating\\nmany snippets for each query that it handles. Rather than cac hing an entire\\ndocument, it is common to cache only a generous but ﬁxed size p reﬁx of\\nthe document, such as perhaps 10,000 characters. For most co mmon, short\\ndocuments, the entire document is thus cached, but huge amou nts of local\\nstorage will not be wasted on potentially vast documents. Su mmaries of\\ndocuments whose length exceeds the preﬁx size will be based o n material\\nin the preﬁx only, which is in general a useful zone in which to look for a\\ndocument summary anyway.\\nIf a document has been updated since it was last processed by a crawler\\nand indexer, these changes will be neither in the cache nor in the index. In\\nthese circumstances, neither the index nor the summary will accurately re-\\nﬂect the current contents of the document, but it is the diffe rences between\\nthe summary and the actual document content that will be more glaringly\\nobvious to the end user.\\n8.8 References and further reading\\nDeﬁnition and implementation of the notion of relevance to a query got off\\nto a rocky start in 1953. Swanson (1988 ) reports that in an evaluation in that\\nyear between two teams, they agreed that 1390 documents were variously\\nrelevant to a set of 98 questions, but disagreed on a further 1 577 documents,\\nand the disagreements were never resolved.\\nRigorous formal testing of IR systems was ﬁrst completed in t he Cranﬁeld\\nexperiments, beginning in the late 1950s. A retrospective d iscussion of the\\nCranﬁeld test collection and experimentation with it can be found in ( Clever-\\ndon 1991 ). The other seminal series of early IR experiments were thos e on the\\nSMART system by Gerard Salton and colleagues ( Salton 1971b ;1991 ). The\\nTREC evaluations are described in detail by Voorhees and Harman (2005 ).\\nOnline information is available at http://trec.nist.gov/ . Initially, few researchers\\ncomputed the statistical signiﬁcance of their experimenta l results, but the IR\\ncommunity increasingly demands this ( Hull 1993 ). User studies of IR system\\neffectiveness began more recently ( Saracevic and Kantor 1988 ;1996 ).\\nThe notions of recall and precision were ﬁrst used by Kent et al. (1955 ),\\nalthough the term precision did not appear until later. The F measure (or, FMEASURE\\nrather its complement E=1−F) was introduced by van Rijsbergen (1979 ).\\nHe provides an extensive theoretical discussion, which sho ws how adopting\\na principle of decreasing marginal relevance (at some point a user will be\\nunwilling to sacriﬁce a unit of precision for an added unit of recall) leads to\\nthe harmonic mean being the appropriate method for combinin g precision\\nand recall (and hence to its adoption rather than the minimum or geometric\\nmean).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 209}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP174 8 Evaluation in information retrieval\\nBuckley and Voorhees (2000 ) compare several evaluation measures, in-\\ncluding precision at k, MAP , and R-precision, and evaluate the error rate of\\neach measure. R-precision was adopted as the ofﬁcial evalua tion metric in R-PRECISION\\nthe TREC HARD track ( Allan 2005 ).Aslam and Yilmaz (2005 ) examine its\\nsurprisingly close correlation to MAP , which had been noted in earlier stud-\\nies (Tague-Sutcliffe and Blustein 1995 ,Buckley and Voorhees 2000 ). A stan-\\ndard program for evaluating IR systems which computes many m easures of\\nranked retrieval effectiveness is Chris Buckley’s trec_eval program used\\nin the TREC evaluations. It can be downloaded from: http://trec.nist.gov/trec_eval/ .\\nKekäläinen and Järvelin (2002 ) argue for the superiority of graded rele-\\nvance judgments when dealing with very large document colle ctions, and\\nJärvelin and Kekäläinen (2002 ) introduce cumulated gain-based methods for\\nIR system evaluation in this context. Sakai (2007 ) does a study of the stabil-\\nity and sensitivity of evaluation measures based on graded r elevance judg-\\nments from NTCIR tasks, and concludes that NDCG is best for ev aluating\\ndocument ranking.\\nSchamber et al. (1990 ) examine the concept of relevance, stressing its multi-\\ndimensional and context-speciﬁc nature, but also arguing t hat it can be mea-\\nsured effectively. ( Voorhees 2000 ) is the standard article for examining vari-\\nation in relevance judgments and their effects on retrieval system scores and\\nranking for the TREC Ad Hoc task. Voorhees concludes that although the\\nnumbers change, the rankings are quite stable. Hersh et al. (1994 ) present\\nsimilar analysis for a medical IR collection. In contrast, Kekäläinen (2005 )\\nanalyze some of the later TRECs, exploring a 4-way relevance judgment and\\nthe notion of cumulative gain, arguing that the relevance me asure used does\\nsubstantially affect system rankings. See also Harter (1998 ).Zobel (1998 )\\nstudies whether the pooling method used by TREC to collect a s ubset of doc-\\numents that will be evaluated for relevance is reliable and f air, and concludes\\nthat it is.\\nThe kappa statistic and its use for language-related purpos es is discussed KAPPA STATISTIC\\nbyCarletta (1996 ). Many standard sources (e.g., Siegel and Castellan 1988 )\\npresent pooled calculation of the expected agreement, but Di Eugenio and\\nGlass (2004 ) argue for preferring the unpooled agreement (though perha ps\\npresenting multiple measures). For further discussion of a lternative mea-\\nsures of agreement, which may in fact be better, see Lombard et al. (2002 )\\nand Krippendorff (2003 ).\\nText summarization has been actively explored for many year s. Modern\\nwork on sentence selection was initiated by Kupiec et al. (1995 ). More recent\\nwork includes ( Barzilay and Elhadad 1997 ) and ( Jing 2000 ), together with\\na broad selection of work appearing at the yearly DUC confere nces and at\\nother NLP venues. Tombros and Sanderson (1998 ) demonstrate the advan-\\ntages of dynamic summaries in the IR context. Turpin et al. (2007 ) address\\nhow to generate snippets efﬁciently.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 210}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP8.8 References and further reading 175\\nClickthrough log analysis is studied in ( Joachims 2002b ,Joachims et al.\\n2005 ).\\nIn a series of papers, Hersh, Turpin and colleagues show how i mprove-\\nments in formal retrieval effectiveness, as evaluated in ba tch experiments, do\\nnot always translate into an improved system for users ( Hersh et al. 2000a ;b;\\n2001 ,Turpin and Hersh 2001 ;2002 ).\\nUser interfaces for IR and human factors such as models of hum an infor-\\nmation seeking and usability testing are outside the scope o f what we cover\\nin this book. More information on these topics can be found in other text-\\nbooks, including ( Baeza-Yates and Ribeiro-Neto 1999 , ch. 10) and ( Korfhage\\n1997 ), and collections focused on cognitive aspects ( Spink and Cole 2005 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 211}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 212}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 177\\n9Relevance feedback and query\\nexpansion\\nIn most collections, the same concept may be referred to usin g different\\nwords. This issue, known as synonymy , has an impact on the recall of most SYNONYMY\\ninformation retrieval systems. For example, you would want a search for\\naircraft to match plane (but only for references to an airplane , not a woodwork-\\ning plane), and for a search on thermodynamics to match references to heat in\\nappropriate discussions. Users often attempt to address th is problem them-\\nselves by manually reﬁning a query, as was discussed in Secti on1.4; in this\\nchapter we discuss ways in which a system can help with query r eﬁnement,\\neither fully automatically or with the user in the loop.\\nThe methods for tackling this problem split into two major cl asses: global\\nmethods and local methods. Global methods are techniques fo r expanding\\nor reformulating query terms independent of the query and re sults returned\\nfrom it, so that changes in the query wording will cause the ne w query to\\nmatch other semantically similar terms. Global methods inc lude:\\n•Query expansion/reformulation with a thesaurus or WordNet (Section 9.2.2 )\\n•Query expansion via automatic thesaurus generation (Secti on9.2.3 )\\n•Techniques like spelling correction (discussed in Chapter 3)\\nLocal methods adjust a query relative to the documents that i nitially appear\\nto match the query. The basic methods here are:\\n•Relevance feedback (Section 9.1)\\n•Pseudo relevance feedback, also known as Blind relevance fe edback (Sec-\\ntion 9.1.6 )\\n•(Global) indirect relevance feedback (Section 9.1.7 )\\nIn this chapter, we will mention all of these approaches, but we will concen-\\ntrate on relevance feedback, which is one of the most used and most success-\\nful approaches.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 213}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP178 9 Relevance feedback and query expansion\\n9.1 Relevance feedback and pseudo relevance feedback\\nThe idea of relevance feedback (RF) is to involve the user in the retrieval process RELEVANCE FEEDBACK\\nso as to improve the ﬁnal result set. In particular, the user g ives feedback on\\nthe relevance of documents in an initial set of results. The b asic procedure is:\\n•The user issues a (short, simple) query.\\n•The system returns an initial set of retrieval results.\\n•The user marks some returned documents as relevant or nonrel evant.\\n•The system computes a better representation of the informat ion need based\\non the user feedback.\\n•The system displays a revised set of retrieval results.\\nRelevance feedback can go through one or more iterations of t his sort. The\\nprocess exploits the idea that it may be difﬁcult to formulat e a good query\\nwhen you don’t know the collection well, but it is easy to judg e particular\\ndocuments, and so it makes sense to engage in iterative query reﬁnement\\nof this sort. In such a scenario, relevance feedback can also be effective in\\ntracking a user’s evolving information need: seeing some do cuments may\\nlead users to reﬁne their understanding of the information t hey are seeking.\\nImage search provides a good example of relevance feedback. Not only is\\nit easy to see the results at work, but this is a domain where a u ser can easily\\nhave difﬁculty formulating what they want in words, but can e asily indicate\\nrelevant or nonrelevant images. After the user enters an ini tial query for bike\\non the demonstration system at:\\nhttp://nayana.ece.ucsb.edu/imsearch/imsearch.html\\nthe initial results (in this case, images) are returned. In F igure 9.1(a), the\\nuser has selected some of them as relevant. These will be used to reﬁne the\\nquery, while other displayed results have no effect on the re formulation. Fig-\\nure9.1(b) then shows the new top-ranked results calculated after t his round\\nof relevance feedback.\\nFigure 9.2shows a textual IR example where the user wishes to ﬁnd out\\nabout new applications of space satellites.\\n9.1.1 The Rocchio algorithm for relevance feedback\\nThe Rocchio Algorithm is the classic algorithm for implemen ting relevance\\nfeedback. It models a way of incorporating relevance feedba ck information\\ninto the vector space model of Section 6.3.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 214}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP9.1 Relevance feedback and pseudo relevance feedback 179\\n(a)\\n(b)\\n◮Figure 9.1 Relevance feedback searching over images. (a) The user view s the\\ninitial query results for a query of bike, selects the ﬁrst, third and fourth result in\\nthe top row and the fourth result in the bottom row as relevant , and submits this\\nfeedback. (b) The users sees the revised result set. Precisi on is greatly improved.\\nFromhttp://nayana.ece.ucsb.edu/imsearch/imsearch.html (Newsam et al. 2001 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 215}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP180 9 Relevance feedback and query expansion\\n(a) Query: New space satellite applications\\n(b) + 1. 0.539, 08/13/91, NASA Hasn’t Scrapped Imaging Spectrom eter\\n+ 2. 0.533, 07/09/91, NASA Scratches Environment Gear From S atel-\\nlite Plan\\n3. 0.528, 04/04/90, Science Panel Backs NASA Satellite Plan , But\\nUrges Launches of Smaller Probes\\n4. 0.526, 09/09/91, A NASA Satellite Project Accomplishes I ncredi-\\nble Feat: Staying Within Budget\\n5. 0.525, 07/24/90, Scientist Who Exposed Global Warming Pr o-\\nposes Satellites for Climate Research\\n6. 0.524, 08/22/90, Report Provides Support for the Critics Of Using\\nBig Satellites to Study Climate\\n7. 0.516, 04/13/87, Arianespace Receives Satellite Launch Pact\\nFrom Telesat Canada\\n+ 8. 0.509, 12/02/87, Telecommunications Tale of Two Compan ies\\n(c) 2.074 new 15.106 space\\n30.816 satellite 5.660 application\\n5.991 nasa 5.196 eos\\n4.196 launch 3.972 aster\\n3.516 instrument 3.446 arianespace\\n3.004 bundespost 2.806 ss\\n2.790 rocket 2.053 scientist\\n2.003 broadcast 1.172 earth\\n0.836 oil 0.646 measure\\n(d) * 1. 0.513, 07/09/91, NASA Scratches Environment Gear From S atel-\\nlite Plan\\n* 2. 0.500, 08/13/91, NASA Hasn’t Scrapped Imaging Spectrom eter\\n3. 0.493, 08/07/89, When the Pentagon Launches a Secret Sate llite,\\nSpace Sleuths Do Some Spy Work of Their Own\\n4. 0.493, 07/31/89, NASA Uses ‘Warm’ Superconductors For Fa st\\nCircuit\\n* 5. 0.492, 12/02/87, Telecommunications Tale of Two Compan ies\\n6. 0.491, 07/09/91, Soviets May Adapt Parts of SS-20 Missile For\\nCommercial Use\\n7. 0.490, 07/12/88, Gaping Gap: Pentagon Lags in Race To Matc h\\nthe Soviets In Rocket Launchers\\n8. 0.490, 06/14/90, Rescue of Satellite By Space Agency To Co st $90\\nMillion\\n◮Figure 9.2 Example of relevance feedback on a text collection. (a) The i nitial query\\n(a). (b) The user marks some relevant documents (shown with a plus sign). (c) The\\nquery is then expanded by 18 terms with weights as shown. (d) T he revised top\\nresults are then shown. A * marks the documents which were jud ged relevant in the\\nrelevance feedback phase.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 216}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP9.1 Relevance feedback and pseudo relevance feedback 181\\n◮Figure 9.3 The Rocchio optimal query for separating relevant and nonre levant\\ndocuments.\\nThe underlying theory. We want to ﬁnd a query vector, denoted as ⃗q, that\\nmaximizes similarity with relevant documents while minimi zing similarity\\nwith nonrelevant documents. If Cris the set of relevant documents and Cnr\\nis the set of nonrelevant documents, then we wish to ﬁnd:1\\n⃗qopt=arg max\\n⃗q[sim(⃗q,Cr)−sim(⃗q,Cnr)], (9.1)\\nwhere sim is deﬁned as in Equation 6.10. Under cosine similarity, the optimal\\nquery vector ⃗qoptfor separating the relevant and nonrelevant documents is:\\n⃗qopt=1\\n|Cr|∑\\n⃗dj∈Cr⃗dj−1\\n|Cnr|∑\\n⃗dj∈Cnr⃗dj (9.2)\\nThat is, the optimal query is the vector difference between t he centroids of the\\nrelevant and nonrelevant documents; see Figure 9.3. However, this observa-\\ntion is not terribly useful, precisely because the full set o f relevant documents\\nis not known: it is what we want to ﬁnd.\\nThe Rocchio (1971 ) algorithm. This was the relevance feedback mecha- ROCCHIO ALGORITHM\\n1. In the equation, arg maxxf(x)returns a value of xwhich maximizes the value of the function\\nf(x). Similarly, arg minxf(x)returns a value of xwhich minimizes the value of the function\\nf(x).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 217}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP182 9 Relevance feedback and query expansion\\n◮Figure 9.4 An application of Rocchio’s algorithm. Some documents have been\\nlabeled as relevant and nonrelevant and the initial query ve ctor is moved in response\\nto this feedback.\\nnism introduced in and popularized by Salton’s SMART system around 1970.\\nIn a real IR query context, we have a user query and partial kno wledge of\\nknown relevant and nonrelevant documents. The algorithm pr oposes using\\nthe modiﬁed query ⃗qm:\\n⃗qm=α⃗q0+β1\\n|Dr|∑\\n⃗dj∈Dr⃗dj−γ1\\n|Dnr|∑\\n⃗dj∈Dnr⃗dj (9.3)\\nwhere q0is the original query vector, Drand Dnrare the set of known rel-\\nevant and nonrelevant documents respectively, and α,β, and γare weights\\nattached to each term. These control the balance between tru sting the judged\\ndocument set versus the query: if we have a lot of judged docum ents, we\\nwould like a higher βand γ. Starting from q0, the new query moves you\\nsome distance toward the centroid of the relevant documents and some dis-\\ntance away from the centroid of the nonrelevant documents. T his new query\\ncan be used for retrieval in the standard vector space model ( see Section 6.3).\\nWe can easily leave the positive quadrant of the vector space by subtracting\\noff a nonrelevant document’s vector. In the Rocchio algorit hm, negative term\\nweights are ignored. That is, the term weight is set to 0. Figu re9.4shows the\\neffect of applying relevance feedback.\\nRelevance feedback can improve both recall and precision. B ut, in prac-\\ntice, it has been shown to be most useful for increasing recal l in situations', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 218}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP9.1 Relevance feedback and pseudo relevance feedback 183\\nwhere recall is important. This is partly because the techni que expands the\\nquery, but it is also partly an effect of the use case: when the y want high\\nrecall, users can be expected to take time to review results a nd to iterate on\\nthe search. Positive feedback also turns out to be much more v aluable than\\nnegative feedback, and so most IR systems set γ<β. Reasonable values\\nmight be α=1,β=0.75, and γ=0.15. In fact, many systems, such as\\nthe image search system in Figure 9.1, allow only positive feedback, which\\nis equivalent to setting γ=0. Another alternative is to use only the marked\\nnonrelevant document which received the highest ranking fr om the IR sys-\\ntem as negative feedback (here, |Dnr|=1 in Equation ( 9.3)). While many of\\nthe experimental results comparing various relevance feed back variants are\\nrather inconclusive, some studies have suggested that this variant, called Ide IDE DEC -HI\\ndec-hi is the most effective or at least the most consistent perform er.\\n✄9.1.2 Probabilistic relevance feedback\\nRather than reweighting the query in a vector space, if a user has told us\\nsome relevant and nonrelevant documents, then we can procee d to build a\\nclassiﬁer. One way of doing this is with a Naive Bayes probabi listic model.\\nIfRis a Boolean indicator variable expressing the relevance of a document,\\nthen we can estimate P(xt=1|R), the probability of a term tappearing in a\\ndocument, depending on whether it is relevant or not, as:\\nˆP(xt=1|R=1) =|VR t|/|VR| (9.4)\\nˆP(xt=1|R=0) = ( d ft−|VR t|)/(N−|VR|)\\nwhere Nis the total number of documents, d ftis the number that contain\\nt,VRis the set of known relevant documents, and VR tis the subset of this\\nset containing t. Even though the set of known relevant documents is a per-\\nhaps small subset of the true set of relevant documents, if we assume that\\nthe set of relevant documents is a small subset of the set of al l documents\\nthen the estimates given above will be reasonable. This give s a basis for\\nanother way of changing the query term weights. We will discu ss such prob-\\nabilistic approaches more in Chapters 11and 13, and in particular outline\\nthe application to relevance feedback in Section 11.3.4 (page 228). For the\\nmoment, observe that using just Equation ( 9.4) as a basis for term-weighting\\nis likely insufﬁcient. The equations use only collection st atistics and infor-\\nmation about the term distribution within the documents jud ged relevant.\\nThey preserve no memory of the original query.\\n9.1.3 When does relevance feedback work?\\nThe success of relevance feedback depends on certain assump tions. Firstly,\\nthe user has to have sufﬁcient knowledge to be able to make an i nitial query', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 219}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP184 9 Relevance feedback and query expansion\\nwhich is at least somewhere close to the documents they desir e. This is\\nneeded anyhow for successful information retrieval in the b asic case, but\\nit is important to see the kinds of problems that relevance fe edback cannot\\nsolve alone. Cases where relevance feedback alone is not suf ﬁcient include:\\n•Misspellings. If the user spells a term in a different way to t he way it\\nis spelled in any document in the collection, then relevance feedback is\\nunlikely to be effective. This can be addressed by the spelli ng correction\\ntechniques of Chapter 3.\\n•Cross-language information retrieval. Documents in anoth er language\\nare not nearby in a vector space based on term distribution. R ather, docu-\\nments in the same language cluster more closely together.\\n•Mismatch of searcher’s vocabulary versus collection vocab ulary. If the\\nuser searches for laptop but all the documents use the term notebook com-\\nputer , then the query will fail, and relevance feedback is again mo st likely\\nineffective.\\nSecondly, the relevance feedback approach requires releva nt documents to\\nbe similar to each other. That is, they should cluster. Ideal ly, the term dis-\\ntribution in all relevant documents will be similar to that i n the documents\\nmarked by the users, while the term distribution in all nonre levant docu-\\nments will be different from those in relevant documents. Th ings will work\\nwell if all relevant documents are tightly clustered around a single proto-\\ntype, or, at least, if there are different prototypes, if the relevant documents\\nhave signiﬁcant vocabulary overlap, while similarities be tween relevant and\\nnonrelevant documents are small. Implicitly, the Rocchio r elevance feedback\\nmodel treats relevant documents as a single cluster , which it models via the\\ncentroid of the cluster. This approach does not work as well i f the relevant\\ndocuments are a multimodal class, that is, they consist of se veral clusters of\\ndocuments within the vector space. This can happen with:\\n•Subsets of the documents using different vocabulary, such a sBurma vs.\\nMyanmar\\n•A query for which the answer set is inherently disjunctive, s uch asPop\\nstarswho once workedat BurgerKing .\\n•Instances of a general concept, which often appear as a disju nction of\\nmore speciﬁc concepts, for example, felines .\\nGood editorial content in the collection can often provide a solution to this\\nproblem. For example, an article on the attitudes of differe nt groups to the\\nsituation in Burma could introduce the terminology used by d ifferent parties,\\nthus linking the document clusters.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 220}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP9.1 Relevance feedback and pseudo relevance feedback 185\\nRelevance feedback is not necessarily popular with users. U sers are often\\nreluctant to provide explicit feedback, or in general do not wish to prolong\\nthe search interaction. Furthermore, it is often harder to u nderstand why a\\nparticular document was retrieved after relevance feedbac k is applied.\\nRelevance feedback can also have practical problems. The lo ng queries\\nthat are generated by straightforward application of relev ance feedback tech-\\nniques are inefﬁcient for a typical IR system. This results i n a high computing\\ncost for the retrieval and potentially long response times f or the user. A par-\\ntial solution to this is to only reweight certain prominent t erms in the relevant\\ndocuments, such as perhaps the top 20 terms by term frequency . Some ex-\\nperimental results have also suggested that using a limited number of terms\\nlike this may give better results ( Harman 1992 ) though other work has sug-\\ngested that using more terms is better in terms of retrieved d ocument quality\\n(Buckley et al. 1994b ).\\n9.1.4 Relevance feedback on the web\\nSome web search engines offer a similar/related pages featu re: the user in-\\ndicates a document in the results set as exemplary from the st andpoint of\\nmeeting his information need and requests more documents li ke it. This can\\nbe viewed as a particular simple form of relevance feedback. However, in\\ngeneral relevance feedback has been little used in web searc h. One exception\\nwas the Excite web search engine, which initially provided f ull relevance\\nfeedback. However, the feature was in time dropped, due to la ck of use. On\\nthe web, few people use advanced search interfaces and most w ould like to\\ncomplete their search in a single interaction. But the lack o f uptake also prob-\\nably reﬂects two other factors: relevance feedback is hard t o explain to the\\naverage user, and relevance feedback is mainly a recall enha ncing strategy,\\nand web search users are only rarely concerned with getting s ufﬁcient recall.\\nSpink et al. (2000 ) present results from the use of relevance feedback in\\nthe Excite search engine. Only about 4% of user query session s used the\\nrelevance feedback option, and these were usually exploiti ng the “More like\\nthis” link next to each result. About 70% of users only looked at the ﬁrst\\npage of results and did not pursue things any further. For peo ple who used\\nrelevance feedback, results were improved about two thirds of the time.\\nAn important more recent thread of work is the use of clickstr eam data\\n(what links a user clicks on) to provide indirect relevance f eedback. Use\\nof this data is studied in detail in ( Joachims 2002b ,Joachims et al. 2005 ).\\nThe very successful use of web link structure (see Chapter 21) can also be\\nviewed as implicit feedback, but provided by page authors ra ther than read-\\ners (though in practice most authors are also readers).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 221}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP186 9 Relevance feedback and query expansion\\n?Exercise 9.1\\nIn Rocchio’s algorithm, what weight setting for α/β/γdoes a “Find pages like this\\none” search correspond to?\\nExercise 9.2 [⋆]\\nGive three reasons why relevance feedback has been little us ed in web search.\\n9.1.5 Evaluation of relevance feedback strategies\\nInteractive relevance feedback can give very substantial g ains in retrieval\\nperformance. Empirically, one round of relevance feedback is often very\\nuseful. Two rounds is sometimes marginally more useful. Suc cessful use of\\nrelevance feedback requires enough judged documents, othe rwise the pro-\\ncess is unstable in that it may drift away from the user’s info rmation need.\\nAccordingly, having at least ﬁve judged documents is recomm ended.\\nThere is some subtlety to evaluating the effectiveness of re levance feed-\\nback in a sound and enlightening way. The obvious ﬁrst strate gy is to start\\nwith an initial query q0and to compute a precision-recall graph. Following\\none round of feedback from the user, we compute the modiﬁed qu eryqm\\nand again compute a precision-recall graph. Here, in both ro unds we assess\\nperformance over all documents in the collection, which mak es comparisons\\nstraightforward. If we do this, we ﬁnd spectacular gains fro m relevance feed-\\nback: gains on the order of 50% in mean average precision. But unfortunately\\nit is cheating. The gains are partly due to the fact that known relevant doc-\\numents (judged by the user) are now ranked higher. Fairness d emands that\\nwe should only evaluate with respect to documents not seen by the user.\\nA second idea is to use documents in the residual collection (the set of doc-\\numents minus those assessed relevant) for the second round o f evaluation.\\nThis seems like a more realistic evaluation. Unfortunately , the measured per-\\nformance can then often be lower than for the original query. This is partic-\\nularly the case if there are few relevant documents, and so a f air proportion\\nof them have been judged by the user in the ﬁrst round. The rela tive per-\\nformance of variant relevance feedback methods can be valid ly compared,\\nbut it is difﬁcult to validly compare performance with and wi thout relevance\\nfeedback because the collection size and the number of relev ant documents\\nchanges from before the feedback to after it.\\nThus neither of these methods is fully satisfactory. A third method is to\\nhave two collections, one which is used for the initial query and relevance\\njudgments, and the second that is then used for comparative e valuation. The\\nperformance of both q0and qmcan be validly compared on the second col-\\nlection.\\nPerhaps the best evaluation of the utility of relevance feed back is to do user\\nstudies of its effectiveness, in particular by doing a time- based comparison:', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 222}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP9.1 Relevance feedback and pseudo relevance feedback 187\\nPrecision at k=50\\nTerm weighting no RF pseudo RF\\nlnc.ltc 64.2% 72.7%\\nLnu.ltu 74.2% 87.0%\\n◮Figure 9.5 Results showing pseudo relevance feedback greatly improvi ng perfor-\\nmance. These results are taken from the Cornell SMART system at TREC 4 ( Buckley\\net al. 1995 ), and also contrast the use of two different length normaliz ation schemes\\n(L vs. l); cf. Figure 6.15 (page 128). Pseudo relevance feedback consisted of adding 20\\nterms to each query.\\nhow fast does a user ﬁnd relevant documents with relevance fe edback vs.\\nanother strategy (such as query reformulation), or alterna tively, how many\\nrelevant documents does a user ﬁnd in a certain amount of time . Such no-\\ntions of user utility are fairest and closest to real system u sage.\\n9.1.6 Pseudo relevance feedback\\nPseudo relevance feedback , also known as blind relevance feedback , provides a PSEUDO RELEVANCE\\nFEEDBACK\\nBLIND RELEVANCE\\nFEEDBACKmethod for automatic local analysis. It automates the manua l part of rele-\\nvance feedback, so that the user gets improved retrieval per formance with-\\nout an extended interaction. The method is to do normal retri eval to ﬁnd an\\ninitial set of most relevant documents, to then assume that the top kranked\\ndocuments are relevant, and ﬁnally to do relevance feedback as before under\\nthis assumption.\\nThis automatic technique mostly works. Evidence suggests t hat it tends\\nto work better than global analysis (Section 9.2). It has been found to im-\\nprove performance in the TREC ad hoc task. See for example the results in\\nFigure 9.5. But it is not without the dangers of an automatic process. Fo r\\nexample, if the query is about copper mines and the top several documents\\nare all about mines in Chile, then there may be query drift in t he direction of\\ndocuments on Chile.\\n9.1.7 Indirect relevance feedback\\nWe can also use indirect sources of evidence rather than expl icit feedback on\\nrelevance as the basis for relevance feedback. This is often called implicit (rel- IMPLICIT RELEVANCE\\nFEEDBACK evance) feedback . Implicit feedback is less reliable than explicit feedback , but is\\nmore useful than pseudo relevance feedback, which contains no evidence of\\nuser judgments. Moreover, while users are often reluctant t o provide explicit\\nfeedback, it is easy to collect implicit feedback in large qu antities for a high\\nvolume system, such as a web search engine.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 223}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP188 9 Relevance feedback and query expansion\\nOn the web, DirectHit introduced the idea of ranking more hig hly docu-\\nments that users chose to look at more often. In other words, c licks on links\\nwere assumed to indicate that the page was likely relevant to the query. This\\napproach makes various assumptions, such as that the docume nt summaries\\ndisplayed in results lists (on whose basis users choose whic h documents to\\nclick on) are indicative of the relevance of these documents . In the original\\nDirectHit search engine, the data about the click rates on pa ges was gathered\\nglobally, rather than being user or query speciﬁc. This is on e form of the gen-\\neral area of clickstream mining . Today, a closely related approach is used in CLICKSTREAM MINING\\nranking the advertisements that match a web search query (Ch apter 19).\\n9.1.8 Summary\\nRelevance feedback has been shown to be very effective at imp roving rele-\\nvance of results. Its successful use requires queries for wh ich the set of rele-\\nvant documents is medium to large. Full relevance feedback i s often onerous\\nfor the user, and its implementation is not very efﬁcient in m ost IR systems.\\nIn many cases, other types of interactive retrieval may impr ove relevance by\\nabout as much with less work.\\nBeyond the core ad hoc retrieval scenario, other uses of rele vance feedback\\ninclude:\\n•Following a changing information need (e.g., names of car mo dels of in-\\nterest change over time)\\n•Maintaining an information ﬁlter (e.g., for a news feed). Su ch ﬁlters are\\ndiscussed further in Chapter 13.\\n•Active learning (deciding which examples it is most useful t o know the\\nclass of to reduce annotation costs).\\n?Exercise 9.3\\nUnder what conditions would the modiﬁed query qmin Equation 9.3be the same as\\nthe original query q0? In all other cases, is qmcloser than q0to the centroid of the\\nrelevant documents?\\nExercise 9.4\\nWhy is positive feedback likely to be more useful than negati ve feedback to an IR\\nsystem? Why might only using one nonrelevant document be mor e effective than\\nusing several?\\nExercise 9.5\\nSuppose that a user’s initial query is cheap CDs cheap DVDs extremely cheap CDs . The\\nuser examines two documents, d1and d2. She judges d1, with the content CDs cheap\\nsoftware cheap CDs relevant and d2with content cheap thrills DVDs nonrelevant. As-\\nsume that we are using direct term frequency (with no scaling and no document', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 224}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP9.2 Global methods for query reformulation 189\\nfrequency). There is no need to length-normalize vectors. U sing Rocchio relevance\\nfeedback as in Equation ( 9.3) what would the revised query vector be after relevance\\nfeedback? Assume α=1,β=0.75, γ=0.25.\\nExercise 9.6 [⋆]\\nOmar has implemented a relevance feedback web search system , where he is going\\nto do relevance feedback based only on words in the title text returned for a page (for\\nefﬁciency). The user is going to rank 3 results. The ﬁrst user , Jinxing, queries for:\\nbananaslug\\nand the top three titles returned are:\\nbanana slug Ariolimax columbianus\\nSanta Cruz mountains banana slug\\nSanta Cruz Campus Mascot\\nJinxing judges the ﬁrst two documents relevant, and the thir d nonrelevant. Assume\\nthat Omar’s search engine uses term frequency but no length n ormalization nor IDF.\\nAssume that he is using the Rocchio relevance feedback mecha nism, with α=β=\\nγ=1. Show the ﬁnal revised query that would be run. (Please list the vector elements\\nin alphabetical order.)\\n9.2 Global methods for query reformulation\\nIn this section we more brieﬂy discuss three global methods f or expanding a\\nquery: by simply aiding the user in doing so, by using a manual thesaurus,\\nand through building a thesaurus automatically.\\n9.2.1 Vocabulary tools for query reformulation\\nVarious user supports in the search process can help the user see how their\\nsearches are or are not working. This includes information a bout words that\\nwere omitted from the query because they were on stop lists, w hat words\\nwere stemmed to, the number of hits on each term or phrase, and whether\\nwords were dynamically turned into phrases. The IR system mi ght also sug-\\ngest search terms by means of a thesaurus or a controlled voca bulary. A user\\ncan also be allowed to browse lists of the terms that are in the inverted index,\\nand thus ﬁnd good terms that appear in the collection.\\n9.2.2 Query expansion\\nIn relevance feedback, users give additional input on docum ents (by mark-\\ning documents in the results set as relevant or not), and this input is used\\nto reweight the terms in the query for documents. In query expansion on the QUERY EXPANSION\\nother hand, users give additional input on query words or phr ases, possibly\\nsuggesting additional query terms. Some search engines (es pecially on the', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 225}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP190 9 Relevance feedback and query expansionY a h o o ! M y Y a h o o ! M a i l W e l c o m e , G u e s t [ S i g n I n ] H e l pS e a r c hp a l m\\nW\\ne b I m a g e s V i d e o L o c a l S h o p p i n g m o r eO p t i o n sA l s o t r y :S P O N S O R R E S U L T S\\np a\\nl m t r e e s ,p a\\nl m s p r i n g s ,p a\\nl m c e n t r o ,p a\\nl mt r e o , M o r e . . .Pa\\nl m b A T & Ta t t . c o m / w i r e l e s s l G o m o b i l e e f f o r t l e s s l y w i t h t h e P A L M T r e o f r o mA\\nT & T ( C i n g u l a r ) .Pa\\nl m H a n d h e l d sP a l m . c o m l O r g a n i z e r , P l a n n e r , W i F i , M u s i c B l u e t o o t h , G a m e s ,P h o t o s & V i d e o .Pa\\nl m , I n c .M a k e r o f h a n d h e l d P DA\\nd e v i c e s t h a t a l l o w m o b i l e u s e r s t o m a n a g es c h e d u l e s , c o n t a c t s , a n d o t h e r p e r s o n a l a n d b u s i n e s s i n f o r m a t i o n .w w w . p a l m . c o m l C a c h e dPa\\nl m , I n c . b T r e o a n d C e n t r o s m a r t p h o n e s , h a n d h e l d s ,a n d a c c e s s o r i e sP a l m , I n c . , i n n o v a t o r o f e a s y l t o l u s e m o b i l e p r o d u c t s i n c l u d i n gP a l m ® T r e o _ a n d C e n t r o _ s m a r t p h o n e s , P a l m h a n d h e l d s , s e r v i c e s ,a n d a c c e s s o r i e s .w w w . p a l m . c o m / u s l C a c h e d\\nS P O N S O R R E S U L T SH a n d h e l d s a t D e l lS t a y C o n n e c t e d w i t hH\\na n d h e l d P C s & P DA\\ns .S h o p a t D e l l ™ O f f i c i a lS i t e .w w w . D e l l . c o mB u y Pa\\nl m C e n t r oC a s e sU\\nl t i m a t e s e l e c t i o n o fc a s e s a n d a c c e s s o r i e sf o r b u s i n e s s d e v i c e s .w w w . C a s e s . c o mF r e e P l a m T r e oG e tA\\nF r e e P a l m T r e o7 0 0\\nW P h o n e . P a r t i c i p a t eT o d a y .E v\\na l u a t i o n N a t i o n . c o m /t r e o\\n1 ª 1 0\\no f a b o u t 5 3 4 ,0 0 0\\n,0 0 0\\nf o r p a l m ( A b o u t t h i s p a g e )ª 0 . 1 1\\ns e c.\\n◮Figure 9.6 An example of query expansion in the interface of the Yahoo! w eb\\nsearch engine in 2006. The expanded query suggestions appea r just below the “Search\\nResults” bar.\\nweb) suggest related queries in response to a query; the user s then opt to use\\none of these alternative query suggestions. Figure 9.6shows an example of\\nquery suggestion options being presented in the Yahoo! web s earch engine.\\nThe central question in this form of query expansion is how to generate al-\\nternative or expanded queries for the user. The most common f orm of query\\nexpansion is global analysis, using some form of thesaurus. For each term\\ntin a query, the query can be automatically expanded with syno nyms and\\nrelated words of tfrom the thesaurus. Use of a thesaurus can be combined\\nwith ideas of term weighting: for instance, one might weight added terms\\nless than original query terms.\\nMethods for building a thesaurus for query expansion includ e:\\n•Use of a controlled vocabulary that is maintained by human ed itors. Here,\\nthere is a canonical term for each concept. The subject headi ngs of tra-\\nditional library subject indexes, such as the Library of Con gress Subject\\nHeadings, or the Dewey Decimal system are examples of a contr olled\\nvocabulary. Use of a controlled vocabulary is quite common f or well-\\nresourced domains. A well-known example is the Uniﬁed Medic al Lan-\\nguage System (UMLS) used with MedLine for querying the biome dical\\nresearch literature. For example, in Figure 9.7,neoplasms was added to a', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 226}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP9.2 Global methods for query reformulation 191\\n• User query:cancer\\n• PubMed query: (“neoplasms”[TIAB] NOT Medline[SB]) OR “neoplasms”[MeSH\\nTerms]OR cancer[TextWord]\\n• User query:skinitch\\n• PubMed query: (“skin”[MeSH Terms] OR (“integumentary system”[TIAB] NOT\\nMedline[SB]) OR “integumentary system”[MeSHTerms] OR ski n[Text Word])AND\\n((“pruritus”[TIAB] NOT Medline[SB]) OR “pruritus”[MeSH T erms] OR itch[Text\\nWord])\\n◮Figure 9.7 Examples of query expansion via the PubMed thesaurus. When a user\\nissues a query on the PubMed interface to Medline at http://www.ncbi.nlm.nih.gov/entrez/ ,\\ntheir query is mapped on to the Medline vocabulary as shown.\\nsearch for cancer . This Medline query expansion also contrasts with the\\nYahoo! example. The Yahoo! interface is a case of interactiv e query expan-\\nsion, whereas PubMed does automatic query expansion. Unles s the user\\nchooses to examine the submitted query, they may not even rea lize that\\nquery expansion has occurred.\\n•A manual thesaurus. Here, human editors have built up sets of synony-\\nmous names for concepts, without designating a canonical te rm. The\\nUMLS metathesaurus is one example of a thesaurus. Statistic s Canada\\nmaintains a thesaurus of preferred terms, synonyms, broade r terms, and\\nnarrower terms for matters on which the government collects statistics,\\nsuch as goods and services. This thesaurus is also bilingual English and\\nFrench.\\n•An automatically derived thesaurus. Here, word co-occurre nce statistics\\nover a collection of documents in a domain are used to automat ically in-\\nduce a thesaurus; see Section 9.2.3 .\\n•Query reformulations based on query log mining. Here, we exp loit the\\nmanual query reformulations of other users to make suggesti ons to a new\\nuser. This requires a huge query volume, and is thus particul arly appro-\\npriate to web search.\\nThesaurus-based query expansion has the advantage of not re quiring any\\nuser input. Use of query expansion generally increases reca ll and is widely\\nused in many science and engineering ﬁelds. As well as such gl obal analysis\\ntechniques, it is also possible to do query expansion by loca l analysis, for\\ninstance, by analyzing the documents in the result set. User input is now', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 227}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP192 9 Relevance feedback and query expansion\\nWord Nearest neighbors\\nabsolutely absurd, whatsoever, totally, exactly, nothing\\nbottomed dip, copper, drops, topped, slide, trimmed\\ncaptivating shimmer, stunningly, superbly, plucky, witty\\ndoghouse dog, porch, crawling, beside, downstairs\\nmakeup repellent, lotion, glossy, sunscreen, skin, gel\\nmediating reconciliation, negotiate, case, conciliation\\nkeeping hoping, bring, wiping, could, some, would\\nlithographs drawings, Picasso, Dali, sculptures, Gauguin\\npathogens toxins, bacteria, organisms, bacterial, parasite\\nsenses grasp, psyche, truly, clumsy, naive, innate\\n◮Figure 9.8 An example of an automatically generated thesaurus. This ex ample\\nis based on the work in Schütze (1998 ), which employs latent semantic indexing (see\\nChapter 18).\\nusually required, but a distinction remains as to whether th e user is giving\\nfeedback on documents or on query terms.\\n9.2.3 Automatic thesaurus generation\\nAs an alternative to the cost of a manual thesaurus, we could a ttempt to\\ngenerate a thesaurus automatically by analyzing a collecti on of documents.\\nThere are two main approaches. One is simply to exploit word c ooccurrence.\\nWe say that words co-occurring in a document or paragraph are likely to be\\nin some sense similar or related in meaning, and simply count text statistics\\nto ﬁnd the most similar words. The other approach is to use a sh allow gram-\\nmatical analysis of the text and to exploit grammatical rela tions or grammat-\\nical dependencies. For example, we say that entities that ar e grown, cooked,\\neaten, and digested, are more likely to be food items. Simply using word\\ncooccurrence is more robust (it cannot be misled by parser er rors), but using\\ngrammatical relations is more accurate.\\nThe simplest way to compute a co-occurrence thesaurus is bas ed on term-\\nterm similarities. We begin with a term-document matrix A, where each cell\\nAt,dis a weighted count wt,dfor term tand document d, with weighting so\\nAhas length-normalized rows. If we then calculate C=AAT, then Cu,vis\\na similarity score between terms uand v, with a larger number being better.\\nFigure 9.8shows an example of a thesaurus derived in basically this man ner,\\nbut with an extra step of dimensionality reduction via Laten t Semantic In-\\ndexing, which we discuss in Chapter 18. While some of the thesaurus terms\\nare good or at least suggestive, others are marginal or bad. T he quality of the\\nassociations is typically a problem. Term ambiguity easily introduces irrel-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 228}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP9.3 References and further reading 193\\nevant statistically correlated terms. For example, a query forApple computer\\nmay expand to Apple red fruit computer . In general these thesauri suffer from\\nboth false positives and false negatives. Moreover, since t he terms in the au-\\ntomatic thesaurus are highly correlated in documents anywa y (and often the\\ncollection used to derive the thesaurus is the same as the one being indexed),\\nthis form of query expansion may not retrieve many additiona l documents.\\nQuery expansion is often effective in increasing recall. Ho wever, there is\\na high cost to manually producing a thesaurus and then updati ng it for sci-\\nentiﬁc and terminological developments within a ﬁeld. In ge neral a domain-\\nspeciﬁc thesaurus is required: general thesauri and dictio naries give far too\\nlittle coverage of the rich domain-particular vocabularie s of most scientiﬁc\\nﬁelds. However, query expansion may also signiﬁcantly decr ease precision,\\nparticularly when the query contains ambiguous terms. For e xample, if the\\nuser searches for interestrate , expanding the query to interestratefascinateeval-\\nuate is unlikely to be useful. Overall, query expansion is less su ccessful than\\nrelevance feedback, though it may be as good as pseudo releva nce feedback.\\nIt does, however, have the advantage of being much more under standable to\\nthe system user.\\n?Exercise 9.7\\nIfAis simply a Boolean cooccurrence matrix, then what do you get as the entries in\\nC?\\n9.3 References and further reading\\nWork in information retrieval quickly confronted the probl em of variant ex-\\npression which meant that the words in a query might not appea r in a doc-\\nument, despite it being relevant to the query. An early exper iment about\\n1960 cited by Swanson (1988 ) found that only 11 out of 23 documents prop-\\nerly indexed under the subject toxicity had any use of a word containing the\\nstem toxi. There is also the issue of translation, of users knowing wha t terms\\na document will use. Blair and Maron (1985 ) conclude that “it is impossibly\\ndifﬁcult for users to predict the exact words, word combinat ions, and phrases\\nthat are used by all (or most) relevant documents and only (or primarily) by\\nthose documents”.\\nThe main initial papers on relevance feedback using vector s pace models\\nall appear in Salton (1971b ), including the presentation of the Rocchio al-\\ngorithm ( Rocchio 1971 ) and the Ide dec-hi variant along with evaluation of\\nseveral variants ( Ide 1971 ). Another variant is to regard alldocuments in\\nthe collection apart from those judged relevant as nonrelev ant, rather than\\nonly ones that are explicitly judged nonrelevant. However, Schütze et al.\\n(1995 ) and Singhal et al. (1997 ) show that better results are obtained for rout-\\ning by using only documents close to the query of interest rat her than all', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 229}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP194 9 Relevance feedback and query expansion\\ndocuments. Other later work includes Salton and Buckley (1990 ),Riezler\\net al. (2007 ) (a statistical NLP approach to RF) and the recent survey pap er\\nRuthven and Lalmas (2003 ).\\nThe effectiveness of interactive relevance feedback syste ms is discussed in\\n(Salton 1989 ,Harman 1992 ,Buckley et al. 1994b ).Koenemann and Belkin\\n(1996 ) do user studies of the effectiveness of relevance feedback .\\nTraditionally Roget’s thesaurus has been the best known Eng lish language\\nthesaurus ( Roget 1946 ). In recent computational work, people almost always\\nuse WordNet ( Fellbaum 1998 ), not only because it is free, but also because of\\nits rich link structure. It is available at: http://wordnet.princeton.edu .\\nQiu and Frei (1993 ) and Schütze (1998 ) discuss automatic thesaurus gener-\\nation. Xu and Croft (1996 ) explore using both local and global query expan-\\nsion.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 230}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 195\\n10 XML retrieval\\nInformation retrieval systems are often contrasted with re lational databases.\\nTraditionally, IR systems have retrieved information from unstructured text\\n– by which we mean “raw” text without markup. Databases are de signed\\nfor querying relational data : sets of records that have values for predeﬁned\\nattributes such as employee number, title and salary. There are fundamental\\ndifferences between information retrieval and database sy stems in terms of\\nretrieval model, data structures and query language as show n in Table 10.1.1\\nSome highly structured text search problems are most efﬁcie ntly handled\\nby a relational database, for example, if the employee table contains an at-\\ntribute for short textual job descriptions and you want to ﬁn d all employees\\nwho are involved with invoicing. In this case, the SQL query:\\nselectlastnamefromemployeeswherejob_desclike’invoic %’;\\nmay be sufﬁcient to satisfy your information need with high p recision and\\nrecall.\\nHowever, many structured data sources containing text are b est modeled\\nas structured documents rather than relational data. We cal l the search over\\nsuch structured documents structured retrieval . Queries in structured retrieval STRUCTURED\\nRETRIEVAL can be either structured or unstructured, but we will assume in this chap-\\nter that the collection consists only of structured documen ts. Applications\\nof structured retrieval include digital libraries, patent databases, blogs, text\\nin which entities like persons and locations have been tagge d (in a process\\ncalled named entity tagging) and output from ofﬁce suites li ke OpenOfﬁce\\nthat save documents as marked up text. In all of these applica tions, we want\\nto be able to run queries that combine textual criteria with s tructural criteria.\\nExamples of such queries are givemeafull-lengtharticleonfastfouriertransforms\\n(digital libraries), givemepatentswhoseclaimsmentionRSApublickeyencrypti on\\n1. In most modern database systems, one can enable full-text search for text columns. This\\nusually means that an inverted index is created and Boolean o r vector space search enabled,\\neffectively combining core database with information retr ieval technologies.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 231}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP196 10 XML retrieval\\nRDB search unstructured retrieval structured retrieval\\nobjects records unstructured documents trees with text at leaves\\nmodel relational model vector space & others ?\\nmain data structure table inverted index ?\\nqueries SQL free text queries ?\\n◮Table 10.1 RDB (relational database) search, unstructured informati on retrieval\\nand structured information retrieval. There is no consensu s yet as to which methods\\nwork best for structured retrieval although many researche rs believe that XQuery\\n(page 215) will become the standard for structured queries.\\nand that cite US patent 4,405,829 (patents), or give me articles about sightseeing\\ntours of the Vatican and the Coliseum (entity-tagged text). These three queries\\nare structured queries that cannot be answered well by an unr anked retrieval\\nsystem. As we argued in Example 1.1(page 15) unranked retrieval models\\nlike the Boolean model suffer from low recall. For instance, an unranked\\nsystem would return a potentially large number of articles t hat mention the\\nVatican, the Coliseum and sightseeing tours without rankin g the ones that\\nare most relevant for the query ﬁrst. Most users are also noto riously bad at\\nprecisely stating structural constraints. For instance, u sers may not know\\nfor which structured elements the search system supports se arch. In our ex-\\nample, the user may be unsure whether to issue the query as sightseeing AND\\n(COUNTRY :Vatican OR LANDMARK :Coliseum) , assightseeing AND(STATE:Vatican OR\\nBUILDING :Coliseum) or in some other form. Users may also be completely un-\\nfamiliar with structured search and advanced search interf aces or unwilling\\nto use them. In this chapter, we look at how ranked retrieval m ethods can be\\nadapted to structured documents to address these problems.\\nWe will only look at one standard for encoding structured doc uments: Ex-\\ntensible Markup Language orXML , which is currently the most widely used XML\\nsuch standard. We will not cover the speciﬁcs that distingui sh XML from\\nother types of markup such as HTML and SGML. But most of what we say\\nin this chapter is applicable to markup languages in general .\\nIn the context of information retrieval, we are only interes ted in XML as\\na language for encoding text and documents. A perhaps more wi despread\\nuse of XML is to encode non-text data. For example, we may want to export\\ndata in XML format from an enterprise resource planning syst em and then\\nread them into an analytics program to produce graphs for a pr esentation.\\nThis type of application of XML is called data-centric because numerical and DATA -CENTRIC XML\\nnon-text attribute-value data dominate and text is usually a small fraction of\\nthe overall data. Most data-centric XML is stored in databas es – in contrast\\nto the inverted index-based methods for text-centric XML th at we present in\\nthis chapter.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 232}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP10.1 Basic XML concepts 197\\nWe call XML retrieval structured retrieval in this chapter. Some researchers\\nprefer the term semistructured retrieval to distinguish XML retrieval from database SEMISTRUCTURED\\nRETRIEVAL querying. We have adopted the terminology that is widesprea d in the XML\\nretrieval community. For instance, the standard way of refe rring to XML\\nqueries is structured queries , not semistructured queries . The term structured\\nretrieval is rarely used for database querying and it always refers to X ML\\nretrieval in this book.\\nThere is a second type of information retrieval problem that is intermediate\\nbetween unstructured retrieval and querying a relational d atabase: paramet-\\nric and zone search, which we discussed in Section 6.1(page 110). In the\\ndata model of parametric and zone search, there are parametr ic ﬁelds (re-\\nlational attributes like date orﬁle-size ) and zones – text attributes that each\\ntake a chunk of unstructured text as value, e.g., author and title in Figure 6.1\\n(page 111). The data model is ﬂat, that is, there is no nesting of attrib utes.\\nThe number of attributes is small. In contrast, XML document s have the\\nmore complex tree structure that we see in Figure 10.2 in which attributes\\nare nested. The number of attributes and nodes is greater tha n in parametric\\nand zone search.\\nAfter presenting the basic concepts of XML in Section 10.1, this chapter\\nﬁrst discusses the challenges we face in XML retrieval (Sect ion10.2). Next we\\ndescribe a vector space model for XML retrieval (Section 10.3). Section 10.4\\npresents INEX, a shared task evaluation that has been held fo r a number of\\nyears and currently is the most important venue for XML retri eval research.\\nWe discuss the differences between data-centric and text-c entric approaches\\nto XML in Section 10.5.\\n10.1 Basic XML concepts\\nAn XML document is an ordered, labeled tree. Each node of the t ree is an\\nXML element and is written with an opening and closing tag. An element can XML ELEMENT\\nhave one or more XML attributes . In the XML document in Figure 10.1, the XML ATTRIBUTE\\nscene element is enclosed by the two tags <scene...> and</scene> . It\\nhas an attribute number with value viiand two child elements, titleand verse .\\nFigure 10.2 shows Figure 10.1 as a tree. The leaf nodes of the tree consist of\\ntext, e.g., Shakespeare ,Macbeth , andMacbeth’s castle . The tree’s internal nodes\\nencode either the structure of the document ( title,act, and scene ) or metadata\\nfunctions ( author ).\\nThe standard for accessing and processing XML documents is t he XML\\nDocument Object Model or DOM . The DOM represents elements, attributes XML DOM\\nand text within elements as nodes in a tree. Figure 10.2 is a simpliﬁed DOM\\nrepresentation of the XML document in Figure 10.1.2With a DOM API, we\\n2. The representation is simpliﬁed in a number of respects. F or example, we do not show the', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 233}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP198 10 XML retrieval\\n<play>\\n<author>Shakespeare</author>\\n<title>Macbeth</title>\\n<actnumber=\"I\">\\n<scenenumber=\"vii\">\\n<title>Macbeth’scastle</title>\\n<verse>WillI withwineandwassail...</verse>\\n</scene>\\n</act>\\n</play>\\n◮Figure 10.1 An XML document.\\nroot element\\nplay\\nelement\\nauthorelement\\nactelement\\ntitle\\ntext\\nShakespearetext\\nMacbeth\\nattribute\\nnumber=\"I\"element\\nscene\\nattribute\\nnumber=\"vii\"element\\nverseelement\\ntitle\\ntext\\nWill I with ...text\\nMacbeth’s castle\\n◮Figure 10.2 The XML document in Figure 10.1 as a simpliﬁed DOM object.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 234}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP10.1 Basic XML concepts 199\\n//article\\n[.//yr= 2001or .//yr= 2002]\\n//section\\n[about(.,summerholidays)]holidays summersectionarticle\\n◮Figure 10.3 An XML query in NEXI format and its partial representation as a tree.\\ncan process an XML document by starting at the root element an d then de-\\nscending down the tree from parents to children.\\nXPath is a standard for enumerating paths in an XML document collec tion. XPATH\\nWe will also refer to paths as XML contexts or simply contexts in this chapter. XML CONTEXT\\nOnly a small subset of XPath is needed for our purposes. The XP ath expres-\\nsionnode selects all nodes of that name. Successive elements of a path are\\nseparated by slashes, so act/scene selects all scene elements whose par-\\nent is an actelement. Double slashes indicate that an arbitrary number o f\\nelements can intervene on a path: play//scene selects all scene elements\\noccurring in a play element. In Figure 10.2 this set consists of a single scene el-\\nement, which is accessible via the path play,act,scene from the top. An initial\\nslash starts the path at the root element. /play/title selects the play’s ti-\\ntle in Figure 10.1,/play//title selects a set with two members (the play’s\\ntitle and the scene’s title), and /scene/title selects no elements. For no-\\ntational convenience, we allow the ﬁnal element of a path to b e a vocabulary\\nterm and separate it from the element path by the symbol #, eve n though this\\ndoes not conform to the XPath standard. For example, title#\"Macbeth\"\\nselects all titles containing the term Macbeth .\\nWe also need the concept of schema in this chapter. A schema puts con- SCHEMA\\nstraints on the structure of allowable XML documents for a pa rticular ap-\\nplication. A schema for Shakespeare’s plays may stipulate t hat scenes can\\nonly occur as children of acts and that only acts and scenes ha ve the num-\\nberattribute. Two standards for schemas for XML documents are XML DTD XML DTD\\n(document type deﬁnition) and XML Schema . Users can only write structured XML S CHEMA\\nqueries for an XML retrieval system if they have some minimal knowledge\\nabout the schema of the collection.\\nrootnode and text is not embedded in textnodes. See http://www.w3.org/DOM/ .', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 235}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP200 10 XML retrieval\\nM’scastletitle\\nWillI ...versescene\\nJuliusCaesartitlebook\\nGallicwartitle\\nJuliusCaesarauthorbook\\nd1q1 q2\\n◮Figure 10.4 Tree representation of XML documents and queries.\\nA common format for XML queries is NEXI (Narrowed Extended XPath NEXI\\nI). We give an example in Figure 10.3. We display the query on four lines for\\ntypographical convenience, but it is intended to be read as o ne unit without\\nline breaks. In particular, //section is embedded under//article .\\nThe query in Figure 10.3 speciﬁes a search for sections about the sum-\\nmer holidays that are part of articles from 2001 or 2002. As in XPath dou-\\nble slashes indicate that an arbitrary number of elements ca n intervene on\\na path. The dot in a clause in square brackets refers to the ele ment the\\nclause modiﬁes. The clause [.//yr=2001or .//yr=2002] mod-\\niﬁes//article . Thus, the dot refers to //article in this case. Similarly,\\nthe dot in[about(.,summerholidays)] refers to the section that the\\nclause modiﬁes.\\nThe twoyrconditions are relational attribute constraints. Only art icles\\nwhoseyrattribute is 2001 or 2002 (or that contain an element whose yr\\nattribute is 2001 or 2002) are to be considered. The about clause is a ranking\\nconstraint: Sections that occur in the right type of article are to be ranked\\naccording to how relevant they are to the topic summerholidays .\\nWe usually handle relational attribute constraints by preﬁ ltering or post-\\nﬁltering: We simply exclude all elements from the result set that do not meet\\nthe relational attribute constraints. In this chapter, we w ill not address how\\nto do this efﬁciently and instead focus on the core informati on retrieval prob-\\nlem in XML retrieval, namely how to rank documents according to the rele-\\nvance criteria expressed in the about conditions of the NEXI query.\\nIf we discard relational attributes, we can represent docum ents as trees\\nwith only one type of node: element nodes. In other words, we r emove\\nall attribute nodes from the XML document, such as the number attribute in\\nFigure 10.1. Figure 10.4 shows a subtree of the document in Figure 10.1 as an\\nelement-node tree (labeled d1).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 236}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP10.2 Challenges in XML retrieval 201\\nWe can represent queries as trees in the same way. This is a que ry-by-\\nexample approach to query language design because users pos e queries by\\ncreating objects that satisfy the same formal description a s documents. In\\nFigure 10.4,q1is a search for books whose titles score highly for the keywor ds\\nJuliusCaesar .q2is a search for books whose author elements score highly for\\nJulius Caesar and whose title elements score highly for Gallic war .3\\n10.2 Challenges in XML retrieval\\nIn this section, we discuss a number of challenges that make s tructured re-\\ntrieval more difﬁcult than unstructured retrieval. Recall from page 195the\\nbasic setting we assume in structured retrieval: the collec tion consists of\\nstructured documents and queries are either structured (as in Figure 10.3)\\nor unstructured (e.g., summerholidays ).\\nThe ﬁrst challenge in structured retrieval is that users wan t us to return\\nparts of documents (i.e., XML elements), not entire documen ts as IR systems\\nusually do in unstructured retrieval. If we query Shakespea re’s plays for\\nMacbeth’s castle , should we return the scene, the act or the entire play in Fig-\\nure10.2? In this case, the user is probably looking for the scene. On t he other\\nhand, an otherwise unspeciﬁed search for Macbeth should return the play of\\nthis name, not a subunit.\\nOne criterion for selecting the most appropriate part of a do cument is the\\nstructured document retrieval principle : STRUCTURED\\nDOCUMENT RETRIEVAL\\nPRINCIPLE Structured document retrieval principle. A system should always re-\\ntrieve the most speciﬁc part of a document answering the quer y.\\nThis principle motivates a retrieval strategy that returns the smallest unit\\nthat contains the information sought, but does not go below t his level. How-\\never, it can be hard to implement this principle algorithmic ally. Consider the\\nquerytitle#\"Macbeth\" applied to Figure 10.2. The title of the tragedy,\\nMacbeth , and the title of Act I, Scene vii, Macbeth’s castle , are both good hits\\nbecause they contain the matching term Macbeth . But in this case, the title of\\nthe tragedy, the higher node, is preferred. Deciding which l evel of the tree is\\nright for answering a query is difﬁcult.\\nParallel to the issue of which parts of a document to return to the user is\\nthe issue of which parts of a document to index. In Section 2.1.2 (page 20), we\\ndiscussed the need for a document unit or indexing unit in indexing and re- INDEXING UNIT\\ntrieval. In unstructured retrieval, it is usually clear wha t the right document\\n3. To represent the semantics of NEXI queries fully we would a lso need to designate one node\\nin the tree as a “target node”, for example, the section in the tree in Figure 10.3. Without the\\ndesignation of a target node, the tree in Figure 10.3 is not a search for sections embedded in\\narticles (as speciﬁed by NEXI), but a search for articles tha t contain sections.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 237}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP202 10 XML retrieval\\n◮Figure 10.5 Partitioning an XML document into non-overlapping indexin g units.\\nunit is: ﬁles on your desktop, email messages, web pages on th e web etc. In\\nstructured retrieval, there are a number of different appro aches to deﬁning\\nthe indexing unit.\\nOne approach is to group nodes into non-overlapping pseudod ocuments\\nas shown in Figure 10.5. In the example, books, chapters and sections have\\nbeen designated to be indexing units, but without overlap. F or example, the\\nleftmost dashed indexing unit contains only those parts of t he tree domi-\\nnated by book that are not already part of other indexing units. The disad-\\nvantage of this approach is that pseudodocuments may not mak e sense to\\nthe user because they are not coherent units. For instance, t he leftmost in-\\ndexing unit in Figure 10.5 merges three disparate elements, the class ,author\\nand titleelements.\\nWe can also use one of the largest elements as the indexing uni t, for exam-\\nple, the book element in a collection of books or the play element for Shake-\\nspeare’s works. We can then postprocess search results to ﬁn d for each book\\nor play the subelement that is the best hit. For example, the q ueryMacbeth’s\\ncastle may return the play Macbeth , which we can then postprocess to identify\\nact I, scene vii as the best-matching subelement. Unfortuna tely, this two-\\nstage retrieval process fails to return the best subelement for many queries\\nbecause the relevance of a whole book is often not a good predi ctor of the\\nrelevance of small subelements within it.\\nInstead of retrieving large units and identifying subeleme nts (top down),\\nwe can also search all leaves, select the most relevant ones a nd then extend\\nthem to larger units in postprocessing (bottom up). For the q ueryMacbeth’s\\ncastle in Figure 10.1, we would retrieve the title Macbeth’s castle in the ﬁrst\\npass and then decide in a postprocessing step whether to retu rn the title, the\\nscene, the act or the play. This approach has a similar proble m as the last one:\\nThe relevance of a leaf element is often not a good predictor o f the relevance', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 238}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP10.2 Challenges in XML retrieval 203\\nof elements it is contained in.\\nThe least restrictive approach is to index all elements. Thi s is also prob-\\nlematic. Many XML elements are not meaningful search result s, e.g., typo-\\ngraphical elements like <b>definitely</b> or an ISBN number which\\ncannot be interpreted without context. Also, indexing all e lements means\\nthat search results will be highly redundant. For the query Macbeth’s castle\\nand the document in Figure 10.1, we would return all of the play,act,scene\\nand title elements on the path between the root node and Macbeth’s castle .\\nThe leaf node would then occur four times in the result set, on ce directly and\\nthree times as part of other elements. We call elements that a re contained\\nwithin each other nested . Returning redundant nested elements in a list of NESTED ELEMENTS\\nreturned hits is not very user-friendly.\\nBecause of the redundancy caused by nested elements it is com mon to re-\\nstrict the set of elements that are eligible to be returned. R estriction strategies\\ninclude:\\n•discard all small elements\\n•discard all element types that users do not look at (this requ ires a working\\nXML retrieval system that logs this information)\\n•discard all element types that assessors generally do not ju dge to be rele-\\nvant (if relevance assessments are available)\\n•only keep element types that a system designer or librarian h as deemed\\nto be useful search results\\nIn most of these approaches, result sets will still contain n ested elements.\\nThus, we may want to remove some elements in a postprocessing step to re-\\nduce redundancy. Alternatively, we can collapse several ne sted elements in\\nthe results list and use highlighting of query terms to draw the user’s atten-\\ntion to the relevant passages. If query terms are highlighte d, then scanning a\\nmedium-sized element (e.g., a section) takes little more ti me than scanning a\\nsmall subelement (e.g., a paragraph). Thus, if the section a nd the paragraph\\nboth occur in the results list, it is sufﬁcient to show the sec tion. An additional\\nadvantage of this approach is that the paragraph is presente d together with\\nits context (i.e., the embedding section). This context may be helpful in in-\\nterpreting the paragraph (e.g., the source of the informati on reported) even\\nif the paragraph on its own satisﬁes the query.\\nIf the user knows the schema of the collection and is able to sp ecify the\\ndesired type of element, then the problem of redundancy is al leviated as few\\nnested elements have the same type. But as we discussed in the introduction,\\nusers often don’t know what the name of an element in the colle ction is (Is the\\nVatican acountry or a city?) or they may not know how to compose structured\\nqueries at all.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 239}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP204 10 XML retrieval\\nGatesbook\\nGatesauthorbook\\nGatescreatorbook\\nGateslastname\\nBillﬁrstnameauthorbook\\nq3 q4 d2 d3\\n◮Figure 10.6 Schema heterogeneity: intervening nodes and mismatched na mes.\\nA challenge in XML retrieval related to nesting is that we may need to\\ndistinguish different contexts of a term when we compute ter m statistics for\\nranking, in particular inverse document frequency (idf) st atistics as deﬁned\\nin Section 6.2.1 (page 117). For example, the term Gates under the node author\\nis unrelated to an occurrence under a content node like section if used to refer\\nto the plural of gate. It makes little sense to compute a single document\\nfrequency for Gates in this example.\\nOne solution is to compute idf for XML-context/term pairs, e .g., to com-\\npute different idf weights for author#\"Gates\" andsection#\"Gates\" .\\nUnfortunately, this scheme will run into sparse data proble ms – that is, many\\nXML-context pairs occur too rarely to reliably estimate df ( see Section 13.2,\\npage 260, for a discussion of sparseness). A compromise is only to con -\\nsider the parent node xof the term and not the rest of the path from the\\nroot to xto distinguish contexts. There are still conﬂations of cont exts that\\nare harmful in this scheme. For instance, we do not distingui sh names of\\nauthors and names of corporations if both have the parent nod ename . But\\nmost important distinctions, like the example contrast author#\"Gates\" vs.\\nsection#\"Gates\" , will be respected.\\nIn many cases, several different XML schemas occur in a colle ction since\\nthe XML documents in an IR application often come from more th an one\\nsource. This phenomenon is called schema heterogeneity orschema diversity SCHEMA\\nHETEROGENEITY and presents yet another challenge. As illustrated in Figur e10.6 comparable\\nelements may have different names: creator ind2vs.author ind3. In other\\ncases, the structural organization of the schemas may be dif ferent: Author', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 240}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP10.2 Challenges in XML retrieval 205\\nnames are direct descendants of the node author inq3, but there are the in-\\ntervening nodes ﬁrstname and lastname ind3. If we employ strict matching\\nof trees, then q3will retrieve neither d2nord3although both documents are\\nrelevant. Some form of approximate matching of element name s in combina-\\ntion with semi-automatic matching of different document st ructures can help\\nhere. Human editing of correspondences of elements in diffe rent schemas\\nwill usually do better than automatic methods.\\nSchema heterogeneity is one reason for query-document mism atches like\\nq3/d2and q3/d3. Another reason is that users often are not familiar with the\\nelement names and the structure of the schemas of collection s they search\\nas mentioned. This poses a challenge for interface design in XML retrieval.\\nIdeally, the user interface should expose the tree structur e of the collection\\nand allow users to specify the elements they are querying. If we take this\\napproach, then designing the query interface in structured retrieval is more\\ncomplex than a search box for keyword queries in unstructure d retrieval.\\nWe can also support the user by interpreting all parent-chil d relationships\\nin queries as descendant relationships with any number of in tervening nodes\\nallowed. We call such queries extended queries . The tree in Figure 10.3 and q4EXTENDED QUERY\\nin Figure 10.6 are examples of extended queries. We show edges that are\\ninterpreted as descendant relationships as dashed arrows. Inq4, a dashed\\narrow connects book andGates . As a pseudo-XPath notation for q4, we adopt\\nbook//#\"Gates\" : a book that somewhere in its structure contains the word\\nGates where the path from the book node to Gates can be arbitrarily long.\\nThe pseudo-XPath notation for the extended query that in add ition speciﬁes\\nthatGates occurs in a section of the book isbook//section//#\"Gates\" .\\nIt is convenient for users to be able to issue such extended qu eries without\\nhaving to specify the exact structural conﬁguration in whic h a query term\\nshould occur – either because they do not care about the exact conﬁguration\\nor because they do not know enough about the schema of the coll ection to be\\nable to specify it.\\nIn Figure 10.7, the user is looking for a chapter entitled FFT (q5). Sup-\\npose there is no such chapter in the collection, but that ther e are references to\\nbooks on FFT ( d4). A reference to a book on FFT is not exactly what the user\\nis looking for, but it is better than returning nothing. Exte nded queries do not\\nhelp here. The extended query q6also returns nothing. This is a case where\\nwe may want to interpret the structural constraints speciﬁe d in the query as\\nhints as opposed to as strict conditions. As we will discuss i n Section 10.4,\\nusers prefer a relaxed interpretation of structural constr aints: Elements that\\ndo not meet structural constraints perfectly should be rank ed lower, but they\\nshould not be omitted from search results.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 241}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP206 10 XML retrieval\\nFFTtitlechapter\\nFFTtitlechapter\\nFFTtitle\\nencryptiontitlereferences chapterbook\\nq5 q6 d4\\n◮Figure 10.7 A structural mismatch between two queries and a document.\\n10.3 A vector space model for XML retrieval\\nIn this section, we present a simple vector space model for XM L retrieval.\\nIt is not intended to be a complete description of a state-of- the-art system.\\nInstead, we want to give the reader a ﬂavor of how documents ca n be repre-\\nsented and retrieved in XML retrieval.\\nTo take account of structure in retrieval in Figure 10.4, we want a book\\nentitled Julius Caesar to be a match for q1and no match (or a lower weighted\\nmatch) for q2. In unstructured retrieval, there would be a single dimensi on\\nof the vector space for Caesar . In XML retrieval, we must separate the title\\nwordCaesar from the author name Caesar . One way of doing this is to have\\neach dimension of the vector space encode a word together wit h its position\\nwithin the XML tree.\\nFigure 10.8 illustrates this representation. We ﬁrst take each text nod e\\n(which in our setup is always a leaf) and break it into multipl e nodes, one for\\neach word. So the leaf node Bill Gates is split into two leaves BillandGates .\\nNext we deﬁne the dimensions of the vector space to be lexicalized subtrees\\nof documents – subtrees that contain at least one vocabulary term. A sub-\\nset of these possible lexicalized subtrees is shown in the ﬁg ure, but there are\\nothers – e.g., the subtree corresponding to the whole docume nt with the leaf\\nnodeGates removed. We can now represent queries and documents as vec-\\ntors in this space of lexicalized subtrees and compute match es between them.\\nThis means that we can use the vector space formalism from Cha pter 6for\\nXML retrieval. The main difference is that the dimensions of vector space', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 242}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP10.3 A vector space model for XML retrieval 207\\n◮Figure 10.8 A mapping of an XML document (left) to a set of lexicalized sub trees\\n(right).\\nin unstructured retrieval are vocabulary terms whereas the y are lexicalized\\nsubtrees in XML retrieval.\\nThere is a tradeoff between the dimensionality of the space a nd accuracy\\nof query results. If we trivially restrict dimensions to voc abulary terms, then\\nwe have a standard vector space retrieval system that will re trieve many\\ndocuments that do not match the structure of the query (e.g., Gates in the\\ntitle as opposed to the author element). If we create a separa te dimension\\nfor each lexicalized subtree occurring in the collection, t he dimensionality of\\nthe space becomes too large. A compromise is to index all path s that end\\nin a single vocabulary term, in other words, all XML-context /term pairs.\\nWe call such an XML-context/term pair a structural term and denote it by STRUCTURAL TERM\\n⟨c,t⟩: a pair of XML-context cand vocabulary term t. The document in\\nFigure 10.8 has nine structural terms. Seven are shown (e.g., \"Bill\" and\\nAuthor#\"Bill\" ) and two are not shown: /Book/Author#\"Bill\" and\\n/Book/Author#\"Gates\" . The tree with the leaves BillandGates is a lexical-\\nized subtree that is not a structural term. We use the previou sly introduced\\npseudo-XPath notation for structural terms.\\nAs we discussed in the last section users are bad at rememberi ng details\\nabout the schema and at constructing queries that comply wit h the schema.\\nWe will therefore interpret all queries as extended queries – that is, there can\\nbe an arbitrary number of intervening nodes in the document f or any parent-\\nchild node pair in the query. For example, we interpret q5in Figure 10.7 as\\nq6.\\nBut we still prefer documents that match the query structure closely by', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 243}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP208 10 XML retrieval\\ninserting fewer additional nodes. We ensure that retrieval results respect this\\npreference by computing a weight for each match. A simple mea sure of the\\nsimilarity of a path cqin a query and a path cdin a document is the following\\ncontext resemblance function C R: CONTEXT\\nRESEMBLANCE\\nCR(cq,cd) ={1+|cq|\\n1+|cd|ifcqmatches cd\\n0 if cqdoes not match cd(10.1)\\nwhere|cq|and|cd|are the number of nodes in the query path and document\\npath, respectively, and cqmatches cdiff we can transform cqinto cdby in-\\nserting additional nodes. Two examples from Figure 10.6 are C R(cq4,cd2) =\\n3/4=0.75 and C R(cq4,cd3) = 3/5=0.6 where cq4,cd2and cd3are the rele-\\nvant paths from top to leaf node in q4,d2and d3, respectively. The value of\\nCR(cq,cd)is 1.0 if qand dare identical.\\nThe ﬁnal score for a document is computed as a variant of the co sine mea-\\nsure (Equation ( 6.10), page 121), which we call S IMNOMERGE for reasons\\nthat will become clear shortly. S IMNOMERGE is deﬁned as follows:\\nSIMNOMERGE(q,d) =∑\\nck∈B∑\\ncl∈BCR(ck,cl)∑\\nt∈Vweight (q,t,ck)weight (d,t,cl)√\\n∑c∈B,t∈Vweight2(d,t,c)(10.2)\\nwhere Vis the vocabulary of non-structural terms; Bis the set of all XML con-\\ntexts; and weight (q,t,c)and weight (d,t,c)are the weights of term tin XML\\ncontext cin query qand document d, respectively. We compute the weights\\nusing one of the weightings from Chapter 6, such as idf t·wft,d. The inverse\\ndocument frequency idf tdepends on which elements we use to compute\\ndftas discussed in Section 10.2. The similarity measure S IMNOMERGE(q,d)\\nis not a true cosine measure since its value can be larger than 1.0 (Exer-\\ncise 10.11 ). We divide by√\\n∑c∈B,t∈Vweight2(d,t,c)to normalize for doc-\\nument length (Section 6.3.1 , page 121). We have omitted query length nor-\\nmalization to simplify the formula. It has no effect on ranki ng since, for\\na given query, the normalizer√\\n∑c∈B,t∈Vweight2(q,t,c)is the same for all\\ndocuments.\\nThe algorithm for computing S IMNOMERGE for all documents in the col-\\nlection is shown in Figure 10.9. The array normalizer in Figure 10.9 contains√\\n∑c∈B,t∈Vweight2(d,t,c)from Equation ( 10.2) for each document.\\nWe give an example of how S IMNOMERGE computes query-document\\nsimilarities in Figure 10.10 .⟨c1,t⟩is one of the structural terms in the query.\\nWe successively retrieve all postings lists for structural terms⟨c′,t⟩with the\\nsame vocabulary term t. Three example postings lists are shown. For the\\nﬁrst one, we have C R(c1,c1) = 1.0 since the two contexts are identical. The', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 244}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP10.3 A vector space model for XML retrieval 209\\nSCORE DOCUMENTS WITHSIMNOMERGE(q,B,V,N,normalizer )\\n1forn←1toN\\n2doscore[n]←0\\n3for each⟨cq,t⟩∈q\\n4dowq←WEIGHT (q,t,cq)\\n5 for each c∈B\\n6 do if CR(cq,c)>0\\n7 then postings←GETPOSTINGS (⟨c,t⟩)\\n8 for each posting∈postings\\n9 dox←CR(cq,c)∗wq∗weight (posting )\\n10 score[docID (posting )] += x\\n11 forn←1toN\\n12 doscore[n]←score[n]/normalizer [n]\\n13 return score\\n◮Figure 10.9 The algorithm for scoring documents with S IMNOMERGE .\\nquery\\n⟨c1,t⟩\\nCR(c1,c1)=1.0\\nCR(c1,c2)=0\\nCR(c1,c3)=0.63inverted index\\n⟨c1,t⟩−→⟨d1, 0.5⟩⟨d4, 0.1⟩⟨d9, 0.2⟩. . .\\n⟨c2,t⟩−→⟨d2, 0.25⟩⟨d3, 0.1⟩⟨d12, 0.9⟩. . .\\n⟨c3,t⟩−→⟨d3, 0.7⟩⟨d6, 0.8⟩⟨d9, 0.6⟩. . .\\n◮Figure 10.10 Scoring of a query with one structural term in S IMNOMERGE .\\nnext context has no context resemblance with c1: CR(c1,c2) =0 and the cor-\\nresponding postings list is ignored. The context match of c1with c3is 0.63>0\\nand it will be processed. In this example, the highest rankin g document is d9\\nwith a similarity of 1.0 ×0.2+0.63×0.6=0.578. To simplify the ﬁgure, the\\nquery weight of⟨c1,t⟩is assumed to be 1.0.\\nThe query-document similarity function in Figure 10.9 is called S IMNOMERGE\\nbecause different XML contexts are kept separate for the pur pose of weight-\\ning. An alternative similarity function is S IMMERGE which relaxes the match-\\ning conditions of query and document further in the followin g three ways.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 245}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP210 10 XML retrieval\\n•We collect the statistics used for computing weight (q,t,c)and weight (d,t,c)\\nfrom allcontexts that have a non-zero resemblance to c(as opposed to just\\nfrom cas in S IMNOMERGE ). For instance, for computing the document\\nfrequency of the structural term atl#\"recognition\" , we also count\\noccurrences of recognition in XML contextsfm/atl ,article//atl etc.\\n•We modify Equation ( 10.2) by merging all structural terms in the docu-\\nment that have a non-zero context resemblance to a given quer y struc-\\ntural term. For example, the contexts /play/act/scene/title and\\n/play/title in the document will be merged when matching against\\nthe query term/play/title#\"Macbeth\" .\\n•The context resemblance function is further relaxed: Conte xts have a non-\\nzero resemblance in many cases where the deﬁnition of C Rin Equation ( 10.1)\\nreturns 0.\\nSee the references in Section 10.6 for details.\\nThese three changes alleviate the problem of sparse term sta tistics dis-\\ncussed in Section 10.2 and increase the robustness of the matching function\\nagainst poorly posed structural queries. The evaluation of SIMNOMERGE\\nand S IMMERGE in the next section shows that the relaxed matching condi-\\ntions of S IMMERGE increase the effectiveness of XML retrieval.\\n?Exercise 10.1\\nConsider computing df for a structural term as the number of t imes that the structural\\nterm occurs under a particular parent node. Assume the follo wing: the structural\\nterm⟨c,t⟩=author#\"Herbert\" occurs once as the child of the node squib ; there are\\n10squib nodes in the collection; ⟨c,t⟩occurs 1000 times as the child of article ; there are\\n1,000,000 article nodes in the collection. The idf weight of ⟨c,t⟩then is log210/1≈3.3\\nwhen occurring as the child of squib and log21,000,000/1000≈10.0 when occurring\\nas the child of article . (i) Explain why this is not an appropriate weighting for ⟨c,t⟩.\\nWhy should⟨c,t⟩not receive a weight that is three times higher in articles th an in\\nsquibs? (ii) Suggest a better way of computing idf.\\nExercise 10.2\\nWrite down all the structural terms occurring in the XML docu ment in Figure 10.8.\\nExercise 10.3\\nHow many structural terms does the document in Figure 10.1 yield?\\n10.4 Evaluation of XML retrieval\\nThe premier venue for research on XML retrieval is the INEX (INitiative for INEX\\ntheEvaluation of XML retrieval ) program, a collaborative effort that has pro-\\nduced reference collections, sets of queries, and relevanc e judgments. A\\nyearly INEX meeting is held to present and discuss research r esults. The', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 246}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP10.4 Evaluation of XML retrieval 211\\n12,107 number of documents\\n494 MB size\\n1995–2002 time of publication of articles\\n1,532 average number of XML nodes per document\\n6.9 average depth of a node\\n30 number of CAS topics\\n30 number of CO topics\\n◮Table 10.2 INEX 2002 collection statistics.\\nIEEE Transac-\\ntion on Pat-\\ntern Analysisjournal title\\nActivity\\nrecognitionarticle title\\nThis work fo-\\ncuses on . . .paragraph\\nIntroductiontitlefront matter sectionbodyarticle\\n◮Figure 10.11 Simpliﬁed schema of the documents in the INEX collection.\\nINEX 2002 collection consisted of about 12,000 articles fro m IEEE journals.\\nWe give collection statistics in Table 10.2 and show part of the schema of\\nthe collection in Figure 10.11 . The IEEE journal collection was expanded in\\n2005. Since 2006 INEX uses the much larger English Wikipedia as a test col-\\nlection. The relevance of documents is judged by human asses sors using the\\nmethodology introduced in Section 8.1(page 152), appropriately modiﬁed\\nfor structured documents as we will discuss shortly.\\nTwo types of information needs or topics in INEX are content- only or CO\\ntopics and content-and-structure (CAS) topics. CO topics are regular key- CO TOPICS\\nword queries as in unstructured information retrieval. CAS topics have struc- CAS TOPICS\\ntural constraints in addition to keywords. We already encou ntered an exam-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 247}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP212 10 XML retrieval\\nple of a CAS topic in Figure 10.3. The keywords in this case are summer and\\nholidays and the structural constraints specify that the keywords oc cur in a\\nsection that in turn is part of an article and that this articl e has an embedded\\nyear attribute with value 2001 or 2002.\\nSince CAS queries have both structural and content criteria , relevance as-\\nsessments are more complicated than in unstructured retrie val. INEX 2002\\ndeﬁned component coverage and topical relevance as orthogo nal dimen-\\nsions of relevance. The component coverage dimension evaluates whether the COMPONENT\\nCOVERAGE element retrieved is “structurally” correct, i.e., neithe r too low nor too high\\nin the tree. We distinguish four cases:\\n•Exact coverage (E). The information sought is the main topic of the com-\\nponent and the component is a meaningful unit of information .\\n•Too small (S). The information sought is the main topic of the component,\\nbut the component is not a meaningful (self-contained) unit of informa-\\ntion.\\n•Too large (L). The information sought is present in the compo nent, but is\\nnot the main topic.\\n•No coverage (N). The information sought is not a topic of the c omponent.\\nThe topical relevance dimension also has four levels: highly relevant (3), TOPICAL RELEVANCE\\nfairly relevant (2), marginally relevant (1) and nonreleva nt (0). Components\\nare judged on both dimensions and the judgments are then comb ined into\\na digit-letter code. 2S is a fairly relevant component that i s too small and\\n3E is a highly relevant component that has exact coverage. In theory, there\\nare 16 combinations of coverage and relevance, but many cann ot occur. For\\nexample, a nonrelevant component cannot have exact coverag e, so the com-\\nbination 3N is not possible.\\nThe relevance-coverage combinations are quantized as foll ows:\\nQ(rel,cov) =\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f31.00 if (rel,cov) =3E\\n0.75 if (rel,cov)∈{2E, 3L}\\n0.50 if (rel,cov)∈{1E, 2L, 2S}\\n0.25 if (rel,cov)∈{1S, 1L}\\n0.00 if (rel,cov) =0N\\nThis evaluation scheme takes account of the fact that binary relevance judg-\\nments, which are standard in unstructured information retr ieval (Section 8.5.1 ,\\npage 166), are not appropriate for XML retrieval. A 2S component prov ides\\nincomplete information and may be difﬁcult to interpret wit hout more con-\\ntext, but it does answer the query partially. The quantizati on function Q\\ndoes not impose a binary choice relevant/nonrelevant and in stead allows us\\nto grade the component as partially relevant.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 248}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP10.4 Evaluation of XML retrieval 213\\nalgorithm average precision\\nSIMNOMERGE 0.242\\nSIMMERGE 0.271\\n◮Table 10.3 INEX 2002 results of the vector space model in Section 10.3 for content-\\nand-structure (CAS) queries and the quantization function Q.\\nThe number of relevant components in a retrieved set Aof components\\ncan then be computed as:\\n#(relevant items retrieved ) =∑\\nc∈AQ(rel(c),cov(c))\\nAs an approximation, the standard deﬁnitions of precision, recall and F from\\nChapter 8can be applied to this modiﬁed deﬁnition of relevant items re -\\ntrieved, with some subtleties because we sum graded as oppos ed to binary\\nrelevance assessments. See the references on focused retri eval in Section 10.6\\nfor further discussion.\\nOne ﬂaw of measuring relevance this way is that overlap is not accounted\\nfor. We discussed the concept of marginal relevance in the co ntext of un-\\nstructured retrieval in Section 8.5.1 (page 166). This problem is worse in\\nXML retrieval because of the problem of multiple nested elem ents occur-\\nring in a search result as we discussed on page 203. Much of the recent focus\\nat INEX has been on developing algorithms and evaluation mea sures that\\nreturn non-redundant results lists and evaluate them prope rly. See the refer-\\nences in Section 10.6.\\nTable 10.3 shows two INEX 2002 runs of the vector space system we de-\\nscribed in Section 10.3. The better run is the S IMMERGE run, which incor-\\nporates few structural constraints and mostly relies on key word matching.\\nSIMMERGE ’s median average precision (where the median is with respec t to\\naverage precision numbers over topics) is only 0.147. Effec tiveness in XML\\nretrieval is often lower than in unstructured retrieval sin ce XML retrieval is\\nharder. Instead of just ﬁnding a document, we have to ﬁnd the s ubpart of a\\ndocument that is most relevant to the query. Also, XML retrie val effective-\\nness – when evaluated as described here – can be lower than uns tructured\\nretrieval effectiveness on a standard evaluation because g raded judgments\\nlower measured performance. Consider a system that returns a document\\nwith graded relevance 0.6 and binary relevance 1 at the top of the retrieved\\nlist. Then, interpolated precision at 0.00 recall (cf. page 158) is 1.0 on a binary\\nevaluation, but can be as low as 0.6 on a graded evaluation.\\nTable 10.3 gives us a sense of the typical performance of XML retrieval,\\nbut it does not compare structured with unstructured retrie val. Table 10.4\\ndirectly shows the effect of using structure in retrieval. T he results are for a', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 249}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP214 10 XML retrieval\\ncontent only full structure improvement\\nprecision at 5 0.2000 0.3265 63.3%\\nprecision at 10 0.1820 0.2531 39.1%\\nprecision at 20 0.1700 0.1796 5.6%\\nprecision at 30 0.1527 0.1531 0.3%\\n◮Table 10.4 A comparison of content-only and full-structure search in I NEX\\n2003/2004.\\nlanguage-model-based system (cf. Chapter 12) that is evaluated on a subset\\nof CAS topics from INEX 2003 and 2004. The evaluation metric i s precision\\natkas deﬁned in Chapter 8(page 161). The discretization function used for\\nthe evaluation maps highly relevant elements (roughly corr esponding to the\\n3E elements deﬁned for Q) to 1 and all other elements to 0. The content-\\nonly system treats queries and documents as unstructured ba gs of words.\\nThe full-structure model ranks elements that satisfy struc tural constraints\\nhigher than elements that do not. For instance, for the query in Figure 10.3\\nan element that contains the phrase summer holidays in a section will be rated\\nhigher than one that contains it in an abstract .\\nThe table shows that structure helps increase precision at t he top of the\\nresults list. There is a large increase of precision at k=5 and at k=10. There\\nis almost no improvement at k=30. These results demonstrate the beneﬁts\\nof structured retrieval. Structured retrieval imposes add itional constraints on\\nwhat to return and documents that pass the structural ﬁlter a re more likely\\nto be relevant. Recall may suffer because some relevant docu ments will be\\nﬁltered out, but for precision-oriented tasks structured r etrieval is superior.\\n10.5 Text-centric vs. data-centric XML retrieval\\nIn the type of structured retrieval we cover in this chapter, XML structure\\nserves as a framework within which we match the text of the que ry with the\\ntext of the XML documents. This exempliﬁes a system that is op timized for\\ntext-centric XML . While both text and structure are important, we give higher TEXT -CENTRIC XML\\npriority to text. We do this by adapting unstructured retrie val methods to\\nhandling additional structural constraints. The premise o f our approach is\\nthat XML document retrieval is characterized by (i) long tex t ﬁelds (e.g., sec-\\ntions of a document), (ii) inexact matching, and (iii) relev ance-ranked results.\\nRelational databases do not deal well with this use case.\\nIn contrast, data-centric XML mainly encodes numerical and non-text attribute- DATA -CENTRIC XML\\nvalue data. When querying data-centric XML, we want to impos e exact\\nmatch conditions in most cases. This puts the emphasis on the structural\\naspects of XML documents and queries. An example is:', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 250}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP10.5 Text-centric vs. data-centric XML retrieval 215\\nFind employees whose salary is the same this month as it was 12 months\\nago.\\nThis query requires no ranking. It is purely structural and a n exact matching\\nof the salaries in the two time periods is probably sufﬁcient to meet the user’s\\ninformation need.\\nText-centric approaches are appropriate for data that are e ssentially text\\ndocuments, marked up as XML to capture document structure. T his is be-\\ncoming a de facto standard for publishing text databases sin ce most text\\ndocuments have some form of interesting structure – paragra phs, sections,\\nfootnotes etc. Examples include assembly manuals, issues o f journals, Shake-\\nspeare’s collected works and newswire articles.\\nData-centric approaches are commonly used for data collect ions with com-\\nplex structures that mainly contain non-text data. A text-c entric retrieval\\nengine will have a hard time with proteomic data in bioinform atics or with\\nthe representation of a city map that (together with street n ames and other\\ntextual descriptions) forms a navigational database.\\nTwo other types of queries that are difﬁcult to handle in a tex t-centric struc-\\ntured retrieval model are joins and ordering constraints. T he query for em-\\nployees with unchanged salary requires a join. The followin g query imposes\\nan ordering constraint:\\nRetrieve the chapter of the book Introduction to algorithms that follows\\nthe chapter Binomial heaps .\\nThis query relies on the ordering of elements in XML – in this c ase the order-\\ning of chapter elements underneath the book node. There are p owerful query\\nlanguages for XML that can handle numerical attributes, joi ns and ordering\\nconstraints. The best known of these is XQuery, a language pr oposed for\\nstandardization by the W3C. It is designed to be broadly appl icable in all ar-\\neas where XML is used. Due to its complexity, it is challengin g to implement\\nan XQuery-based ranked retrieval system with the performan ce characteris-\\ntics that users have come to expect in information retrieval . This is currently\\none of the most active areas of research in XML retrieval.\\nRelational databases are better equipped to handle many str uctural con-\\nstraints, particularly joins (but ordering is also difﬁcul t in a database frame-\\nwork – the tuples of a relation in the relational calculus are not ordered). For\\nthis reason, most data-centric XML retrieval systems are ex tensions of rela-\\ntional databases (see the references in Section 10.6). If text ﬁelds are short,\\nexact matching meets user needs and retrieval results in for m of unordered\\nsets are acceptable, then using a relational database for XM L retrieval is ap-\\npropriate.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 251}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP216 10 XML retrieval\\n10.6 References and further reading\\nThere are many good introductions to XML, including ( Harold and Means\\n2004 ). Table 10.1 is inspired by a similar table in ( van Rijsbergen 1979 ). Sec-\\ntion 10.4 follows the overview of INEX 2002 by Gövert and Kazai (2003 ),\\npublished in the proceedings of the meeting ( Fuhr et al. 2003a ). The pro-\\nceedings of the four following INEX meetings were published asFuhr et al.\\n(2003b ),Fuhr et al. (2005 ),Fuhr et al. (2006 ), and Fuhr et al. (2007 ). An up-\\ntodate overview article is Fuhr and Lalmas (2007 ). The results in Table 10.4\\nare from ( Kamps et al. 2006 ).Chu-Carroll et al. (2006 ) also present evidence\\nthat XML queries increase precision compared with unstruct ured queries.\\nInstead of coverage and relevance, INEX now evaluates on the related but\\ndifferent dimensions of exhaustivity and speciﬁcity ( Lalmas and Tombros\\n2007 ).Trotman et al. (2006 ) relate the tasks investigated at INEX to real world\\nuses of structured retrieval such as structured book search on internet book-\\nstore sites.\\nThe structured document retrieval principle is due to Chiaramella et al.\\n(1996 ). Figure 10.5 is from ( Fuhr and Großjohann 2004 ).Rahm and Bernstein\\n(2001 ) give a survey of automatic schema matching that is applicab le to XML.\\nThe vector-space based XML retrieval method in Section 10.3 is essentially\\nIBM Haifa’s JuruXML system as presented by Mass et al. (2003 ) and Carmel\\net al. (2003 ).Schlieder and Meuss (2002 ) and Grabs and Schek (2002 ) describe\\nsimilar approaches. Carmel et al. (2003 ) represent queries as XML fragments . XML FRAGMENT\\nThe trees that represent XML queries in this chapter are all X ML fragments,\\nbut XML fragments also permit the operators +,−and phrase on content\\nnodes.\\nWe chose to present the vector space model for XML retrieval b ecause it\\nis simple and a natural extension of the unstructured vector space model\\nin Chapter 6. But many other unstructured retrieval methods have been\\napplied to XML retrieval with at least as much success as the v ector space\\nmodel. These methods include language models (cf. Chapter 12, e.g., Kamps\\net al. (2004 ),List et al. (2005 ),Ogilvie and Callan (2005 )), systems that use\\na relational database as a backend ( Mihajlovi´ c et al. 2005 ,Theobald et al.\\n2005 ;2008 ), probabilistic weighting ( Lu et al. 2007 ), and fusion ( Larson 2005 ).\\nThere is currently no consensus as to what the best approach t o XML retrieval\\nis.\\nMost early work on XML retrieval accomplished relevance ran king by fo-\\ncusing on individual terms, including their structural con texts, in query and\\ndocument. As in unstructured information retrieval, there is a trend in more\\nrecent work to model relevance ranking as combining evidenc e from dis-\\nparate measurements about the query, the document and their match. The\\ncombination function can be tuned manually ( Arvola et al. 2005 ,Sigurbjörns-\\nson et al. 2004 ) or trained using machine learning methods ( Vittaut and Gal-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 252}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP10.7 Exercises 217\\nlinari (2006 ), cf. Section 15.4.1 , page 341).\\nAn active area of XML retrieval research is focused retrieval (Trotman et al. FOCUSED RETRIEVAL\\n2007 ), which aims to avoid returning nested elements that share o ne or more\\ncommon subelements (cf. discussion in Section 10.2, page 203). There is ev-\\nidence that users dislike redundancy caused by nested eleme nts ( Betsi et al.\\n2006 ). Focused retrieval requires evaluation measures that pen alize redun-\\ndant results lists ( Kazai and Lalmas 2006 ,Lalmas et al. 2007 ).Trotman and\\nGeva (2006 ) argue that XML retrieval is a form of passage retrieval . In passage PASSAGE RETRIEVAL\\nretrieval ( Salton et al. 1993 ,Hearst and Plaunt 1993 ,Zobel et al. 1995 ,Hearst\\n1997 ,Kaszkiel and Zobel 1997 ), the retrieval system returns short passages\\ninstead of documents in response to a user query. While eleme nt bound-\\naries in XML documents are cues for identifying good segment boundaries\\nbetween passages, the most relevant passage often does not c oincide with an\\nXML element.\\nIn the last several years, the query format at INEX has been th e NEXI stan-\\ndard proposed by Trotman and Sigurbjörnsson (2004 ). Figure 10.3 is from\\ntheir paper. O’Keefe and Trotman (2004 ) give evidence that users cannot reli-\\nably distinguish the child and descendant axes. This justiﬁ es only permitting\\ndescendant axes in NEXI (and XML fragments). These structur al constraints\\nwere only treated as “hints” in recent INEXes. Assessors can judge an ele-\\nment highly relevant even though it violates one of the struc tural constraints\\nspeciﬁed in a NEXI query.\\nAn alternative to structured query languages like NEXI is a m ore sophisti-\\ncated user interface for query formulation ( Tannier and Geva 2005 ,van Zwol\\net al. 2006 ,Woodley and Geva 2006 ).\\nA broad overview of XML retrieval that covers database as wel l as IR ap-\\nproaches is given by Amer-Yahia and Lalmas (2006 ) and an extensive refer-\\nence list on the topic can be found in ( Amer-Yahia et al. 2005 ). Chapter 6\\nofGrossman and Frieder (2004 ) is a good introduction to structured text re-\\ntrieval from a database perspective. The proposed standard for XQuery is\\navailable at http://www.w3.org/TR/xquery/ including an extension for full-text\\nqueries ( Amer-Yahia et al. 2006 ):http://www.w3.org/TR/xquery-full-text/ . Work\\nthat has looked at combining the relational database and the unstructured\\ninformation retrieval approaches includes ( Fuhr and Rölleke 1997 ), (Navarro\\nand Baeza-Yates 1997 ), (Cohen 1998 ), and ( Chaudhuri et al. 2006 ).\\n10.7 Exercises\\n?Exercise 10.4\\nFind a reasonably sized XML document collection (or a collec tion using a markup lan-\\nguage different from XML like HTML) on the web and download it . Jon Bosak’s XML\\nedition of Shakespeare and of various religious works at http://www.ibiblio.org/bosak/ or\\nthe ﬁrst 10,000 documents of the Wikipedia are good choices. Create three CAS topics', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 253}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP218 10 XML retrieval\\nof the type shown in Figure 10.3 that you would expect to do better than analogous\\nCO topics. Explain why an XML retrieval system would be able t o exploit the XML\\nstructure of the documents to achieve better retrieval resu lts on the topics than an\\nunstructured retrieval system.\\nExercise 10.5\\nFor the collection and the topics in Exercise 10.4, (i) are there pairs of elements e1and\\ne2, with e2a subelement of e1such that both answer one of the topics? Find one case\\neach where (ii) e1(iii)e2is the better answer to the query.\\nExercise 10.6\\nImplement the (i) S IMMERGE (ii) S IMNOMERGE algorithm in Section 10.3 and run it\\nfor the collection and the topics in Exercise 10.4. (iii) Evaluate the results by assigning\\nbinary relevance judgments to the ﬁrst ﬁve documents of the t hree retrieved lists for\\neach algorithm. Which algorithm performs better?\\nExercise 10.7\\nAre all of the elements in Exercise 10.4 appropriate to be returned as hits to a user or\\nare there elements (as in the example <b>definitely</b> on page 203) that you\\nwould exclude?\\nExercise 10.8\\nWe discussed the tradeoff between accuracy of results and di mensionality of the vec-\\ntor space on page 207. Give an example of an information need that we can answer\\ncorrectly if we index all lexicalized subtrees, but cannot a nswer if we only index struc-\\ntural terms.\\nExercise 10.9\\nIf we index all structural terms, what is the size of the index as a function of text size?\\nExercise 10.10\\nIf we index all lexicalized subtrees, what is the size of the i ndex as a function of text\\nsize?\\nExercise 10.11\\nGive an example of a query-document pair for which S IMNOMERGE(q,d)is larger\\nthan 1.0.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 254}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 219\\n11Probabilistic information\\nretrieval\\nDuring the discussion of relevance feedback in Section 9.1.2 , we observed\\nthat if we have some known relevant and nonrelevant document s, then we\\ncan straightforwardly start to estimate the probability of a term tappearing\\nin a relevant document P(t|R=1), and that this could be the basis of a\\nclassiﬁer that decides whether documents are relevant or no t. In this chapter,\\nwe more systematically introduce this probabilistic appro ach to IR, which\\nprovides a different formal basis for a retrieval model and r esults in different\\ntechniques for setting term weights.\\nUsers start with information needs , which they translate into query repre-\\nsentations . Similarly, there are documents , which are converted into document\\nrepresentations (the latter differing at least by how text is tokenized, but p er-\\nhaps containing fundamentally less information, as when a n on-positional\\nindex is used). Based on these two representations, a system tries to de-\\ntermine how well documents satisfy information needs. In th e Boolean or\\nvector space models of IR, matching is done in a formally deﬁn ed but seman-\\ntically imprecise calculus of index terms. Given only a quer y, an IR system\\nhas an uncertain understanding of the information need. Giv en the query\\nand document representations, a system has an uncertain gue ss of whether\\na document has content relevant to the information need. Pro bability theory\\nprovides a principled foundation for such reasoning under u ncertainty. This\\nchapter provides one answer as to how to exploit this foundat ion to estimate\\nhow likely it is that a document is relevant to an information need.\\nThere is more than one possible retrieval model which has a pr obabilistic\\nbasis. Here, we will introduce probability theory and the Pr obability Rank-\\ning Principle (Sections 11.1–11.2), and then concentrate on the Binary Inde-\\npendence Model (Section 11.3), which is the original and still most inﬂuential\\nprobabilistic retrieval model. Finally, we will introduce related but extended\\nmethods which use term counts, including the empirically su ccessful Okapi\\nBM25 weighting scheme, and Bayesian Network models for IR (S ection 11.4).\\nIn Chapter 12, we then present the alternative probabilistic language mo del-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 255}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP220 11 Probabilistic information retrieval\\ning approach to IR, which has been developed with considerab le success in\\nrecent years.\\n11.1 Review of basic probability theory\\nWe hope that the reader has seen a little basic probability th eory previously.\\nWe will give a very quick review; some references for further reading appear\\nat the end of the chapter. A variable Arepresents an event (a subset of the\\nspace of possible outcomes). Equivalently, we can represen t the subset via a\\nrandom variable , which is a function from outcomes to real numbers; the sub- RANDOM VARIABLE\\nset is the domain over which the random variable Ahas a particular value.\\nOften we will not know with certainty whether an event is true in the world.\\nWe can ask the probability of the event 0 ≤P(A)≤1. For two events Aand\\nB, the joint event of both events occurring is described by the joint probabil-\\nityP(A,B). The conditional probability P(A|B)expresses the probability of\\nevent Agiven that event Boccurred. The fundamental relationship between\\njoint and conditional probabilities is given by the chain rule : CHAIN RULE\\nP(A,B) =P(A∩B) =P(A|B)P(B) =P(B|A)P(A) (11.1)\\nWithout making any assumptions, the probability of a joint e vent equals the\\nprobability of one of the events multiplied by the probabili ty of the other\\nevent conditioned on knowing the ﬁrst event happened.\\nWriting P(A)for the complement of an event, we similarly have:\\nP(A,B) =P(B|A)P(A) (11.2)\\nProbability theory also has a partition rule , which says that if an event Bcan PARTITION RULE\\nbe divided into an exhaustive set of disjoint subcases, then the probability of\\nBis the sum of the probabilities of the subcases. A special cas e of this rule\\ngives that:\\nP(B) =P(A,B) +P(A,B) (11.3)\\nFrom these we can derive Bayes’ Rule for inverting conditional probabili- BAYES ’ RULE\\nties:\\nP(A|B) =P(B|A)P(A)\\nP(B)=[\\nP(B|A)\\n∑X∈{A,A}P(B|X)P(X)]\\nP(A) (11.4)\\nThis equation can also be thought of as a way of updating proba bilities. We\\nstart off with an initial estimate of how likely the event Ais when we do\\nnot have any other information; this is the prior probability P (A). Bayes’ rule PRIOR PROBABILITY\\nlets us derive a posterior probability P (A|B)after having seen the evidence B, POSTERIOR\\nPROBABILITY', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 256}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP11.2 The Probability Ranking Principle 221\\nbased on the likelihood ofBoccurring in the two cases that Adoes or does not\\nhold.1\\nFinally, it is often useful to talk about the odds of an event, which provide ODDS\\na kind of multiplier for how probabilities change:\\nOdds: O(A) =P(A)\\nP(A)=P(A)\\n1−P(A)(11.5)\\n11.2 The Probability Ranking Principle\\n11.2.1 The 1/0 loss case\\nWe assume a ranked retrieval setup as in Section 6.3, where there is a collec-\\ntion of documents, the user issues a query, and an ordered lis t of documents\\nis returned. We also assume a binary notion of relevance as in Chapter 8. For\\na query qand a document din the collection, let Rd,qbe an indicator random\\nvariable that says whether dis relevant with respect to a given query q. That\\nis, it takes on a value of 1 when the document is relevant and 0 o therwise. In\\ncontext we will often write just RforRd,q.\\nUsing a probabilistic model, the obvious order in which to pr esent doc-\\numents to the user is to rank documents by their estimated pro bability of\\nrelevance with respect to the information need: P(R=1|d,q). This is the ba-\\nsis of the Probability Ranking Principle (PRP) ( van Rijsbergen 1979 , 113–114): PROBABILITY\\nRANKING PRINCIPLE\\n“If a reference retrieval system’s response to each request is a ranking\\nof the documents in the collection in order of decreasing pro bability\\nof relevance to the user who submitted the request, where the prob-\\nabilities are estimated as accurately as possible on the bas is of what-\\never data have been made available to the system for this purp ose, the\\noverall effectiveness of the system to its user will be the be st that is\\nobtainable on the basis of those data.”\\nIn the simplest case of the PRP , there are no retrieval costs o r other utility\\nconcerns that would differentially weight actions or error s. You lose a point\\nfor either returning a nonrelevant document or failing to re turn a relevant\\ndocument (such a binary situation where you are evaluated on your accuracy\\nis called 1/0 loss ). The goal is to return the best possible results as the top k 1/0 LOSS\\ndocuments, for any value of kthe user chooses to examine. The PRP then\\nsays to simply rank all documents in decreasing order of P(R=1|d,q). If\\na set of retrieval results is to be returned, rather than an or dering, the Bayes BAYES OPTIMAL\\nDECISION RULE\\n1. The term likelihood is just a synonym of probability . It is the probability of an event or data\\naccording to a model. The term is usually used when people are thinking of holding the data\\nﬁxed, while varying the model.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 257}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP222 11 Probabilistic information retrieval\\nOptimal Decision Rule , the decision which minimizes the risk of loss, is to\\nsimply return documents that are more likely relevant than n onrelevant:\\ndis relevant iff P(R=1|d,q)>P(R=0|d,q) (11.6)\\nTheorem 11.1. The PRP is optimal, in the sense that it minimizes the expecte d loss\\n(also known as the Bayes risk ) under 1/0 loss. BAYES RISK\\nThe proof can be found in Ripley (1996 ). However, it requires that all proba-\\nbilities are known correctly. This is never the case in pract ice. Nevertheless,\\nthe PRP still provides a very useful foundation for developi ng models of IR.\\n11.2.2 The PRP with retrieval costs\\nSuppose, instead, that we assume a model of retrieval costs. LetC1be the\\ncost of not retrieving a relevant document and C0the cost of retrieval of a\\nnonrelevant document. Then the Probability Ranking Princi ple says that if\\nfor a speciﬁc document dand for all documents d′not yet retrieved\\nC0·P(R=0|d)−C1·P(R=1|d)≤C0·P(R=0|d′)−C1·P(R=1|d′) (11.7)\\nthen dis the next document to be retrieved. Such a model gives a form al\\nframework where we can model differential costs of false pos itives and false\\nnegatives and even system performance issues at the modelin g stage, rather\\nthan simply at the evaluation stage, as we did in Section 8.6(page 168). How-\\never, we will not further consider loss/utility models in th is chapter.\\n11.3 The Binary Independence Model\\nThe Binary Independence Model (BIM) we present in this section is the model BINARY\\nINDEPENDENCE\\nMODELthat has traditionally been used with the PRP. It introduces some simple as-\\nsumptions, which make estimating the probability function P(R|d,q)prac-\\ntical. Here, “binary” is equivalent to Boolean: documents a nd queries are\\nboth represented as binary term incidence vectors. That is, a document d\\nis represented by the vector ⃗x= (x1, . . . , xM)where xt=1 if term tis\\npresent in document dand xt=0 if tis not present in d. With this rep-\\nresentation, many possible documents have the same vector r epresentation.\\nSimilarly, we represent qby the incidence vector ⃗q(the distinction between\\nqand⃗qis less central since commonly qis in the form of a set of words).\\n“Independence” means that terms are modeled as occurring in documents\\nindependently. The model recognizes no association betwee n terms. This\\nassumption is far from correct, but it nevertheless often gi ves satisfactory\\nresults in practice; it is the “naive” assumption of Naive Ba yes models, dis-\\ncussed further in Section 13.4 (page 265). Indeed, the Binary Independence', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 258}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP11.3 The Binary Independence Model 223\\nModel is exactly the same as the multivariate Bernoulli Naiv e Bayes model\\npresented in Section 13.3 (page 263). In a sense this assumption is equivalent\\nto an assumption of the vector space model, where each term is a dimension\\nthat is orthogonal to all other terms.\\nWe will ﬁrst present a model which assumes that the user has a s ingle\\nstep information need. As discussed in Chapter 9, seeing a range of results\\nmight let the user reﬁne their information need. Fortunatel y, as mentioned\\nthere, it is straightforward to extend the Binary Independe nce Model so as to\\nprovide a framework for relevance feedback, and we present t his model in\\nSection 11.3.4 .\\nTo make a probabilistic retrieval strategy precise, we need to estimate how\\nterms in documents contribute to relevance, speciﬁcally, w e wish to know\\nhow term frequency, document frequency, document length, a nd other statis-\\ntics that we can compute inﬂuence judgments about document r elevance,\\nand how they can be reasonably combined to estimate the proba bility of doc-\\nument relevance. We then order documents by decreasing esti mated proba-\\nbility of relevance.\\nWe assume here that the relevance of each document is indepen dent of the\\nrelevance of other documents. As we noted in Section 8.5.1 (page 166), this\\nis incorrect: the assumption is especially harmful in pract ice if it allows a\\nsystem to return duplicate or near duplicate documents. Und er the BIM, we\\nmodel the probability P(R|d,q)that a document is relevant via the probabil-\\nity in terms of term incidence vectors P(R|⃗x,⃗q). Then, using Bayes rule, we\\nhave:\\nP(R=1|⃗x,⃗q) =P(⃗x|R=1,⃗q)P(R=1|⃗q)\\nP(⃗x|⃗q)(11.8)\\nP(R=0|⃗x,⃗q) =P(⃗x|R=0,⃗q)P(R=0|⃗q)\\nP(⃗x|⃗q)\\nHere, P(⃗x|R=1,⃗q)and P(⃗x|R=0,⃗q)are the probability that if a relevant or\\nnonrelevant, respectively, document is retrieved, then th at document’s rep-\\nresentation is ⃗x. You should think of this quantity as deﬁned with respect to\\na space of possible documents in a domain. How do we compute al l these\\nprobabilities? We never know the exact probabilities, and s o we have to use\\nestimates: Statistics about the actual document collectio n are used to estimate\\nthese probabilities. P(R=1|⃗q)and P(R=0|⃗q)indicate the prior probability\\nof retrieving a relevant or nonrelevant document respectiv ely for a query ⃗q.\\nAgain, if we knew the percentage of relevant documents in the collection,\\nthen we could use this number to estimate P(R=1|⃗q)and P(R=0|⃗q). Since\\na document is either relevant or nonrelevant to a query, we mu st have that:\\nP(R=1|⃗x,⃗q) +P(R=0|⃗x,⃗q) =1 (11.9)', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 259}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP224 11 Probabilistic information retrieval\\n11.3.1 Deriving a ranking function for query terms\\nGiven a query q, we wish to order returned documents by descending P(R=\\n1|d,q). Under the BIM, this is modeled as ordering by P(R=1|⃗x,⃗q). Rather\\nthan estimating this probability directly, because we are i nterested only in the\\nranking of documents, we work with some other quantities whi ch are easier\\nto compute and which give the same ordering of documents. In p articular,\\nwe can rank documents by their odds of relevance (as the odds o f relevance\\nis monotonic with the probability of relevance). This makes things easier,\\nbecause we can ignore the common denominator in ( 11.8), giving:\\nO(R|⃗x,⃗q) =P(R=1|⃗x,⃗q)\\nP(R=0|⃗x,⃗q)=P(R=1|⃗q)P(⃗x|R=1,⃗q)\\nP(⃗x|⃗q)\\nP(R=0|⃗q)P(⃗x|R=0,⃗q)\\nP(⃗x|⃗q)=P(R=1|⃗q)\\nP(R=0|⃗q)·P(⃗x|R=1,⃗q)\\nP(⃗x|R=0,⃗q)(11.10)\\nThe left term in the rightmost expression of Equation ( 11.10 ) is a constant for\\na given query. Since we are only ranking documents, there is t hus no need\\nfor us to estimate it. The right-hand term does, however, req uire estimation,\\nand this initially appears to be difﬁcult: How can we accurat ely estimate the\\nprobability of an entire term incidence vector occurring? I t is at this point that\\nwe make the Naive Bayes conditional independence assumption that the presence NAIVE BAYES\\nASSUMPTION or absence of a word in a document is independent of the presen ce or absence\\nof any other word (given the query):\\nP(⃗x|R=1,⃗q)\\nP(⃗x|R=0,⃗q)=M\\n∏\\nt=1P(xt|R=1,⃗q)\\nP(xt|R=0,⃗q)(11.11)\\nSo:\\nO(R|⃗x,⃗q) =O(R|⃗q)·M\\n∏\\nt=1P(xt|R=1,⃗q)\\nP(xt|R=0,⃗q)(11.12)\\nSince each xtis either 0 or 1, we can separate the terms to give:\\nO(R|⃗x,⃗q) =O(R|⃗q)·∏\\nt:xt=1P(xt=1|R=1,⃗q)\\nP(xt=1|R=0,⃗q)·∏\\nt:xt=0P(xt=0|R=1,⃗q)\\nP(xt=0|R=0,⃗q)(11.13)\\nHenceforth, let pt=P(xt=1|R=1,⃗q)be the probability of a term appear-\\ning in a document relevant to the query, and ut=P(xt=1|R=0,⃗q)be the\\nprobability of a term appearing in a nonrelevant document. T hese quantities\\ncan be visualized in the following contingency table where t he columns add\\nto 1:\\n(11.14)\\ndocument relevant ( R=1) nonrelevant ( R=0)\\nTerm present xt=1 pt ut\\nTerm absent xt=0 1−pt 1−ut', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 260}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP11.3 The Binary Independence Model 225\\nLet us make an additional simplifying assumption that terms not occur-\\nring in the query are equally likely to occur in relevant and n onrelevant doc-\\numents: that is, if qt=0 then pt=ut. (This assumption can be changed,\\nas when doing relevance feedback in Section 11.3.4 .) Then we need only\\nconsider terms in the products that appear in the query, and s o,\\nO(R|⃗q,⃗x) =O(R|⃗q)·∏\\nt:xt=qt=1pt\\nut·∏\\nt:xt=0,qt=11−pt\\n1−ut(11.15)\\nThe left product is over query terms found in the document and the right\\nproduct is over query terms not found in the document.\\nWe can manipulate this expression by including the query ter ms found in\\nthe document into the right product, but simultaneously div iding through\\nby them in the left product, so the value is unchanged. Then we have:\\nO(R|⃗q,⃗x) =O(R|⃗q)·∏\\nt:xt=qt=1pt(1−ut)\\nut(1−pt)·∏\\nt:qt=11−pt\\n1−ut(11.16)\\nThe left product is still over query terms found in the docume nt, but the right\\nproduct is now over all query terms. That means that this righ t product is a\\nconstant for a particular query, just like the odds O(R|⃗q). So the only quantity\\nthat needs to be estimated to rank documents for relevance to a query is the\\nleft product. We can equally rank documents by the logarithm of this term,\\nsince log is a monotonic function. The resulting quantity us ed for ranking is\\ncalled the Retrieval Status Value (RSV) in this model: RETRIEVAL STATUS\\nVALUE\\nRSV d=log ∏\\nt:xt=qt=1pt(1−ut)\\nut(1−pt)=∑\\nt:xt=qt=1logpt(1−ut)\\nut(1−pt)(11.17)\\nSo everything comes down to computing the RSV . Deﬁne ct:\\nct=logpt(1−ut)\\nut(1−pt)=logpt\\n(1−pt)+log1−ut\\nut(11.18)\\nThe ctterms are log odds ratios for the terms in the query. We have th e\\nodds of the term appearing if the document is relevant ( pt/(1−pt)) and the\\nodds of the term appearing if the document is nonrelevant ( ut/(1−ut)). The\\nodds ratio is the ratio of two such odds, and then we ﬁnally take the log of that ODDS RATIO\\nquantity. The value will be 0 if a term has equal odds of appear ing in relevant\\nand nonrelevant documents, and positive if it is more likely to appear in\\nrelevant documents. The ctquantities function as term weights in the model,\\nand the document score for a query is RSV d=∑xt=qt=1ct. Operationally, we\\nsum them in accumulators for query terms appearing in docume nts, just as\\nfor the vector space model calculations discussed in Sectio n7.1(page 135).\\nWe now turn to how we estimate these ctquantities for a particular collection\\nand query.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 261}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP226 11 Probabilistic information retrieval\\n11.3.2 Probability estimates in theory\\nFor each term t, what would these ctnumbers look like for the whole collec-\\ntion? ( 11.19 ) gives a contingency table of counts of documents in the coll ec-\\ntion, where df tis the number of documents that contain term t:\\n(11.19)\\ndocuments relevant nonrelevant Total\\nTerm present xt=1 s dft−s dft\\nTerm absent xt=0 S−s(N−dft)−(S−s)N−dft\\nTotal S N−S N\\nUsing this, pt=s/Sand ut= (dft−s)/(N−S)and\\nct=K(N, dft,S,s) =logs/(S−s)\\n(dft−s)/((N−dft)−(S−s))(11.20)\\nTo avoid the possibility of zeroes (such as if every or no rele vant document\\nhas a particular term) it is fairly standard to add1\\n2to each of the quantities\\nin the center 4 terms of ( 11.19 ), and then to adjust the marginal counts (the\\ntotals) accordingly (so, the bottom right cell totals N+2). Then we have:\\nˆct=K(N, dft,S,s) =log(s+1\\n2)/(S−s+1\\n2)\\n(dft−s+1\\n2)/(N−dft−S+s+1\\n2)(11.21)\\nAdding1\\n2in this way is a simple form of smoothing. For trials with cat-\\negorical outcomes (such as noting the presence or absence of a term), one\\nway to estimate the probability of an event from data is simpl y to count the\\nnumber of times an event occurred divided by the total number of trials.\\nThis is referred to as the relative frequency of the event. Estimating the prob- RELATIVE FREQUENCY\\nability as the relative frequency is the maximum likelihood estimate (orMLE ), MAXIMUM LIKELIHOOD\\nESTIMATE\\nMLEbecause this value makes the observed data maximally likely . However, if\\nwe simply use the MLE, then the probability given to events we happened to\\nsee is usually too high, whereas other events may be complete ly unseen and\\ngiving them as a probability estimate their relative freque ncy of 0 is both an\\nunderestimate, and normally breaks our models, since anyth ing multiplied\\nby 0 is 0. Simultaneously decreasing the estimated probabil ity of seen events\\nand increasing the probability of unseen events is referred to as smoothing . SMOOTHING\\nOne simple way of smoothing is to add a number αto each of the observed\\ncounts. These pseudocounts correspond to the use of a uniform distribution PSEUDOCOUNTS\\nover the vocabulary as a Bayesian prior , following Equation ( 11.4). We ini- BAYESIAN PRIOR\\ntially assume a uniform distribution over events, where the size of αdenotes\\nthe strength of our belief in uniformity, and we then update t he probability\\nbased on observed events. Since our belief in uniformity is w eak, we use', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 262}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP11.3 The Binary Independence Model 227\\nα=1\\n2. This is a form of maximum a posteriori (MAP ) estimation, where we MAXIMUM A\\nPOSTERIORI\\nMAPchoose the most likely point value for probabilities based o n the prior and\\nthe observed evidence, following Equation ( 11.4). We will further discuss\\nmethods of smoothing estimated counts to give probability m odels in Sec-\\ntion 12.2.2 (page 243); the simple method of adding1\\n2to each observed count\\nwill do for now.\\n11.3.3 Probability estimates in practice\\nUnder the assumption that relevant documents are a very smal l percentage\\nof the collection, it is plausible to approximate statistic s for nonrelevant doc-\\numents by statistics from the whole collection. Under this a ssumption, ut\\n(the probability of term occurrence in nonrelevant documen ts for a query) is\\ndft/Nand\\nlog[(1−ut)/ut] =log[(N−dft)/df t]≈logN/df t (11.22)\\nIn other words, we can provide a theoretical justiﬁcation fo r the most fre-\\nquently used form of idf weighting, which we saw in Section 6.2.1 .\\nThe approximation technique in Equation ( 11.22 ) cannot easily be extended\\nto relevant documents. The quantity ptcan be estimated in various ways:\\n1.We can use the frequency of term occurrence in known relevant docu-\\nments (if we know some). This is the basis of probabilistic ap proaches to\\nrelevance feedback weighting in a feedback loop, discussed in the next\\nsubsection.\\n2.Croft and Harper (1979 ) proposed using a constant in their combination\\nmatch model. For instance, we might assume that ptis constant over all\\nterms xtin the query and that pt=0.5. This means that each term has\\neven odds of appearing in a relevant document, and so the ptand(1−pt)\\nfactors cancel out in the expression for RSV . Such an estimate is weak, but\\ndoesn’t disagree violently with our hopes for the search ter ms appearing\\nin many but not all relevant documents. Combining this metho d with our\\nearlier approximation for ut, the document ranking is determined simply\\nby which query terms occur in documents scaled by their idf we ighting.\\nFor short documents (titles or abstracts) in situations in w hich iterative\\nsearching is undesirable, using this weighting term alone c an be quite\\nsatisfactory, although in many other circumstances we woul d like to do\\nbetter.\\n3.Greiff (1998 ) argues that the constant estimate of ptin the Croft and Harper\\n(1979 ) model is theoretically problematic and not observed empir ically: as\\nmight be expected, ptis shown to rise with df t. Based on his data analysis,\\na plausible proposal would be to use the estimate pt=1\\n3+2\\n3dft/N.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 263}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP228 11 Probabilistic information retrieval\\nIterative methods of estimation, which combine some of the a bove ideas,\\nare discussed in the next subsection.\\n11.3.4 Probabilistic approaches to relevance feedback\\nWe can use (pseudo-)relevance feedback, perhaps in an itera tive process of\\nestimation, to get a more accurate estimate of pt. The probabilistic approach\\nto relevance feedback works as follows:\\n1.Guess initial estimates of ptand ut. This can be done using the probability\\nestimates of the previous section. For instance, we can assu me that ptis\\nconstant over all xtin the query, in particular, perhaps taking pt=1\\n2.\\n2.Use the current estimates of ptand utto determine a best guess at the set\\nof relevant documents R={d:Rd,q=1}. Use this model to retrieve a set\\nof candidate relevant documents, which we present to the use r.\\n3.We interact with the user to reﬁne the model of R. We do this by learn-\\ning from the user relevance judgments for some subset of docu ments V.\\nBased on relevance judgments, Vis partitioned into two subsets: VR=\\n{d∈V,Rd,q=1}⊂ Rand VNR ={d∈V,Rd,q=0}, which is disjoint\\nfrom R.\\n4.We reestimate ptand uton the basis of known relevant and nonrelevant\\ndocuments. If the sets VRand VNR are large enough, we may be able\\nto estimate these quantities directly from these documents as maximum\\nlikelihood estimates:\\npt=|VR t|/|VR| (11.23)\\n(where VR tis the set of documents in VR containing xt). In practice,\\nwe usually need to smooth these estimates. We can do this by ad ding\\n1\\n2to both the count |VR t|and to the number of relevant documents not\\ncontaining the term, giving:\\npt=|VR t|+1\\n2\\n|VR|+1(11.24)\\nHowever, the set of documents judged by the user ( V) is usually very\\nsmall, and so the resulting statistical estimate is quite un reliable (noisy),\\neven if the estimate is smoothed. So it is often better to comb ine the new\\ninformation with the original guess in a process of Bayesian updating. In\\nthis case we have:\\np(k+1)\\nt=|VR t|+κp(k)\\nt\\n|VR|+κ(11.25)', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 264}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP11.3 The Binary Independence Model 229\\nHere p(k)\\ntis the kthestimate for ptin an iterative updating process and\\nis used as a Bayesian prior in the next iteration with a weight ing of κ.\\nRelating this equation back to Equation ( 11.4) requires a bit more proba-\\nbility theory than we have presented here (we need to use a bet a distribu-\\ntion prior, conjugate to the Bernoulli random variable Xt). But the form\\nof the resulting equation is quite straightforward: rather than uniformly\\ndistributing pseudocounts, we now distribute a total of κpseudocounts\\naccording to the previous estimate, which acts as the prior d istribution.\\nIn the absence of other evidence (and assuming that the user i s perhaps\\nindicating roughly 5 relevant or nonrelevant documents) th en a value\\nof around κ=5 is perhaps appropriate. That is, the prior is strongly\\nweighted so that the estimate does not change too much from th e evi-\\ndence provided by a very small number of documents.\\n5.Repeat the above process from step 2, generating a successio n of approxi-\\nmations to Rand hence pt, until the user is satisﬁed.\\nIt is also straightforward to derive a pseudo-relevance fee dback version of\\nthis algorithm, where we simply pretend that VR=V. More brieﬂy:\\n1.Assume initial estimates for ptand utas above.\\n2.Determine a guess for the size of the relevant document set. I f unsure, a\\nconservative (too small) guess is likely to be best. This mot ivates use of a\\nﬁxed size set Vof highest ranked documents.\\n3.Improve our guesses for ptand ut. We choose from the methods of Equa-\\ntions ( 11.23 ) and ( 11.25 ) for re-estimating pt, except now based on the set\\nVinstead of VR. If we let Vtbe the subset of documents in Vcontaining\\nxtand use add1\\n2smoothing, we get:\\npt=|Vt|+1\\n2\\n|V|+1(11.26)\\nand if we assume that documents that are not retrieved are non relevant\\nthen we can update our utestimates as:\\nut=dft−|Vt|+1\\n2\\nN−|V|+1(11.27)\\n4.Go to step 2 until the ranking of the returned results converg es.\\nOnce we have a real estimate for ptthen the ctweights used in the RSV\\nvalue look almost like a tf-idf value. For instance, using Eq uation ( 11.18 ),', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 265}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP230 11 Probabilistic information retrieval\\nEquation ( 11.22 ), and Equation ( 11.26 ), we have:\\nct=log[pt\\n1−pt·1−ut\\nut]\\n≈log[\\n|Vt|+1\\n2\\n|V|−|Vt|+1·N\\ndft]\\n(11.28)\\nBut things aren’t quite the same: pt/(1−pt)measures the (estimated) pro-\\nportion of relevant documents that the term toccurs in, not term frequency.\\nMoreover, if we apply log identities:\\nct=log|Vt|+1\\n2\\n|V|−|Vt|+1+logN\\ndft(11.29)\\nwe see that we are now adding the two log scaled components rather than\\nmultiplying them.\\n?Exercise 11.1\\nWork through the derivation of Equation ( 11.20 ) from Equations ( 11.18 ) and ( 11.19 ).\\nExercise 11.2\\nWhat are the differences between standard vector space tf-i df weighting and the BIM\\nprobabilistic retrieval model (in the case where no documen t relevance information\\nis available)?\\nExercise 11.3 [⋆⋆]\\nLetXtbe a random variable indicating whether the term tappears in a document.\\nSuppose we have |R|relevant documents in the document collection and that Xt=1\\ninsof the documents. Take the observed data to be just these obse rvations of Xtfor\\neach document in R. Show that the MLE for the parameter pt=P(Xt=1|R=1,⃗q),\\nthat is, the value for ptwhich maximizes the probability of the observed data, is\\npt=s/|R|.\\nExercise 11.4\\nDescribe the differences between vector space relevance fe edback and probabilistic\\nrelevance feedback.\\n11.4 An appraisal and some extensions\\n11.4.1 An appraisal of probabilistic models\\nProbabilistic methods are one of the oldest formal models in IR. Already\\nin the 1970s they were held out as an opportunity to place IR on a ﬁrmer\\ntheoretical footing, and with the resurgence of probabilis tic methods in com-\\nputational linguistics in the 1990s, that hope has returned , and probabilis-\\ntic methods are again one of the currently hottest topics in I R. Traditionally,\\nprobabilistic IR has had neat ideas but the methods have neve r won on per-\\nformance. Getting reasonable approximations of the needed probabilities for', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 266}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP11.4 An appraisal and some extensions 231\\na probabilistic IR model is possible, but it requires some ma jor assumptions.\\nIn the BIM these are:\\n•a Boolean representation of documents/queries/relevance\\n•term independence\\n•terms not in the query don’t affect the outcome\\n•document relevance values are independent\\nIt is perhaps the severity of the modeling assumptions that m akes achieving\\ngood performance difﬁcult. A general problem seems to be tha t probabilistic\\nmodels either require partial relevance information or els e only allow for\\nderiving apparently inferior term weighting models.\\nThings started to change in the 1990s when the BM25 weighting scheme,\\nwhich we discuss in the next section, showed very good perfor mance, and\\nstarted to be adopted as a term weighting scheme by many group s. The\\ndifference between “vector space” and “probabilistic” IR s ystems is not that\\ngreat: in either case, you build an information retrieval sc heme in the exact\\nsame way that we discussed in Chapter 7. For a probabilistic IR system, it’s\\njust that, at the end, you score queries not by cosine similar ity and tf-idf in\\na vector space, but by a slightly different formula motivate d by probability\\ntheory. Indeed, sometimes people have changed an existing v ector-space\\nIR system into an effectively probabilistic system simply b y adopted term\\nweighting formulas from probabilistic models. In this sect ion, we brieﬂy\\npresent three extensions of the traditional probabilistic model, and in the next\\nchapter, we look at the somewhat different probabilistic la nguage modeling\\napproach to IR.\\n11.4.2 Tree-structured dependencies between terms\\nSome of the assumptions of the BIM can be removed. For example , we can\\nremove the assumption that terms are independent. This assu mption is very\\nfar from true in practice. A case that particularly violates this assumption is\\nterm pairs like Hong andKong , which are strongly dependent. But dependen-\\ncies can occur in various complex conﬁgurations, such as bet ween the set of\\ntermsNew,York,England ,City,Stock ,Exchange , andUniversity .van Rijsbergen\\n(1979 ) proposed a simple, plausible model which allowed a tree str ucture of\\nterm dependencies, as in Figure 11.1. In this model each term can be directly\\ndependent on only one other term, giving a tree structure of d ependencies.\\nWhen it was invented in the 1970s, estimation problems held b ack the practi-\\ncal success of this model, but the idea was reinvented as the T ree Augmented\\nNaive Bayes model by Friedman and Goldszmidt (1996 ), who used it with\\nsome success on various machine learning data sets.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 267}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP232 11 Probabilistic information retrieval\\nx1\\nx2\\nx3 x4 x5\\nx6 x7\\n◮Figure 11.1 A tree of dependencies between terms. In this graphical mode l rep-\\nresentation, a term xiis directly dependent on a term xkif there is an arrow xk→xi.\\n11.4.3 Okapi BM25: a non-binary model\\nThe BIM was originally designed for short catalog records an d abstracts of\\nfairly consistent length, and it works reasonably in these c ontexts, but for\\nmodern full-text search collections, it seems clear that a m odel should pay\\nattention to term frequency and document length, as in Chapt er6. The BM25 BM25 WEIGHTS\\nweighting scheme , often called Okapi weighting , after the system in which it was OKAPI WEIGHTING\\nﬁrst implemented, was developed as a way of building a probab ilistic model\\nsensitive to these quantities while not introducing too man y additional pa-\\nrameters into the model ( Spärck Jones et al. 2000 ). We will not develop the\\nfull theory behind the model here, but just present a series o f forms that\\nbuild up to the standard form now used for document scoring. T he simplest\\nscore for document dis just idf weighting of the query terms present, as in\\nEquation ( 11.22 ):\\nRSV d=∑\\nt∈qlogN\\ndft(11.30)\\nSometimes, an alternative version of idf is used. If we start with the formula\\nin Equation ( 11.21 ) but in the absence of relevance feedback information we\\nestimate that S=s=0, then we get an alternative idf formulation as follows:\\nRSV d=∑\\nt∈qlogN−dft+1\\n2\\ndft+1\\n2(11.31)', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 268}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP11.4 An appraisal and some extensions 233\\nThis variant behaves slightly strangely: if a term occurs in over half the doc-\\numents in the collection then this model gives a negative ter m weight, which\\nis presumably undesirable. But, assuming the use of a stop li st, this normally\\ndoesn’t happen, and the value for each summand can be given a ﬂ oor of 0.\\nWe can improve on Equation ( 11.30 ) by factoring in the frequency of each\\nterm and document length:\\nRSV d=∑\\nt∈qlog[N\\ndft]\\n·(k1+1)tftd\\nk1((1−b) +b×(Ld/Lave)) + tftd(11.32)\\nHere, tf tdis the frequency of term tin document d, and Ldand Laveare the\\nlength of document dand the average document length for the whole col-\\nlection. The variable k1is a positive tuning parameter that calibrates the\\ndocument term frequency scaling. A k1value of 0 corresponds to a binary\\nmodel (no term frequency), and a large value corresponds to u sing raw term\\nfrequency. bis another tuning parameter (0 ≤b≤1) which determines\\nthe scaling by document length: b=1 corresponds to fully scaling the term\\nweight by the document length, while b=0 corresponds to no length nor-\\nmalization.\\nIf the query is long, then we might also use similar weighting for query\\nterms. This is appropriate if the queries are paragraph long information\\nneeds, but unnecessary for short queries.\\nRSV d=∑\\nt∈q[\\nlogN\\ndft]\\n·(k1+1)tftd\\nk1((1−b) +b×(Ld/Lave)) + tftd·(k3+1)tftq\\nk3+tftq(11.33)\\nwith tf tqbeing the frequency of term tin the query q, and k3being another\\npositive tuning parameter that this time calibrates term fr equency scaling\\nof the query. In the equation presented, there is no length no rmalization of\\nqueries (it is as if b=0 here). Length normalization of the query is unnec-\\nessary because retrieval is being done with respect to a sing le ﬁxed query.\\nThe tuning parameters of these formulas should ideally be se t to optimize\\nperformance on a development test collection (see page 153). That is, we\\ncan search for values of these parameters that maximize perf ormance on a\\nseparate development test collection (either manually or w ith optimization\\nmethods such as grid search or something more advanced), and then use\\nthese parameters on the actual test collection. In the absen ce of such opti-\\nmization, experiments have shown reasonable values are to s etk1and k3to\\na value between 1.2 and 2 and b=0.75.\\nIf we have relevance judgments available, then we can use the full form of\\n(11.21 ) in place of the approximation log (N/df t)introduced in ( 11.22 ):\\nRSV d=∑\\nt∈qlog[[\\n(|VR t|+1\\n2)/(|VNR t|+1\\n2)\\n(dft−|VR t|+1\\n2)/(N−dft−|VR|+|VR t|+1\\n2)]\\n(11.34)', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 269}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP234 11 Probabilistic information retrieval\\n×(k1+1)tftd\\nk1((1−b) +b(Ld/Lave)) + tftd×(k3+1)tftq\\nk3+tftq]\\nHere, VR t,NVR t, and VRare used as in Section 11.3.4 . The ﬁrst part of the\\nexpression reﬂects relevance feedback (or just idf weighti ng if no relevance\\ninformation is available), the second implements document term frequency\\nand document length scaling, and the third considers term fr equency in the\\nquery.\\nRather than just providing a term weighting method for terms in a user’s\\nquery, relevance feedback can also involve augmenting the q uery (automat-\\nically or with manual review) with some (say, 10–20) of the to p terms in the\\nknown-relevant documents as ordered by the relevance facto rˆctfrom Equa-\\ntion ( 11.21 ), and the above formula can then be used with such an augmente d\\nquery vector ⃗q.\\nThe BM25 term weighting formulas have been used quite widely and quite\\nsuccessfully across a range of collections and search tasks . Especially in the\\nTREC evaluations, they performed well and were widely adopt ed by many\\ngroups. See Spärck Jones et al. (2000 ) for extensive motivation and discussion\\nof experimental results.\\n11.4.4 Bayesian network approaches to IR\\nTurtle and Croft (1989 ;1991 ) introduced into information retrieval the use\\nofBayesian networks (Jensen and Jensen 2001 ), a form of probabilistic graph- BAYESIAN NETWORKS\\nical model. We skip the details because fully introducing th e formalism of\\nBayesian networks would require much too much space, but con ceptually,\\nBayesian networks use directed graphs to show probabilisti c dependencies\\nbetween variables, as in Figure 11.1, and have led to the development of so-\\nphisticated algorithms for propagating inﬂuence so as to al low learning and\\ninference with arbitrary knowledge within arbitrary direc ted acyclic graphs.\\nTurtle and Croft used a sophisticated network to better mode l the complex\\ndependencies between a document and a user’s information ne ed.\\nThe model decomposes into two parts: a document collection n etwork and\\na query network. The document collection network is large, b ut can be pre-\\ncomputed: it maps from documents to terms to concepts. The co ncepts are\\na thesaurus-based expansion of the terms appearing in the do cument. The\\nquery network is relatively small but a new network needs to b e built each\\ntime a query comes in, and then attached to the document netwo rk. The\\nquery network maps from query terms, to query subexpression s (built us-\\ning probabilistic or “noisy” versions of AND and ORoperators), to the user’s\\ninformation need.\\nThe result is a ﬂexible probabilistic network which can gene ralize vari-\\nous simpler Boolean and probabilistic models. Indeed, this is the primary', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 270}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP11.5 References and further reading 235\\ncase of a statistical ranked retrieval model that naturally supports structured\\nquery operators. The system allowed efﬁcient large-scale r etrieval, and was\\nthe basis of the InQuery text retrieval system, built at the U niversity of Mas-\\nsachusetts. This system performed very well in TREC evaluat ions and for a\\ntime was sold commercially. On the other hand, the model stil l used various\\napproximations and independence assumptions to make param eter estima-\\ntion and computation possible. There has not been much follo w-on work\\nalong these lines, but we would note that this model was actua lly built very\\nearly on in the modern era of using Bayesian networks, and the re have been\\nmany subsequent developments in the theory, and the time is p erhaps right\\nfor a new generation of Bayesian network-based information retrieval sys-\\ntems.\\n11.5 References and further reading\\nLonger introductions to probability theory can be found in m ost introduc-\\ntory probability and statistics books, such as ( Grinstead and Snell 1997 ,Rice\\n2006 ,Ross 2006 ). An introduction to Bayesian utility theory can be found in\\n(Ripley 1996 ).\\nThe probabilistic approach to IR originated in the UK in the 1 950s. The\\nﬁrst major presentation of a probabilistic model is Maron and Kuhns (1960 ).\\nRobertson and Jones (1976 ) introduce the main foundations of the BIM and\\nvan Rijsbergen (1979 ) presents in detail the classic BIM probabilistic model.\\nThe idea of the PRP is variously attributed to S. E. Robertson , M. E. Maron\\nand W. S. Cooper (the term “Probabilistic Ordering Principl e” is used in\\nRobertson and Jones (1976 ), but PRP dominates in later work). Fuhr (1992 )\\nis a more recent presentation of probabilistic IR, which inc ludes coverage of\\nother approaches such as probabilistic logics and Bayesian networks. Crestani\\net al. (1998 ) is another survey. Spärck Jones et al. (2000 ) is the deﬁnitive pre-\\nsentation of probabilistic IR experiments by the “London sc hool”, and Robert-\\nson(2005 ) presents a retrospective on the group’s participation in T REC eval-\\nuations, including detailed discussion of the Okapi BM25 sc oring function\\nand its development. Robertson et al. (2004 ) extend BM25 to the case of mul-\\ntiple weighted ﬁelds.\\nThe open-source Indri search engine, which is distributed w ith the Lemur\\ntoolkit (http://www.lemurproject.org/ ) merges ideas from Bayesian inference net-\\nworks and statistical language modeling approaches (see Ch apter 12), in par-\\nticular preserving the former’s support for structured que ry operators.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 271}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 272}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 237\\n12Language models for information\\nretrieval\\nA common suggestion to users for coming up with good queries i s to think\\nof words that would likely appear in a relevant document, and to use those\\nwords as the query. The language modeling approach to IR dire ctly models\\nthat idea: a document is a good match to a query if the document model\\nis likely to generate the query, which will in turn happen if t he document\\ncontains the query words often. This approach thus provides a different real-\\nization of some of the basic ideas for document ranking which we saw in Sec-\\ntion 6.2(page 117). Instead of overtly modeling the probability P(R=1|q,d)\\nof relevance of a document dto a query q, as in the traditional probabilis-\\ntic approach to IR (Chapter 11), the basic language modeling approach in-\\nstead builds a probabilistic language model Mdfrom each document d, and\\nranks documents based on the probability of the model genera ting the query:\\nP(q|Md).\\nIn this chapter, we ﬁrst introduce the concept of language mo dels (Sec-\\ntion 12.1) and then describe the basic and most commonly used language\\nmodeling approach to IR, the Query Likelihood Model (Sectio n12.2). Af-\\nter some comparisons between the language modeling approac h and other\\napproaches to IR (Section 12.3), we ﬁnish by brieﬂy describing various ex-\\ntensions to the language modeling approach (Section 12.4).\\n12.1 Language models\\n12.1.1 Finite automata and language models\\nWhat do we mean by a document model generating a query? A tradi tional\\ngenerative model of a language, of the kind familiar from formal language GENERATIVE MODEL\\ntheory, can be used either to recognize or to generate string s. For example,\\nthe ﬁnite automaton shown in Figure 12.1 can generate strings that include\\nthe examples shown. The full set of strings that can be genera ted is called\\nthelanguage of the automaton.1LANGUAGE', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 273}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP238 12 Language models for information retrieval\\nI wishI wish\\nI wish I wish\\nI wish I wish I wish\\nI wish I wish I wish I wish I wish I wish\\n. . .\\nCANNOT GENERATE : wish I wish\\n◮Figure 12.1 A simple ﬁnite automaton and some of the strings in the langua ge it\\ngenerates.→shows the start state of the automaton and a double circle ind icates a\\n(possible) ﬁnishing state.\\nq1\\nP(STOP|q1) =0.2the 0.2\\na 0.1\\nfrog 0.01\\ntoad 0.01\\nsaid 0.03\\nlikes 0.02\\nthat 0.04\\n. . . . . .\\n◮Figure 12.2 A one-state ﬁnite automaton that acts as a unigram language m odel.\\nWe show a partial speciﬁcation of the state emission probabi lities.\\nIf instead each node has a probability distribution over gen erating differ-\\nent terms, we have a language model. The notion of a language m odel is\\ninherently probabilistic. A language model is a function that puts a probability LANGUAGE MODEL\\nmeasure over strings drawn from some vocabulary. That is, fo r a language\\nmodel Mover an alphabet Σ:\\n∑\\ns∈Σ∗P(s) =1 (12.1)\\nOne simple kind of language model is equivalent to a probabil istic ﬁnite\\nautomaton consisting of just a single node with a single prob ability distri-\\nbution over producing different terms, so that ∑t∈VP(t) = 1, as shown\\nin Figure 12.2. After generating each word, we decide whether to stop or\\nto loop around and then produce another word, and so the model also re-\\nquires a probability of stopping in the ﬁnishing state. Such a model places a\\nprobability distribution over any sequence of words. By con struction, it also\\nprovides a model for generating text according to its distri bution.\\n1. Finite automata can have outputs attached to either their states or their arcs; we use states\\nhere, because that maps directly on to the way probabilistic automata are usually formalized.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 274}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP12.1 Language models 239\\nModel M1 Model M2\\nthe 0.2 the 0.15\\na 0.1 a 0.12\\nfrog 0.01 frog 0.0002\\ntoad 0.01 toad 0.0001\\nsaid 0.03 said 0.03\\nlikes 0.02 likes 0.04\\nthat 0.04 that 0.04\\ndog 0.005 dog 0.01\\ncat 0.003 cat 0.015\\nmonkey 0.001 monkey 0.002\\n. . . . . . . . . . . .\\n◮Figure 12.3 Partial speciﬁcation of two unigram language models.\\n✎Example 12.1: To ﬁnd the probability of a word sequence, we just multiply th e\\nprobabilities which the model gives to each word in the seque nce, together with the\\nprobability of continuing or stopping after producing each word. For example,\\nP(frog said that toad likes frog ) = ( 0.01×0.03×0.04×0.01×0.02×0.01) (12.2)\\n×(0.8×0.8×0.8×0.8×0.8×0.8×0.2)\\n≈ 0.000000000001573\\nAs you can see, the probability of a particular string/docum ent, is usually a very\\nsmall number! Here we stopped after generating frogthe second time. The ﬁrst line of\\nnumbers are the term emission probabilities, and the second line gives the probabil-\\nity of continuing or stopping after generating each word. An explicit stop probability\\nis needed for a ﬁnite automaton to be a well-formed language m odel according to\\nEquation ( 12.1). Nevertheless, most of the time, we will omit to include STOP and\\n(1−STOP)probabilities (as do most other authors). To compare two mod els for a\\ndata set, we can calculate their likelihood ratio , which results from simply dividing the LIKELIHOOD RATIO\\nprobability of the data according to one model by the probabi lity of the data accord-\\ning to the other model. Providing that the stop probability i s ﬁxed, its inclusion will\\nnot alter the likelihood ratio that results from comparing t he likelihood of two lan-\\nguage models generating a string. Hence, it will not alter th e ranking of documents.2\\nNevertheless, formally, the numbers will no longer truly be probabilities, but only\\nproportional to probabilities. See Exercise 12.4.\\n✎Example 12.2: Suppose, now, that we have two language models M1and M2,\\nshown partially in Figure 12.3. Each gives a probability estimate to a sequence of\\n2. In the IR context that we are leading up to, taking the stop p robability to be ﬁxed across\\nmodels seems reasonable. This is because we are generating q ueries, and the length distribution\\nof queries is ﬁxed and independent of the document from which we are generating the language\\nmodel.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 275}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP240 12 Language models for information retrieval\\nterms, as already illustrated in Example 12.1. The language model that gives the\\nhigher probability to the sequence of terms is more likely to have generated the term\\nsequence. This time, we will omit STOP probabilities from our calculations. For the\\nsequence shown, we get:\\n(12.3) s frog said that toad likes that dog\\nM10.01 0.03 0.04 0.01 0.02 0.04 0.005\\nM20.0002 0.03 0.04 0.0001 0.04 0.04 0.01\\nP(s|M1) =0.00000000000048\\nP(s|M2) =0.000000000000000384\\nand we see that P(s|M1)>P(s|M2). We present the formulas here in terms of prod-\\nucts of probabilities, but, as is common in probabilistic ap plications, in practice it is\\nusually best to work with sums of log probabilities (cf. page 258).\\n12.1.2 Types of language models\\nHow do we build probabilities over sequences of terms? We can always\\nuse the chain rule from Equation ( 11.1) to decompose the probability of a\\nsequence of events into the probability of each successive e vent conditioned\\non earlier events:\\nP(t1t2t3t4) =P(t1)P(t2|t1)P(t3|t1t2)P(t4|t1t2t3) (12.4)\\nThe simplest form of language model simply throws away all co nditioning\\ncontext, and estimates each term independently. Such a mode l is called a\\nunigram language model : UNIGRAM LANGUAGE\\nMODEL\\nPuni(t1t2t3t4) =P(t1)P(t2)P(t3)P(t4) (12.5)\\nThere are many more complex kinds of language models, such as bigram BIGRAM LANGUAGE\\nMODEL language models , which condition on the previous term,\\nPbi(t1t2t3t4) =P(t1)P(t2|t1)P(t3|t2)P(t4|t3) (12.6)\\nand even more complex grammar-based language models such as proba-\\nbilistic context-free grammars. Such models are vital for t asks like speech\\nrecognition, spelling correction, and machine translatio n, where you need\\nthe probability of a term conditioned on surrounding contex t. However,\\nmost language-modeling work in IR has used unigram language models.\\nIR is not the place where you most immediately need complex la nguage\\nmodels, since IR does not directly depend on the structure of sentences to\\nthe extent that other tasks like speech recognition do. Unig ram models are\\noften sufﬁcient to judge the topic of a text. Moreover, as we s hall see, IR lan-\\nguage models are frequently estimated from a single documen t and so it is', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 276}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP12.1 Language models 241\\nquestionable whether there is enough training data to do mor e. Losses from\\ndata sparseness (see the discussion on page 260) tend to outweigh any gains\\nfrom richer models. This is an example of the bias-variance t radeoff (cf. Sec-\\ntion 14.6, page 308): With limited training data, a more constrained model\\ntends to perform better. In addition, unigram models are mor e efﬁcient to\\nestimate and apply than higher-order models. Nevertheless , the importance\\nof phrase and proximity queries in IR in general suggests tha t future work\\nshould make use of more sophisticated language models, and s ome has be-\\ngun to (see Section 12.5, page 252). Indeed, making this move parallels the\\nmodel of van Rijsbergen in Chapter 11(page 231).\\n12.1.3 Multinomial distributions over words\\nUnder the unigram language model the order of words is irrele vant, and so\\nsuch models are often called “bag of words” models, as discus sed in Chap-\\nter6(page 117). Even though there is no conditioning on preceding context ,\\nthis model nevertheless still gives the probability of a par ticular ordering of\\nterms. However, any other ordering of this bag of terms will h ave the same\\nprobability. So, really, we have a multinomial distribution over words. So long MULTINOMIAL\\nDISTRIBUTION as we stick to unigram models, the language model name and mot ivation\\ncould be viewed as historical rather than necessary. We coul d instead just\\nrefer to the model as a multinomial model. From this perspect ive, the equa-\\ntions presented above do not present the multinomial probab ility of a bag of\\nwords, since they do not sum over all possible orderings of th ose words, as\\nis done by the multinomial coefﬁcient (the ﬁrst term on the ri ght-hand side)\\nin the standard presentation of a multinomial model:\\nP(d) =Ld!\\ntft1,d!tft2,d!···tftM,d!P(t1)tft1,dP(t2)tft2,d···P(tM)tftM,d(12.7)\\nHere, Ld=∑1≤i≤Mtfti,dis the length of document d,Mis the size of the term\\nvocabulary, and the products are now over the terms in the voc abulary, not\\nthe positions in the document. However, just as with STOP probabilities, in\\npractice we can also leave out the multinomial coefﬁcient in our calculations,\\nsince, for a particular bag of words, it will be a constant, an d so it has no effect\\non the likelihood ratio of two different models generating a particular bag of\\nwords. Multinomial distributions also appear in Section 13.2 (page 258).\\nThe fundamental problem in designing language models is tha t we do not\\nknow what exactly we should use as the model Md. However, we do gener-\\nally have a sample of text that is representative of that mode l. This problem\\nmakes a lot of sense in the original, primary uses of language models. For ex-\\nample, in speech recognition, we have a training sample of (s poken) text. But\\nwe have to expect that, in the future, users will use differen t words and in', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 277}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP242 12 Language models for information retrieval\\ndifferent sequences, which we have never observed before, a nd so the model\\nhas to generalize beyond the observed data to allow unknown w ords and se-\\nquences. This interpretation is not so clear in the IR case, w here a document\\nis ﬁnite and usually ﬁxed. The strategy we adopt in IR is as fol lows. We\\npretend that the document dis only a representative sample of text drawn\\nfrom a model distribution, treating it like a ﬁne-grained to pic. We then esti-\\nmate a language model from this sample, and use that model to c alculate the\\nprobability of observing any word sequence, and, ﬁnally, we rank documents\\naccording to their probability of generating the query.\\n?Exercise 12.1 [⋆]\\nIncluding stop probabilities in the calculation, what will the sum of the probability\\nestimates of all strings in the language of length 1 be? Assum e that you generate a\\nword and then decide whether to stop or not (i.e., the null str ing is not part of the\\nlanguage).\\nExercise 12.2 [⋆]\\nIf the stop probability is omitted from calculations, what w ill the sum of the scores\\nassigned to strings in the language of length 1 be?\\nExercise 12.3 [⋆]\\nWhat is the likelihood ratio of the document according to M1and M2in Exam-\\nple12.2?\\nExercise 12.4 [⋆]\\nNo explicit STOP probability appeared in Example 12.2. Assuming that the STOP\\nprobability of each model is 0.1, does this change the likeli hood ratio of a document\\naccording to the two models?\\nExercise 12.5 [⋆⋆]\\nHow might a language model be used in a spelling correction sy stem? In particular,\\nconsider the case of context-sensitive spelling correctio n, and correcting incorrect us-\\nages of words, such as their inAre you their? (See Section 3.5(page 65) for pointers to\\nsome literature on this topic.)\\n12.2 The query likelihood model\\n12.2.1 Using query likelihood language models in IR\\nLanguage modeling is a quite general formal approach to IR, w ith many vari-\\nant realizations. The original and basic method for using la nguage models\\nin IR is the query likelihood model . In it, we construct from each document d QUERY LIKELIHOOD\\nMODEL in the collection a language model Md. Our goal is to rank documents by\\nP(d|q), where the probability of a document is interpreted as the li kelihood\\nthat it is relevant to the query. Using Bayes rule (as introdu ced in Section 11.1,\\npage 220), we have:\\nP(d|q) =P(q|d)P(d)/P(q)', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 278}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP12.2 The query likelihood model 243\\nP(q)is the same for all documents, and so can be ignored. The prior prob-\\nability of a document P(d)is often treated as uniform across all dand so it\\ncan also be ignored, but we could implement a genuine prior wh ich could in-\\nclude criteria like authority, length, genre, newness, and number of previous\\npeople who have read the document. But, given these simpliﬁc ations, we\\nreturn results ranked by simply P(q|d), the probability of the query qunder\\nthe language model derived from d. The Language Modeling approach thus\\nattempts to model the query generation process: Documents a re ranked by\\nthe probability that a query would be observed as a random sam ple from the\\nrespective document model.\\nThe most common way to do this is using the multinomial unigra m lan-\\nguage model, which is equivalent to a multinomial Naive Baye s model (page 263),\\nwhere the documents are the classes, each treated in the esti mation as a sep-\\narate “language”. Under this model, we have that:\\nP(q|Md) =Kq∏\\nt∈VP(t|Md)tft,d (12.8)\\nwhere, again Kq=Ld!/(tft1,d!tft2,d!···tftM,d!)is the multinomial coefﬁcient\\nfor the query q, which we will henceforth ignore, since it is a constant for a\\nparticular query.\\nFor retrieval based on a language model (henceforth LM), we t reat the\\ngeneration of queries as a random process. The approach is to\\n1.Infer a LM for each document.\\n2.Estimate P(q|Mdi), the probability of generating the query according to\\neach of these document models.\\n3.Rank the documents according to these probabilities.\\nThe intuition of the basic model is that the user has a prototy pe document in\\nmind, and generates a query based on words that appear in this document.\\nOften, users have a reasonable idea of terms that are likely t o occur in doc-\\numents of interest and they will choose query terms that dist inguish these\\ndocuments from others in the collection.3Collection statistics are an integral\\npart of the language model, rather than being used heuristic ally as in many\\nother approaches.\\n12.2.2 Estimating the query generation probability\\nIn this section we describe how to estimate P(q|Md). The probability of pro-\\nducing the query given the LM Mdof document dusing maximum likelihood\\n3. Of course, in other cases, they do not. The answer to this wi thin the language modeling\\napproach is translation language models, as brieﬂy discuss ed in Section 12.4.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 279}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP244 12 Language models for information retrieval\\nestimation (MLE) and the unigram assumption is:\\nˆP(q|Md) =∏\\nt∈qˆPmle(t|Md) =∏\\nt∈qtft,d\\nLd(12.9)\\nwhere Mdis the language model of document d, tft,dis the (raw) term fre-\\nquency of term tin document d, and Ldis the number of tokens in docu-\\nment d. That is, we just count up how often each word occurred, and di vide\\nthrough by the total number of words in the document d. This is the same\\nmethod of calculating an MLE as we saw in Section 11.3.2 (page 226), but\\nnow using a multinomial over word counts.\\nThe classic problem with using language models is one of esti mation (the\\nˆsymbol on the P’s is used above to stress that the model is esti mated):\\nterms appear very sparsely in documents. In particular, som e words will\\nnot have appeared in the document at all, but are possible wor ds for the in-\\nformation need, which the user may have used in the query. If w e estimate\\nˆP(t|Md) = 0 for a term missing from a document d, then we get a strict\\nconjunctive semantics: documents will only give a query non -zero probabil-\\nity if all of the query terms appear in the document. Zero prob abilities are\\nclearly a problem in other uses of language models, such as wh en predicting\\nthe next word in a speech recognition application, because m any words will\\nbe sparsely represented in the training data. It may seem rat her less clear\\nwhether this is problematic in an IR application. This could be thought of\\nas a human-computer interface issue: vector space systems h ave generally\\npreferred more lenient matching, though recent web search d evelopments\\nhave tended more in the direction of doing searches with such conjunctive\\nsemantics. Regardless of the approach here, there is a more g eneral prob-\\nlem of estimation: occurring words are also badly estimated ; in particular,\\nthe probability of words occurring once in the document is no rmally over-\\nestimated, since their one occurrence was partly by chance. The answer to\\nthis (as we saw in Section 11.3.2 , page 226) is smoothing. But as people have\\ncome to understand the LM approach better, it has become appa rent that the\\nrole of smoothing in this model is not only to avoid zero proba bilities. The\\nsmoothing of terms actually implements major parts of the te rm weighting\\ncomponent (Exercise 12.8). It is not just that an unsmoothed model has con-\\njunctive semantics; an unsmoothed model works badly becaus e it lacks parts\\nof the term weighting component.\\nThus, we need to smooth probabilities in our document langua ge mod-\\nels: to discount non-zero probabilities and to give some pro bability mass to\\nunseen words. There’s a wide space of approaches to smoothin g probabil-\\nity distributions to deal with this problem. In Section 11.3.2 (page 226), we\\nalready discussed adding a number (1, 1/2, or a small α) to the observed', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 280}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP12.2 The query likelihood model 245\\ncounts and renormalizing to give a probability distributio n.4In this sec-\\ntion we will mention a couple of other smoothing methods, whi ch involve\\ncombining observed counts with a more general reference pro bability distri-\\nbution. The general approach is that a non-occurring term sh ould be possi-\\nble in a query, but its probability should be somewhat close t o but no more\\nlikely than would be expected by chance from the whole collec tion. That is,\\nif tf t,d=0 then\\nˆP(t|Md)≤cft/T\\nwhere cf tis the raw count of the term in the collection, and Tis the raw size\\n(number of tokens) of the entire collection. A simple idea th at works well in\\npractice is to use a mixture between a document-speciﬁc mult inomial distri-\\nbution and a multinomial distribution estimated from the en tire collection:\\nˆP(t|d) =λˆPmle(t|Md) + ( 1−λ)ˆPmle(t|Mc) (12.10)\\nwhere 0 <λ<1 and Mcis a language model built from the entire doc-\\nument collection. This mixes the probability from the docum ent with the\\ngeneral collection frequency of the word. Such a model is ref erred to as a\\nlinear interpolation language model.5Correctly setting λis important to the LINEAR\\nINTERPOLATION good performance of this model.\\nAn alternative is to use a language model built from the whole collection\\nas a prior distribution in a Bayesian updating process (rather than a uniform BAYESIAN SMOOTHING\\ndistribution, as we saw in Section 11.3.2 ). We then get the following equation:\\nˆP(t|d) =tft,d+αˆP(t|Mc)\\nLd+α(12.11)\\nBoth of these smoothing methods have been shown to perform we ll in IR\\nexperiments; we will stick with the linear interpolation sm oothing method\\nfor the rest of this section. While different in detail, they are both conceptu-\\nally similar: in both cases the probability estimate for a wo rd present in the\\ndocument combines a discounted MLE and a fraction of the esti mate of its\\nprevalence in the whole collection, while for words not pres ent in a docu-\\nment, the estimate is just a fraction of the estimate of the pr evalence of the\\nword in the whole collection.\\nThe role of smoothing in LMs for IR is not simply or principall y to avoid es-\\ntimation problems. This was not clear when the models were ﬁr st proposed,\\nbut it is now understood that smoothing is essential to the go od properties\\n4. In the context of probability theory, (re)normalization refers to summing numbers that cover\\nan event space and dividing them through by their sum, so that the result is a probability distri-\\nbution which sums to 1. This is distinct from both the concept of term normalization in Chapter 2\\nand the concept of length normalization in Chapter 6, which is done with a L2norm.\\n5. It is also referred to as Jelinek-Mercer smoothing.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 281}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP246 12 Language models for information retrieval\\nof the models. The reason for this is explored in Exercise 12.8. The extent\\nof smoothing in these two models is controlled by the λand αparameters: a\\nsmall value of λor a large value of αmeans more smoothing. This parameter\\ncan be tuned to optimize performance using a line search (or, for the linear\\ninterpolation model, by other methods, such as the expectat ion maximimiza-\\ntion algorithm; see Section 16.5, page 368). The value need not be a constant.\\nOne approach is to make the value a function of the query size. This is useful\\nbecause a small amount of smoothing (a “conjunctive-like” s earch) is more\\nsuitable for short queries, while a lot of smoothing is more s uitable for long\\nqueries.\\nTo summarize, the retrieval ranking for a query qunder the basic LM for\\nIR we have been considering is given by:\\nP(d|q)∝P(d)∏\\nt∈q((1−λ)P(t|Mc) +λP(t|Md)) (12.12)\\nThis equation captures the probability that the document th at the user had\\nin mind was in fact d.\\n✎Example 12.3: Suppose the document collection contains two documents:\\n•d1: Xyzzy reports a proﬁt but revenue is down\\n•d2: Quorus narrows quarter loss but revenue decreases further\\nThe model will be MLE unigram models from the documents and co llection, mixed\\nwith λ=1/2.\\nSuppose the query is revenue down . Then:\\nP(q|d1) = [( 1/8+2/16)/2]×[(1/8+1/16)/2] (12.13)\\n= 1/8×3/32=3/256\\nP(q|d2) = [( 1/8+2/16)/2]×[(0/8+1/16)/2]\\n= 1/8×1/32=1/256\\nSo, the ranking is d1>d2.\\n12.2.3 Ponte and Croft’s Experiments\\nPonte and Croft (1998 ) present the ﬁrst experiments on the language model-\\ning approach to information retrieval. Their basic approac h is the model that\\nwe have presented until now. However, we have presented an ap proach\\nwhere the language model is a mixture of two multinomials, mu ch as in\\n(Miller et al. 1999 ,Hiemstra 2000 ) rather than Ponte and Croft’s multivari-\\nate Bernoulli model. The use of multinomials has been standa rd in most\\nsubsequent work in the LM approach and experimental results in IR, as\\nwell as evidence from text classiﬁcation which we consider i n Section 13.3', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 282}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP12.2 The query likelihood model 247\\nPrecision\\nRec. tf-idf LM %chg\\n0.0 0.7439 0.7590 +2.0\\n0.1 0.4521 0.4910 +8.6\\n0.2 0.3514 0.4045 +15.1 *\\n0.3 0.2761 0.3342 +21.0 *\\n0.4 0.2093 0.2572 +22.9 *\\n0.5 0.1558 0.2061 +32.3 *\\n0.6 0.1024 0.1405 +37.1 *\\n0.7 0.0451 0.0760 +68.7 *\\n0.8 0.0160 0.0432 +169.6 *\\n0.9 0.0033 0.0063 +89.3\\n1.0 0.0028 0.0050 +76.9\\nAve 0.1868 0.2233 +19.55 *\\n◮Figure 12.4 Results of a comparison of tf-idf with language modeling (LM ) term\\nweighting by Ponte and Croft (1998 ). The version of tf-idf from the INQUERY IR sys-\\ntem includes length normalization of tf. The table gives an e valuation according to\\n11-point average precision with signiﬁcance marked with a * according to a Wilcoxon\\nsigned rank test. The language modeling approach always doe s better in these exper-\\niments, but note that where the approach shows signiﬁcant ga ins is at higher levels\\nof recall.\\n(page 263), suggests that it is superior. Ponte and Croft argued stron gly for\\nthe effectiveness of the term weights that come from the lang uage modeling\\napproach over traditional tf-idf weights. We present a subs et of their results\\nin Figure 12.4 where they compare tf-idf to language modeling by evaluatin g\\nTREC topics 202–250 over TREC disks 2 and 3. The queries are se ntence-\\nlength natural language queries. The language modeling app roach yields\\nsigniﬁcantly better results than their baseline tf-idf bas ed term weighting ap-\\nproach. And indeed the gains shown here have been extended in subsequent\\nwork.\\n?Exercise 12.6 [⋆]\\nConsider making a language model from the following trainin g text:\\nthe martian has landed on the latin pop sensation ricky marti n\\na.Under a MLE-estimated unigram probability model, what are P(the)and P(martian )?\\nb.Under a MLE-estimated bigram model, what are P(sensation|pop)and P(pop|the)?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 283}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP248 12 Language models for information retrieval\\nExercise 12.7 [⋆⋆]\\nSuppose we have a collection that consists of the 4 documents given in the below\\ntable.\\ndocID Document text\\n1 click go the shears boys click click click\\n2 click click\\n3 metal here\\n4 metal shears click here\\nBuild a query likelihood language model for this document co llection. Assume a\\nmixture model between the documents and the collection, wit h both weighted at 0.5.\\nMaximum likelihood estimation (mle) is used to estimate bot h as unigram models.\\nWork out the model probabilities of the queries click,shears , and hence clickshears for\\neach document, and use those probabilities to rank the docum ents returned by each\\nquery. Fill in these probabilities in the below table:\\nQuery Doc 1 Doc 2 Doc 3 Doc 4\\nclick\\nshears\\nclick shears\\nWhat is the ﬁnal ranking of the documents for the query click shears ?\\nExercise 12.8 [⋆⋆]\\nUsing the calculations in Exercise 12.7 as inspiration or as examples where appro-\\npriate, write one sentence each describing the treatment th at the model in Equa-\\ntion ( 12.10 ) gives to each of the following quantities. Include whether it is present\\nin the model or not and whether the effect is raw or scaled.\\na.Term frequency in a document\\nb.Collection frequency of a term\\nc.Document frequency of a term\\nd.Length normalization of a term\\nExercise 12.9 [⋆⋆]\\nIn the mixture model approach to the query likelihood model ( Equation ( 12.12 )), the\\nprobability estimate of a term is based on the term frequency of a word in a document,\\nand the collection frequency of the word. Doing this certain ly guarantees that each\\nterm of a query (in the vocabulary) has a non-zero chance of be ing generated by each\\ndocument. But it has a more subtle but important effect of imp lementing a form of\\nterm weighting, related to what we saw in Chapter 6. Explain how this works. In\\nparticular, include in your answer a concrete numeric examp le showing this term\\nweighting at work.\\n12.3 Language modeling versus other approaches in IR\\nThe language modeling approach provides a novel way of looki ng at the\\nproblem of text retrieval, which links it with a lot of recent work in speech', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 284}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP12.3 Language modeling versus other approaches in IR 249\\nand language processing. As Ponte and Croft (1998 ) emphasize, the language\\nmodeling approach to IR provides a different approach to sco ring matches\\nbetween queries and documents, and the hope is that the proba bilistic lan-\\nguage modeling foundation improves the weights that are use d, and hence\\nthe performance of the model. The major issue is estimation o f the docu-\\nment model, such as choices of how to smooth it effectively. T he model\\nhas achieved very good retrieval results. Compared to other probabilistic\\napproaches, such as the BIM from Chapter 11, the main difference initially\\nappears to be that the LM approach does away with explicitly m odeling rel-\\nevance (whereas this is the central variable evaluated in th e BIM approach).\\nBut this may not be the correct way to think about things, as so me of the\\npapers in Section 12.5 further discuss. The LM approach assumes that docu-\\nments and expressions of information needs are objects of th e same type, and\\nassesses their match by importing the tools and methods of la nguage mod-\\neling from speech and natural language processing. The resu lting model is\\nmathematically precise, conceptually simple, computatio nally tractable, and\\nintuitively appealing. This seems similar to the situation with XML retrieval\\n(Chapter 10): there the approaches that assume queries and documents ar e\\nobjects of the same type are also among the most successful.\\nOn the other hand, like all IR models, you can also raise objec tions to the\\nmodel. The assumption of equivalence between document and i nformation\\nneed representation is unrealistic. Current LM approaches use very simple\\nmodels of language, usually unigram models. Without an expl icit notion of\\nrelevance, relevance feedback is difﬁcult to integrate int o the model, as are\\nuser preferences. It also seems necessary to move beyond a un igram model\\nto accommodate notions of phrase or passage matching or Bool ean retrieval\\noperators. Subsequent work in the LM approach has looked at a ddressing\\nsome of these concerns, including putting relevance back in to the model and\\nallowing a language mismatch between the query language and the docu-\\nment language.\\nThe model has signiﬁcant relations to traditional tf-idf mo dels. Term fre-\\nquency is directly represented in tf-idf models, and much re cent work has\\nrecognized the importance of document length normalizatio n. The effect of\\ndoing a mixture of document generation probability with col lection gener-\\nation probability is a little like idf: terms rare in the gene ral collection but\\ncommon in some documents will have a greater inﬂuence on the r anking of\\ndocuments. In most concrete realizations, the models share treating terms as\\nif they were independent. On the other hand, the intuitions a re probabilistic\\nrather than geometric, the mathematical models are more pri ncipled rather\\nthan heuristic, and the details of how statistics like term f requency and doc-\\nument length are used differ. If you are concerned mainly wit h performance\\nnumbers, recent work has shown the LM approach to be very effe ctive in re-\\ntrieval experiments, beating tf-idf and BM25 weights. Neve rtheless, there is', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 285}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP250 12 Language models for information retrieval\\nQuery Query model P(t|Query )\\nDocument Doc. model P(t|Document )(a)\\n(b)(c)\\n◮Figure 12.5 Three ways of developing the language modeling approach: (a ) query\\nlikelihood, (b) document likelihood, and (c) model compari son.\\nperhaps still insufﬁcient evidence that its performance so greatly exceeds that\\nof a well-tuned traditional vector space retrieval system a s to justify chang-\\ning an existing implementation.\\n12.4 Extended language modeling approaches\\nIn this section we brieﬂy mention some of the work that extend s the basic\\nlanguage modeling approach.\\nThere are other ways to think of using the language modeling i dea in IR\\nsettings, and many of them have been tried in subsequent work . Rather than\\nlooking at the probability of a document language model Mdgenerating the\\nquery, you can look at the probability of a query language mod elMqgener-\\nating the document. The main reason that doing things in this direction and\\ncreating a document likelihood model is less appealing is that there is much less DOCUMENT\\nLIKELIHOOD MODEL text available to estimate a language model based on the quer y text, and so\\nthe model will be worse estimated, and will have to depend mor e on being\\nsmoothed with some other language model. On the other hand, i t is easy to\\nsee how to incorporate relevance feedback into such a model: you can ex-\\npand the query with terms taken from relevant documents in th e usual way\\nand hence update the language model Mq(Zhai and Lafferty 2001a ). Indeed,\\nwith appropriate modeling choices, this approach leads to t he BIM model of\\nChapter 11. The relevance model of Lavrenko and Croft (2001 ) is an instance\\nof a document likelihood model, which incorporates pseudo- relevance feed-\\nback into a language modeling approach. It achieves very str ong empirical\\nresults.\\nRather than directly generating in either direction, we can make a lan-\\nguage model from both the document and query, and then ask how different\\nthese two language models are from each other. Lafferty and Zhai (2001 ) lay', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 286}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP12.4 Extended language modeling approaches 251\\nout these three ways of thinking about the problem, which we s how in Fig-\\nure12.5, and develop a general risk minimization approach for docum ent\\nretrieval. For instance, one way to model the risk of returni ng a document d\\nas relevant to a query qis to use the Kullback-Leibler (KL) divergence between KULLBACK -LEIBLER\\nDIVERGENCE their respective language models:\\nR(d;q) =KL(Md∥Mq) =∑\\nt∈VP(t|Mq)logP(t|Mq)\\nP(t|Md)(12.14)\\nKL divergence is an asymmetric divergence measure originat ing in informa-\\ntion theory, which measures how bad the probability distrib ution Mqis at\\nmodeling Md(Cover and Thomas 1991 ,Manning and Schütze 1999 ).Laf-\\nferty and Zhai (2001 ) present results suggesting that a model comparison\\napproach outperforms both query-likelihood and document- likelihood ap-\\nproaches. One disadvantage of using KL divergence as a ranki ng function\\nis that scores are not comparable across queries. This does n ot matter for ad\\nhoc retrieval, but is important in other applications such a s topic tracking.\\nKraaij and Spitters (2003 ) suggest an alternative proposal which models sim-\\nilarity as a normalized log-likelihood ratio (or, equivale ntly, as a difference\\nbetween cross-entropies).\\nBasic LMs do not address issues of alternate expression, tha t is, synonymy,\\nor any deviation in use of language between queries and docum ents. Berger\\nand Lafferty (1999 ) introduce translation models to bridge this query-docume nt\\ngap. A translation model lets you generate query words not in a document by TRANSLATION MODEL\\ntranslation to alternate terms with similar meaning. This a lso provides a ba-\\nsis for performing cross-language IR. We assume that the tra nslation model\\ncan be represented by a conditional probability distributi onT(·|·)between\\nvocabulary terms. The form of the translation query generat ion model is\\nthen:\\nP(q|Md) =∏\\nt∈q∑\\nv∈VP(v|Md)T(t|v) (12.15)\\nThe term P(v|Md)is the basic document language model, and the term T(t|v)\\nperforms translation. This model is clearly more computati onally intensive\\nand we need to build a translation model. The translation mod el is usually\\nbuilt using separate resources (such as a traditional thesa urus or bilingual\\ndictionary or a statistical machine translation system’s t ranslation diction-\\nary), but can be built using the document collection if there are pieces of\\ntext that naturally paraphrase or summarize other pieces of text. Candi-\\ndate examples are documents and their titles or abstracts, o r documents and\\nanchor-text pointing to them in a hypertext environment.\\nBuilding extended LM approaches remains an active area of re search. In\\ngeneral, translation models, relevance feedback models, a nd model compar-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 287}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP252 12 Language models for information retrieval\\nison approaches have all been demonstrated to improve perfo rmance over\\nthe basic query likelihood LM.\\n12.5 References and further reading\\nFor more details on the basic concepts of probabilistic lang uage models and\\ntechniques for smoothing, see either Manning and Schütze (1999 , Chapter 6)\\norJurafsky and Martin (2008 , Chapter 4).\\nThe important initial papers that originated the language m odeling ap-\\nproach to IR are: ( Ponte and Croft 1998 ,Hiemstra 1998 ,Berger and Lafferty\\n1999 ,Miller et al. 1999 ). Other relevant papers can be found in the next sev-\\neral years of SIGIR proceedings. ( Croft and Lafferty 2003 ) contains a col-\\nlection of papers from a workshop on language modeling appro aches and\\nHiemstra and Kraaij (2005 ) review one prominent thread of work on using\\nlanguage modeling approaches for TREC tasks. Zhai and Lafferty (2001b )\\nclarify the role of smoothing in LMs for IR and present detail ed empirical\\ncomparisons of different smoothing methods. Zaragoza et al. (2003 ) advo-\\ncate using full Bayesian predictive distributions rather t han MAP point es-\\ntimates, but while they outperform Bayesian smoothing, the y fail to outper-\\nform a linear interpolation. Zhai and Lafferty (2002 ) argue that a two-stage\\nsmoothing model with ﬁrst Bayesian smoothing followed by li near interpo-\\nlation gives a good model of the task, and performs better and more stably\\nthan a single form of smoothing. A nice feature of the LM appro ach is that it\\nprovides a convenient and principled way to put various kind s of prior infor-\\nmation into the model; Kraaij et al. (2002 ) demonstrate this by showing the\\nvalue of link information as a prior in improving web entry pa ge retrieval\\nperformance. As brieﬂy discussed in Chapter 16(page 353),Liu and Croft\\n(2004 ) show some gains by smoothing a document LM with estimates fr om\\na cluster of similar documents; Tao et al. (2006 ) report larger gains by doing\\ndocument-similarity based smoothing.\\nHiemstra and Kraaij (2005 ) present TREC results showing a LM approach\\nbeating use of BM25 weights. Recent work has achieved some ga ins by\\ngoing beyond the unigram model, providing the higher order m odels are\\nsmoothed with lower order models ( Gao et al. 2004 ,Cao et al. 2005 ), though\\nthe gains to date remain modest. Spärck Jones (2004 ) presents a critical view-\\npoint on the rationale for the language modeling approach, b utLafferty and\\nZhai (2003 ) argue that a uniﬁed account can be given of the probabilisti c\\nsemantics underlying both the language modeling approach p resented in\\nthis chapter and the classical probabilistic information r etrieval approach of\\nChapter 11. The Lemur Toolkit ( http://www.lemurproject.org/ ) provides a ﬂexi-\\nble open source framework for investigating language model ing approaches\\nto IR.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 288}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 253\\n13Text classiﬁcation and Naive\\nBayes\\nThus far, this book has mainly discussed the process of ad hoc retrieval , where\\nusers have transient information needs that they try to addr ess by posing\\none or more queries to a search engine. However, many users ha ve ongoing\\ninformation needs. For example, you might need to track deve lopments in\\nmulticore computer chips . One way of doing this is to issue the query multi-\\ncoreANDcomputer ANDchip against an index of recent newswire articles each\\nmorning. In this and the following two chapters we examine th e question:\\nHow can this repetitive task be automated? To this end, many s ystems sup-\\nport standing queries . A standing query is like any other query except that it STANDING QUERY\\nis periodically executed on a collection to which new docume nts are incre-\\nmentally added over time.\\nIf your standing query is just multicore ANDcomputer ANDchip, you will tend\\nto miss many relevant new articles which use other terms such asmulticore\\nprocessors . To achieve good recall, standing queries thus have to be reﬁ ned\\nover time and can gradually become quite complex. In this exa mple, using a\\nBoolean search engine with stemming, you might end up with a q uery like\\n(multicore ORmulti-core) AND(chipORprocessor ORmicroprocessor) .\\nTo capture the generality and scope of the problem space to wh ich stand-\\ning queries belong, we now introduce the general notion of a classiﬁcation CLASSIFICATION\\nproblem. Given a set of classes , we seek to determine which class(es) a given\\nobject belongs to. In the example, the standing query serves to divide new\\nnewswire articles into the two classes: documentsaboutmulticorecomputerchips\\nanddocuments not about multicore computer chips . We refer to this as two-class\\nclassiﬁcation . Classiﬁcation using standing queries is also called routing or ROUTING\\nﬁltering and will be discussed further in Section 15.3.1 (page 335). FILTERING\\nA class need not be as narrowly focused as the standing query multicore\\ncomputer chips . Often, a class is a more general subject area like China orcoffee .\\nSuch more general classes are usually referred to as topics , and the classiﬁca-\\ntion task is then called text classiﬁcation ,text categorization ,topic classiﬁcation , TEXT CLASSIFICATION\\nortopic spotting . An example for China appears in Figure 13.1. Standing\\nqueries and topics differ in their degree of speciﬁcity, but the methods for', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 289}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP254 13 Text classiﬁcation and Naive Bayes\\nsolving routing, ﬁltering, and text classiﬁcation are esse ntially the same. We\\ntherefore include routing and ﬁltering under the rubric of t ext classiﬁcation\\nin this and the following chapters.\\nThe notion of classiﬁcation is very general and has many appl ications within\\nand beyond information retrieval (IR). For instance, in com puter vision, a\\nclassiﬁer may be used to divide images into classes such as landscape ,por-\\ntrait, and neither . We focus here on examples from information retrieval such\\nas:\\n•Several of the preprocessing steps necessary for indexing a s discussed in\\nChapter 2: detecting a document’s encoding (ASCII, Unicode UTF-8 etc ;\\npage 20); word segmentation (Is the white space between two letters a\\nword boundary or not? page 24 ) ; truecasing (page 30); and identifying\\nthe language of a document (page 46).\\n•The automatic detection of spam pages (which then are not inc luded in\\nthe search engine index).\\n•The automatic detection of sexually explicit content (whic h is included in\\nsearch results only if the user turns an option such as SafeSe arch off).\\n•Sentiment detection or the automatic classiﬁcation of a movie or product SENTIMENT DETECTION\\nreview as positive or negative. An example application is a u ser search-\\ning for negative reviews before buying a camera to make sure i t has no\\nundesirable features or quality problems.\\n•Personal email sorting . A user may have folders like talk announcements , EMAIL SORTING\\nelectronic bills ,email from family and friends , and so on, and may want a\\nclassiﬁer to classify each incoming email and automaticall y move it to the\\nappropriate folder. It is easier to ﬁnd messages in sorted fo lders than in\\na very large inbox. The most common case of this application i s a spam\\nfolder that holds all suspected spam messages.\\n•Topic-speciﬁc or vertical search. Vertical search engines restrict searches to VERTICAL SEARCH\\nENGINE a particular topic. For example, the query computer science on a vertical\\nsearch engine for the topic China will return a list of Chinese computer\\nscience departments with higher precision and recall than t he query com-\\nputerscienceChina on a general purpose search engine. This is because the\\nvertical search engine does not include web pages in its inde x that contain\\nthe term china in a different sense (e.g., referring to a hard white ceramic ),\\nbut does include relevant pages even if they do not explicitl y mention the\\ntermChina .\\n•Finally, the ranking function in ad hoc information retriev al can also be\\nbased on a document classiﬁer as we will explain in Section 15.4 (page 341).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 290}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP255\\nThis list shows the general importance of classiﬁcation in I R. Most retrieval\\nsystems today contain multiple components that use some for m of classiﬁer.\\nThe classiﬁcation task we will use as an example in this book i s text classiﬁ-\\ncation.\\nA computer is not essential for classiﬁcation. Many classiﬁ cation tasks\\nhave traditionally been solved manually. Books in a library are assigned\\nLibrary of Congress categories by a librarian. But manual cl assiﬁcation is\\nexpensive to scale. The multicore computer chips example illustrates one al-\\nternative approach: classiﬁcation by the use of standing qu eries – which can\\nbe thought of as rules – most commonly written by hand. As in our exam- RULES IN TEXT\\nCLASSIFICATION ple(multicore ORmulti-core) AND(chipORprocessor ORmicroprocessor) , rules are\\nsometimes equivalent to Boolean expressions.\\nA rule captures a certain combination of keywords that indic ates a class.\\nHand-coded rules have good scaling properties, but creatin g and maintain-\\ning them over time is labor intensive. A technically skilled person (e.g., a\\ndomain expert who is good at writing regular expressions) ca n create rule\\nsets that will rival or exceed the accuracy of the automatica lly generated clas-\\nsiﬁers we will discuss shortly; however, it can be hard to ﬁnd someone with\\nthis specialized skill.\\nApart from manual classiﬁcation and hand-crafted rules, th ere is a third\\napproach to text classiﬁcation, namely, machine learning- based text classiﬁ-\\ncation. It is the approach that we focus on in the next several chapters. In\\nmachine learning, the set of rules or, more generally, the de cision criterion of\\nthe text classiﬁer, is learned automatically from training data. This approach\\nis also called statistical text classiﬁcation if the learning method is statistical. STATISTICAL TEXT\\nCLASSIFICATION In statistical text classiﬁcation, we require a number of go od example docu-\\nments (or training documents) for each class. The need for ma nual classiﬁ-\\ncation is not eliminated because the training documents com e from a person\\nwho has labeled them – where labeling refers to the process of annotating LABELING\\neach document with its class. But labeling is arguably an eas ier task than\\nwriting rules. Almost anybody can look at a document and deci de whether\\nor not it is related to China. Sometimes such labeling is alre ady implicitly\\npart of an existing workﬂow. For instance, you may go through the news\\narticles returned by a standing query each morning and give r elevance feed-\\nback (cf. Chapter 9) by moving the relevant articles to a special folder like\\nmulticore-processors .\\nWe begin this chapter with a general introduction to the text classiﬁcation\\nproblem including a formal deﬁnition (Section 13.1); we then cover Naive\\nBayes, a particularly simple and effective classiﬁcation m ethod (Sections 13.2–\\n13.4). All of the classiﬁcation algorithms we study represent do cuments in\\nhigh-dimensional spaces. To improve the efﬁciency of these algorithms, it\\nis generally desirable to reduce the dimensionality of thes e spaces; to this\\nend, a technique known as feature selection is commonly applied in text clas-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 291}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP256 13 Text classiﬁcation and Naive Bayes\\nsiﬁcation as discussed in Section 13.5. Section 13.6 covers evaluation of text\\nclassiﬁcation. In the following chapters, Chapters 14and 15, we look at two\\nother families of classiﬁcation methods, vector space clas siﬁers and support\\nvector machines.\\n13.1 The text classiﬁcation problem\\nIn text classiﬁcation, we are given a description d∈Xof a document, where\\nXis the document space ; and a ﬁxed set of classes C={c1,c2, . . . , cJ}. Classes DOCUMENT SPACE\\nCLASS are also called categories orlabels . Typically, the document space Xis some\\ntype of high-dimensional space, and the classes are human de ﬁned for the\\nneeds of an application, as in the examples China and documents that talk\\nabout multicore computer chips above. We are given a training set Dof labeled TRAINING SET\\ndocuments⟨d,c⟩,where⟨d,c⟩∈X×C. For example:\\n⟨d,c⟩=⟨Beijing joins the World Trade Organization, China⟩\\nfor the one-sentence document Beijing joins the World Trade Organization and\\nthe class (or label) China .\\nUsing a learning method orlearning algorithm , we then wish to learn a clas- LEARNING METHOD\\nsiﬁer or classiﬁcation function γthat maps documents to classes: CLASSIFIER\\nγ:X→C (13.1)\\nThis type of learning is called supervised learning because a supervisor (the SUPERVISED LEARNING\\nhuman who deﬁnes the classes and labels training documents) serves as a\\nteacher directing the learning process. We denote the super vised learning\\nmethod by Γand write Γ(D) =γ. The learning method Γtakes the training\\nsetDas input and returns the learned classiﬁcation function γ.\\nMost names for learning methods Γare also used for classiﬁers γ. We\\ntalk about the Naive Bayes (NB) learning method Γwhen we say that “Naive\\nBayes is robust,” meaning that it can be applied to many diffe rent learning\\nproblems and is unlikely to produce classiﬁers that fail cat astrophically. But\\nwhen we say that “Naive Bayes had an error rate of 20%,” we are d escribing\\nan experiment in which a particular NB classiﬁer γ(which was produced by\\nthe NB learning method) had a 20% error rate in an application .\\nFigure 13.1 shows an example of text classiﬁcation from the Reuters-RCV 1\\ncollection, introduced in Section 4.2, page 69. There are six classes ( UK,China ,\\n. . . ,sports ), each with three training documents. We show a few mnemonic\\nwords for each document’s content. The training set provide s some typical\\nexamples for each class, so that we can learn the classiﬁcati on function γ.\\nOnce we have learned γ, we can apply it to the test set (ortest data ), for ex- TEST SET\\nample, the new document ﬁrst private Chinese airline whose class is unknown.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 292}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.1 The text classiﬁcation problem 257\\nclasses:\\ntraining\\nset:test\\nset:regions industries subject areasγ(d′) =China\\nﬁrst\\nprivate\\nChinese\\nairlineUK China poultry coffee elections sports\\nLondoncongestion\\nBig BenParliament\\nthe QueenWindsorBeijingOlympics\\nGreatWalltourism\\ncommunistMaochickenfeed\\nduckspate\\nturkeybirdﬂubeansroasting\\nrobustaarabica\\nharvestKenyavotesrecount\\nrun-offseat\\ncampaignTV adsbaseballdiamond\\nsoccerforward\\ncaptainteamd′\\n◮Figure 13.1 Classes, training set, and test set in text classiﬁcation .\\nIn Figure 13.1, the classiﬁcation function assigns the new document to cla ss\\nγ(d) =China , which is the correct assignment.\\nThe classes in text classiﬁcation often have some interesti ng structure such\\nas the hierarchy in Figure 13.1. There are two instances each of region cate-\\ngories, industry categories, and subject area categories. A hierarchy can be\\nan important aid in solving a classiﬁcation problem; see Sec tion 15.3.2 for\\nfurther discussion. Until then, we will make the assumption in the text clas-\\nsiﬁcation chapters that the classes form a set with no subset relationships\\nbetween them.\\nDeﬁnition ( 13.1) stipulates that a document is a member of exactly one\\nclass. This is not the most appropriate model for the hierarc hy in Figure 13.1.\\nFor instance, a document about the 2008 Olympics should be a m ember of\\ntwo classes: the China class and the sports class. This type of classiﬁcation\\nproblem is referred to as an any-of problem and we will return to it in Sec-\\ntion 14.5 (page 306). For the time being, we only consider one-of problems\\nwhere a document is a member of exactly one class.\\nOur goal in text classiﬁcation is high accuracy on test data o rnew data – for\\nexample, the newswire articles that we will encounter tomor row morning\\nin the multicore chip example. It is easy to achieve high accu racy on the\\ntraining set (e.g., we can simply memorize the labels). But h igh accuracy on\\nthe training set in general does not mean that the classiﬁer w ill work well on', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 293}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP258 13 Text classiﬁcation and Naive Bayes\\nnew data in an application. When we use the training set to lea rn a classiﬁer\\nfor test data, we make the assumption that training data and t est data are\\nsimilar or from the same distribution . We defer a precise deﬁnition of this\\nnotion to Section 14.6 (page 308).\\n13.2 Naive Bayes text classiﬁcation\\nThe ﬁrst supervised learning method we introduce is the multinomial Naive MULTINOMIAL NAIVE\\nBAYES Bayes ormultinomial NB model, a probabilistic learning method. The proba-\\nbility of a document dbeing in class cis computed as\\nP(c|d)∝P(c)∏\\n1≤k≤ndP(tk|c) (13.2)\\nwhere P(tk|c)is the conditional probability of term tkoccurring in a docu-\\nment of class c.1We interpret P(tk|c)as a measure of how much evidence\\ntkcontributes that cis the correct class. P(c)is the prior probability of a\\ndocument occurring in class c. If a document’s terms do not provide clear\\nevidence for one class versus another, we choose the one that has a higher\\nprior probability.⟨t1,t2, . . . , tnd⟩are the tokens in dthat are part of the vocab-\\nulary we use for classiﬁcation and ndis the number of such tokens in d. For\\nexample,⟨t1,t2, . . . , tnd⟩for the one-sentence document Beijing and Taipei join\\nthe WTO might be⟨Beijing ,Taipei ,join,WTO⟩, with nd=4, if we treat the terms\\nandandtheas stop words.\\nIn text classiﬁcation, our goal is to ﬁnd the bestclass for the document. The\\nbest class in NB classiﬁcation is the most likely or maximum a posteriori (MAP) MAXIMUM A\\nPOSTERIORI CLASS class cmap:\\ncmap=arg max\\nc∈CˆP(c|d) =arg max\\nc∈CˆP(c)∏\\n1≤k≤ndˆP(tk|c). (13.3)\\nWe write ˆPforPbecause we do not know the true values of the parameters\\nP(c)and P(tk|c), but estimate them from the training set as we will see in a\\nmoment.\\nIn Equation ( 13.3), many conditional probabilities are multiplied, one for\\neach position 1≤k≤nd. This can result in a ﬂoating point underﬂow.\\nIt is therefore better to perform the computation by adding l ogarithms of\\nprobabilities instead of multiplying probabilities. The c lass with the highest\\nlog probability score is still the most probable; log (xy) = log(x) +log(y)\\nand the logarithm function is monotonic. Hence, the maximiz ation that is\\n1. We will explain in the next section why P(c|d)is proportional to ( ∝), not equal to the quantity\\non the right.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 294}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.2 Naive Bayes text classiﬁcation 259\\nactually done in most implementations of NB is:\\ncmap=arg max\\nc∈C[logˆP(c) +∑\\n1≤k≤ndlogˆP(tk|c)]. (13.4)\\nEquation ( 13.4) has a simple interpretation. Each conditional parameter\\nlogˆP(tk|c)is a weight that indicates how good an indicator tkis for c. Sim-\\nilarly, the prior log ˆP(c)is a weight that indicates the relative frequency of\\nc. More frequent classes are more likely to be the correct clas s than infre-\\nquent classes. The sum of log prior and term weights is then a m easure of\\nhow much evidence there is for the document being in the class , and Equa-\\ntion ( 13.4) selects the class for which we have the most evidence.\\nWe will initially work with this intuitive interpretation o f the multinomial\\nNB model and defer a formal derivation to Section 13.4.\\nHow do we estimate the parameters ˆP(c)and ˆP(tk|c)? We ﬁrst try the\\nmaximum likelihood estimate (MLE; Section 11.3.2 , page 226), which is sim-\\nply the relative frequency and corresponds to the most likel y value of each\\nparameter given the training data. For the priors this estim ate is:\\nˆP(c) =Nc\\nN, (13.5)\\nwhere Ncis the number of documents in class cand Nis the total number of\\ndocuments.\\nWe estimate the conditional probability ˆP(t|c)as the relative frequency of\\nterm tin documents belonging to class c:\\nˆP(t|c) =Tct\\n∑t′∈VTct′, (13.6)\\nwhere Tctis the number of occurrences of tin training documents from class\\nc, including multiple occurrences of a term in a document. We h ave made the\\npositional independence assumption here, which we will discuss in more detail\\nin the next section: Tctis a count of occurrences in all positions kin the doc-\\numents in the training set. Thus, we do not compute different estimates for\\ndifferent positions and, for example, if a word occurs twice in a document,\\nin positions k1and k2, then ˆP(tk1|c) = ˆP(tk2|c).\\nThe problem with the MLE estimate is that it is zero for a term– class combi-\\nnation that did not occur in the training data. If the term WTO in the training\\ndata only occurred in China documents, then the MLE estimates for the other\\nclasses, for example UK, will be zero:\\nˆP(WTO|UK) =0.\\nNow, the one-sentence document Britain is a member of the WTO will get a\\nconditional probability of zero for UKbecause we are multiplying the condi-\\ntional probabilities for all terms in Equation ( 13.2). Clearly, the model should', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 295}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP260 13 Text classiﬁcation and Naive Bayes\\nTRAIN MULTINOMIAL NB(C,D)\\n1V←EXTRACT VOCABULARY (D)\\n2N←COUNT DOCS(D)\\n3for each c∈C\\n4doNc←COUNT DOCSINCLASS(D,c)\\n5 prior[c]←Nc/N\\n6 text c←CONCATENATE TEXTOFALLDOCSINCLASS(D,c)\\n7 for each t∈V\\n8 doTct←COUNT TOKENS OFTERM(text c,t)\\n9 for each t∈V\\n10 docondprob [t][c]←Tct+1\\n∑t′(Tct′+1)\\n11 return V,prior ,condprob\\nAPPLY MULTINOMIAL NB(C,V,prior ,condprob ,d)\\n1W←EXTRACT TOKENS FROM DOC(V,d)\\n2for each c∈C\\n3doscore[c]←logprior[c]\\n4 for each t∈W\\n5 doscore[c] += logcondprob [t][c]\\n6return arg maxc∈Cscore[c]\\n◮Figure 13.2 Naive Bayes algorithm (multinomial model): Training and te sting.\\nassign a high probability to the UKclass because the term Britain occurs. The\\nproblem is that the zero probability for WTO cannot be “conditioned away,”\\nno matter how strong the evidence for the class UKfrom other features. The\\nestimate is 0 because of sparseness : The training data are never large enough SPARSENESS\\nto represent the frequency of rare events adequately, for ex ample, the fre-\\nquency of WTO occurring in UKdocuments.\\nTo eliminate zeros, we use add-one orLaplace smoothing , which simply adds ADD -ONE SMOOTHING\\none to each count (cf. Section 11.3.2 ):\\nˆP(t|c) =Tct+1\\n∑t′∈V(Tct′+1)=Tct+1\\n(∑t′∈VTct′) +B, (13.7)\\nwhere B=|V|is the number of terms in the vocabulary. Add-one smoothing\\ncan be interpreted as a uniform prior (each term occurs once f or each class)\\nthat is then updated as evidence from the training data comes in. Note that\\nthis is a prior probability for the occurrence of a term as opposed to the prior\\nprobability of a class which we estimate in Equation ( 13.5) on the document\\nlevel.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 296}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.2 Naive Bayes text classiﬁcation 261\\n◮Table 13.1 Data for parameter estimation examples.\\ndocID words in document in c=China ?\\ntraining set 1 Chinese Beijing Chinese yes\\n2 Chinese Chinese Shanghai yes\\n3 Chinese Macao yes\\n4 Tokyo Japan Chinese no\\ntest set 5 Chinese Chinese Chinese Tokyo Japan ?\\n◮Table 13.2 Training and test times for NB.\\nmode time complexity\\ntraining Θ(|D|Lave+|C||V|)\\ntesting Θ(La+|C|Ma) =Θ(|C|Ma)\\nWe have now introduced all the elements we need for training a nd apply-\\ning an NB classiﬁer. The complete algorithm is described in F igure 13.2.\\n✎Example 13.1: For the example in Table 13.1, the multinomial parameters we\\nneed to classify the test document are the priors ˆP(c) =3/4 and ˆP(c) =1/4 and the\\nfollowing conditional probabilities:\\nˆP(Chinese|c) = ( 5+1)/(8+6) =6/14=3/7\\nˆP(Tokyo|c) = ˆP(Japan|c) = ( 0+1)/(8+6) =1/14\\nˆP(Chinese|c) = ( 1+1)/(3+6) =2/9\\nˆP(Tokyo|c) = ˆP(Japan|c) = ( 1+1)/(3+6) =2/9\\nThe denominators are (8+6)and(3+6)because the lengths of text cand text care 8\\nand 3, respectively, and because the constant Bin Equation ( 13.7) is 6 as the vocabu-\\nlary consists of six terms.\\nWe then get:\\nˆP(c|d5)∝3/4·(3/7)3·1/14·1/14≈0.0003.\\nˆP(c|d5)∝1/4·(2/9)3·2/9·2/9≈0.0001.\\nThus, the classiﬁer assigns the test document to c=China . The reason for this clas-\\nsiﬁcation decision is that the three occurrences of the posi tive indicator Chinese ind5\\noutweigh the occurrences of the two negative indicators Japan andTokyo .\\nWhat is the time complexity of NB? The complexity of computin g the pa-\\nrameters is Θ(|C||V|)because the set of parameters consists of |C||V|con-\\nditional probabilities and |C|priors. The preprocessing necessary for com-\\nputing the parameters (extracting the vocabulary, countin g terms, etc.) can\\nbe done in one pass through the training data. The time comple xity of this', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 297}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP262 13 Text classiﬁcation and Naive Bayes\\ncomponent is therefore Θ(|D|Lave), where|D|is the number of documents\\nand Laveis the average length of a document.\\nWe use Θ(|D|Lave)as a notation for Θ(T)here, where Tis the length of the\\ntraining collection. This is nonstandard; Θ(.)is not deﬁned for an average.\\nWe prefer expressing the time complexity in terms of Dand Lavebecause\\nthese are the primary statistics used to characterize train ing collections.\\nThe time complexity of A PPLY MULTINOMIAL NB in Figure 13.2 isΘ(|C|La).\\nLaand Maare the numbers of tokens and types, respectively, in the tes t doc-\\nument. A PPLY MULTINOMIAL NB can be modiﬁed to be Θ(La+|C|Ma)(Ex-\\nercise 13.8). Finally, assuming that the length of test documents is bou nded,\\nΘ(La+|C|Ma) =Θ(|C|Ma)because La<b|C|Mafor a ﬁxed constant b.2\\nTable 13.2 summarizes the time complexities. In general, we have |C||V|<\\n|D|Lave, so both training and testing complexity are linear in the ti me it takes\\nto scan the data. Because we have to look at the data at least on ce, NB can be\\nsaid to have optimal time complexity. Its efﬁciency is one re ason why NB is\\na popular text classiﬁcation method.\\n13.2.1 Relation to multinomial unigram language model\\nThe multinomial NB model is formally identical to the multin omial unigram\\nlanguage model (Section 12.2.1 , page 242). In particular, Equation ( 13.2) is\\na special case of Equation ( 12.12 ) from page 243, which we repeat here for\\nλ=1:\\nP(d|q)∝P(d)∏\\nt∈qP(t|Md). (13.8)\\nThe document din text classiﬁcation (Equation ( 13.2)) takes the role of the\\nquery in language modeling (Equation ( 13.8)) and the classes cin text clas-\\nsiﬁcation take the role of the documents din language modeling. We used\\nEquation ( 13.8) to rank documents according to the probability that they ar e\\nrelevant to the query q. In NB classiﬁcation, we are usually only interested\\nin the top-ranked class.\\nWe also used MLE estimates in Section 12.2.2 (page 243) and encountered\\nthe problem of zero estimates owing to sparse data (page 244); but instead\\nof add-one smoothing, we used a mixture of two distributions to address the\\nproblem there. Add-one smoothing is closely related to add-1\\n2smoothing in\\nSection 11.3.4 (page 228).\\n?Exercise 13.1\\nWhy is|C||V|<|D|Lavein Table 13.2 expected to hold for most text collections?\\n2. Our assumption here is that the length of test documents is bounded. Lawould exceed\\nb|C|Mafor extremely long test documents.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 298}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.3 The Bernoulli model 263\\nTRAIN BERNOULLI NB(C,D)\\n1V←EXTRACT VOCABULARY (D)\\n2N←COUNT DOCS(D)\\n3for each c∈C\\n4doNc←COUNT DOCSINCLASS(D,c)\\n5 prior[c]←Nc/N\\n6 for each t∈V\\n7 doNct←COUNT DOCSINCLASS CONTAINING TERM(D,c,t)\\n8 condprob [t][c]←(Nct+1)/(Nc+2)\\n9return V,prior ,condprob\\nAPPLY BERNOULLI NB(C,V,prior ,condprob ,d)\\n1Vd←EXTRACT TERMS FROM DOC(V,d)\\n2for each c∈C\\n3doscore[c]←logprior[c]\\n4 for each t∈V\\n5 do if t∈Vd\\n6 then score[c] += logcondprob [t][c]\\n7 else score[c] += log(1−condprob [t][c])\\n8return arg maxc∈Cscore[c]\\n◮Figure 13.3 NB algorithm (Bernoulli model): Training and testing. The a dd-one\\nsmoothing in Line 8 (top) is in analogy to Equation ( 13.7) with B=2.\\n13.3 The Bernoulli model\\nThere are two different ways we can set up an NB classiﬁer. The model we in-\\ntroduced in the previous section is the multinomial model. I t generates one\\nterm from the vocabulary in each position of the document, wh ere we as-\\nsume a generative model that will be discussed in more detail in Section 13.4\\n(see also page 237).\\nAn alternative to the multinomial model is the multivariate Bernoulli model\\norBernoulli model . It is equivalent to the binary independence model of Sec- BERNOULLI MODEL\\ntion 11.3 (page 222), which generates an indicator for each term of the vo-\\ncabulary, either 1 indicating presence of the term in the doc ument or 0 indi-\\ncating absence. Figure 13.3 presents training and testing algorithms for the\\nBernoulli model. The Bernoulli model has the same time compl exity as the\\nmultinomial model.\\nThe different generation models imply different estimatio n strategies and\\ndifferent classiﬁcation rules. The Bernoulli model estima tesˆP(t|c)as the frac-\\ntion of documents of class cthat contain term t(Figure 13.3, TRAIN BERNOULLI -', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 299}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP264 13 Text classiﬁcation and Naive Bayes\\nNB, line 8). In contrast, the multinomial model estimates ˆP(t|c)as the frac-\\ntion of tokens orfraction of positions in documents of class cthat contain term\\nt(Equation ( 13.7)). When classifying a test document, the Bernoulli model\\nuses binary occurrence information, ignoring the number of occurrences,\\nwhereas the multinomial model keeps track of multiple occur rences. As a\\nresult, the Bernoulli model typically makes many mistakes w hen classifying\\nlong documents. For example, it may assign an entire book to t he class China\\nbecause of a single occurrence of the term China .\\nThe models also differ in how nonoccurring terms are used in c lassiﬁca-\\ntion. They do not affect the classiﬁcation decision in the mu ltinomial model;\\nbut in the Bernoulli model the probability of nonoccurrence is factored in\\nwhen computing P(c|d)(Figure 13.3, APPLY BERNOULLI NB, Line 7). This is\\nbecause only the Bernoulli NB model models absence of terms e xplicitly.\\n✎Example 13.2: Applying the Bernoulli model to the example in Table 13.1, we\\nhave the same estimates for the priors as before: ˆP(c) = 3/4, ˆP(c) = 1/4. The\\nconditional probabilities are:\\nˆP(Chinese|c) = ( 3+1)/(3+2) =4/5\\nˆP(Japan|c) = ˆP(Tokyo|c) = ( 0+1)/(3+2) =1/5\\nˆP(Beijing|c) = ˆP(Macao|c) = ˆP(Shanghai|c) = ( 1+1)/(3+2) =2/5\\nˆP(Chinese|c) = ( 1+1)/(1+2) =2/3\\nˆP(Japan|c) = ˆP(Tokyo|c) = ( 1+1)/(1+2) =2/3\\nˆP(Beijing|c) = ˆP(Macao|c) = ˆP(Shanghai|c) = ( 0+1)/(1+2) =1/3\\nThe denominators are (3+2)and(1+2)because there are three documents in c\\nand one document in cand because the constant Bin Equation ( 13.7) is 2 – there are\\ntwo cases to consider for each term, occurrence and nonoccur rence.\\nThe scores of the test document for the two classes are\\nˆP(c|d5)∝ ˆP(c)·ˆP(Chinese|c)·ˆP(Japan|c)·ˆP(Tokyo|c)\\n·(1−ˆP(Beijing|c))·(1−ˆP(Shanghai|c))·(1−ˆP(Macao|c))\\n= 3/4·4/5·1/5·1/5·(1−2/5)·(1−2/5)·(1−2/5)\\n≈ 0.005\\nand, analogously,\\nˆP(c|d5)∝1/4·2/3·2/3·2/3·(1−1/3)·(1−1/3)·(1−1/3)\\n≈ 0.022\\nThus, the classiﬁer assigns the test document to c=not-China . When looking only\\nat binary occurrence and not at term frequency, Japan andTokyo are indicators for c\\n(2/3>1/5) and the conditional probabilities of Chinese forcand care not different\\nenough (4/5 vs. 2/3) to affect the classiﬁcation decision.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 300}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.4 Properties of Naive Bayes 265\\n13.4 Properties of Naive Bayes\\nTo gain a better understanding of the two models and the assum ptions they\\nmake, let us go back and examine how we derived their classiﬁc ation rules in\\nChapters 11and 12. We decide class membership of a document by assigning\\nit to the class with the maximum a posteriori probability (cf . Section 11.3.2 ,\\npage 226), which we compute as follows:\\ncmap=arg max\\nc∈CP(c|d)\\n=arg max\\nc∈CP(d|c)P(c)\\nP(d)(13.9)\\n=arg max\\nc∈CP(d|c)P(c), (13.10)\\nwhere Bayes’ rule (Equation ( 11.4), page 220) is applied in ( 13.9) and we drop\\nthe denominator in the last step because P(d)is the same for all classes and\\ndoes not affect the argmax.\\nWe can interpret Equation ( 13.10 ) as a description of the generative process\\nwe assume in Bayesian text classiﬁcation. To generate a docu ment, we ﬁrst\\nchoose class cwith probability P(c)(top nodes in Figures 13.4 and 13.5). The\\ntwo models differ in the formalization of the second step, th e generation of\\nthe document given the class, corresponding to the conditio nal distribution\\nP(d|c):\\nMultinomial P(d|c) = P(⟨t1, . . . , tk, . . . , tnd⟩|c) (13.11)\\nBernoulli P(d|c) = P(⟨e1, . . . , ei, . . . , eM⟩|c), (13.12)\\nwhere⟨t1, . . . , tnd⟩is the sequence of terms as it occurs in d(minus terms\\nthat were excluded from the vocabulary) and ⟨e1, . . . , ei, . . . , eM⟩is a binary\\nvector of dimensionality Mthat indicates for each term whether it occurs in\\ndor not.\\nIt should now be clearer why we introduced the document space Xin\\nEquation ( 13.1) when we deﬁned the classiﬁcation problem. A critical step\\nin solving a text classiﬁcation problem is to choose the docu ment represen-\\ntation.⟨t1, . . . , tnd⟩and⟨e1, . . . , eM⟩are two different document representa-\\ntions. In the ﬁrst case, Xis the set of all term sequences (or, more precisely,\\nsequences of term tokens). In the second case, Xis{0, 1}M.\\nWe cannot use Equations ( 13.11 ) and ( 13.12 ) for text classiﬁcation directly.\\nFor the Bernoulli model, we would have to estimate 2M|C|different param-\\neters, one for each possible combination of Mvalues eiand a class. The\\nnumber of parameters in the multinomial case has the same ord er of magni-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 301}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP266 13 Text classiﬁcation and Naive Bayes\\nC=China\\nX1=Beijing X2=and X3=Taipei X4=join X5=WTO\\n◮Figure 13.4 The multinomial NB model.\\ntude.3This being a very large quantity, estimating these paramete rs reliably\\nis infeasible.\\nTo reduce the number of parameters, we make the Naive Bayes conditional CONDITIONAL\\nINDEPENDENCE\\nASSUMPTIONindependence assumption . We assume that attribute values are independent of\\neach other given the class:\\nMultinomial P(d|c) = P(⟨t1, . . . , tnd⟩|c) =∏\\n1≤k≤ndP(Xk=tk|c) (13.13)\\nBernoulli P(d|c) = P(⟨e1, . . . , eM⟩|c) =∏\\n1≤i≤MP(Ui=ei|c). (13.14)\\nWe have introduced two random variables here to make the two d ifferent\\ngenerative models explicit. Xkis the random variable for position kin the RANDOM VARIABLE X\\ndocument and takes as values terms from the vocabulary. P(Xk=t|c)is the\\nprobability that in a document of class cthe term twill occur in position k.UiRANDOM VARIABLE U\\nis the random variable for vocabulary term iand takes as values 0 (absence)\\nand 1 (presence). ˆP(Ui=1|c)is the probability that in a document of class c\\nthe term tiwill occur – in any position and possibly multiple times.\\nWe illustrate the conditional independence assumption in F igures 13.4 and 13.5.\\nThe class China generates values for each of the ﬁve term attributes (multi-\\nnomial) or six binary attributes (Bernoulli) with a certain probability, inde-\\npendent of the values of the other attributes. The fact that a document in the\\nclass China contains the term Taipei does not make it more likely or less likely\\nthat it also contains Beijing .\\nIn reality, the conditional independence assumption does n ot hold for text\\ndata. Terms areconditionally dependent on each other. But as we will dis-\\ncuss shortly, NB models perform well despite the conditiona l independence\\nassumption.\\n3. In fact, if the length of documents is not bounded, the numb er of parameters in the multino-\\nmial case is inﬁnite.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 302}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.4 Properties of Naive Bayes 267\\nUAlaska =0 UBeijing =1 UIndia=0 Ujoin=1 UTaipei =1 UWTO =1C=China\\n◮Figure 13.5 The Bernoulli NB model.\\nEven when assuming conditional independence, we still have too many\\nparameters for the multinomial model if we assume a differen t probability\\ndistribution for each position kin the document. The position of a term in a\\ndocument by itself does not carry information about the clas s. Although\\nthere is a difference between China sues France and France sues China , the\\noccurrence of China in position 1 versus position 3 of the document is not\\nuseful in NB classiﬁcation because we look at each term separ ately. The con-\\nditional independence assumption commits us to this way of p rocessing the\\nevidence.\\nAlso, if we assumed different term distributions for each po sition k, we\\nwould have to estimate a different set of parameters for each k. The probabil-\\nity ofbean appearing as the ﬁrst term of a coffee document could be different\\nfrom it appearing as the second term, and so on. This again cau ses problems\\nin estimation owing to data sparseness.\\nFor these reasons, we make a second independence assumption for the\\nmultinomial model, positional independence : The conditional probabilities for POSITIONAL\\nINDEPENDENCE a term are the same independent of position in the document.\\nP(Xk1=t|c) =P(Xk2=t|c)\\nfor all positions k1,k2, terms tand classes c. Thus, we have a single dis-\\ntribution of terms that is valid for all positions kiand we can use Xas its\\nsymbol.4Positional independence is equivalent to adopting the bag o f words\\nmodel, which we introduced in the context of ad hoc retrieval in Chapter 6\\n(page 117).\\nWith conditional and positional independence assumptions , we only need\\nto estimate Θ(M|C|)parameters P(tk|c)(multinomial model) or P(ei|c)(Bernoulli\\n4. Our terminology is nonstandard. The random variable Xis a categorical variable, not a multi-\\nnomial variable, and the corresponding NB model should perh aps be called a sequence model . We\\nhave chosen to present this sequence model and the multinomi al model in Section 13.4.1 as the\\nsame model because they are computationally identical.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 303}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP268 13 Text classiﬁcation and Naive Bayes\\n◮Table 13.3 Multinomial versus Bernoulli model.\\nmultinomial model Bernoulli model\\nevent model generation of token generation of document\\nrandom variable(s) X=tifftoccurs at given pos Ut=1 iff toccurs in doc\\ndocument representation d=⟨t1, . . . , tk, . . . , tnd⟩,tk∈V d =⟨e1, . . . , ei, . . . , eM⟩,\\nei∈{0, 1}\\nparameter estimation ˆP(X=t|c) ˆP(Ui=e|c)\\ndecision rule: maximize ˆP(c)∏1≤k≤ndˆP(X=tk|c) ˆP(c)∏ti∈VˆP(Ui=ei|c)\\nmultiple occurrences taken into account ignored\\nlength of docs can handle longer docs works best for short doc s\\n# features can handle more works best with fewer\\nestimate for term the ˆP(X=the|c)≈0.05 ˆP(Uthe=1|c)≈1.0\\nmodel), one for each term–class combination, rather than a n umber that is\\nat least exponential in M, the size of the vocabulary. The independence\\nassumptions reduce the number of parameters to be estimated by several\\norders of magnitude.\\nTo summarize, we generate a document in the multinomial mode l (Fig-\\nure13.4) by ﬁrst picking a class C=cwith P(c)where Cis a random variable RANDOM VARIABLE C\\ntaking values from Cas values. Next we generate term tkin position kwith\\nP(Xk=tk|c)for each of the ndpositions of the document. The Xkall have\\nthe same distribution over terms for a given c. In the example in Figure 13.4,\\nwe show the generation of ⟨t1,t2,t3,t4,t5⟩=⟨Beijing ,and,Taipei ,join,WTO⟩,\\ncorresponding to the one-sentence document Beijing and Taipei join WTO .\\nFor a completely speciﬁed document generation model, we wou ld also\\nhave to deﬁne a distribution P(nd|c)over lengths. Without it, the multino-\\nmial model is a token generation model rather than a document generation\\nmodel.\\nWe generate a document in the Bernoulli model (Figure 13.5) by ﬁrst pick-\\ning a class C=cwith P(c)and then generating a binary indicator eifor each\\nterm tiof the vocabulary (1 ≤i≤M). In the example in Figure 13.5, we\\nshow the generation of ⟨e1,e2,e3,e4,e5,e6⟩=⟨0, 1, 0, 1, 1, 1⟩, corresponding,\\nagain, to the one-sentence document Beijing and Taipei join WTO where we\\nhave assumed that andis a stop word.\\nWe compare the two models in Table 13.3, including estimation equations\\nand decision rules.\\nNaive Bayes is so called because the independence assumptio ns we have\\njust made are indeed very naive for a model of natural languag e. The condi-\\ntional independence assumption states that features are in dependent of each\\nother given the class. This is hardly ever true for terms in do cuments. In\\nmany cases, the opposite is true. The pairs hong andkong orlondon anden-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 304}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.4 Properties of Naive Bayes 269\\n◮Table 13.4 Correct estimation implies accurate prediction, but accur ate predic-\\ntion does not imply correct estimation.\\nc1 c2 class selected\\ntrue probability P(c|d) 0.6 0.4 c1\\nˆP(c)∏1≤k≤ndˆP(tk|c)(Equation ( 13.13 )) 0.00099 0.00001\\nNB estimate ˆP(c|d) 0.99 0.01 c1\\nglish in Figure 13.7 are examples of highly dependent terms. In addition, the\\nmultinomial model makes an assumption of positional indepe ndence. The\\nBernoulli model ignores positions in documents altogether because it only\\ncares about absence or presence. This bag-of-words model di scards all in-\\nformation that is communicated by the order of words in natur al language\\nsentences. How can NB be a good text classiﬁer when its model o f natural\\nlanguage is so oversimpliﬁed?\\nThe answer is that even though the probability estimates of NB are of low\\nquality, its classiﬁcation decisions are surprisingly good. Consider a document\\ndwith true probabilities P(c1|d) = 0.6 and P(c2|d) = 0.4 as shown in Ta-\\nble13.4. Assume that dcontains many terms that are positive indicators for\\nc1and many terms that are negative indicators for c2. Thus, when using the\\nmultinomial model in Equation ( 13.13 ),ˆP(c1)∏1≤k≤ndˆP(tk|c1)will be much\\nlarger than ˆP(c2)∏1≤k≤ndˆP(tk|c2)(0.00099 vs. 0.00001 in the table). After di-\\nvision by 0.001 to get well-formed probabilities for P(c|d), we end up with\\none estimate that is close to 1.0 and one that is close to 0.0. T his is common:\\nThe winning class in NB classiﬁcation usually has a much larg er probabil-\\nity than the other classes and the estimates diverge very sig niﬁcantly from\\nthe true probabilities. But the classiﬁcation decision is b ased on which class\\ngets the highest score. It does not matter how accurate the es timates are. De-\\nspite the bad estimates, NB estimates a higher probability f orc1and therefore\\nassigns dto the correct class in Table 13.4.Correct estimation implies accurate\\nprediction, but accurate prediction does not imply correct estimation. NB classiﬁers\\nestimate badly, but often classify well.\\nEven if it is not the method with the highest accuracy for text , NB has many\\nvirtues that make it a strong contender for text classiﬁcati on. It excels if there\\nare many equally important features that jointly contribut e to the classiﬁ-\\ncation decision. It is also somewhat robust to noise feature s (as deﬁned in\\nthe next section) and concept drift – the gradual change over time of the con- CONCEPT DRIFT\\ncept underlying a class like US president from Bill Clinton to George W. Bush\\n(see Section 13.7). Classiﬁers like kNN (Section 14.3, page 297) can be care-\\nfully tuned to idiosyncratic properties of a particular tim e period. This will\\nthen hurt them when documents in the following time period ha ve slightly', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 305}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP270 13 Text classiﬁcation and Naive Bayes\\n◮Table 13.5 A set of documents for which the NB independence assumptions are\\nproblematic.\\n(1) He moved from London, Ontario, to London, England.\\n(2) He moved from London, England, to London, Ontario.\\n(3) He moved from England to London, Ontario.\\ndifferent properties.\\nThe Bernoulli model is particularly robust with respect to c oncept drift.\\nWe will see in Figure 13.8 that it can have decent performance when using\\nfewer than a dozen terms. The most important indicators for a class are less\\nlikely to change. Thus, a model that only relies on these feat ures is more\\nlikely to maintain a certain level of accuracy in concept dri ft.\\nNB’s main strength is its efﬁciency: Training and classiﬁca tion can be ac-\\ncomplished with one pass over the data. Because it combines e fﬁciency with\\ngood accuracy it is often used as a baseline in text classiﬁca tion research.\\nIt is often the method of choice if (i) squeezing out a few extr a percentage\\npoints of accuracy is not worth the trouble in a text classiﬁc ation application,\\n(ii) a very large amount of training data is available and the re is more to be\\ngained from training on a lot of data than using a better class iﬁer on a smaller\\ntraining set, or (iii) if its robustness to concept drift can be exploited.\\nIn this book, we discuss NB as a classiﬁer for text. The indepe ndence as-\\nsumptions do not hold for text. However, it can be shown that N B is an\\noptimal classiﬁer (in the sense of minimal error rate on new data) for data OPTIMAL CLASSIFIER\\nwhere the independence assumptions do hold.\\n13.4.1 A variant of the multinomial model\\nAn alternative formalization of the multinomial model repr esents each doc-\\nument das an M-dimensional vector of counts ⟨tft1,d, . . . , tf tM,d⟩where tf ti,d\\nis the term frequency of tiind.P(d|c)is then computed as follows (cf. Equa-\\ntion ( 12.8), page 243);\\nP(d|c) =P(⟨tft1,d, . . . , tf tM,d⟩|c)∝∏\\n1≤i≤MP(X=ti|c)tfti,d(13.15)\\nNote that we have omitted the multinomial factor. See Equati on (12.8) (page 243).\\nEquation ( 13.15 ) is equivalent to the sequence model in Equation ( 13.2) as\\nP(X=ti|c)tfti,d=1 for terms that do not occur in d(tfti,d=0) and a term\\nthat occurs tf ti,d≥1 times will contribute tf ti,dfactors both in Equation ( 13.2)\\nand in Equation ( 13.15 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 306}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.5 Feature selection 271\\nSELECT FEATURES (D,c,k)\\n1V←EXTRACT VOCABULARY (D)\\n2L←[]\\n3for each t∈V\\n4doA(t,c)←COMPUTE FEATURE UTILITY (D,t,c)\\n5 A PPEND (L,⟨A(t,c),t⟩)\\n6return FEATURES WITHLARGEST VALUES (L,k)\\n◮Figure 13.6 Basic feature selection algorithm for selecting the kbest features.\\n?Exercise 13.2 [⋆]\\nWhich of the documents in Table 13.5 have identical and different bag of words rep-\\nresentations for (i) the Bernoulli model (ii) the multinomi al model? If there are differ-\\nences, describe them.\\nExercise 13.3\\nThe rationale for the positional independence assumption i s that there is no useful\\ninformation in the fact that a term occurs in position kof a document. Find exceptions.\\nConsider formulaic documents with a ﬁxed document structur e.\\nExercise 13.4\\nTable 13.3 gives Bernoulli and multinomial estimates for the word the. Explain the\\ndifference.\\n13.5 Feature selection\\nFeature selection is the process of selecting a subset of the terms occurring FEATURE SELECTION\\nin the training set and using only this subset as features in t ext classiﬁca-\\ntion. Feature selection serves two main purposes. First, it makes training\\nand applying a classiﬁer more efﬁcient by decreasing the siz e of the effective\\nvocabulary. This is of particular importance for classiﬁer s that, unlike NB,\\nare expensive to train. Second, feature selection often inc reases classiﬁca-\\ntion accuracy by eliminating noise features. A noise feature is one that, when NOISE FEATURE\\nadded to the document representation, increases the classi ﬁcation error on\\nnew data. Suppose a rare term, say arachnocentric , has no information about\\na class, say China , but all instances of arachnocentric happen to occur in China\\ndocuments in our training set. Then the learning method migh t produce a\\nclassiﬁer that misassigns test documents containing arachnocentric toChina .\\nSuch an incorrect generalization from an accidental proper ty of the training\\nset is called overﬁtting . OVERFITTING\\nWe can view feature selection as a method for replacing a comp lex clas-\\nsiﬁer (using all features) with a simpler one (using a subset of the features).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 307}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP272 13 Text classiﬁcation and Naive Bayes\\nIt may appear counterintuitive at ﬁrst that a seemingly weak er classiﬁer is\\nadvantageous in statistical text classiﬁcation, but when d iscussing the bias-\\nvariance tradeoff in Section 14.6 (page 308), we will see that weaker models\\nare often preferable when limited training data are availab le.\\nThe basic feature selection algorithm is shown in Figure 13.6. For a given\\nclass c, we compute a utility measure A(t,c)for each term of the vocabulary\\nand select the kterms that have the highest values of A(t,c). All other terms\\nare discarded and not used in classiﬁcation. We will introdu ce three different\\nutility measures in this section: mutual information, A(t,c) = I(Ut;Cc); the\\nχ2test, A(t,c) =X2(t,c); and frequency, A(t,c) =N(t,c).\\nOf the two NB models, the Bernoulli model is particularly sen sitive to\\nnoise features. A Bernoulli NB classiﬁer requires some form of feature se-\\nlection or else its accuracy will be low.\\nThis section mainly addresses feature selection for two-cl ass classiﬁcation\\ntasks like China versus not-China . Section 13.5.5 brieﬂy discusses optimiza-\\ntions for systems with more than two classes.\\n13.5.1 Mutual information\\nA common feature selection method is to compute A(t,c)as the expected\\nmutual information (MI) of term tand class c.5MI measures how much in- MUTUAL INFORMATION\\nformation the presence/absence of a term contributes to mak ing the correct\\nclassiﬁcation decision on c. Formally:\\nI(U;C) = ∑\\net∈{1,0}∑\\nec∈{1,0}P(U=et,C=ec)log2P(U=et,C=ec)\\nP(U=et)P(C=ec), (13.16)\\nwhere Uis a random variable that takes values et=1 (the document contains\\nterm t) and et=0 (the document does not contain t), as deﬁned on page 266,\\nand Cis a random variable that takes values ec=1 (the document is in class\\nc) and ec=0 (the document is not in class c). We write Utand Ccif it is not\\nclear from context which term tand class cwe are referring to.\\nForMLEs of the probabilities, Equation ( 13.16 ) is equivalent to Equation ( 13.17 ):\\nI(U;C) =N11\\nNlog2NN 11\\nN1.N.1+N01\\nNlog2NN 01\\nN0.N.1(13.17)\\n+N10\\nNlog2NN 10\\nN1.N.0+N00\\nNlog2NN 00\\nN0.N.0\\nwhere the Ns are counts of documents that have the values of etand ecthat\\nare indicated by the two subscripts. For example, N10is the number of doc-\\n5. Take care not to confuse expected mutual information with pointwise mutual information ,\\nwhich is deﬁned as log N11/E11where N11and E11are deﬁned as in Equation ( 13.18 ). The\\ntwo measures have different properties. See Section 13.7.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 308}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.5 Feature selection 273\\numents that contain t(et=1) and are not in c(ec=0).N1.=N10+N11is\\nthe number of documents that contain t(et=1) and we count documents\\nindependent of class membership ( ec∈{0, 1}).N=N00+N01+N10+N11\\nis the total number of documents. An example of one of the MLE e stimates\\nthat transform Equation ( 13.16 ) into Equation ( 13.17 ) isP(U=1,C=1) =\\nN11/N.\\n✎Example 13.3: Consider the class poultry and the term export in Reuters-RCV1.\\nThe counts of the number of documents with the four possible c ombinations of indi-\\ncator values are as follows:\\nec=epoultry =1ec=epoultry =0\\net=eexport=1 N11=49 N10=27,652\\net=eexport=0 N01=141 N00=774,106\\nAfter plugging these values into Equation ( 13.17 ) we get:\\nI(U;C) =49\\n801,948log2801,948·49\\n(49+27,652 )(49+141)\\n+141\\n801,948log2801,948·141\\n(141+774,106 )(49+141)\\n+27,652\\n801,948log2801,948·27,652\\n(49+27,652 )(27,652 +774,106 )\\n+774,106\\n801,948log2801,948·774,106\\n(141+774,106 )(27,652 +774,106 )\\n≈ 0.0001105\\nTo select kterms t1, . . . , tkfor a given class, we use the feature selection al-\\ngorithm in Figure 13.6: We compute the utility measure as A(t,c) =I(Ut,Cc)\\nand select the kterms with the largest values.\\nMutual information measures how much information – in the in formation-\\ntheoretic sense – a term contains about the class. If a term’s distribution is\\nthe same in the class as it is in the collection as a whole, then I(U;C) =\\n0. MI reaches its maximum value if the term is a perfect indica tor for class\\nmembership, that is, if the term is present in a document if an d only if the\\ndocument is in the class.\\nFigure 13.7 shows terms with high mutual information scores for the six\\nclasses in Figure 13.1.6The selected terms (e.g., london ,uk,british for the class\\nUK) are of obvious utility for making classiﬁcation decisions for their respec-\\ntive classes. At the bottom of the list for UKwe ﬁnd terms like peripherals\\nandtonight (not shown in the ﬁgure) that are clearly not helpful in decid ing\\n6. Feature scores were computed on the ﬁrst 100,000 document s, except for poultry , a rare class,\\nfor which 800,000 documents were used. We have omitted numbe rs and other special words\\nfrom the top ten lists.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 309}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP274 13 Text classiﬁcation and Naive Bayes\\nUK\\nlondon 0.1925\\nuk 0.0755\\nbritish 0.0596\\nstg 0.0555\\nbritain 0.0469\\nplc 0.0357\\nengland 0.0238\\npence 0.0212\\npounds 0.0149\\nenglish 0.0126China\\nchina 0.0997\\nchinese 0.0523\\nbeijing 0.0444\\nyuan 0.0344\\nshanghai 0.0292\\nhong 0.0198\\nkong 0.0195\\nxinhua 0.0155\\nprovince 0.0117\\ntaiwan 0.0108poultry\\npoultry 0.0013\\nmeat 0.0008\\nchicken 0.0006\\nagriculture 0.0005\\navian 0.0004\\nbroiler 0.0003\\nveterinary 0.0003\\nbirds 0.0003\\ninspection 0.0003\\npathogenic 0.0003\\ncoffee\\ncoffee 0.0111\\nbags 0.0042\\ngrowers 0.0025\\nkg 0.0019\\ncolombia 0.0018\\nbrazil 0.0016\\nexport 0.0014\\nexporters 0.0013\\nexports 0.0013\\ncrop 0.0012elections\\nelection 0.0519\\nelections 0.0342\\npolls 0.0339\\nvoters 0.0315\\nparty 0.0303\\nvote 0.0299\\npoll 0.0225\\ncandidate 0.0202\\ncampaign 0.0202\\ndemocratic 0.0198sports\\nsoccer 0.0681\\ncup 0.0515\\nmatch 0.0441\\nmatches 0.0408\\nplayed 0.0388\\nleague 0.0386\\nbeat 0.0301\\ngame 0.0299\\ngames 0.0284\\nteam 0.0264\\n◮Figure 13.7 Features with high mutual information scores for six Reuter s-RCV1\\nclasses.\\nwhether the document is in the class. As you might expect, kee ping the in-\\nformative terms and eliminating the non-informative ones t ends to reduce\\nnoise and improve the classiﬁer’s accuracy.\\nSuch an accuracy increase can be observed in Figure 13.8, which shows\\nF1as a function of vocabulary size after feature selection for Reuters-RCV1.7\\nComparing F1at 132,776 features (corresponding to selection of all feat ures)\\nand at 10–100 features, we see that MI feature selection incr eases F1by about\\n0.1 for the multinomial model and by more than 0.2 for the Bern oulli model.\\nFor the Bernoulli model, F1peaks early, at ten features selected. At that point,\\nthe Bernoulli model is better than the multinomial model. Wh en basing a\\nclassiﬁcation decision on only a few features, it is more rob ust to consider bi-\\nnary occurrence only. For the multinomial model (MI feature selection), the\\npeak occurs later, at 100 features, and its effectiveness re covers somewhat at\\n7. We trained the classiﬁers on the ﬁrst 100,000 documents an d computed F1on the next 100,000.\\nThe graphs are averages over ﬁve classes.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 310}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.5 Feature selection 275\\n## ######\\n#\\n##### #\\n11 0 100 1000 100000.0 0.2 0.4 0.6 0.8\\nnumber of features selectedF1 measure\\noooooooooo\\noooo o\\nxx\\nxx\\nx\\nx\\nxxxx xxxx x\\nbbbbbbb\\nb\\nb\\nb\\nbb bb b\\n#\\no\\nx\\nbmultinomial, MI\\nmultinomial, chisquare\\nmultinomial, frequency\\nbinomial, MI\\n◮Figure 13.8 Effect of feature set size on accuracy for multinomial and Be rnoulli\\nmodels.\\nthe end when we use all features. The reason is that the multin omial takes\\nthe number of occurrences into account in parameter estimat ion and clas-\\nsiﬁcation and therefore better exploits a larger number of f eatures than the\\nBernoulli model. Regardless of the differences between the two methods,\\nusing a carefully selected subset of the features results in better effectiveness\\nthan using all features.\\n13.5.2 χ2Feature selection\\nAnother popular feature selection method is χ2. In statistics, the χ2test is χ2FEATURE SELECTION\\napplied to test the independence of two events, where two eve nts A and B are\\ndeﬁned to be independent ifP(AB) = P(A)P(B)or, equivalently, P(A|B) = INDEPENDENCE\\nP(A)and P(B|A) =P(B). In feature selection, the two events are occurrence\\nof the term and occurrence of the class. We then rank terms wit h respect to\\nthe following quantity:\\nX2(D,t,c) = ∑\\net∈{0,1}∑\\nec∈{0,1}(Netec−Eetec)2\\nEetec(13.18)', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 311}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP276 13 Text classiﬁcation and Naive Bayes\\nwhere etand ecare deﬁned as in Equation ( 13.16 ).Nis the observed frequency\\ninDand Etheexpected frequency. For example, E11is the expected frequency\\noftand coccurring together in a document assuming that term and clas s are\\nindependent.\\n✎Example 13.4: We ﬁrst compute E11for the data in Example 13.3:\\nE11= N×P(t)×P(c) =N×N11+N10\\nN×N11+N01\\nN\\n= N×49+141\\nN×49+27652\\nN≈6.6\\nwhere Nis the total number of documents as before.\\nWe compute the other Eetecin the same way:\\nepoultry =1 epoultry =0\\neexport=1 N11=49 E11≈6.6 N10=27,652 E10≈27,694.4\\neexport=0 N01=141 E01≈183.4 N00=774,106 E00≈774,063.6\\nPlugging these values into Equation ( 13.18 ), we get a X2value of 284:\\nX2(D,t,c) = ∑\\net∈{0,1}∑\\nec∈{0,1}(Netec−Eetec)2\\nEetec≈284\\nX2is a measure of how much expected counts Eand observed counts N\\ndeviate from each other. A high value of X2indicates that the hypothesis of\\nindependence, which implies that expected and observed cou nts are similar,\\nis incorrect. In our example, X2≈284>10.83. Based on Table 13.6, we\\ncan reject the hypothesis that poultry andexport are independent with only a\\n0.001 chance of being wrong.8Equivalently, we say that the outcome X2≈\\n284>10.83 is statistically signiﬁcant at the 0.001 level. If the two events are STATISTICAL\\nSIGNIFICANCE dependent, then the occurrence of the term makes the occurre nce of the class\\nmore likely (or less likely), so it should be helpful as a feat ure. This is the\\nrationale of χ2feature selection.\\nAn arithmetically simpler way of computing X2is the following:\\nX2(D,t,c) =(N11+N10+N01+N00)×(N11N00−N10N01)2\\n(N11+N01)×(N11+N10)×(N10+N00)×(N01+N00)(13.19)\\nThis is equivalent to Equation ( 13.18 ) (Exercise 13.14 ).\\n8. We can make this inference because, if the two events are in dependent, then X2∼χ2, where\\nχ2is the χ2distribution. See, for example, Rice (2006 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 312}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.5 Feature selection 277\\n◮Table 13.6 Critical values of the χ2distribution with one degree of freedom. For\\nexample, if the two events are independent, then P(X2>6.63)<0.01. So for X2>\\n6.63 the assumption of independence can be rejected with 99% conﬁdence.\\np χ2critical value\\n0.1 2.71\\n0.05 3.84\\n0.01 6.63\\n0.005 7.88\\n0.001 10.83\\n✄Assessing χ2as a feature selection method\\nFrom a statistical point of view, χ2feature selection is problematic. For a\\ntest with one degree of freedom, the so-called Yates correct ion should be\\nused (see Section 13.7), which makes it harder to reach statistical signiﬁcance.\\nAlso, whenever a statistical test is used multiple times, th en the probability\\nof getting at least one error increases. If 1,000 hypotheses are rejected, each\\nwith 0.05 error probability, then 0.05 ×1000=50 calls of the test will be\\nwrong on average. However, in text classiﬁcation it rarely m atters whether a\\nfew additional terms are added to the feature set or removed f rom it. Rather,\\ntherelative importance of features is important. As long as χ2feature selec-\\ntion only ranks features with respect to their usefulness an d is not used to\\nmake statements about statistical dependence or independe nce of variables,\\nwe need not be overly concerned that it does not adhere strict ly to statistical\\ntheory.\\n13.5.3 Frequency-based feature selection\\nA third feature selection method is frequency-based feature selection , that is,\\nselecting the terms that are most common in the class. Freque ncy can be\\neither deﬁned as document frequency (the number of document s in the class\\ncthat contain the term t) or as collection frequency (the number of tokens of\\ntthat occur in documents in c). Document frequency is more appropriate for\\nthe Bernoulli model, collection frequency for the multinom ial model.\\nFrequency-based feature selection selects some frequent t erms that have\\nno speciﬁc information about the class, for example, the day s of the week\\n(Monday ,Tuesday , . . . ), which are frequent across classes in newswire text.\\nWhen many thousands of features are selected, then frequenc y-based fea-\\nture selection often does well. Thus, if somewhat suboptima l accuracy is\\nacceptable, then frequency-based feature selection can be a good alternative\\nto more complex methods. However, Figure 13.8 is a case where frequency-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 313}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP278 13 Text classiﬁcation and Naive Bayes\\nbased feature selection performs a lot worse than MI and χ2and should not\\nbe used.\\n13.5.4 Feature selection for multiple classiﬁers\\nIn an operational system with a large number of classiﬁers, i t is desirable\\nto select a single set of features instead of a different one f or each classiﬁer.\\nOne way of doing this is to compute the X2statistic for an n×2 table where\\nthe columns are occurrence and nonoccurrence of the term and each row\\ncorresponds to one of the classes. We can then select the kterms with the\\nhighest X2statistic as before.\\nMore commonly, feature selection statistics are ﬁrst compu ted separately\\nfor each class on the two-class classiﬁcation task cversus cand then com-\\nbined. One combination method computes a single ﬁgure of mer it for each\\nfeature, for example, by averaging the values A(t,c)for feature t, and then\\nselects the kfeatures with highest ﬁgures of merit. Another frequently u sed\\ncombination method selects the top k/nfeatures for each of nclassiﬁers and\\nthen combines these nsets into one global feature set.\\nClassiﬁcation accuracy often decreases when selecting kcommon features\\nfor a system with nclassiﬁers as opposed to ndifferent sets of size k. But even\\nif it does, the gain in efﬁciency owing to a common document re presentation\\nmay be worth the loss in accuracy.\\n13.5.5 Comparison of feature selection methods\\nMutual information and χ2represent rather different feature selection meth-\\nods. The independence of term tand class ccan sometimes be rejected with\\nhigh conﬁdence even if tcarries little information about membership of a\\ndocument in c. This is particularly true for rare terms. If a term occurs on ce\\nin a large collection and that one occurrence is in the poultry class, then this\\nis statistically signiﬁcant. But a single occurrence is not very informative\\naccording to the information-theoretic deﬁnition of infor mation. Because\\nits criterion is signiﬁcance, χ2selects more rare terms (which are often less\\nreliable indicators) than mutual information. But the sele ction criterion of\\nmutual information also does not necessarily select the ter ms that maximize\\nclassiﬁcation accuracy.\\nDespite the differences between the two methods, the classi ﬁcation accu-\\nracy of feature sets selected with χ2and MI does not seem to differ systemat-\\nically. In most text classiﬁcation problems, there are a few strong indicators\\nand many weak indicators. As long as all strong indicators an d a large num-\\nber of weak indicators are selected, accuracy is expected to be good. Both\\nmethods do this.\\nFigure 13.8 compares MI and χ2feature selection for the multinomial model.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 314}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.6 Evaluation of text classiﬁcation 279\\nPeak effectiveness is virtually the same for both methods. χ2reaches this\\npeak later, at 300 features, probably because the rare, but h ighly signiﬁcant\\nfeatures it selects initially do not cover all documents in t he class. However,\\nfeatures selected later (in the range of 100–300) are of bett er quality than those\\nselected by MI.\\nAll three methods – MI, χ2and frequency based – are greedy methods. GREEDY FEATURE\\nSELECTION They may select features that contribute no incremental inf ormation over\\npreviously selected features. In Figure 13.7,kong is selected as the seventh\\nterm even though it is highly correlated with previously sel ectedhong and\\ntherefore redundant. Although such redundancy can negativ ely impact ac-\\ncuracy, non-greedy methods (see Section 13.7 for references) are rarely used\\nin text classiﬁcation due to their computational cost.\\n?Exercise 13.5\\nConsider the following frequencies for the class coffee for four terms in the ﬁrst 100,000\\ndocuments of Reuters-RCV1:\\nterm N00 N01 N10 N11\\nbrazil 98,012 102 1835 51\\ncouncil 96,322 133 3525 20\\nproducers 98,524 119 1118 34\\nroasted 99,824 143 23 10\\nSelect two of these four terms based on (i) χ2, (ii) mutual information, (iii) frequency.\\n13.6 Evaluation of text classiﬁcation\\n] Historically, the classic Reuters-21578 collection was t he main benchmark\\nfor text classiﬁcation evaluation. This is a collection of 2 1,578 newswire ar-\\nticles, originally collected and labeled by Carnegie Group , Inc. and Reuters,\\nLtd. in the course of developing the CONSTRUE text classiﬁca tion system.\\nIt is much smaller than and predates the Reuters-RCV1 collec tion discussed\\nin Chapter 4(page 69). The articles are assigned classes from a set of 118\\ntopic categories. A document may be assigned several classe s or none, but\\nthe commonest case is single assignment (documents with at l east one class\\nreceived an average of 1.24 classes). The standard approach to this any-of\\nproblem (Chapter 14, page 306) is to learn 118 two-class classiﬁers, one for\\neach class, where the two-class classiﬁer for class cis the classiﬁer for the two TWO -CLASS CLASSIFIER\\nclasses cand its complement c.\\nFor each of these classiﬁers, we can measure recall, precisi on, and accu-\\nracy. In recent work, people almost invariably use the ModApte split , which MODAPTE SPLIT\\nincludes only documents that were viewed and assessed by a hu man indexer,', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 315}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP280 13 Text classiﬁcation and Naive Bayes\\n◮Table 13.7 The ten largest classes in the Reuters-21578 collection wit h number of\\ndocuments in training and test sets.\\nclass # train # testclass # train # test\\nearn 2877 1087 trade 369 119\\nacquisitions 1650 179 interest 347 131\\nmoney -fx 538 179 ship 197 89\\ngrain 433 149 wheat 212 71\\ncrude 389 189 corn 182 56\\nand comprises 9,603 training documents and 3,299 test docum ents. The dis-\\ntribution of documents in classes is very uneven, and some wo rk evaluates\\nsystems on only documents in the ten largest classes. They ar e listed in Ta-\\nble13.7. A typical document with topics is shown in Figure 13.9.\\nIn Section 13.1, we stated as our goal in text classiﬁcation the minimizatio n\\nof classiﬁcation error on test data. Classiﬁcation error is 1.0 minus classiﬁca-\\ntion accuracy, the proportion of correct decisions, a measu re we introduced\\nin Section 8.3(page 155). This measure is appropriate if the percentage of\\ndocuments in the class is high, perhaps 10% to 20% and higher. But as we\\ndiscussed in Section 8.3, accuracy is not a good measure for “small” classes\\nbecause always saying no, a strategy that defeats the purpos e of building a\\nclassiﬁer, will achieve high accuracy. The always-no class iﬁer is 99% accurate\\nfor a class with relative frequency 1%. For small classes, pr ecision, recall and\\nF1are better measures.\\nWe will use effectiveness as a generic term for measures that evaluate the EFFECTIVENESS\\nquality of classiﬁcation decisions, including precision, recall, F1, and accu-\\nracy. Performance refers to the computational efﬁciency of classiﬁcation and PERFORMANCE\\nEFFICIENCY IR systems in this book. However, many researchers mean effe ctiveness, not\\nefﬁciency of text classiﬁcation when they use the term perfo rmance.\\nWhen we process a collection with several two-class classiﬁ ers (such as\\nReuters-21578 with its 118 classes), we often want to comput e a single ag-\\ngregate measure that combines the measures for individual c lassiﬁers. There\\nare two methods for doing this. Macroaveraging computes a simple aver- MACROAVERAGING\\nage over classes. Microaveraging pools per-document decisions across classes, MICROAVERAGING\\nand then computes an effectiveness measure on the pooled con tingency ta-\\nble. Table 13.8 gives an example.\\nThe differences between the two methods can be large. Macroa veraging\\ngives equal weight to each class, whereas microaveraging gi ves equal weight\\nto each per-document classiﬁcation decision. Because the F1measure ignores\\ntrue negatives and its magnitude is mostly determined by the number of\\ntrue positives, large classes dominate small classes in mic roaveraging. In the\\nexample, microaveraged precision (0.83) is much closer to t he precision of', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 316}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.6 Evaluation of text classiﬁcation 281\\n<REUTERSTOPICS=’’YES’’LEWISSPLIT=’’TRAIN’’\\nCGISPLIT=’’TRAINING-SET’’OLDID=’’12981’’NEWID=’’798 ’’>\\n<DATE>2-MAR-198716:51:43.42</DATE>\\n<TOPICS><D>livestock</D><D>hog</D></TOPICS>\\n<TITLE>AMERICANPORKCONGRESSKICKSOFFTOMORROW</TITLE>\\n<DATELINE>CHICAGO,March2- </DATELINE><BODY>TheAmeric anPork\\nCongresskicksofftomorrow,March3, inIndianapoliswith1 60\\nofthenationsporkproducersfrom44 memberstatesdetermin ing\\nindustrypositionson anumberofissues,accordingtothe\\nNationalPorkProducersCouncil,NPPC.\\nDelegatestothe threeday Congresswillbeconsidering26\\nresolutionsconcerningvariousissues,includingthefutu re\\ndirectionoffarmpolicyandthe taxlawasit appliesto the\\nagriculturesector.Thedelegateswillalsodebatewhether to\\nendorseconceptsofa nationalPRV(pseudorabiesvirus)con trol\\nanderadicationprogram,theNPPCsaid.A large\\ntradeshow,in conjunctionwiththecongress,willfeature\\nthelatestintechnologyinall areasoftheindustry,the NPP C\\nadded.Reuter\\n\\\\&\\\\#3;</BODY></TEXT></REUTERS>\\n◮Figure 13.9 A sample document from the Reuters-21578 collection.\\nc2(0.9) than to the precision of c1(0.5) because c2is ﬁve times larger than\\nc1. Microaveraged results are therefore really a measure of ef fectiveness on\\nthe large classes in a test collection. To get a sense of effec tiveness on small\\nclasses, you should compute macroaveraged results.\\nIn one-of classiﬁcation (Section 14.5, page 306), microaveraged F1is the\\nsame as accuracy (Exercise 13.6).\\nTable 13.9 gives microaveraged and macroaveraged effectiveness of Na ive\\nBayes for the ModApte split of Reuters-21578. To give a sense of the relative\\neffectiveness of NB, we compare it with linear SVMs (rightmo st column; see\\nChapter 15), one of the most effective classiﬁers, but also one that is m ore\\nexpensive to train than NB. NB has a microaveraged F1of 80%, which is\\n9% less than the SVM (89%), a 10% relative decrease (row “micr o-avg-L (90\\nclasses)”). So there is a surprisingly small effectiveness penalty for its sim-\\nplicity and efﬁciency. However, on small classes, some of wh ich only have on\\nthe order of ten positive examples in the training set, NB doe s much worse.\\nIts macroaveraged F1is 13% below the SVM, a 22% relative decrease (row\\n“macro-avg (90 classes)”).\\nThe table also compares NB with the other classiﬁers we cover in this book:', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 317}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP282 13 Text classiﬁcation and Naive Bayes\\n◮Table 13.8 Macro- and microaveraging. “Truth” is the true class and “ca ll” the\\ndecision of the classiﬁer. In this example, macroaveraged p recision is [10/(10+10) +\\n90/(10+90)]/2= (0.5+0.9)/2=0.7. Microaveraged precision is 100/ (100+20)≈\\n0.83.\\nclass 1\\ntruth: truth:\\nyes no\\ncall:\\nyes10 10\\ncall:\\nno10 970class 2\\ntruth: truth:\\nyes no\\ncall:\\nyes90 10\\ncall:\\nno10 890pooled table\\ntruth: truth:\\nyes no\\ncall:\\nyes100 20\\ncall:\\nno20 1860\\n◮Table 13.9 Text classiﬁcation effectiveness numbers on Reuters-2157 8 for F 1(in\\npercent). Results from Li and Yang (2003 ) (a), Joachims (1998 ) (b: kNN) and Dumais\\net al. (1998 ) (b: NB, Rocchio, trees, SVM).\\n(a) NB Rocchio kNN SVM\\nmicro-avg-L (90 classes) 80 85 86 89\\nmacro-avg (90 classes) 47 59 60 60\\n(b) NB Rocchio kNN trees SVM\\nearn 96 93 97 98 98\\nacq 88 65 92 90 94\\nmoney-fx 57 47 78 66 75\\ngrain 79 68 82 85 95\\ncrude 80 70 86 85 89\\ntrade 64 65 77 73 76\\ninterest 65 63 74 67 78\\nship 85 49 79 74 86\\nwheat 70 69 77 93 92\\ncorn 65 48 78 92 90\\nmicro-avg (top 10) 82 65 82 88 92\\nmicro-avg-D (118 classes) 75 62 n/a n/a 87\\nRocchio and kNN. In addition, we give numbers for decision trees , an impor- DECISION TREES\\ntant classiﬁcation method we do not cover. The bottom part of the table\\nshows that there is considerable variation from class to cla ss. For instance,\\nNB beats kNN on ship, but is much worse on money-fx .\\nComparing parts (a) and (b) of the table, one is struck by the d egree to\\nwhich the cited papers’ results differ. This is partly due to the fact that the\\nnumbers in (b) are break-even scores (cf. page 161) averaged over 118 classes,\\nwhereas the numbers in (a) are true F1scores (computed without any know-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 318}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.6 Evaluation of text classiﬁcation 283\\nledge of the test set) averaged over ninety classes. This is u nfortunately typ-\\nical of what happens when comparing different results in tex t classiﬁcation:\\nThere are often differences in the experimental setup or the evaluation that\\ncomplicate the interpretation of the results.\\nThese and other results have shown that the average effectiv eness of NB\\nis uncompetitive with classiﬁers like SVMs when trained and tested on inde-\\npendent and identically distributed (i.i.d.) data, that is, uniform data with all the\\ngood properties of statistical sampling. However, these di fferences may of-\\nten be invisible or even reverse themselves when working in t he real world\\nwhere, usually, the training sample is drawn from a subset of the data to\\nwhich the classiﬁer will be applied, the nature of the data dr ifts over time\\nrather than being stationary (the problem of concept drift w e mentioned on\\npage 269), and there may well be errors in the data (among other proble ms).\\nMany practitioners have had the experience of being unable t o build a fancy\\nclassiﬁer for a certain problem that consistently performs better than NB.\\nOur conclusion from the results in Table 13.9 is that, although most re-\\nsearchers believe that an SVM is better than kNN and kNN bette r than NB,\\nthe ranking of classiﬁers ultimately depends on the class, t he document col-\\nlection, and the experimental setup. In text classiﬁcation , there is always\\nmore to know than simply which machine learning algorithm wa s used, as\\nwe further discuss in Section 15.3 (page 334).\\nWhen performing evaluations like the one in Table 13.9, it is important to\\nmaintain a strict separation between the training set and th e test set. We can\\neasily make correct classiﬁcation decisions on the test set by using informa-\\ntion we have gleaned from the test set, such as the fact that a p articular term\\nis a good predictor in the test set (even though this is not the case in the train-\\ning set). A more subtle example of using knowledge about the t est set is to\\ntry a large number of values of a parameter (e.g., the number o f selected fea-\\ntures) and select the value that is best for the test set. As a r ule, accuracy on\\nnew data – the type of data we will encounter when we use the cla ssiﬁer in\\nan application – will be much lower than accuracy on a test set that the clas-\\nsiﬁer has been tuned for. We discussed the same problem in ad h oc retrieval\\nin Section 8.1(page 153).\\nIn a clean statistical text classiﬁcation experiment, you s hould never run\\nany program on or even look at the test set while developing a t ext classiﬁca-\\ntion system. Instead, set aside a development set for testing while you develop DEVELOPMENT SET\\nyour method. When such a set serves the primary purpose of ﬁnd ing a good\\nvalue for a parameter, for example, the number of selected fe atures, then it\\nis also called held-out data . Train the classiﬁer on the rest of the training set HELD -OUT DATA\\nwith different parameter values, and then select the value t hat gives best re-\\nsults on the held-out part of the training set. Ideally, at th e very end, when\\nall parameters have been set and the method is fully speciﬁed , you run one\\nﬁnal experiment on the test set and publish the results. Beca use no informa-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 319}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP284 13 Text classiﬁcation and Naive Bayes\\n◮Table 13.10 Data for parameter estimation exercise.\\ndocID words in document in c=China ?\\ntraining set 1 Taipei Taiwan yes\\n2 Macao Taiwan Shanghai yes\\n3 Japan Sapporo no\\n4 Sapporo Osaka Taiwan no\\ntest set 5 Taiwan Taiwan Sapporo ?\\ntion about the test set was used in developing the classiﬁer, the results of this\\nexperiment should be indicative of actual performance in pr actice.\\nThis ideal often cannot be met; researchers tend to evaluate several sys-\\ntems on the same test set over a period of several years. But it is neverthe-\\nless highly important to not look at the test data and to run sy stems on it as\\nsparingly as possible. Beginners often violate this rule, a nd their results lose\\nvalidity because they have implicitly tuned their system to the test data sim-\\nply by running many variant systems and keeping the tweaks to the system\\nthat worked best on the test set.\\n?Exercise 13.6 [⋆⋆]\\nAssume a situation where every document in the test collecti on has been assigned\\nexactly one class, and that a classiﬁer also assigns exactly one class to each document.\\nThis setup is called one-of classiﬁcation (Section 14.5, page 306). Show that in one-of\\nclassiﬁcation (i) the total number of false positive decisi ons equals the total number\\nof false negative decisions and (ii) microaveraged F1and accuracy are identical.\\nExercise 13.7\\nThe class priors in Figure 13.2 are computed as the fraction of documents in the class\\nas opposed to the fraction of tokens in the class. Why?\\nExercise 13.8\\nThe function A PPLY MULTINOMIAL NB in Figure 13.2 has time complexity Θ(La+\\n|C|La). How would you modify the function so that its time complexit y is Θ(La+\\n|C|Ma)?\\nExercise 13.9\\nBased on the data in Table 13.10 , (i) estimate a multinomial Naive Bayes classiﬁer, (ii)\\napply the classiﬁer to the test document, (iii) estimate a Be rnoulli NB classiﬁer, (iv)\\napply the classiﬁer to the test document. You need not estima te parameters that you\\ndon’t need for classifying the test document.\\nExercise 13.10\\nYour task is to classify words as English or not English. Word s are generated by a\\nsource with the following distribution:', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 320}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.6 Evaluation of text classiﬁcation 285\\nevent word English? probability\\n1 ozb no 4/9\\n2 uzu no 4/9\\n3 zoo yes 1/18\\n4 bun yes 1/18\\n(i) Compute the parameters (priors and conditionals) of a mu ltinomial NB classi-\\nﬁer that uses the letters b, n, o, u, and z as features. Assume a training set that\\nreﬂects the probability distribution of the source perfect ly. Make the same indepen-\\ndence assumptions that are usually made for a multinomial cl assiﬁer that uses terms\\nas features for text classiﬁcation. Compute parameters usi ng smoothing, in which\\ncomputed-zero probabilities are smoothed into probabilit y 0.01, and computed-nonzero\\nprobabilities are untouched. (This simplistic smoothing m ay cause P(A) +P(A)>1.\\nSolutions are not required to correct this.) (ii) How does th e classiﬁer classify the\\nwordzoo? (iii) Classify the word zoousing a multinomial classiﬁer as in part (i), but\\ndo not make the assumption of positional independence. That is, estimate separate\\nparameters for each position in a word. You only need to compu te the parameters\\nyou need for classifying zoo.\\nExercise 13.11\\nWhat are the values of I(Ut;Cc)and X2(D,t,c)if term and class are completely inde-\\npendent? What are the values if they are completely dependen t?\\nExercise 13.12\\nThe feature selection method in Equation ( 13.16 ) is most appropriate for the Bernoulli\\nmodel. Why? How could one modify it for the multinomial model ?\\nExercise 13.13\\nFeatures can also be selected according to information gain (IG), which is deﬁned as: INFORMATION GAIN\\nIG(D,t,c) =H(pD)− ∑\\nx∈{Dt+,Dt−}|x|\\n|D|H(px)\\nwhere His entropy, Dis the training set, and Dt+, and Dt−are the subset of Dwith\\nterm t, and the subset of Dwithout term t, respectively. pAis the class distribution\\nin (sub)collection A, e.g., pA(c) =0.25, pA(c) =0.75 if a quarter of the documents in\\nAare in class c.\\nShow that mutual information and information gain are equiv alent.\\nExercise 13.14\\nShow that the two X2formulas (Equations ( 13.18 ) and ( 13.19 )) are equivalent.\\nExercise 13.15\\nIn the χ2example on page 276we have|N11−E11|=|N10−E10|=|N01−E01|=\\n|N00−E00|. Show that this holds in general.\\nExercise 13.16\\nχ2and mutual information do not distinguish between positive ly and negatively cor-\\nrelated features. Because most good text classiﬁcation fea tures are positively corre-\\nlated (i.e., they occur more often in cthan in c), one may want to explicitly rule out\\nthe selection of negative indicators. How would you do this?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 321}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP286 13 Text classiﬁcation and Naive Bayes\\n13.7 References and further reading\\nGeneral introductions to statistical classiﬁcation and ma chine learning can be\\nfound in ( Hastie et al. 2001 ), (Mitchell 1997 ), and ( Duda et al. 2000 ), including\\nmany important methods (e.g., decision trees and boosting) that we do not\\ncover. A comprehensive review of text classiﬁcation method s and results is\\n(Sebastiani 2002 ).Manning and Schütze (1999 , Chapter 16) give an accessible\\nintroduction to text classiﬁcation with coverage of decisi on trees, perceptrons\\nand maximum entropy models. More information on the superli near time\\ncomplexity of learning methods that are more accurate than N aive Bayes can\\nbe found in ( Perkins et al. 2003 ) and ( Joachims 2006a ).\\nMaron and Kuhns (1960 ) described one of the ﬁrst NB text classiﬁers. Lewis\\n(1998 ) focuses on the history of NB classiﬁcation. Bernoulli and m ultinomial\\nmodels and their accuracy for different collections are dis cussed by McCal-\\nlum and Nigam (1998 ).Eyheramendy et al. (2003 ) present additional NB\\nmodels. Domingos and Pazzani (1997 ),Friedman (1997 ), and Hand and Yu\\n(2001 ) analyze why NB performs well although its probability esti mates are\\npoor. The ﬁrst paper also discusses NB’s optimality when the independence\\nassumptions are true of the data. Pavlov et al. (2004 ) propose a modiﬁed\\ndocument representation that partially addresses the inap propriateness of\\nthe independence assumptions. Bennett (2000 ) attributes the tendency of NB\\nprobability estimates to be close to either 0 or 1 to the effec t of document\\nlength. Ng and Jordan (2001 ) show that NB is sometimes (although rarely)\\nsuperior to discriminative methods because it more quickly reaches its opti-\\nmal error rate. The basic NB model presented in this chapter c an be tuned for\\nbetter effectiveness ( Rennie et al. 2003 ;Kołcz and Yih 2007 ). The problem of\\nconcept drift and other reasons why state-of-the-art class iﬁers do not always\\nexcel in practice are discussed by Forman (2006 ) and Hand (2006 ).\\nEarly uses of mutual information and χ2for feature selection in text clas-\\nsiﬁcation are Lewis and Ringuette (1994 ) and Schütze et al. (1995 ), respec-\\ntively. Yang and Pedersen (1997 ) review feature selection methods and their\\nimpact on classiﬁcation effectiveness. They ﬁnd that pointwise mutual infor- POINTWISE MUTUAL\\nINFORMATION mation is not competitive with other methods. Yang and Pedersen refer to\\nexpected mutual information (Equation ( 13.16 )) as information gain (see Ex-\\nercise 13.13 , page 285). (Snedecor and Cochran 1989 ) is a good reference for\\ntheχ2test in statistics, including the Yates’ correction for con tinuity for 2×2\\ntables. Dunning (1993 ) discusses problems of the χ2test when counts are\\nsmall. Nongreedy feature selection techniques are describ ed by Hastie et al.\\n(2001 ).Cohen (1995 ) discusses the pitfalls of using multiple signiﬁcance test s\\nand methods to avoid them. Forman (2004 ) evaluates different methods for\\nfeature selection for multiple classiﬁers.\\nDavid D. Lewis deﬁnes the ModApte split at www.daviddlewis.com/resources/testcollections/reute rs21578/readme\\nbased on Apté et al. (1994 ).Lewis (1995 ) describes utility measures for the UTILITY MEASURE', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 322}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP13.7 References and further reading 287\\nevaluation of text classiﬁcation systems. Yang and Liu (1999 ) employ signif-\\nicance tests in the evaluation of text classiﬁcation method s.\\nLewis et al. (2004 ) ﬁnd that SVMs (Chapter 15) perform better on Reuters-\\nRCV1 than kNN and Rocchio (Chapter 14).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 323}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 324}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 289\\n14 Vector space classiﬁcation\\nThe document representation in Naive Bayes is a sequence of t erms or a bi-\\nnary vector⟨e1, . . . , e|V|⟩∈{ 0, 1}|V|. In this chapter we adopt a different\\nrepresentation for text classiﬁcation, the vector space mo del, developed in\\nChapter 6. It represents each document as a vector with one real-value d com-\\nponent, usually a tf-idf weight, for each term. Thus, the doc ument space X,\\nthe domain of the classiﬁcation function γ, isR|V|. This chapter introduces a\\nnumber of classiﬁcation methods that operate on real-value d vectors.\\nThe basic hypothesis in using the vector space model for clas siﬁcation is\\nthecontiguity hypothesis . CONTIGUITY\\nHYPOTHESISContiguity hypothesis. Documents in the same class form a contigu-\\nous region and regions of different classes do not overlap.\\nThere are many classiﬁcation tasks, in particular the type o f text classiﬁcation\\nthat we encountered in Chapter 13, where classes can be distinguished by\\nword patterns. For example, documents in the class China tend to have high\\nvalues on dimensions like Chinese ,Beijing , andMao whereas documents in the\\nclass UKtend to have high values for London ,British andQueen . Documents\\nof the two classes therefore form distinct contiguous regio ns as shown in\\nFigure 14.1 and we can draw boundaries that separate them and classify ne w\\ndocuments. How exactly this is done is the topic of this chapt er.\\nWhether or not a set of documents is mapped into a contiguous r egion de-\\npends on the particular choices we make for the document repr esentation:\\ntype of weighting, stop list etc. To see that the document rep resentation is\\ncrucial, consider the two classes written by a group vs.written by a single per-\\nson. Frequent occurrence of the ﬁrst person pronoun Iis evidence for the\\nsingle-person class. But that information is likely delete d from the document\\nrepresentation if we use a stop list. If the document represe ntation chosen\\nis unfavorable, the contiguity hypothesis will not hold and successful vector\\nspace classiﬁcation is not possible.\\nThe same considerations that led us to prefer weighted repre sentations, in\\nparticular length-normalized tf-idf representations, in Chapters 6and 7also', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 325}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP290 14 Vector space classiﬁcation\\nxx\\nx\\nx⋄\\n⋄\\n⋄⋄\\n⋄\\n⋄\\nChina\\nKenyaUK\\n⋆\\n◮Figure 14.1 Vector space classiﬁcation into three classes.\\napply here. For example, a term with 5 occurrences in a docume nt should get\\na higher weight than a term with one occurrence, but a weight 5 times larger\\nwould give too much emphasis to the term. Unweighted and unno rmalized\\ncounts should not be used in vector space classiﬁcation.\\nWe introduce two vector space classiﬁcation methods in this chapter, Roc-\\nchio and kNN. Rocchio classiﬁcation (Section 14.2) divides the vector space\\ninto regions centered on centroids or prototypes , one for each class, computed PROTOTYPE\\nas the center of mass of all documents in the class. Rocchio cl assiﬁcation is\\nsimple and efﬁcient, but inaccurate if classes are not appro ximately spheres\\nwith similar radii.\\nkNN or knearest neighbor classiﬁcation (Section 14.3) assigns the majority\\nclass of the knearest neighbors to a test document. kNN requires no explic it\\ntraining and can use the unprocessed training set directly i n classiﬁcation.\\nIt is less efﬁcient than other classiﬁcation methods in clas sifying documents.\\nIf the training set is large, then kNN can handle non-spheric al and other\\ncomplex classes better than Rocchio.\\nA large number of text classiﬁers can be viewed as linear clas siﬁers – clas-\\nsiﬁers that classify based on a simple linear combination of the features (Sec-\\ntion 14.4). Such classiﬁers partition the space of features into regi ons sepa-\\nrated by linear decision hyperplanes , in a manner to be detailed below. Because\\nof the bias-variance tradeoff (Section 14.6) more complex nonlinear models', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 326}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP14.1 Document representations and measures of relatedness in vector spaces 291\\ndtrue\\ndprojectedx1x2x3x4\\nx5\\nx′\\n1x′\\n2x′\\n3x′\\n4x′\\n5x′\\n1x′\\n2x′\\n3x′\\n4x′\\n5\\n◮Figure 14.2 Projections of small areas of the unit sphere preserve dista nces. Left:\\nA projection of the 2D semicircle to 1D. For the points x1,x2,x3,x4,x5at x coordinates\\n−0.9,−0.2, 0, 0.2, 0.9 the distance |x2x3|≈ 0.201 only differs by 0.5% from |x′\\n2x′\\n3|=\\n0.2; but|x1x3|/|x′\\n1x′\\n3|=dtrue/dprojected≈1.06/0.9≈1.18 is an example of a large\\ndistortion (18%) when projecting a large area. Right: The co rresponding projection of\\nthe 3D hemisphere to 2D.\\nare not systematically better than linear models. Nonlinea r models have\\nmore parameters to ﬁt on a limited amount of training data and are more\\nlikely to make mistakes for small and noisy data sets.\\nWhen applying two-class classiﬁers to problems with more th an two classes,\\nthere are one-of tasks – a document must be assigned to exactly one of several\\nmutually exclusive classes – and any-of tasks – a document can be assigned to\\nany number of classes as we will explain in Section 14.5. Two-class classiﬁers\\nsolve any-of problems and can be combined to solve one-of pro blems.\\n14.1 Document representations and measures of relatedness in vec-\\ntor spaces\\nAs in Chapter 6, we represent documents as vectors in R|V|in this chapter.\\nTo illustrate properties of document vectors in vector clas siﬁcation, we will\\nrender these vectors as points in a plane as in the example in F igure 14.1.\\nIn reality, document vectors are length-normalized unit ve ctors that point\\nto the surface of a hypersphere. We can view the 2D planes in ou r ﬁgures\\nas projections onto a plane of the surface of a (hyper-)spher e as shown in\\nFigure 14.2. Distances on the surface of the sphere and on the projection\\nplane are approximately the same as long as we restrict ourse lves to small\\nareas of the surface and choose an appropriate projection (E xercise 14.1).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 327}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP292 14 Vector space classiﬁcation\\nDecisions of many vector space classiﬁers are based on a noti on of dis-\\ntance, e.g., when computing the nearest neighbors in kNN cla ssiﬁcation.\\nWe will use Euclidean distance in this chapter as the underly ing distance\\nmeasure. We observed earlier (Exercise 6.18, page 131) that there is a direct\\ncorrespondence between cosine similarity and Euclidean di stance for length-\\nnormalized vectors. In vector space classiﬁcation, it rare ly matters whether\\nthe relatedness of two documents is expressed in terms of sim ilarity or dis-\\ntance.\\nHowever, in addition to documents, centroids or averages of vectors also\\nplay an important role in vector space classiﬁcation. Centr oids are not length-\\nnormalized. For unnormalized vectors, dot product, cosine similarity and\\nEuclidean distance all have different behavior in general ( Exercise 14.6). We\\nwill be mostly concerned with small local regions when compu ting the sim-\\nilarity between a document and a centroid, and the smaller th e region the\\nmore similar the behavior of the three measures is.\\n?Exercise 14.1\\nFor small areas, distances on the surface of the hypersphere are approximated well\\nby distances on its projection (Figure 14.2) because α≈sinαfor small angles. For\\nwhat size angle is the distortion α/ sin(α)(i) 1.01, (ii) 1.05 and (iii) 1.1?\\n14.2 Rocchio classiﬁcation\\nFigure 14.1 shows three classes, China ,UKand Kenya , in a two-dimensional\\n(2D) space. Documents are shown as circles, diamonds and X’s . The bound-\\naries in the ﬁgure, which we call decision boundaries , are chosen to separate DECISION BOUNDARY\\nthe three classes, but are otherwise arbitrary. To classify a new document,\\ndepicted as a star in the ﬁgure, we determine the region it occ urs in and as-\\nsign it the class of that region – China in this case. Our task in vector space\\nclassiﬁcation is to devise algorithms that compute good bou ndaries where\\n“good” means high classiﬁcation accuracy on data unseen dur ing training.\\nPerhaps the best-known way of computing good class boundari es is Roc- ROCCHIO\\nCLASSIFICATION chio classiﬁcation , which uses centroids to deﬁne the boundaries. The centroid\\nCENTROIDof a class cis computed as the vector average or center of mass of its mem-\\nbers:\\n⃗µ(c) =1\\n|Dc|∑\\nd∈Dc⃗v(d) (14.1)\\nwhere Dcis the set of documents in Dwhose class is c:Dc={d:⟨d,c⟩∈D}.\\nWe denote the normalized vector of dby⃗v(d)(Equation ( 6.11), page 122).\\nThree example centroids are shown as solid circles in Figure 14.3.\\nThe boundary between two classes in Rocchio classiﬁcation i s the set of\\npoints with equal distance from the two centroids. For examp le,|a1|=|a2|,', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 328}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP14.2 Rocchio classiﬁcation 293\\nxx\\nx\\nx⋄\\n⋄\\n⋄⋄\\n⋄\\n⋄\\nChina\\nKenyaUK\\n⋆a1\\na2b1\\nb2c1\\nc2\\n◮Figure 14.3 Rocchio classiﬁcation.\\n|b1|=|b2|, and|c1|=|c2|in the ﬁgure. This set of points is always a line.\\nThe generalization of a line in M-dimensional space is a hyperplane, which\\nwe deﬁne as the set of points ⃗xthat satisfy:\\n⃗wT⃗x=b (14.2)\\nwhere ⃗wis the M-dimensional normal vector1of the hyperplane and bis a NORMAL VECTOR\\nconstant. This deﬁnition of hyperplanes includes lines (an y line in 2D can\\nbe deﬁned by w1x1+w2x2=b) and 2-dimensional planes (any plane in 3D\\ncan be deﬁned by w1x1+w2x2+w3x3=b). A line divides a plane in two,\\na plane divides 3-dimensional space in two, and hyperplanes divide higher-\\ndimensional spaces in two.\\nThus, the boundaries of class regions in Rocchio classiﬁcat ion are hyper-\\nplanes. The classiﬁcation rule in Rocchio is to classify a po int in accordance\\nwith the region it falls into. Equivalently, we determine th e centroid ⃗µ(c)that\\nthe point is closest to and then assign it to c. As an example, consider the star\\nin Figure 14.3. It is located in the China region of the space and Rocchio\\ntherefore assigns it to China . We show the Rocchio algorithm in pseudocode\\nin Figure 14.4.\\n1. Recall from basic linear algebra that ⃗v·⃗w=⃗vT⃗w, i.e., the dot product of ⃗vand⃗wequals the\\nproduct by matrix multiplication of the transpose of ⃗vand⃗w.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 329}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP294 14 Vector space classiﬁcation\\nterm weights\\nvector Chinese Japan Tokyo Macao Beijing Shanghai\\n⃗d1 0 0 0 0 1.0 0\\n⃗d2 0 0 0 0 0 1.0\\n⃗d3 0 0 0 1.0 0 0\\n⃗d4 0 0.71 0.71 0 0 0\\n⃗d5 0 0.71 0.71 0 0 0\\n⃗µc 0 0 0 0.33 0.33 0.33\\n⃗µc 0 0.71 0.71 0 0 0\\n◮Table 14.1 Vectors and class centroids for the data in Table 13.1.\\n✎Example 14.1: Table 14.1 shows the tf-idf vector representations of the ﬁve docu-\\nments in Table 13.1 (page 261), using the formula (1+log10tft,d)log10(4/df t)if tf t,d>\\n0 (Equation ( 6.14), page 127). The two class centroids are µc=1/3·(⃗d1+⃗d2+⃗d3)\\nand µc=1/1·(⃗d4). The distances of the test document from the centroids are\\n|µc−⃗d5|≈1.15 and|µc−⃗d5|=0.0. Thus, Rocchio assigns d5toc.\\nThe separating hyperplane in this case has the following par ameters:\\n⃗w≈(0−0.71−0.71 1/3 1/3 1/3 )T\\nb=−1/3\\nSee Exercise 14.15 for how to compute ⃗wand b. We can easily verify that this hy-\\nperplane separates the documents as desired: ⃗wT⃗d1≈0·0+−0.71·0+−0.71·0+\\n1/3·0+1/3·1.0+1/3·0=1/3>b(and, similarly, ⃗wT⃗di>bfori=2 and i=3)\\nand⃗wT⃗d4=−1<b. Thus, documents in care above the hyperplane ( ⃗wT⃗d>b) and\\ndocuments in care below the hyperplane ( ⃗wT⃗d<b).\\nThe assignment criterion in Figure 14.4 is Euclidean distance (A PPLY ROC-\\nCHIO , line 1). An alternative is cosine similarity:\\nAssign dto class c=arg max\\nc′cos(⃗µ(c′),⃗v(d))\\nAs discussed in Section 14.1, the two assignment criteria will sometimes\\nmake different classiﬁcation decisions. We present the Euc lidean distance\\nvariant of Rocchio classiﬁcation here because it emphasize s Rocchio’s close\\ncorrespondence to K-means clustering (Section 16.4, page 360).\\nRocchio classiﬁcation is a form of Rocchio relevance feedba ck (Section 9.1.1 ,\\npage 178). The average of the relevant documents, corresponding to t he most\\nimportant component of the Rocchio vector in relevance feed back (Equa-\\ntion ( 9.3), page 182), is the centroid of the “class” of relevant documents.\\nWe omit the query component of the Rocchio formula in Rocchio classiﬁca-\\ntion since there is no query in text classiﬁcation. Rocchio c lassiﬁcation can be', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 330}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP14.2 Rocchio classiﬁcation 295\\nTRAIN ROCCHIO (C,D)\\n1for each cj∈C\\n2doDj←{ d:⟨d,cj⟩∈D}\\n3 ⃗µj←1\\n|Dj|∑d∈Dj⃗v(d)\\n4return{⃗µ1, . . . ,⃗µJ}\\nAPPLY ROCCHIO ({⃗µ1, . . . ,⃗µJ},d)\\n1return arg minj|⃗µj−⃗v(d)|\\n◮Figure 14.4 Rocchio classiﬁcation: Training and testing.\\na\\naa\\naa\\naaa\\na\\naaa\\naaa a\\na aa\\naa\\na\\naa\\naa\\naa\\naaa\\naaaa\\na\\naaa\\na\\nbb\\nb\\nbbbbbb\\nbb\\nb\\nbb\\nbb\\nb\\nbbX X A\\nBo\\n◮Figure 14.5 The multimodal class “a” consists of two different clusters (small\\nupper circles centered on X’s). Rocchio classiﬁcation will misclassify “o” as “a”\\nbecause it is closer to the centroid A of the “a” class than to t he centroid B of the “b”\\nclass.\\napplied to J>2 classes whereas Rocchio relevance feedback is designed to\\ndistinguish only two classes, relevant and nonrelevant.\\nIn addition to respecting contiguity, the classes in Rocchi o classiﬁcation\\nmust be approximate spheres with similar radii. In Figure 14.3, the solid\\nsquare just below the boundary between UKand Kenya is a better ﬁt for the\\nclass UKsince UKis more scattered than Kenya . But Rocchio assigns it to\\nKenya because it ignores details of the distribution of points in a class and\\nonly uses distance from the centroid for classiﬁcation.\\nThe assumption of sphericity also does not hold in Figure 14.5. We can-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 331}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP296 14 Vector space classiﬁcation\\nmode time complexity\\ntraining Θ(|D|Lave+|C||V|)\\ntesting Θ(La+|C|Ma) =Θ(|C|Ma)\\n◮Table 14.2 Training and test times for Rocchio classiﬁcation. Laveis the average\\nnumber of tokens per document. Laand Maare the numbers of tokens and types,\\nrespectively, in the test document. Computing Euclidean di stance between the class\\ncentroids and a document is Θ(|C|Ma).\\nnot represent the “a” class well with a single prototype beca use it has two\\nclusters. Rocchio often misclassiﬁes this type of multimodal class . A text clas- MULTIMODAL CLASS\\nsiﬁcation example for multimodality is a country like Burma , which changed\\nits name to Myanmar in 1989. The two clusters before and after the name\\nchange need not be close to each other in space. We also encoun tered the\\nproblem of multimodality in relevance feedback (Section 9.1.2 , page 184).\\nTwo-class classiﬁcation is another case where classes are r arely distributed\\nlike spheres with similar radii. Most two-class classiﬁers distinguish between\\na class like China that occupies a small region of the space and its widely\\nscattered complement. Assuming equal radii will result in a large number\\nof false positives. Most two-class classiﬁcation problems therefore require a\\nmodiﬁed decision rule of the form:\\nAssign dto class ciff|⃗µ(c)−⃗v(d)|<|⃗µ(c)−⃗v(d)|−b\\nfor a positive constant b. As in Rocchio relevance feedback, the centroid of\\nthe negative documents is often not used at all, so that the de cision criterion\\nsimpliﬁes to|⃗µ(c)−⃗v(d)|<b′for a positive constant b′.\\nTable 14.2 gives the time complexity of Rocchio classiﬁcation.2Adding all\\ndocuments to their respective (unnormalized) centroid is Θ(|D|Lave)(as op-\\nposed to Θ(|D||V|)) since we need only consider non-zero entries. Dividing\\neach vector sum by the size of its class to compute the centroi d is Θ(|V|).\\nOverall, training time is linear in the size of the collectio n (cf. Exercise 13.1).\\nThus, Rocchio classiﬁcation and Naive Bayes have the same li near training\\ntime complexity.\\nIn the next section, we will introduce another vector space c lassiﬁcation\\nmethod, kNN, that deals better with classes that have non-sp herical, discon-\\nnected or other irregular shapes.\\n?Exercise 14.2 [⋆]\\nShow that Rocchio classiﬁcation can assign a label to a docum ent that is different from\\nits training set label.\\n2. We write Θ(|D|Lave)forΘ(T)and assume that the length of test documents is bounded as\\nwe did on page 262.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 332}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP14.3 k nearest neighbor 297\\nxx\\nxxxxx\\nxxx\\nx⋄\\n⋄\\n⋄\\n⋄⋄\\n⋄\\n⋄\\n⋄⋄\\n⋄⋄⋆\\n◮Figure 14.6 Voronoi tessellation and decision boundaries (double line s) in 1NN\\nclassiﬁcation. The three classes are: X, circle and diamond .\\n14.3 knearest neighbor\\nUnlike Rocchio, k nearest neighbor orkNN classiﬁcation determines the deci- kNEAREST NEIGHBOR\\nCLASSIFICATION sion boundary locally. For 1NN we assign each document to the class of its\\nclosest neighbor. For kNN we assign each document to the majo rity class of\\nitskclosest neighbors where kis a parameter. The rationale of kNN classiﬁ-\\ncation is that, based on the contiguity hypothesis, we expec t a test document\\ndto have the same label as the training documents located in th e local region\\nsurrounding d.\\nDecision boundaries in 1NN are concatenated segments of the Voronoi tes- VORONOI\\nTESSELLATION sellation as shown in Figure 14.6. The Voronoi tessellation of a set of objects\\ndecomposes space into Voronoi cells, where each object’s ce ll consists of all\\npoints that are closer to the object than to other objects. In our case, the ob-\\njects are documents. The Voronoi tessellation then partiti ons the plane into\\n|D|convex polygons, each containing its corresponding docume nt (and no\\nother) as shown in Figure 14.6, where a convex polygon is a convex region in\\n2-dimensional space bounded by lines.\\nFor general k∈Nin kNN, consider the region in the space for which the\\nset of knearest neighbors is the same. This again is a convex polygon and the\\nspace is partitioned into convex polygons, within each of wh ich the set of k', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 333}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP298 14 Vector space classiﬁcation\\nTRAIN -KNN(C,D)\\n1D′←PREPROCESS (D)\\n2k←SELECT -K(C,D′)\\n3return D′,k\\nAPPLY -KNN(C,D′,k,d)\\n1Sk←COMPUTE NEAREST NEIGHBORS (D′,k,d)\\n2for each cj∈C\\n3dopj←|Sk∩cj|/k\\n4return arg maxjpj\\n◮Figure 14.7 kNN training (with preprocessing) and testing. pjis an estimate for\\nP(cj|Sk) =P(cj|d).cjdenotes the set of all documents in the class cj.\\nnearest neighbors is invariant (Exercise 14.11 ).3\\n1NN is not very robust. The classiﬁcation decision of each te st document\\nrelies on the class of a single training document, which may b e incorrectly\\nlabeled or atypical. kNN for k>1 is more robust. It assigns documents to\\nthe majority class of their kclosest neighbors, with ties broken randomly.\\nThere is a probabilistic version of this kNN classiﬁcation a lgorithm. We\\ncan estimate the probability of membership in class cas the proportion of the\\nknearest neighbors in c. Figure 14.6 gives an example for k=3. Probabil-\\nity estimates for class membership of the star are ˆP(circle class|star) = 1/3,\\nˆP(X class|star) = 2/3, and ˆP(diamond class|star) = 0. The 3nn estimate\\n(ˆP1(circle class|star) = 1/3) and the 1nn estimate ( ˆP1(circle class|star) = 1)\\ndiffer with 3nn preferring the X class and 1nn preferring the circle class .\\nThe parameter kin kNN is often chosen based on experience or knowledge\\nabout the classiﬁcation problem at hand. It is desirable for kto be odd to\\nmake ties less likely. k=3 and k=5 are common choices, but much larger\\nvalues between 50 and 100 are also used. An alternative way of setting the\\nparameter is to select the kthat gives best results on a held-out portion of the\\ntraining set.\\nWe can also weight the “votes” of the knearest neighbors by their cosine\\n3. The generalization of a polygon to higher dimensions is a p olytope. A polytope is a region\\ninM-dimensional space bounded by (M−1)-dimensional hyperplanes. In Mdimensions, the\\ndecision boundaries for kNN consist of segments of (M−1)-dimensional hyperplanes that form\\nthe Voronoi tessellation into convex polytopes for the trai ning set of documents. The decision\\ncriterion of assigning a document to the majority class of it sknearest neighbors applies equally\\ntoM=2 (tessellation into polygons) and M>2 (tessellation into polytopes).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 334}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP14.3 k nearest neighbor 299\\nkNN with preprocessing of training set\\ntraining Θ(|D|Lave)\\ntesting Θ(La+|D|MaveMa) =Θ(|D|MaveMa)\\nkNN without preprocessing of training set\\ntraining Θ(1)\\ntesting Θ(La+|D|LaveMa) =Θ(|D|LaveMa)\\n◮Table 14.3 Training and test times for kNN classiﬁcation. Maveis the average size\\nof the vocabulary of documents in the collection.\\nsimilarity. In this scheme, a class’s score is computed as:\\nscore(c,d) = ∑\\nd′∈Sk(d)Ic(d′)cos(⃗v(d′),⃗v(d))\\nwhere Sk(d)is the set of d’sknearest neighbors and Ic(d′) =1 iff d′is in class\\ncand 0 otherwise. We then assign the document to the class with the highest\\nscore. Weighting by similarities is often more accurate tha n simple voting.\\nFor example, if two classes have the same number of neighbors in the top k,\\nthe class with the more similar neighbors wins.\\nFigure 14.7 summarizes the kNN algorithm.\\n✎Example 14.2: The distances of the test document from the four training doc u-\\nments in Table 14.1 are|⃗d1−⃗d5|=|⃗d2−⃗d5|=|⃗d3−⃗d5|≈1.41 and|⃗d4−⃗d5|=0.0.\\nd5’s nearest neighbor is therefore d4and 1NN assigns d5tod4’s class, c.\\n✄14.3.1 Time complexity and optimality of kNN\\nTable 14.3 gives the time complexity of kNN. kNN has properties that are\\nquite different from most other classiﬁcation algorithms. Training a kNN\\nclassiﬁer simply consists of determining kand preprocessing documents. In\\nfact, if we preselect a value for kand do not preprocess, then kNN requires\\nno training at all. In practice, we have to perform preproces sing steps like\\ntokenization. It makes more sense to preprocess training do cuments once\\nas part of the training phase rather than repeatedly every ti me we classify a\\nnew test document.\\nTest time is Θ(|D|MaveMa)for kNN. It is linear in the size of the training\\nset as we need to compute the distance of each training docume nt from the\\ntest document. Test time is independent of the number of clas sesJ. kNN\\ntherefore has a potential advantage for problems with large J.\\nIn kNN classiﬁcation, we do not perform any estimation of par ameters as\\nwe do in Rocchio classiﬁcation (centroids) or in Naive Bayes (priors and con-\\nditional probabilities). kNN simply memorizes all example s in the training', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 335}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP300 14 Vector space classiﬁcation\\nset and then compares the test document to them. For this reas on, kNN is\\nalso called memory-based learning orinstance-based learning . It is usually desir- MEMORY -BASED\\nLEARNING able to have as much training data as possible in machine lear ning. But in\\nkNN large training sets come with a severe efﬁciency penalty in classiﬁca-\\ntion.\\nCan kNN testing be made more efﬁcient than Θ(|D|MaveMa)or, ignoring\\nthe length of documents, more efﬁcient than Θ(|D|)? There are fast kNN\\nalgorithms for small dimensionality M(Exercise 14.12 ). There are also ap-\\nproximations for large Mthat give error bounds for speciﬁc efﬁciency gains\\n(see Section 14.7). These approximations have not been extensively tested\\nfor text classiﬁcation applications, so it is not clear whet her they can achieve\\nmuch better efﬁciency than Θ(|D|)without a signiﬁcant loss of accuracy.\\nThe reader may have noticed the similarity between the probl em of ﬁnding\\nnearest neighbors of a test document and ad hoc retrieval, wh ere we search\\nfor the documents with the highest similarity to the query (S ection 6.3.2 ,\\npage 123). In fact, the two problems are both knearest neighbor problems\\nand only differ in the relative density of (the vector of) the test document\\nin kNN (10s or 100s of non-zero entries) versus the sparsenes s of (the vec-\\ntor of) the query in ad hoc retrieval (usually fewer than 10 no n-zero entries).\\nWe introduced the inverted index for efﬁcient ad hoc retriev al in Section 1.1\\n(page 6). Is the inverted index also the solution for efﬁcient kNN?\\nAn inverted index restricts a search to those documents that have at least\\none term in common with the query. Thus in the context of kNN, t he in-\\nverted index will be efﬁcient if the test document has no term overlap with a\\nlarge number of training documents. Whether this is the case depends on the\\nclassiﬁcation problem. If documents are long and no stop lis t is used, then\\nless time will be saved. But with short documents and a large s top list, an\\ninverted index may well cut the average test time by a factor o f 10 or more.\\nThe search time in an inverted index is a function of the lengt h of the post-\\nings lists of the terms in the query. Postings lists grow subl inearly with the\\nlength of the collection since the vocabulary increases acc ording to Heaps’\\nlaw – if the probability of occurrence of some terms increase s, then the prob-\\nability of occurrence of others must decrease. However, mos t new terms are\\ninfrequent. We therefore take the complexity of inverted in dex search to be\\nΘ(T)(as discussed in Section 2.4.2 , page 41) and, assuming average docu-\\nment length does not change over time, Θ(T) =Θ(|D|).\\nAs we will see in the next chapter, kNN’s effectiveness is clo se to that of the\\nmost accurate learning methods in text classiﬁcation (Tabl e15.2, page 334). A\\nmeasure of the quality of a learning method is its Bayes error rate , the average BAYES ERROR RATE\\nerror rate of classiﬁers learned by it for a particular probl em. kNN is not\\noptimal for problems with a non-zero Bayes error rate – that i s, for problems\\nwhere even the best possible classiﬁer has a non-zero classi ﬁcation error. The\\nerror of 1NN is asymptotically (as the training set increase s) bounded by', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 336}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP14.4 Linear versus nonlinear classiﬁers 301\\n◮Figure 14.8 There are an inﬁnite number of hyperplanes that separate two linearly\\nseparable classes.\\ntwice the Bayes error rate. That is, if the optimal classiﬁer has an error rate\\nofx, then 1NN has an asymptotic error rate of less than 2 x. This is due to the\\neffect of noise – we already saw one example of noise in the for m of noisy\\nfeatures in Section 13.5 (page 271), but noise can also take other forms as we\\nwill discuss in the next section. Noise affects two componen ts of kNN: the\\ntest document and the closest training document. The two sou rces of noise\\nare additive, so the overall error of 1NN is twice the optimal error rate. For\\nproblems with Bayes error rate 0, the error rate of 1NN will ap proach 0 as\\nthe size of the training set increases.\\n?Exercise 14.3\\nExplain why kNN handles multimodal classes better than Rocc hio.\\n14.4 Linear versus nonlinear classiﬁers\\nIn this section, we show that the two learning methods Naive B ayes and\\nRocchio are instances of linear classiﬁers, the perhaps mos t important group\\nof text classiﬁers, and contrast them with nonlinear classi ﬁers. To simplify\\nthe discussion, we will only consider two-class classiﬁers in this section and\\ndeﬁne a linear classiﬁer as a two-class classiﬁer that decides class membership LINEAR CLASSIFIER\\nby comparing a linear combination of the features to a thresh old.\\nIn two dimensions, a linear classiﬁer is a line. Five example s are shown\\nin Figure 14.8. These lines have the functional form w1x1+w2x2=b. The\\nclassiﬁcation rule of a linear classiﬁer is to assign a docum ent to cifw1x1+\\nw2x2>band to cifw1x1+w2x2≤b. Here, (x1,x2)Tis the two-dimensional\\nvector representation of the document and (w1,w2)Tis the parameter vector', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 337}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP302 14 Vector space classiﬁcation\\nAPPLY LINEAR CLASSIFIER (⃗w,b,⃗x)\\n1score←∑M\\ni=1wixi\\n2ifscore>b\\n3 then return 1\\n4 else return 0\\n◮Figure 14.9 Linear classiﬁcation algorithm.\\nthat deﬁnes (together with b) the decision boundary. An alternative geomet-\\nric interpretation of a linear classiﬁer is provided in Figu re15.7 (page 343).\\nWe can generalize this 2D linear classiﬁer to higher dimensi ons by deﬁning\\na hyperplane as we did in Equation ( 14.2), repeated here as Equation ( 14.3):\\n⃗wT⃗x=b (14.3)\\nThe assignment criterion then is: assign to cif⃗wT⃗x>band to cif⃗wT⃗x≤b.\\nWe call a hyperplane that we use as a linear classiﬁer a decision hyperplane . DECISION HYPERPLANE\\nThe corresponding algorithm for linear classiﬁcation in Mdimensions is\\nshown in Figure 14.9. Linear classiﬁcation at ﬁrst seems trivial given the\\nsimplicity of this algorithm. However, the difﬁculty is in t raining the lin-\\near classiﬁer, that is, in determining the parameters ⃗wand bbased on the\\ntraining set. In general, some learning methods compute muc h better param-\\neters than others where our criterion for evaluating the qua lity of a learning\\nmethod is the effectiveness of the learned linear classiﬁer on new data.\\nWe now show that Rocchio and Naive Bayes are linear classiﬁer s. To see\\nthis for Rocchio, observe that a vector ⃗xis on the decision boundary if it has\\nequal distance to the two class centroids:\\n|⃗µ(c1)−⃗x|=|⃗µ(c2)−⃗x| (14.4)\\nSome basic arithmetic shows that this corresponds to a linea r classiﬁer with\\nnormal vector ⃗w=⃗µ(c1)−⃗µ(c2)and b=0.5∗(|⃗µ(c1)|2−|⃗µ(c2)|2)(Exer-\\ncise 14.15 ).\\nWe can derive the linearity of Naive Bayes from its decision r ule, which\\nchooses the category cwith the largest ˆP(c|d)(Figure 13.2, page 260) where:\\nˆP(c|d)∝ˆP(c)∏\\n1≤k≤ndˆP(tk|c)\\nand ndis the number of tokens in the document that are part of the voc abu-\\nlary. Denoting the complement category as ¯c, we obtain for the log odds:\\nlogˆP(c|d)\\nˆP(¯c|d)=logˆP(c)\\nˆP(¯c)+∑\\n1≤k≤ndlogˆP(tk|c)\\nˆP(tk|¯c)(14.5)', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 338}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP14.4 Linear versus nonlinear classiﬁers 303\\nti wi d1id2iti wi d1id2i\\nprime 0.70 0 1 dlrs -0.71 1 1\\nrate 0.67 1 0 world -0.35 1 0\\ninterest 0.63 0 0 sees -0.33 0 0\\nrates 0.60 0 0 year -0.25 0 0\\ndiscount 0.46 1 0 group -0.24 0 0\\nbundesbank 0.43 0 0 dlr -0.24 0 0\\n◮Table 14.4 A linear classiﬁer. The dimensions tiand parameters wiof a linear\\nclassiﬁer for the class interest (as ininterest rate ) in Reuters-21578. The threshold is\\nb=0. Terms like dlrandworld have negative weights because they are indicators for\\nthe competing class currency .\\nWe choose class cif the odds are greater than 1 or, equivalently, if the log\\nodds are greater than 0. It is easy to see that Equation ( 14.5) is an instance\\nof Equation ( 14.3) for wi=log[ˆP(ti|c)/ˆP(ti|¯c)],xi=number of occurrences\\noftiind, and b=−log[ˆP(c)/ˆP(¯c)]. Here, the index i, 1≤i≤M, refers\\nto terms of the vocabulary (not to positions in daskdoes; cf. Section 13.4.1 ,\\npage 270) and ⃗xand⃗wareM-dimensional vectors. So in log space, Naive\\nBayes is a linear classiﬁer.\\n✎Example 14.3: Table 14.4 deﬁnes a linear classiﬁer for the category interest in\\nReuters-21578 (see Section 13.6, page 279). We assign document ⃗d1“rate discount\\ndlrs world” to interest since⃗wT⃗d1=0.67·1+0.46·1+ (−0.71)·1+ (−0.35)·1=\\n0.07>0=b. We assign ⃗d2“prime dlrs” to the complement class (not in interest ) since\\n⃗wT⃗d2=−0.01≤b. For simplicity, we assume a simple binary vector represent ation\\nin this example: 1 for occurring terms, 0 for non-occurring t erms.\\nFigure 14.10 is a graphical example of a linear problem , which we deﬁne to\\nmean that the underlying distributions P(d|c)and P(d|c)of the two classes\\nare separated by a line. We call this separating line the class boundary . It is CLASS BOUNDARY\\nthe “true” boundary of the two classes and we distinguish it f rom the deci-\\nsion boundary that the learning method computes to approxim ate the class\\nboundary.\\nAs is typical in text classiﬁcation, there are some noise documents in Fig- NOISE DOCUMENT\\nure14.10 (marked with arrows) that do not ﬁt well into the overall dist ri-\\nbution of the classes. In Section 13.5 (page 271), we deﬁned a noise feature\\nas a misleading feature that, when included in the document r epresentation,\\non average increases the classiﬁcation error. Analogously , a noise document\\nis a document that, when included in the training set, mislea ds the learn-\\ning method and increases classiﬁcation error. Intuitively , the underlying\\ndistribution partitions the representation space into are as with mostly ho-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 339}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP304 14 Vector space classiﬁcation\\n◮Figure 14.10 A linear problem with noise. In this hypothetical web page cl assiﬁ-\\ncation scenario, Chinese-only web pages are solid circles a nd mixed Chinese-English\\nweb pages are squares. The two classes are separated by a line ar class boundary\\n(dashed line, short dashes), except for three noise documen ts (marked with arrows).\\nmogeneous class assignments. A document that does not confo rm with the\\ndominant class in its area is a noise document.\\nNoise documents are one reason why training a linear classiﬁ er is hard. If\\nwe pay too much attention to noise documents when choosing th e decision\\nhyperplane of the classiﬁer, then it will be inaccurate on ne w data. More\\nfundamentally, it is usually difﬁcult to determine which do cuments are noise\\ndocuments and therefore potentially misleading.\\nIf there exists a hyperplane that perfectly separates the tw o classes, then\\nwe call the two classes linearly separable . In fact, if linear separability holds, LINEAR SEPARABILITY\\nthen there is an inﬁnite number of linear separators (Exerci se14.4) as illus-\\ntrated by Figure 14.8, where the number of possible separating hyperplanes\\nis inﬁnite.\\nFigure 14.8 illustrates another challenge in training a linear classiﬁ er. If we\\nare dealing with a linearly separable problem, then we need a criterion for\\nselecting among all decision hyperplanes that perfectly se parate the training\\ndata. In general, some of these hyperplanes will do well on ne w data, some', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 340}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP14.4 Linear versus nonlinear classiﬁers 305\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\\n◮Figure 14.11 A nonlinear problem.\\nwill not.\\nAn example of a nonlinear classiﬁer is kNN. The nonlinearity of kNN is NONLINEAR\\nCLASSIFIER intuitively clear when looking at examples like Figure 14.6. The decision\\nboundaries of kNN (the double lines in Figure 14.6) are locally linear seg-\\nments, but in general have a complex shape that is not equival ent to a line in\\n2D or a hyperplane in higher dimensions.\\nFigure 14.11 is another example of a nonlinear problem: there is no good\\nlinear separator between the distributions P(d|c)and P(d|c)because of the\\ncircular “enclave” in the upper left part of the graph. Linea r classiﬁers mis-\\nclassify the enclave, whereas a nonlinear classiﬁer like kN N will be highly\\naccurate for this type of problem if the training set is large enough.\\nIf a problem is nonlinear and its class boundaries cannot be a pproximated\\nwell with linear hyperplanes, then nonlinear classiﬁers ar e often more accu-\\nrate than linear classiﬁers. If a problem is linear, it is bes t to use a simpler\\nlinear classiﬁer.\\n?Exercise 14.4\\nProve that the number of linear separators of two classes is e ither inﬁnite or zero.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 341}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP306 14 Vector space classiﬁcation\\n14.5 Classiﬁcation with more than two classes\\nWe can extend two-class linear classiﬁers to J>2 classes. The method to use\\ndepends on whether the classes are mutually exclusive or not .\\nClassiﬁcation for classes that are not mutually exclusive i s called any-of , ANY -OF\\nCLASSIFICATION multilabel , ormultivalue classiﬁcation . In this case, a document can belong to\\nseveral classes simultaneously, or to a single class, or to n one of the classes.\\nA decision on one class leaves all options open for the others . It is some-\\ntimes said that the classes are independent of each other, but this is misleading\\nsince the classes are rarely statistically independent in t he sense deﬁned on\\npage 275. In terms of the formal deﬁnition of the classiﬁcation probl em in\\nEquation ( 13.1) (page 256), we learn Jdifferent classiﬁers γjin any-of classi-\\nﬁcation, each returning either cjorcj:γj(d)∈{cj,cj}.\\nSolving an any-of classiﬁcation task with linear classiﬁer s is straightfor-\\nward:\\n1.Build a classiﬁer for each class, where the training set cons ists of the set\\nof documents in the class (positive labels) and its compleme nt (negative\\nlabels).\\n2.Given the test document, apply each classiﬁer separately. T he decision of\\none classiﬁer has no inﬂuence on the decisions of the other cl assiﬁers.\\nThe second type of classiﬁcation with more than two classes i sone-of clas- ONE -OF\\nCLASSIFICATION siﬁcation . Here, the classes are mutually exclusive. Each document mu st\\nbelong to exactly one of the classes. One-of classiﬁcation i s also called multi-\\nnomial ,polytomous4,multiclass , orsingle-label classiﬁcation . Formally, there is a\\nsingle classiﬁcation function γin one-of classiﬁcation whose range is C, i.e.,\\nγ(d)∈{c1, . . . , cJ}. kNN is a (nonlinear) one-of classiﬁer.\\nTrue one-of problems are less common in text classiﬁcation t han any-of\\nproblems. With classes like UK,China ,poultry , orcoffee , a document can be\\nrelevant to many topics simultaneously – as when the prime mi nister of the\\nUK visits China to talk about the coffee and poultry trade.\\nNevertheless, we will often make a one-of assumption, as we d id in Fig-\\nure14.1, even if classes are not really mutually exclusive. For the c lassiﬁca-\\ntion problem of identifying the language of a document, the o ne-of assump-\\ntion is a good approximation as most text is written in only on e language.\\nIn such cases, imposing a one-of constraint can increase the classiﬁer’s ef-\\nfectiveness because errors that are due to the fact that the a ny-of classiﬁers\\nassigned a document to either no class or more than one class a re eliminated.\\nJhyperplanes do not divide R|V|into Jdistinct regions as illustrated in\\nFigure 14.12 . Thus, we must use a combination method when using two-\\nclass linear classiﬁers for one-of classiﬁcation. The simp lest method is to\\n4. A synonym of polytomous ispolychotomous .', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 342}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP14.5 Classiﬁcation with more than two classes 307\\n?\\n◮Figure 14.12 Jhyperplanes do not divide space into Jdisjoint regions.\\nrank classes and then select the top-ranked class. Geometri cally, the ranking\\ncan be with respect to the distances from the Jlinear separators. Documents\\nclose to a class’s separator are more likely to be misclassiﬁ ed, so the greater\\nthe distance from the separator, the more plausible it is tha t a positive clas-\\nsiﬁcation decision is correct. Alternatively, we can use a d irect measure of\\nconﬁdence to rank classes, e.g., probability of class membe rship. We can\\nstate this algorithm for one-of classiﬁcation with linear c lassiﬁers as follows:\\n1.Build a classiﬁer for each class, where the training set cons ists of the set\\nof documents in the class (positive labels) and its compleme nt (negative\\nlabels).\\n2.Given the test document, apply each classiﬁer separately.\\n3.Assign the document to the class with\\n•the maximum score,\\n•the maximum conﬁdence value,\\n•or the maximum probability.\\nAn important tool for analyzing the performance of a classiﬁ er for J>2\\nclasses is the confusion matrix . The confusion matrix shows for each pair of CONFUSION MATRIX\\nclasses⟨c1,c2⟩, how many documents from c1were incorrectly assigned to c2.\\nIn Table 14.5, the classiﬁer manages to distinguish the three ﬁnancial cl asses\\nmoney-fx ,trade , and interest from the three agricultural classes wheat ,corn,\\nand grain , but makes many errors within these two groups. The confusio n\\nmatrix can help pinpoint opportunities for improving the ac curacy of the', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 343}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP308 14 Vector space classiﬁcation\\nassigned class money-fx trade interest wheat corn grain\\ntrue class\\nmoney-fx 95 0 10 0 0 0\\ntrade 1 1 90 0 1 0\\ninterest 13 0 0 0 0 0\\nwheat 0 0 1 34 3 7\\ncorn 1 0 2 13 26 5\\ngrain 0 0 2 14 5 10\\n◮Table 14.5 A confusion matrix for Reuters-21578. For example, 14 docum ents\\nfrom grain were incorrectly assigned to wheat . Adapted from Picca et al. (2006 ).\\nsystem. For example, to address the second largest error in T able 14.5 (14 in\\nthe row grain ), one could attempt to introduce features that distinguish wheat\\ndocuments from grain documents.\\n?Exercise 14.5\\nCreate a training set of 300 documents, 100 each from three di fferent languages (e.g.,\\nEnglish, French, Spanish). Create a test set by the same proc edure, but also add 100\\ndocuments from a fourth language. Train (i) a one-of classiﬁ er (ii) an any-of classi-\\nﬁer on this training set and evaluate it on the test set. (iii) Are there any interesting\\ndifferences in how the two classiﬁers behave on this task?\\n✄14.6 The bias-variance tradeoff\\nNonlinear classiﬁers are more powerful than linear classiﬁ ers. For some\\nproblems, there exists a nonlinear classiﬁer with zero clas siﬁcation error, but\\nno such linear classiﬁer. Does that mean that we should alway s use nonlinear\\nclassiﬁers for optimal effectiveness in statistical text c lassiﬁcation?\\nTo answer this question, we introduce the bias-variance tra deoff in this sec-\\ntion, one of the most important concepts in machine learning . The tradeoff\\nhelps explain why there is no universally optimal learning m ethod. Selecting\\nan appropriate learning method is therefore an unavoidable part of solving\\na text classiﬁcation problem.\\nThroughout this section, we use linear and nonlinear classi ﬁers as proto-\\ntypical examples of “less powerful” and “more powerful” lea rning, respec-\\ntively. This is a simpliﬁcation for a number of reasons. Firs t, many nonlinear\\nmodels subsume linear models as a special case. For instance , a nonlinear\\nlearning method like kNN will in some cases produce a linear c lassiﬁer. Sec-\\nond, there are nonlinear models that are less complex than li near models.\\nFor instance, a quadratic polynomial with two parameters is less powerful\\nthan a 10,000-dimensional linear classiﬁer. Third, the com plexity of learn-\\ning is not really a property of the classiﬁer because there ar e many aspects', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 344}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP14.6 The bias-variance tradeoff 309\\nof learning (such as feature selection, cf. (Section 13.5, page 271), regulariza-\\ntion, and constraints such as margin maximization in Chapte r15) that make\\na learning method either more powerful or less powerful with out affecting\\nthe type of classiﬁer that is the ﬁnal result of learning – reg ardless of whether\\nthat classiﬁer is linear or nonlinear. We refer the reader to the publications\\nlisted in Section 14.7 for a treatment of the bias-variance tradeoff that takes\\ninto account these complexities. In this section, linear an d nonlinear classi-\\nﬁers will simply serve as proxies for weaker and stronger lea rning methods\\nin text classiﬁcation.\\nWe ﬁrst need to state our objective in text classiﬁcation mor e precisely. In\\nSection 13.1 (page 256), we said that we want to minimize classiﬁcation er-\\nror on the test set. The implicit assumption was that trainin g documents\\nand test documents are generated according to the same under lying distri-\\nbution. We will denote this distribution P(⟨d,c⟩)where dis the document\\nand cits label or class. Figures 13.4 and 13.5 were examples of generative\\nmodels that decompose P(⟨d,c⟩)into the product of P(c)and P(d|c). Fig-\\nures 14.10 and 14.11 depict generative models for ⟨d,c⟩with d∈R2and\\nc∈{square, solid circle }.\\nIn this section, instead of using the number of correctly cla ssiﬁed test doc-\\numents (or, equivalently, the error rate on test documents) as evaluation\\nmeasure, we adopt an evaluation measure that addresses the i nherent un-\\ncertainty of labeling. In many text classiﬁcation problems , a given document\\nrepresentation can arise from documents belonging to diffe rent classes. This\\nis because documents from different classes can be mapped to the same doc-\\nument representation. For example, the one-sentence docum ents China sues\\nFrance and France sues China are mapped to the same document representa-\\ntion d′={China ,France ,sues}in a bag of words model. But only the latter\\ndocument is relevant to the class c′=legal actions brought by France (which\\nmight be deﬁned, for example, as a standing query by an intern ational trade\\nlawyer).\\nTo simplify the calculations in this section, we do not count the number\\nof errors on the test set when evaluating a classiﬁer, but ins tead look at how\\nwell the classiﬁer estimates the conditional probability P(c|d)of a document\\nbeing in a class. In the above example, we might have P(c′|d′) =0.5.\\nOur goal in text classiﬁcation then is to ﬁnd a classiﬁer γsuch that, aver-\\naged over documents d,γ(d)is as close as possible to the true probability\\nP(c|d). We measure this using mean squared error:\\nMSE(γ) =Ed[γ(d)−P(c|d)]2(14.6)\\nwhere Edis the expectation with respect to P(d). The mean squared error\\nterm gives partial credit for decisions by γthat are close if not completely\\nright.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 345}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP310 14 Vector space classiﬁcation\\nE[x−α]2=Ex2−2Exα+α2(14.8)\\n= ( Ex)2−2Exα+α2\\n+Ex2−2(Ex)2+ (Ex)2\\n= [ Ex−α]2\\n+Ex2−E2x(Ex) +E(Ex)2\\n= [ Ex−α]2+E[x−Ex]2\\nEDEd[ΓD(d)−P(c|d)]2=EdED[ΓD(d)−P(c|d)]2(14.9)\\n=Ed[ [EDΓD(d)−P(c|d)]2\\n+ED[ΓD(d)−EDΓD(d)]2]\\n◮Figure 14.13 Arithmetic transformations for the bias-variance decompo sition.\\nFor the derivation of Equation ( 14.9), we set α=P(c|d)and x=ΓD(d)in Equa-\\ntion ( 14.8).\\nWe deﬁne a classiﬁer γto be optimal for a distribution P(⟨d,c⟩)if it mini- OPTIMAL CLASSIFIER\\nmizes MSE (γ).\\nMinimizing MSE is a desideratum for classiﬁers . We also need a criterion\\nforlearning methods . Recall that we deﬁned a learning method Γas a function\\nthat takes a labeled training set Das input and returns a classiﬁer γ.\\nFor learning methods, we adopt as our goal to ﬁnd a Γthat, averaged over\\ntraining sets, learns classiﬁers γwith minimal MSE. We can formalize this as\\nminimizing learning error : LEARNING ERROR\\nlearning-error (Γ) =ED[MSE(Γ(D))] (14.7)\\nwhere EDis the expectation over labeled training sets. To keep thing s simple,\\nwe can assume that training sets have a ﬁxed size – the distrib ution P(⟨d,c⟩)\\nthen deﬁnes a distribution P(D)over training sets.\\nWe can use learning error as a criterion for selecting a learn ing method in\\nstatistical text classiﬁcation. A learning method Γisoptimal for a distribution OPTIMAL LEARNING\\nMETHOD P(D)if it minimizes the learning error.\\nWriting ΓDforΓ(D)for better readability, we can transform Equation ( 14.7)\\nas follows:\\nlearning-error (Γ) = ED[MSE(ΓD)]\\n=EDEd[ΓD(d)−P(c|d)]2(14.10)\\n=Ed[bias(Γ,d) +variance (Γ,d)] (14.11)', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 346}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP14.6 The bias-variance tradeoff 311\\nbias(Γ,d) = [ P(c|d)−EDΓD(d)]2(14.12)\\nvariance (Γ,d) = ED[ΓD(d)−EDΓD(d)]2(14.13)\\nwhere the equivalence between Equations ( 14.10 ) and ( 14.11 ) is shown in\\nEquation ( 14.9) in Figure 14.13 . Note that dand Dare independent of each\\nother. In general, for a random document dand a random training set D,D\\ndoes not contain a labeled instance of d.\\nBias is the squared difference between P(c|d), the true conditional prob- BIAS\\nability of dbeing in c, and ΓD(d), the prediction of the learned classiﬁer,\\naveraged over training sets. Bias is large if the learning me thod produces\\nclassiﬁers that are consistently wrong. Bias is small if (i) the classiﬁers are\\nconsistently right or (ii) different training sets cause er rors on different docu-\\nments or (iii) different training sets cause positive and ne gative errors on the\\nsame documents, but that average out to close to 0. If one of th ese three con-\\nditions holds, then EDΓD(d), the expectation over all training sets, is close to\\nP(c|d).\\nLinear methods like Rocchio and Naive Bayes have a high bias f or non-\\nlinear problems because they can only model one type of class boundary, a\\nlinear hyperplane. If the generative model P(⟨d,c⟩)has a complex nonlinear\\nclass boundary, the bias term in Equation ( 14.11 ) will be high because a large\\nnumber of points will be consistently misclassiﬁed. For exa mple, the circular\\nenclave in Figure 14.11 does not ﬁt a linear model and will be misclassiﬁed\\nconsistently by linear classiﬁers.\\nWe can think of bias as resulting from our domain knowledge (o r lack\\nthereof) that we build into the classiﬁer. If we know that the true boundary\\nbetween the two classes is linear, then a learning method tha t produces linear\\nclassiﬁers is more likely to succeed than a nonlinear method . But if the true\\nclass boundary is not linear and we incorrectly bias the clas siﬁer to be linear,\\nthen classiﬁcation accuracy will be low on average.\\nNonlinear methods like kNN have low bias. We can see in Figure 14.6 that\\nthe decision boundaries of kNN are variable – depending on th e distribu-\\ntion of documents in the training set, learned decision boun daries can vary\\ngreatly. As a result, each document has a chance of being clas siﬁed correctly\\nfor some training sets. The average prediction EDΓD(d)is therefore closer to\\nP(c|d)and bias is smaller than for a linear learning method.\\nVariance is the variation of the prediction of learned classiﬁers: th e aver- VARIANCE\\nage squared difference between ΓD(d)and its average EDΓD(d). Variance is\\nlarge if different training sets Dgive rise to very different classiﬁers ΓD. It is\\nsmall if the training set has a minor effect on the classiﬁcat ion decisions ΓD\\nmakes, be they correct or incorrect. Variance measures how i nconsistent the\\ndecisions are, not whether they are correct or incorrect.\\nLinear learning methods have low variance because most rand omly drawn\\ntraining sets produce similar decision hyperplanes. The de cision lines pro-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 347}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP312 14 Vector space classiﬁcation\\nduced by linear learning methods in Figures 14.10 and 14.11 will deviate\\nslightly from the main class boundaries, depending on the tr aining set, but\\nthe class assignment for the vast majority of documents (wit h the exception\\nof those close to the main boundary) will not be affected. The circular enclave\\nin Figure 14.11 will be consistently misclassiﬁed.\\nNonlinear methods like kNN have high variance. It is apparen t from Fig-\\nure14.6 that kNN can model very complex boundaries between two class es.\\nIt is therefore sensitive to noise documents of the sort depi cted in Figure 14.10 .\\nAs a result the variance term in Equation ( 14.11 ) is large for kNN: Test doc-\\numents are sometimes misclassiﬁed – if they happen to be clos e to a noise\\ndocument in the training set – and sometimes correctly class iﬁed – if there\\nare no noise documents in the training set near them. This res ults in high\\nvariation from training set to training set.\\nHigh-variance learning methods are prone to overﬁtting the training data. OVERFITTING\\nThe goal in classiﬁcation is to ﬁt the training data to the ext ent that we cap-\\nture true properties of the underlying distribution P(⟨d,c⟩). In overﬁtting,\\nthe learning method also learns from noise. Overﬁtting incr eases MSE and\\nfrequently is a problem for high-variance learning methods .\\nWe can also think of variance as the model complexity or, equivalently, mem- MEMORY CAPACITY\\nory capacity of the learning method – how detailed a characterization of t he\\ntraining set it can remember and then apply to new data. This c apacity corre-\\nsponds to the number of independent parameters available to ﬁt the training\\nset. Each kNN neighborhood Skmakes an independent classiﬁcation deci-\\nsion. The parameter in this case is the estimate ˆP(c|Sk)from Figure 14.7.\\nThus, kNN’s capacity is only limited by the size of the traini ng set. It can\\nmemorize arbitrarily large training sets. In contrast, the number of parame-\\nters of Rocchio is ﬁxed – Jparameters per dimension, one for each centroid\\n– and independent of the size of the training set. The Rocchio classiﬁer (in\\nform of the centroids deﬁning it) cannot “remember” ﬁne-gra ined details of\\nthe distribution of the documents in the training set.\\nAccording to Equation ( 14.7), our goal in selecting a learning method is to\\nminimize learning error. The fundamental insight captured by Equation ( 14.11 ),\\nwhich we can succinctly state as: learning-error = bias + var iance, is that the\\nlearning error has two components, bias and variance, which in general can-\\nnot be minimized simultaneously. When comparing two learni ng methods\\nΓ1and Γ2, in most cases the comparison comes down to one method having\\nhigher bias and lower variance and the other lower bias and hi gher variance.\\nThe decision for one learning method vs. another is then not s imply a mat-\\nter of selecting the one that reliably produces good classiﬁ ers across training\\nsets (small variance) or the one that can learn classiﬁcatio n problems with\\nvery difﬁcult decision boundaries (small bias). Instead, w e have to weigh\\nthe respective merits of bias and variance in our applicatio n and choose ac-\\ncordingly. This tradeoff is called the bias-variance tradeoff . BIAS -VARIANCE\\nTRADEOFF', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 348}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP14.6 The bias-variance tradeoff 313\\nFigure 14.10 provides an illustration, which is somewhat contrived, but\\nwill be useful as an example for the tradeoff. Some Chinese te xt contains\\nEnglish words written in the Roman alphabet like CPU,ONLINE , andGPS.\\nConsider the task of distinguishing Chinese-only web pages from mixed\\nChinese-English web pages. A search engine might offer Chin ese users with-\\nout knowledge of English (but who understand loanwords like CPU) the op-\\ntion of ﬁltering out mixed pages. We use two features for this classiﬁcation\\ntask: number of Roman alphabet characters and number of Chin ese char-\\nacters on the web page. As stated earlier, the distribution P(⟨d,c⟩) of the\\ngenerative model generates most mixed (respectively, Chin ese) documents\\nabove (respectively, below) the short-dashed line, but the re are a few noise\\ndocuments.\\nIn Figure 14.10 , we see three classiﬁers:\\n•One-feature classiﬁer. Shown as a dotted horizontal line. This classiﬁer\\nuses only one feature, the number of Roman alphabet characte rs. Assum-\\ning a learning method that minimizes the number of misclassi ﬁcations\\nin the training set, the position of the horizontal decision boundary is\\nnot greatly affected by differences in the training set (e.g ., noise docu-\\nments). So a learning method producing this type of classiﬁe r has low\\nvariance. But its bias is high since it will consistently mis classify squares\\nin the lower left corner and “solid circle” documents with mo re than 50\\nRoman characters.\\n•Linear classiﬁer. Shown as a dashed line with long dashes. Learning lin-\\near classiﬁers has less bias since only noise documents and p ossibly a few\\ndocuments close to the boundary between the two classes are m isclassi-\\nﬁed. The variance is higher than for the one-feature classiﬁ ers, but still\\nsmall: The dashed line with long dashes deviates only slight ly from the\\ntrue boundary between the two classes, and so will almost all linear de-\\ncision boundaries learned from training sets. Thus, very fe w documents\\n(documents close to the class boundary) will be inconsisten tly classiﬁed.\\n•“Fit-training-set-perfectly” classiﬁer. Shown as a solid line. Here, the\\nlearning method constructs a decision boundary that perfec tly separates\\nthe classes in the training set. This method has the lowest bi as because\\nthere is no document that is consistently misclassiﬁed – the classiﬁers\\nsometimes even get noise documents in the test set right. But the variance\\nof this learning method is high. Because noise documents can move the\\ndecision boundary arbitrarily, test documents close to noi se documents\\nin the training set will be misclassiﬁed – something that a li near learning\\nmethod is unlikely to do.\\nIt is perhaps surprising that so many of the best-known text c lassiﬁcation\\nalgorithms are linear. Some of these methods, in particular linear SVMs, reg-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 349}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP314 14 Vector space classiﬁcation\\nularized logistic regression and regularized linear regre ssion, are among the\\nmost effective known methods. The bias-variance tradeoff p rovides insight\\ninto their success. Typical classes in text classiﬁcation a re complex and seem\\nunlikely to be modeled well linearly. However, this intuiti on is misleading\\nfor the high-dimensional spaces that we typically encounte r in text appli-\\ncations. With increased dimensionality, the likelihood of linear separability\\nincreases rapidly (Exercise 14.17 ). Thus, linear models in high-dimensional\\nspaces are quite powerful despite their linearity. Even mor e powerful nonlin-\\near learning methods can model decision boundaries that are more complex\\nthan a hyperplane, but they are also more sensitive to noise i n the training\\ndata. Nonlinear learning methods sometimes perform better if the training\\nset is large, but by no means in all cases.\\n14.7 References and further reading\\nAs discussed in Chapter 9, Rocchio relevance feedback is due to Rocchio\\n(1971 ).Joachims (1997 ) presents a probabilistic analysis of the method. Roc-\\nchio classiﬁcation was widely used as a classiﬁcation metho d in TREC in the\\n1990s ( Buckley et al. 1994a ;b,Voorhees and Harman 2005 ). Initially, it was\\nused as a form of routing . Routing merely ranks documents according to rel- ROUTING\\nevance to a class without assigning them. Early work on ﬁltering , a true clas- FILTERING\\nsiﬁcation approach that makes an assignment decision on eac h document,\\nwas published by Ittner et al. (1995 ) and Schapire et al. (1998 ). The deﬁnition\\nof routing we use here should not be confused with another sen se. Routing\\ncan also refer to the electronic distribution of documents t o subscribers, the\\nso-called push model of document distribution. In a pull model , each transfer PUSH MODEL\\nPULL MODEL of a document to the user is initiated by the user – for example , by means\\nof search or by selecting it from a list of documents on a news a ggregation\\nwebsite.\\nSome authors restrict the name Roccchio classiﬁcation to two-class problems\\nand use the terms cluster-based (Iwayama and Tokunaga 1995 ) and centroid- CENTROID -BASED\\nCLASSIFICATION based classiﬁcation (Han and Karypis 2000 ,Tan and Cheng 2007 ) for Rocchio\\nclassiﬁcation with J>2.\\nA more detailed treatment of kNN can be found in ( Hastie et al. 2001 ), in-\\ncluding methods for tuning the parameter k. An example of an approximate\\nfast kNN algorithm is locality-based hashing ( Andoni et al. 2006 ).Klein-\\nberg (1997 ) presents an approximate Θ((Mlog2M)(M+logN))kNN algo-\\nrithm (where Mis the dimensionality of the space and Nthe number of data\\npoints), but at the cost of exponential storage requirement s:Θ((NlogM)2M).\\nIndyk (2004 ) surveys nearest neighbor methods in high-dimensional spa ces.\\nEarly work on kNN in text classiﬁcation was motivated by the a vailability\\nof massively parallel hardware architectures ( Creecy et al. 1992 ).Yang (1994 )', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 350}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP14.8 Exercises 315\\nuses an inverted index to speed up kNN classiﬁcation. The opt imality result\\nfor 1NN (twice the Bayes error rate asymptotically) is due to Cover and Hart\\n(1967 ).\\nThe effectiveness of Rocchio classiﬁcation and kNN is highl y dependent\\non careful parameter tuning (in particular, the parameters b′for Rocchio on\\npage 296and kfor kNN), feature engineering (Section 15.3, page 334) and\\nfeature selection (Section 13.5, page 271).Buckley and Salton (1995 ),Schapire\\net al. (1998 ),Yang and Kisiel (2003 ) and Moschitti (2003 ) address these issues\\nfor Rocchio and Yang (2001 ) and Ault and Yang (2002 ) for kNN. Zavrel et al.\\n(2000 ) compare feature selection methods for kNN.\\nThe bias-variance tradeoff was introduced by Geman et al. (1992 ). The\\nderivation in Section 14.6 is for MSE (γ), but the tradeoff applies to many\\nloss functions (cf. Friedman (1997 ),Domingos (2000 )).Schütze et al. (1995 )\\nand Lewis et al. (1996 ) discuss linear classiﬁers for text and Hastie et al. (2001 )\\nlinear classiﬁers in general. Readers interested in the alg orithms mentioned,\\nbut not described in this chapter may wish to consult Bishop (2006 ) for neu-\\nral networks, Hastie et al. (2001 ) for linear and logistic regression, and Min-\\nsky and Papert (1988 ) for the perceptron algorithm. Anagnostopoulos et al.\\n(2006 ) show that an inverted index can be used for highly efﬁcient d ocument\\nclassiﬁcation with any linear classiﬁer, provided that the classiﬁer is still ef-\\nfective when trained on a modest number of features via featu re selection.\\nWe have only presented the simplest method for combining two -class clas-\\nsiﬁers into a one-of classiﬁer. Another important method is the use of error-\\ncorrecting codes, where a vector of decisions of different t wo-class classiﬁers\\nis constructed for each document. A test document’s decisio n vector is then\\n“corrected” based on the distribution of decision vectors i n the training set,\\na procedure that incorporates information from all two-cla ss classiﬁers and\\ntheir correlations into the ﬁnal classiﬁcation decision ( Dietterich and Bakiri\\n1995 ).Ghamrawi and McCallum (2005 ) also exploit dependencies between\\nclasses in any-of classiﬁcation. Allwein et al. (2000 ) propose a general frame-\\nwork for combining two-class classiﬁers.\\n14.8 Exercises\\n?Exercise 14.6\\nIn Figure 14.14 , which of the three vectors ⃗a,⃗b, and⃗cis (i) most similar to ⃗xaccording\\nto dot product similarity, (ii) most similar to ⃗xaccording to cosine similarity, (iii)\\nclosest to ⃗xaccording to Euclidean distance?\\nExercise 14.7\\nDownload Reuters-21578 and train and test Rocchio and kNN cl assiﬁers for the classes\\nacquisitions ,corn,crude ,earn,grain ,interest ,money-fx ,ship,trade , and wheat . Use the\\nModApte split. You may want to use one of a number of software p ackages that im-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 351}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP316 14 Vector space classiﬁcation\\n0 1 2 3 4 5 6 7 8012345678\\naxbc\\n◮Figure 14.14 Example for differences between Euclidean distance, dot pr oduct\\nsimilarity and cosine similarity. The vectors are ⃗a= (0.5 1.5 )T,⃗x= (2 2)T,⃗b=\\n(4 4)T, and⃗c= (8 6)T.\\nplement Rocchio classiﬁcation and kNN classiﬁcation, for e xample, the Bow toolkit\\n(McCallum 1996 ).\\nExercise 14.8\\nDownload 20 Newgroups (page 154) and train and test Rocchio and kNN classiﬁers\\nfor its 20 classes.\\nExercise 14.9\\nShow that the decision boundaries in Rocchio classiﬁcation are, as in kNN, given by\\nthe Voronoi tessellation.\\nExercise 14.10 [⋆]\\nComputing the distance between a dense centroid and a sparse vector is Θ(M)for\\na naive implementation that iterates over all Mdimensions. Based on the equality\\n∑(xi−µi)2=1.0+∑µ2\\ni−2∑xiµiand assuming that ∑µ2\\nihas been precomputed,\\nwrite down an algorithm that is Θ(Ma)instead, where Mais the number of distinct\\nterms in the test document.\\nExercise 14.11 [⋆ ⋆ ⋆ ]\\nProve that the region of the plane consisting of all points wi th the same knearest\\nneighbors is a convex polygon.\\nExercise 14.12\\nDesign an algorithm that performs an efﬁcient 1NN search in 1 dimension (where\\nefﬁciency is with respect to the number of documents N). What is the time complexity\\nof the algorithm?\\nExercise 14.13 [⋆ ⋆ ⋆ ]\\nDesign an algorithm that performs an efﬁcient 1NN search in 2 dimensions with at\\nmost polynomial (in N) preprocessing time.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 352}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP14.8 Exercises 317\\n/Bullet\\n/Bullet\\n◮Figure 14.15 A simple non-separable set of points.\\nExercise 14.14 [⋆ ⋆ ⋆ ]\\nCan one design an exact efﬁcient algorithm for 1NN for very la rgeMalong the ideas\\nyou used to solve the last exercise?\\nExercise 14.15\\nShow that Equation ( 14.4) deﬁnes a hyperplane with ⃗w=⃗µ(c1)−⃗µ(c2)and b=\\n0.5∗(|⃗µ(c1)|2−|⃗µ(c2)|2).\\nExercise 14.16\\nWe can easily construct non-separable data sets in high dime nsions by embedding\\na non-separable set like the one shown in Figure 14.15 . Consider embedding Fig-\\nure14.15 in 3D and then perturbing the 4 points slightly (i.e., moving them a small\\ndistance in a random direction). Why would you expect the res ulting conﬁguration\\nto be linearly separable? How likely is then a non-separable set of m≪Mpoints in\\nM-dimensional space?\\nExercise 14.17\\nAssuming two classes, show that the percentage of non-separ able assignments of the\\nvertices of a hypercube decreases with dimensionality MforM>1. For example,\\nforM=1 the proportion of non-separable assignments is 0, for M=2, it is 2/16.\\nOne of the two non-separable cases for M=2 is shown in Figure 14.15 , the other is\\nits mirror image. Solve the exercise either analytically or by simulation.\\nExercise 14.18\\nAlthough we point out the similarities of Naive Bayes with li near vector space classi-\\nﬁers, it does not make sense to represent count vectors (the d ocument representations\\nin NB) in a continuous vector space. There is however a formal ization of NB that is\\nanalogous to Rocchio. Show that NB assigns a document to the c lass (represented as\\na parameter vector) whose Kullback-Leibler (KL) divergenc e (Section 12.4, page 251)\\nto the document (represented as a count vector as in Section 13.4.1 (page 270), nor-\\nmalized to sum to 1) is smallest.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 353}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 354}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 319\\n15Support vector machines and\\nmachine learning on documents\\nImproving classiﬁer effectiveness has been an area of inten sive machine-\\nlearning research over the last two decades, and this work ha s led to a new\\ngeneration of state-of-the-art classiﬁers, such as suppor t vector machines,\\nboosted decision trees, regularized logistic regression, neural networks, and\\nrandom forests. Many of these methods, including support ve ctor machines\\n(SVMs), the main topic of this chapter, have been applied wit h success to\\ninformation retrieval problems, particularly text classi ﬁcation. An SVM is a\\nkind of large-margin classiﬁer: it is a vector space based ma chine learning\\nmethod where the goal is to ﬁnd a decision boundary between tw o classes\\nthat is maximally far from any point in the training data (pos sibly discount-\\ning some points as outliers or noise).\\nWe will initially motivate and develop SVMs for the case of tw o-class data\\nsets that are separable by a linear classiﬁer (Section 15.1), and then extend the\\nmodel in Section 15.2 to non-separable data, multi-class problems, and non-\\nlinear models, and also present some additional discussion of SVM perfor-\\nmance. The chapter then moves to consider the practical depl oyment of text\\nclassiﬁers in Section 15.3: what sorts of classiﬁers are appropriate when, and\\nhow can you exploit domain-speciﬁc text features in classiﬁ cation? Finally,\\nwe will consider how the machine learning technology that we have been\\nbuilding for text classiﬁcation can be applied back to the pr oblem of learning\\nhow to rank documents in ad hoc retrieval (Section 15.4). While several ma-\\nchine learning methods have been applied to this task, use of SVMs has been\\nprominent. Support vector machines are not necessarily bet ter than other\\nmachine learning methods (except perhaps in situations wit h little training\\ndata), but they perform at the state-of-the-art level and ha ve much current\\ntheoretical and empirical appeal.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 355}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP320 15 Support vector machines and machine learning on document s\\n/Bullet\\n/Bullet/Bullet\\n/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet/Bullet/SolidTriangle/Triangle\\n/SolidTriangle/Triangle/SolidTriangle/Triangle\\n/SolidTriangle/Triangle\\n/SolidTriangle/Triangle/SolidTriangle/Triangle/SolidTriangle/TriangleSupport vectors Maximum\\nmargin\\ndecision\\nhyperplane\\nMargin is\\nmaximized\\n◮Figure 15.1 The support vectors are the 5 points right up against the marg in of\\nthe classiﬁer.\\n15.1 Support vector machines: The linearly separable case\\nFor two-class, separable training data sets, such as the one in Figure 14.8\\n(page 301), there are lots of possible linear separators. Intuitivel y, a decision\\nboundary drawn in the middle of the void between data items of the two\\nclasses seems better than one which approaches very close to examples of\\none or both classes. While some learning methods such as the p erceptron\\nalgorithm (see references in Section 14.7, page 314) ﬁnd just any linear sepa-\\nrator, others, like Naive Bayes, search for the best linear s eparator according\\nto some criterion. The SVM in particular deﬁnes the criterio n to be looking\\nfor a decision surface that is maximally far away from any dat a point. This\\ndistance from the decision surface to the closest data point determines the\\nmargin of the classiﬁer. This method of construction necessarily m eans that MARGIN\\nthe decision function for an SVM is fully speciﬁed by a (usual ly small) sub-\\nset of the data which deﬁnes the position of the separator. Th ese points are\\nreferred to as the support vectors (in a vector space, a point can be thought of SUPPORT VECTOR\\nas a vector between the origin and that point). Figure 15.1 shows the margin\\nand support vectors for a sample problem. Other data points p lay no part in\\ndetermining the decision surface that is chosen.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 356}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP15.1 Support vector machines: The linearly separable case 321\\n◮Figure 15.2 An intuition for large-margin classiﬁcation. Insisting on a large mar-\\ngin reduces the capacity of the model: the range of angles at w hich the fat deci-\\nsion surface can be placed is smaller than for a decision hype rplane (cf. Figure 14.8,\\npage 301).\\nMaximizing the margin seems good because points near the dec ision sur-\\nface represent very uncertain classiﬁcation decisions: th ere is almost a 50%\\nchance of the classiﬁer deciding either way. A classiﬁer wit h a large margin\\nmakes no low certainty classiﬁcation decisions. This gives you a classiﬁca-\\ntion safety margin: a slight error in measurement or a slight document vari-\\nation will not cause a misclassiﬁcation. Another intuition motivating SVMs\\nis shown in Figure 15.2. By construction, an SVM classiﬁer insists on a large\\nmargin around the decision boundary. Compared to a decision hyperplane,\\nif you have to place a fat separator between classes, you have fewer choices\\nof where it can be put. As a result of this, the memory capacity of the model\\nhas been decreased, and hence we expect that its ability to co rrectly general-\\nize to test data is increased (cf. the discussion of the bias- variance tradeoff in\\nChapter 14, page 312).\\nLet us formalize an SVM with algebra. A decision hyperplane ( page 302)\\ncan be deﬁned by an intercept term band a decision hyperplane normal vec-\\ntor⃗wwhich is perpendicular to the hyperplane. This vector is com monly', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 357}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP322 15 Support vector machines and machine learning on document s\\nreferred to in the machine learning literature as the weight vector . To choose WEIGHT VECTOR\\namong all the hyperplanes that are perpendicular to the norm al vector, we\\nspecify the intercept term b. Because the hyperplane is perpendicular to the\\nnormal vector, all points ⃗xon the hyperplane satisfy ⃗wT⃗x=−b. Now sup-\\npose that we have a set of training data points D={(⃗xi,yi)}, where each\\nmember is a pair of a point ⃗xiand a class label yicorresponding to it.1For\\nSVMs, the two data classes are always named +1 and−1 (rather than 1 and\\n0), and the intercept term is always explicitly represented asb(rather than\\nbeing folded into the weight vector ⃗wby adding an extra always-on feature).\\nThe math works out much more cleanly if you do things this way, as we will\\nsee almost immediately in the deﬁnition of functional margi n. The linear\\nclassiﬁer is then:\\nf(⃗x) =sign(⃗wT⃗x+b) (15.1)\\nA value of−1 indicates one class, and a value of +1 the other class.\\nWe are conﬁdent in the classiﬁcation of a point if it is far awa y from the\\ndecision boundary. For a given data set and decision hyperpl ane, we deﬁne\\nthefunctional margin of the ithexample ⃗xiwith respect to a hyperplane ⟨⃗w,b⟩ FUNCTIONAL MARGIN\\nas the quantity yi(⃗wT⃗xi+b). The functional margin of a data set with re-\\nspect to a decision surface is then twice the functional marg in of any of the\\npoints in the data set with minimal functional margin (the fa ctor of 2 comes\\nfrom measuring across the whole width of the margin, as in Fig ure15.3).\\nHowever, there is a problem with using this deﬁnition as is: t he value is un-\\nderconstrained, because we can always make the functional m argin as big\\nas we wish by simply scaling up ⃗wand b. For example, if we replace ⃗wby\\n5⃗wand bby 5 bthen the functional margin yi(5⃗wT⃗xi+5b)is ﬁve times as\\nlarge. This suggests that we need to place some constraint on the size of the\\n⃗wvector. To get a sense of how to do that, let us look at the actua l geometry.\\nWhat is the Euclidean distance from a point ⃗xto the decision boundary? In\\nFigure 15.3, we denote by rthis distance. We know that the shortest distance\\nbetween a point and a hyperplane is perpendicular to the plan e, and hence,\\nparallel to ⃗w. A unit vector in this direction is ⃗w/|⃗w|. The dotted line in the\\ndiagram is then a translation of the vector r⃗w/|⃗w|. Let us label the point on\\nthe hyperplane closest to ⃗xas⃗x′. Then:\\n⃗x′=⃗x−yr⃗w\\n|⃗w|(15.2)\\nwhere multiplying by yjust changes the sign for the two cases of ⃗xbeing on\\neither side of the decision surface. Moreover, ⃗x′lies on the decision boundary\\n1. As discussed in Section 14.1 (page 291), we present the general case of points in a vector\\nspace, but if the points are length normalized document vect ors, then all the action is taking\\nplace on the surface of a unit sphere, and the decision surfac e intersects the sphere’s surface.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 358}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP15.1 Support vector machines: The linearly separable case 323\\n0 1 2 3 4 5 6 7 801234567\\n/Bullet\\n/Bullet/Bullet\\n/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet/Bullet/SolidTriangle/Triangle\\n/SolidTriangle/Triangle/SolidTriangle/Triangle⃗x\\n+\\n⃗x′r/SolidTriangle/Triangle\\n/SolidTriangle/Triangle/SolidTriangle/Triangle/SolidTriangle/Triangle\\nρ\\n⃗w\\n◮Figure 15.3 The geometric margin of a point ( r) and a decision boundary ( ρ).\\nand so satisﬁes ⃗wT⃗x′+b=0. Hence:\\n⃗wT(⃗x−yr⃗w\\n|⃗w|)+b=0 (15.3)\\nSolving for rgives:2\\nr=y⃗wT⃗x+b\\n|⃗w|(15.4)\\nAgain, the points closest to the separating hyperplane are s upport vectors.\\nThe geometric margin of the classiﬁer is the maximum width of the band that GEOMETRIC MARGIN\\ncan be drawn separating the support vectors of the two classe s. That is, it is\\ntwice the minimum value over data points for rgiven in Equation ( 15.4), or,\\nequivalently, the maximal width of one of the fat separators shown in Fig-\\nure15.2. The geometric margin is clearly invariant to scaling of par ameters:\\nif we replace ⃗wby 5⃗wand bby 5b, then the geometric margin is the same, be-\\ncause it is inherently normalized by the length of ⃗w. This means that we can\\nimpose any scaling constraint we wish on ⃗wwithout affecting the geometric\\nmargin. Among other choices, we could use unit vectors, as in Chapter 6, by\\n2. Recall that|⃗w|=√\\n⃗wT⃗w.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 359}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP324 15 Support vector machines and machine learning on document s\\nrequiring that|⃗w|=1. This would have the effect of making the geometric\\nmargin the same as the functional margin.\\nSince we can scale the functional margin as we please, for con venience in\\nsolving large SVMs, let us choose to require that the functio nal margin of all\\ndata points is at least 1 and that it is equal to 1 for at least on e data vector.\\nThat is, for all items in the data:\\nyi(⃗wT⃗xi+b)≥1 (15.5)\\nand there exist support vectors for which the inequality is a n equality. Since\\neach example’s distance from the hyperplane is ri=yi(⃗wT⃗xi+b)/|⃗w|, the\\ngeometric margin is ρ=2/|⃗w|. Our desire is still to maximize this geometric\\nmargin. That is, we want to ﬁnd ⃗wand bsuch that:\\n•ρ=2/|⃗w|is maximized\\n•For all (⃗xi,yi)∈D,yi(⃗wT⃗xi+b)≥1\\nMaximizing 2/|⃗w|is the same as minimizing |⃗w|/2. This gives the ﬁnal stan-\\ndard formulation of an SVM as a minimization problem:\\n(15.6) Find⃗wand bsuch that:\\n•1\\n2⃗wT⃗wis minimized, and\\n•for all{(⃗xi,yi)},yi(⃗wT⃗xi+b)≥1\\nWe are now optimizing a quadratic function subject to linear constraints.\\nQuadratic optimization problems are a standard, well-known class of mathe- QUADRATIC\\nPROGRAMMING matical optimization problems, and many algorithms exist f or solving them.\\nWe could in principle build our SVM using standard quadratic programming\\n(QP) libraries, but there has been much recent research in th is area aiming to\\nexploit the structure of the kind of QP that emerges from an SV M. As a result,\\nthere are more intricate but much faster and more scalable li braries available\\nespecially for building SVMs, which almost everyone uses to build models.\\nWe will not present the details of such algorithms here.\\nHowever, it will be helpful to what follows to understand the shape of the\\nsolution of such an optimization problem. The solution invo lves construct-\\ning a dual problem where a Lagrange multiplier αiis associated with each\\nconstraint yi(⃗wT⃗xi+b)≥1 in the primal problem:\\n(15.7) Find α1, . . .αNsuch that ∑αi−1\\n2∑i∑jαiαjyiyj⃗xiT⃗xjis maximized, and\\n•∑iαiyi=0\\n•αi≥0 for all 1≤i≤N\\nThe solution is then of the form:', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 360}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP15.1 Support vector machines: The linearly separable case 325\\n0 1 2 30123\\n/Bullet\\n/Bullet/SolidTriangle/Triangle\\n◮Figure 15.4 A tiny 3 data point training set for an SVM.\\n(15.8) ⃗w=∑αiyi⃗xi\\nb=yk−⃗wT⃗xkfor any ⃗xksuch that αk̸=0\\nIn the solution, most of the αiare zero. Each non-zero αiindicates that the\\ncorresponding ⃗xiis a support vector. The classiﬁcation function is then:\\nf(⃗x) =sign(∑iαiyi⃗xiT⃗x+b) (15.9)\\nBoth the term to be maximized in the dual problem and the class ifying func-\\ntion involve a dot product between pairs of points ( ⃗xand⃗xior⃗xiand⃗xj), and\\nthat is the only way the data are used – we will return to the sig niﬁcance of\\nthis later.\\nTo recap, we start with a training data set. The data set uniqu ely deﬁnes\\nthe best separating hyperplane, and we feed the data through a quadratic\\noptimization procedure to ﬁnd this plane. Given a new point ⃗xto classify,\\nthe classiﬁcation function f(⃗x)in either Equation ( 15.1) or Equation ( 15.9) is\\ncomputing the projection of the point onto the hyperplane no rmal. The sign\\nof this function determines the class to assign to the point. If the point is\\nwithin the margin of the classiﬁer (or another conﬁdence thr eshold tthat we\\nmight have determined to minimize classiﬁcation mistakes) then the classi-\\nﬁer can return “don’t know” rather than one of the two classes . The value\\noff(⃗x)may also be transformed into a probability of classiﬁcation ; ﬁtting\\na sigmoid to transform the values is standard ( Platt 2000 ). Also, since the\\nmargin is constant, if the model includes dimensions from va rious sources,\\ncareful rescaling of some dimensions may be required. Howev er, this is not\\na problem if our documents (points) are on the unit hypersphe re.\\n✎Example 15.1: Consider building an SVM over the (very little) data set show n in\\nFigure 15.4. Working geometrically, for an example like this, the maxim um margin\\nweight vector will be parallel to the shortest line connecti ng points of the two classes,\\nthat is, the line between (1, 1)and(2, 3), giving a weight vector of (1, 2). The opti-\\nmal decision surface is orthogonal to that line and intersec ts it at the halfway point.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 361}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP326 15 Support vector machines and machine learning on document s\\nTherefore, it passes through (1.5, 2). So, the SVM decision boundary is:\\ny=x1+2x2−5.5\\nWorking algebraically, with the standard constraint that s ign(yi(⃗wT⃗xi+b))≥1,\\nwe seek to minimize |⃗w|. This happens when this constraint is satisﬁed with equalit y\\nby the two support vectors. Further we know that the solution is⃗w= (a, 2a)for some\\na. So we have that:\\na+2a+b=−1\\n2a+6a+b= 1\\nTherefore, a=2/5 and b=−11/5. So the optimal hyperplane is given by ⃗w=\\n(2/5, 4/5 )and b=−11/5.\\nThe margin ρis 2/|⃗w|=2/√\\n4/25+16/25 =2/(2√\\n5/5) =√\\n5. This answer can\\nbe conﬁrmed geometrically by examining Figure 15.4.\\n?Exercise 15.1 [⋆]\\nWhat is the minimum number of support vectors that there can b e for a data set\\n(which contains instances of each class)?\\nExercise 15.2 [⋆⋆]\\nThe basis of being able to use kernels in SVMs (see Section 15.2.3 ) is that the classiﬁca-\\ntion function can be written in the form of Equation ( 15.9) (where, for large problems,\\nmost αiare 0). Show explicitly how the classiﬁcation function coul d be written in this\\nform for the data set from Example 15.1. That is, write fas a function where the data\\npoints appear and the only variable is ⃗x.\\nExercise 15.3 [⋆⋆]\\nInstall an SVM package such as SVMlight ( http://svmlight.joachims.org/ ), and build an\\nSVM for the data set discussed in Example 15.1. Conﬁrm that the program gives the\\nsame solution as the text. For SVMlight, or another package t hat accepts the same\\ntraining data format, the training ﬁle would be:\\n+11:2 2:3\\n−11:2 2:0\\n−11:1 2:1\\nThe training command for SVMlight is then:\\nsvm_learn -c 1 -aalphas.dat train.dat model.dat\\nThe-c 1 option is needed to turn off use of the slack variables that we discuss in\\nSection 15.2.1 . Check that the norm of the weight vector agrees with what we f ound\\nin Example 15.1. Examine the ﬁle alphas.dat which contains the αivalues, and check\\nthat they agree with your answers in Exercise 15.2.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 362}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP15.2 Extensions to the SVM model 327\\n/Bullet\\n/Bullet/Bullet\\n/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet/Bullet\\n/Bullet/Bullet⃗xi\\nξi/SolidTriangle/Triangle\\n/SolidTriangle/Triangle/SolidTriangle/Triangle\\n/SolidTriangle/Triangle\\n/SolidTriangle/Triangle/SolidTriangle/Triangle/SolidTriangle/Triangle\\n/SolidTriangle/Triangle/SolidTriangle/Triangle\\n/SolidTriangle/Triangle\\n⃗xjξj\\n◮Figure 15.5 Large margin classiﬁcation with slack variables.\\n15.2 Extensions to the SVM model\\n15.2.1 Soft margin classiﬁcation\\nFor the very high dimensional problems common in text classi ﬁcation, some-\\ntimes the data are linearly separable. But in the general cas e they are not, and\\neven if they are, we might prefer a solution that better separ ates the bulk of\\nthe data while ignoring a few weird noise documents.\\nIf the training set Dis not linearly separable, the standard approach is to\\nallow the fat decision margin to make a few mistakes (some poi nts – outliers\\nor noisy examples – are inside or on the wrong side of the margi n). We then\\npay a cost for each misclassiﬁed example, which depends on ho w far it is\\nfrom meeting the margin requirement given in Equation ( 15.5). To imple-\\nment this, we introduce slack variables ξi. A non-zero value for ξiallows ⃗xito SLACK VARIABLES\\nnot meet the margin requirement at a cost proportional to the value of ξi. See\\nFigure 15.5.\\nThe formulation of the SVM optimization problem with slack v ariables is:\\n(15.10) Find⃗w,b, and ξi≥0 such that:\\n•1\\n2⃗wT⃗w+C∑iξiis minimized\\n•and for all{(⃗xi,yi)},yi(⃗wT⃗xi+b)≥1−ξi', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 363}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP328 15 Support vector machines and machine learning on document s\\nThe optimization problem is then trading off how fat it can ma ke the margin\\nversus how many points have to be moved around to allow this ma rgin.\\nThe margin can be less than 1 for a point ⃗xiby setting ξi>0, but then one\\npays a penalty of Cξiin the minimization for having done that. The sum of\\ntheξigives an upper bound on the number of training errors. Soft-m argin\\nSVMs minimize training error traded off against margin. The parameter C\\nis a regularization term, which provides a way to control overﬁtting: as C REGULARIZATION\\nbecomes large, it is unattractive to not respect the data at t he cost of reducing\\nthe geometric margin; when it is small, it is easy to account f or some data\\npoints with the use of slack variables and to have a fat margin placed so it\\nmodels the bulk of the data.\\nThe dual problem for soft margin classiﬁcation becomes:\\n(15.11) Find α1, . . .αNsuch that ∑αi−1\\n2∑i∑jαiαjyiyj⃗xiT⃗xjis maximized, and\\n•∑iαiyi=0\\n•0≤αi≤Cfor all 1≤i≤N\\nNeither the slack variables ξinor Lagrange multipliers for them appear in the\\ndual problem. All we are left with is the constant Cbounding the possible\\nsize of the Lagrange multipliers for the support vector data points. As before,\\nthe⃗xiwith non-zero αiwill be the support vectors. The solution of the dual\\nproblem is of the form:\\n(15.12) ⃗w=∑αyi⃗xi\\nb=yk(1−ξk)−⃗wT⃗xkfork=arg maxkαk\\nAgain ⃗wis not needed explicitly for classiﬁcation, which can be don e in terms\\nof dot products with data points, as in Equation ( 15.9).\\nTypically, the support vectors will be a small proportion of the training\\ndata. However, if the problem is non-separable or with small margin, then\\nevery data point which is misclassiﬁed or within the margin w ill have a non-\\nzero αi. If this set of points becomes large, then, for the nonlinear case which\\nwe turn to in Section 15.2.3 , this can be a major slowdown for using SVMs at\\ntest time.\\nThe complexity of training and testing with linear SVMs is sh own in Ta-\\nble15.1.3The time for training an SVM is dominated by the time for solvi ng\\nthe underlying QP , and so the theoretical and empirical comp lexity varies de-\\npending on the method used to solve it. The standard result fo r solving QPs\\nis that it takes time cubic in the size of the data set ( Kozlov et al. 1979 ). All the\\nrecent work on SVM training has worked to reduce that complex ity, often by\\n3. We write Θ(|D|Lave)forΘ(T)(page 262) and assume that the length of test documents is\\nbounded as we did on page 262.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 364}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP15.2 Extensions to the SVM model 329\\nClassiﬁer Mode Method Time complexity\\nNB training Θ(|D|Lave+|C||V|)\\nNB testing Θ(|C|Ma)\\nRocchio training Θ(|D|Lave+|C||V|)\\nRocchio testing Θ(|C|Ma)\\nkNN training preprocessing Θ(|D|Lave)\\nkNN testing preprocessing Θ(|D|MaveMa)\\nkNN training no preprocessing Θ(1)\\nkNN testing no preprocessing Θ(|D|LaveMa)\\nSVM training conventional O(|C||D|3Mave);\\n≈O(|C||D|1.7Mave), empirically\\nSVM training cutting planes O(|C||D|Mave)\\nSVM testing O(|C|Ma)\\n◮Table 15.1 Training and testing complexity of various classiﬁers incl uding SVMs.\\nTraining is the time the learning method takes to learn a clas siﬁer over D, while test-\\ning is the time it takes a classiﬁer to classify one document. For SVMs, multiclass\\nclassiﬁcation is assumed to be done by a set of |C|one-versus-rest classiﬁers. Laveis\\nthe average number of tokens per document, while Maveis the average vocabulary\\n(number of non-zero features) of a document. Laand Maare the numbers of tokens\\nand types, respectively, in the test document.\\nbeing satisﬁed with approximate solutions. Standardly, em pirical complex-\\nity is about O(|D|1.7)(Joachims 2006a ). Nevertheless, the super-linear train-\\ning time of traditional SVM algorithms makes them difﬁcult o r impossible\\nto use on very large training data sets. Alternative traditi onal SVM solu-\\ntion algorithms which are linear in the number of training ex amples scale\\nbadly with a large number of features, which is another stand ard attribute\\nof text problems. However, a new training algorithm based on cutting plane\\ntechniques gives a promising answer to this issue by having r unning time\\nlinear in the number of training examples and the number of no n-zero fea-\\ntures in examples ( Joachims 2006a ). Nevertheless, the actual speed of doing\\nquadratic optimization remains much slower than simply cou nting terms as\\nis done in a Naive Bayes model. Extending SVM algorithms to no nlinear\\nSVMs, as in the next section, standardly increases training complexity by a\\nfactor of|D|(since dot products between examples need to be calculated) ,\\nmaking them impractical. In practice it can often be cheaper to materialize', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 365}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP330 15 Support vector machines and machine learning on document s\\nthe higher-order features and to train a linear SVM.4\\n15.2.2 Multiclass SVMs\\nSVMs are inherently two-class classiﬁers. The traditional way to do mul-\\nticlass classiﬁcation with SVMs is to use one of the methods d iscussed in\\nSection 14.5 (page 306). In particular, the most common technique in prac-\\ntice has been to build |C|one-versus-rest classiﬁers (commonly referred to as\\n“one-versus-all” or OVA classiﬁcation), and to choose the c lass which classi-\\nﬁes the test datum with greatest margin. Another strategy is to build a set\\nof one-versus-one classiﬁers, and to choose the class that i s selected by the\\nmost classiﬁers. While this involves building |C|(|C|−1)/2 classiﬁers, the\\ntime for training classiﬁers may actually decrease, since t he training data set\\nfor each classiﬁer is much smaller.\\nHowever, these are not very elegant approaches to solving mu lticlass prob-\\nlems. A better alternative is provided by the construction o f multiclass SVMs,\\nwhere we build a two-class classiﬁer over a feature vector Φ(⃗x,y)derived\\nfrom the pair consisting of the input features and the class o f the datum. At\\ntest time, the classiﬁer chooses the class y=arg maxy′⃗wTΦ(⃗x,y′). The mar-\\ngin during training is the gap between this value for the corr ect class and\\nfor the nearest other class, and so the quadratic program for mulation will\\nrequire that∀i∀y̸=yi⃗wTΦ(⃗xi,yi)−⃗wTΦ(⃗xi,y)≥1−ξi. This general\\nmethod can be extended to give a multiclass formulation of va rious kinds of\\nlinear classiﬁers. It is also a simple instance of a generali zation of classiﬁca-\\ntion where the classes are not just a set of independent, cate gorical labels, but\\nmay be arbitrary structured objects with relationships deﬁ ned between them.\\nIn the SVM world, such work comes under the label of structural SVMs . We STRUCTURAL SVM S\\nmention them again in Section 15.4.2 .\\n15.2.3 Nonlinear SVMs\\nWith what we have presented so far, data sets that are linearl y separable (per-\\nhaps with a few exceptions or some noise) are well-handled. B ut what are\\nwe going to do if the data set just doesn’t allow classiﬁcatio n by a linear clas-\\nsiﬁer? Let us look at a one-dimensional case. The top data set in Figure 15.6\\nis straightforwardly classiﬁed by a linear classiﬁer but th e middle data set is\\nnot. We instead need to be able to pick out an interval. One way to solve this\\nproblem is to map the data on to a higher dimensional space and then to use\\na linear classiﬁer in the higher dimensional space. For exam ple, the bottom\\npart of the ﬁgure shows that a linear separator can easily cla ssify the data\\n4. Materializing the features refers to directly calculati ng higher order and interaction terms\\nand then putting them into a linear model.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 366}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP15.2 Extensions to the SVM model 331\\n◮Figure 15.6 Projecting data that is not linearly separable into a higher dimensional\\nspace can make it linearly separable.\\nif we use a quadratic function to map the data into two dimensi ons (a po-\\nlar coordinates projection would be another possibility). The general idea is\\nto map the original feature space to some higher-dimensiona l feature space\\nwhere the training set is separable. Of course, we would want to do so in\\nways that preserve relevant dimensions of relatedness betw een data points,\\nso that the resultant classiﬁer should still generalize wel l.\\nSVMs, and also a number of other linear classiﬁers, provide a n easy and\\nefﬁcient way of doing this mapping to a higher dimensional sp ace, which is\\nreferred to as “the kernel trick ”. It’s not really a trick: it just exploits the math KERNEL TRICK\\nthat we have seen. The SVM linear classiﬁer relies on a dot pro duct between\\ndata point vectors. Let K(⃗xi,⃗xj) =⃗xiT⃗xj. Then the classiﬁer we have seen so', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 367}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP332 15 Support vector machines and machine learning on document s\\nfar is:\\nf(⃗x) =sign(∑\\niαiyiK(⃗xi,⃗x) +b) (15.13)\\nNow suppose we decide to map every data point into a higher dim ensional\\nspace via some transformation Φ:⃗x↦→φ(⃗x). Then the dot product becomes\\nφ(⃗xi)Tφ(⃗xj). If it turned out that this dot product (which is just a real nu m-\\nber) could be computed simply and efﬁciently in terms of the o riginal data\\npoints, then we wouldn’t have to actually map from ⃗x↦→φ(⃗x). Rather, we\\ncould simply compute the quantity K(⃗xi,⃗xj) =φ(⃗xi)Tφ(⃗xj), and then use the\\nfunction’s value in Equation ( 15.13 ). A kernel function K is such a function KERNEL FUNCTION\\nthat corresponds to a dot product in some expanded feature sp ace.\\n✎Example 15.2: The quadratic kernel in two dimensions. For 2-dimensional\\nvectors ⃗u= (u1u2),⃗v= (v1v2), consider K(⃗u,⃗v) = ( 1+⃗uT⃗v)2. We wish to\\nshow that this is a kernel, i.e., that K(⃗u,⃗v) =φ(⃗u)Tφ(⃗v)for some φ. Consider φ(⃗u) =\\n(1u2\\n1√\\n2u1u2u2\\n2√\\n2u1√\\n2u2). Then:\\nK(⃗u,⃗v) = ( 1+⃗uT⃗v)2(15.14)\\n= 1+u2\\n1v2\\n1+2u1v1u2v2+u2\\n2v2\\n2+2u1v1+2u2v2\\n= ( 1u2\\n1√\\n2u1u2u2\\n2√\\n2u1√\\n2u2)T(1v2\\n1√\\n2v1v2v2\\n2√\\n2v1√\\n2v2)\\n= φ(⃗u)Tφ(⃗v)\\nIn the language of functional analysis, what kinds of functi ons are valid\\nkernel functions ? Kernel functions are sometimes more precisely referred to KERNEL\\nasMercer kernels , because they must satisfy Mercer’s condition: for any g(⃗x) MERCER KERNEL\\nsuch that∫\\ng(⃗x)2d⃗xis ﬁnite, we must have that:\\n∫\\nK(⃗x,⃗z)g(⃗x)g(⃗z)d⃗xd⃗z≥0 . (15.15)\\nA kernel function Kmust be continuous, symmetric, and have a positive def-\\ninite gram matrix. Such a Kmeans that there exists a mapping to a reproduc-\\ning kernel Hilbert space (a Hilbert space is a vector space cl osed under dot\\nproducts) such that the dot product there gives the same valu e as the function\\nK. If a kernel does not satisfy Mercer’s condition, then the co rresponding QP\\nmay have no solution. If you would like to better understand t hese issues,\\nyou should consult the books on SVMs mentioned in Section 15.5. Other-\\nwise, you can content yourself with knowing that 90% of work w ith kernels\\nuses one of two straightforward families of functions of two vectors, which\\nwe deﬁne below, and which deﬁne valid kernels.\\nThe two commonly used families of kernels are polynomial ker nels and\\nradial basis functions. Polynomial kernels are of the form K(⃗x,⃗z) = ( 1+', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 368}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP15.2 Extensions to the SVM model 333\\n⃗xT⃗z)d. The case of d=1 is a linear kernel, which is what we had before the\\nstart of this section (the constant 1 just changing the thres hold). The case of\\nd=2 gives a quadratic kernel, and is very commonly used. We illu strated\\nthe quadratic kernel in Example 15.2.\\nThe most common form of radial basis function is a Gaussian di stribution,\\ncalculated as:\\nK(⃗x,⃗z) =e−(⃗x−⃗z)2/(2σ2)(15.16)\\nA radial basis function (rbf) is equivalent to mapping the da ta into an inﬁ-\\nnite dimensional Hilbert space, and so we cannot illustrate the radial basis\\nfunction concretely, as we did a quadratic kernel. Beyond th ese two families,\\nthere has been interesting work developing other kernels, s ome of which is\\npromising for text applications. In particular, there has b een investigation of\\nstring kernels (see Section 15.5).\\nThe world of SVMs comes with its own language, which is rather different\\nfrom the language otherwise used in machine learning. The te rminology\\ndoes have deep roots in mathematics, but it’s important not t o be too awed\\nby that terminology. Really, we are talking about some quite simple things. A\\npolynomial kernel allows us to model feature conjunctions ( up to the order of\\nthe polynomial). That is, if we want to be able to model occurr ences of pairs\\nof words, which give distinctive information about topic cl assiﬁcation, not\\ngiven by the individual words alone, like perhaps operating AND system or\\nethnic AND cleansing , then we need to use a quadratic kernel. If occurrences\\nof triples of words give distinctive information, then we ne ed to use a cubic\\nkernel. Simultaneously you also get the powers of the basic f eatures – for\\nmost text applications, that probably isn’t useful, but jus t comes along with\\nthe math and hopefully doesn’t do harm. A radial basis functi on allows you\\nto have features that pick out circles (hyperspheres) – alth ough the decision\\nboundaries become much more complex as multiple such featur es interact. A\\nstring kernel lets you have features that are character subs equences of terms.\\nAll of these are straightforward notions which have also bee n used in many\\nother places under different names.\\n15.2.4 Experimental results\\nWe presented results in Section 13.6 showing that an SVM is a very effec-\\ntive text classiﬁer. The results of Dumais et al. (1998 ) given in Table 13.9\\nshow SVMs clearly performing the best. This was one of severa l pieces of\\nwork from this time that established the strong reputation o f SVMs for text\\nclassiﬁcation. Another pioneering work on scaling and eval uating SVMs\\nfor text classiﬁcation was ( Joachims 1998 ). We present some of his results', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 369}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP334 15 Support vector machines and machine learning on document s\\nRoc- Dec. linear SVM rbf-SVM\\nNB chio Trees kNN C=0.5 C=1.0 σ≈7\\nearn 96.0 96.1 96.1 97.8 98.0 98.2 98.1\\nacq 90.7 92.1 85.3 91.8 95.5 95.6 94.7\\nmoney-fx 59.6 67.6 69.4 75.4 78.8 78.5 74.3\\ngrain 69.8 79.5 89.1 82.6 91.9 93.1 93.4\\ncrude 81.2 81.5 75.5 85.8 89.4 89.4 88.7\\ntrade 52.2 77.4 59.2 77.9 79.2 79.2 76.6\\ninterest 57.6 72.5 49.1 76.7 75.6 74.8 69.1\\nship 80.9 83.1 80.9 79.8 87.4 86.5 85.8\\nwheat 63.4 79.4 85.5 72.9 86.6 86.8 82.4\\ncorn 45.2 62.2 87.7 71.4 87.5 87.8 84.6\\nmicroavg. 72.3 79.9 79.4 82.6 86.7 87.5 86.4\\n◮Table 15.2 SVM classiﬁer break-even F 1from ( Joachims 2002a , p. 114). Results\\nare shown for the 10 largest categories and for microaverage d performance over all\\n90 categories on the Reuters-21578 data set.\\nfrom ( Joachims 2002a ) in Table 15.2.5Joachims used a large number of term\\nfeatures in contrast to Dumais et al. (1998 ), who used MI feature selection\\n(Section 13.5.1 , page 272) to build classiﬁers with a much more limited num-\\nber of features. The success of the linear SVM mirrors the res ults discussed\\nin Section 14.6 (page 308) on other linear approaches like Naive Bayes. It\\nseems that working with simple term features can get one a lon g way. It is\\nagain noticeable the extent to which different papers’ resu lts for the same ma-\\nchine learning methods differ. In particular, based on repl ications by other\\nresearchers, the Naive Bayes results of ( Joachims 1998 ) appear too weak, and\\nthe results in Table 13.9 should be taken as representative.\\n15.3 Issues in the classiﬁcation of text documents\\nThere are lots of applications of text classiﬁcation in the c ommercial world;\\nemail spam ﬁltering is perhaps now the most ubiquitous. Jackson and Mou-\\nlinier (2002 ) write: “There is no question concerning the commercial val ue of\\nbeing able to classify documents automatically by content. There are myriad\\n5. These results are in terms of the break-even F1(see Section 8.4). Many researchers disprefer\\nthis measure for text classiﬁcation evaluation, since its c alculation may involve interpolation\\nrather than an actual parameter setting of the system and it i s not clear why this value should\\nbe reported rather than maximal F1or another point on the precision/recall curve motivated by\\nthe task at hand. While earlier results in ( Joachims 1998 ) suggested notable gains on this task\\nfrom the use of higher order polynomial or rbf kernels, this w as with hard-margin SVMs. With\\nsoft-margin SVMs, a simple linear SVM with the default C=1 performs best.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 370}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP15.3 Issues in the classiﬁcation of text documents 335\\npotential applications of such a capability for corporate I ntranets, govern-\\nment departments, and Internet publishers.”\\nMost of our discussion of classiﬁcation has focused on intro ducing various\\nmachine learning methods rather than discussing particula r features of text\\ndocuments relevant to classiﬁcation. This bias is appropri ate for a textbook,\\nbut is misplaced for an application developer. It is frequen tly the case that\\ngreater performance gains can be achieved from exploiting d omain-speciﬁc\\ntext features than from changing from one machine learning m ethod to an-\\nother. Jackson and Moulinier (2002 ) suggest that “Understanding the data\\nis one of the keys to successful categorization, yet this is a n area in which\\nmost categorization tool vendors are extremely weak. Many o f the ‘one size\\nﬁts all’ tools on the market have not been tested on a wide rang e of content\\ntypes.” In this section we wish to step back a little and consi der the applica-\\ntions of text classiﬁcation, the space of possible solution s, and the utility of\\napplication-speciﬁc heuristics.\\n15.3.1 Choosing what kind of classiﬁer to use\\nWhen confronted with a need to build a text classiﬁer, the ﬁrs t question to\\nask is how much training data is there currently available? N one? Very little?\\nQuite a lot? Or a huge amount, growing every day? Often one of t he biggest\\npractical challenges in ﬁelding a machine learning classiﬁ er in real applica-\\ntions is creating or obtaining enough training data. For man y problems and\\nalgorithms, hundreds or thousands of examples from each cla ss are required\\nto produce a high performance classiﬁer and many real world c ontexts in-\\nvolve large sets of categories. We will initially assume tha t the classiﬁer is\\nneeded as soon as possible; if a lot of time is available for im plementation,\\nmuch of it might be spent on assembling data resources.\\nIf you have no labeled training data, and especially if there are existing\\nstaff knowledgeable about the domain of the data, then you sh ould never\\nforget the solution of using hand-written rules. That is, yo u write standing\\nqueries, as we touched on at the beginning of Chapter 13. For example:\\nIF(wheatORgrain)AND NOT(wholeORbread)THEN c=grain\\nIn practice, rules get a lot bigger than this, and can be phras ed using more\\nsophisticated query languages than just Boolean expressio ns, including the\\nuse of numeric scores. With careful crafting (that is, by hum ans tuning the\\nrules on development data), the accuracy of such rules can be come very high.\\nJacobs and Rau (1990 ) report identifying articles about takeovers with 92%\\nprecision and 88.5% recall, and Hayes and Weinstein (1990 ) report 94% re-\\ncall and 84% precision over 675 categories on Reuters newswi re documents.\\nNevertheless the amount of work to create such well-tuned ru les is very\\nlarge. A reasonable estimate is 2 days per class, and extra ti me has to go', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 371}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP336 15 Support vector machines and machine learning on document s\\ninto maintenance of rules, as the content of documents in cla sses drifts over\\ntime (cf. page 269).\\nIf you have fairly little data and you are going to train a supe rvised clas-\\nsiﬁer, then machine learning theory says you should stick to a classiﬁer with\\nhigh bias, as we discussed in Section 14.6 (page 308). For example, there\\nare theoretical and empirical results that Naive Bayes does well in such cir-\\ncumstances ( Ng and Jordan 2001 ,Forman and Cohen 2004 ), although this\\neffect is not necessarily observed in practice with regular ized models over\\ntextual data ( Klein and Manning 2002 ). At any rate, a very low bias model\\nlike a nearest neighbor model is probably counterindicated . Regardless, the\\nquality of the model will be adversely affected by the limite d training data.\\nHere, the theoretically interesting answer is to try to appl ysemi-supervised SEMI -SUPERVISED\\nLEARNING training methods . This includes methods such as bootstrapping or the EM\\nalgorithm, which we will introduce in Section 16.5 (page 368). In these meth-\\nods, the system gets some labeled documents, and a further la rge supply\\nof unlabeled documents over which it can attempt to learn. On e of the big\\nadvantages of Naive Bayes is that it can be straightforwardl y extended to\\nbe a semi-supervised learning algorithm, but for SVMs, ther e is also semi-\\nsupervised learning work which goes under the title of transductive SVMs . TRANSDUCTIVE SVM S\\nSee the references for pointers.\\nOften, the practical answer is to work out how to get more labe led data as\\nquickly as you can. The best way to do this is to insert yoursel f into a process\\nwhere humans will be willing to label data for you as part of th eir natural\\ntasks. For example, in many cases humans will sort or route em ail for their\\nown purposes, and these actions give information about clas ses. The alter-\\nnative of getting human labelers expressly for the task of tr aining classiﬁers\\nis often difﬁcult to organize, and the labeling is often of lo wer quality, be-\\ncause the labels are not embedded in a realistic task context . Rather than\\ngetting people to label all or a random sample of documents, t here has also\\nbeen considerable research on active learning , where a system is built which ACTIVE LEARNING\\ndecides which documents a human should label. Usually these are the ones\\non which a classiﬁer is uncertain of the correct classiﬁcati on. This can be ef-\\nfective in reducing annotation costs by a factor of 2–4, but h as the problem\\nthat the good documents to label to train one type of classiﬁe r often are not\\nthe good documents to label to train a different type of class iﬁer.\\nIf there is a reasonable amount of labeled data, then you are i n the per-\\nfect position to use everything that we have presented about text classiﬁ-\\ncation. For instance, you may wish to use an SVM. However, if y ou are\\ndeploying a linear classiﬁer such as an SVM, you should proba bly design\\nan application that overlays a Boolean rule-based classiﬁe r over the machine\\nlearning classiﬁer. Users frequently like to adjust things that do not come\\nout quite right, and if management gets on the phone and wants the classi-\\nﬁcation of a particular document ﬁxed right now, then this is much easier to', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 372}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP15.3 Issues in the classiﬁcation of text documents 337\\ndo by hand-writing a rule than by working out how to adjust the weights\\nof an SVM without destroying the overall classiﬁcation accu racy. This is one\\nreason why machine learning models like decision trees whic h produce user-\\ninterpretable Boolean-like models retain considerable po pularity.\\nIf a huge amount of data are available, then the choice of clas siﬁer probably\\nhas little effect on your results and the best choice may be un clear (cf. Banko\\nand Brill 2001 ). It may be best to choose a classiﬁer based on the scalabilit y\\nof training or even runtime efﬁciency. To get to this point, y ou need to have\\nhuge amounts of data. The general rule of thumb is that each do ubling of\\nthe training data size produces a linear increase in classiﬁ er performance,\\nbut with very large amounts of data, the improvement becomes sub-linear.\\n15.3.2 Improving classiﬁer performance\\nFor any particular application, there is usually signiﬁcan t room for improv-\\ning classiﬁer effectiveness through exploiting features s peciﬁc to the domain\\nor document collection. Often documents will contain zones which are espe-\\ncially useful for classiﬁcation. Often there will be partic ular subvocabularies\\nwhich demand special treatment for optimal classiﬁcation e ffectiveness.\\nLarge and difﬁcult category taxonomies\\nIf a text classiﬁcation problem consists of a small number of well-separated\\ncategories, then many classiﬁcation algorithms are likely to work well. But\\nmany real classiﬁcation problems consist of a very large num ber of often\\nvery similar categories. The reader might think of examples like web direc-\\ntories (the Yahoo! Directory or the Open Directory Project) , library classi-\\nﬁcation schemes (Dewey Decimal or Library of Congress) or th e classiﬁca-\\ntion schemes used in legal or medical applications. For inst ance, the Yahoo!\\nDirectory consists of over 200,000 categories in a deep hier archy. Accurate\\nclassiﬁcation over large sets of closely related classes is inherently difﬁcult.\\nMost large sets of categories have a hierarchical structure , and attempting\\nto exploit the hierarchy by doing hierarchical classiﬁcation is a promising ap- HIERARCHICAL\\nCLASSIFICATION proach. However, at present the effectiveness gains from do ing this rather\\nthan just working with the classes that are the leaves of the h ierarchy re-\\nmain modest.6But the technique can be very useful simply to improve the\\nscalability of building classiﬁers over large hierarchies . Another simple way\\nto improve the scalability of classiﬁers over large hierarc hies is the use of\\naggressive feature selection. We provide references to som e work on hierar-\\nchical classiﬁcation in Section 15.5.\\n6. Using the small hierarchy in Figure 13.1 (page 257) as an example, the leaf classes are ones\\nlikepoultry and coffee , as opposed to higher-up classes like industries .', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 373}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP338 15 Support vector machines and machine learning on document s\\nA general result in machine learning is that you can always ge t a small\\nboost in classiﬁcation accuracy by combining multiple clas siﬁers, provided\\nonly that the mistakes that they make are at least somewhat in dependent.\\nThere is now a large literature on techniques such as voting, bagging, and\\nboosting multiple classiﬁers. Again, there are some pointe rs in the refer-\\nences. Nevertheless, ultimately a hybrid automatic/manua l solution may be\\nneeded to achieve sufﬁcient classiﬁcation accuracy. A comm on approach in\\nsuch situations is to run a classiﬁer ﬁrst, and to accept all i ts high conﬁdence\\ndecisions, but to put low conﬁdence decisions in a queue for m anual review.\\nSuch a process also automatically leads to the production of new training\\ndata which can be used in future versions of the machine learn ing classiﬁer.\\nHowever, note that this is a case in point where the resulting training data is\\nclearly notrandomly sampled from the space of documents.\\nFeatures for text\\nThe default in both ad hoc retrieval and text classiﬁcation i s to use terms\\nas features. However, for text classiﬁcation, a great deal o f mileage can be\\nachieved by designing additional features which are suited to a speciﬁc prob-\\nlem. Unlike the case of IR query languages, since these featu res are internal\\nto the classiﬁer, there is no problem of communicating these features to an\\nend user. This process is generally referred to as feature engineering . At pre- FEATURE ENGINEERING\\nsent, feature engineering remains a human craft, rather tha n something done\\nby machine learning. Good feature engineering can often mar kedly improve\\nthe performance of a text classiﬁer. It is especially beneﬁc ial in some of the\\nmost important applications of text classiﬁcation, like sp am and porn ﬁlter-\\ning.\\nClassiﬁcation problems will often contain large numbers of terms which\\ncan be conveniently grouped, and which have a similar vote in text classi-\\nﬁcation problems. Typical examples might be year mentions o r strings of\\nexclamation marks. Or they may be more specialized tokens li ke ISBNs or\\nchemical formulas. Often, using them directly in a classiﬁe r would greatly in-\\ncrease the vocabulary without providing classiﬁcatory pow er beyond know-\\ning that, say, a chemical formula is present. In such cases, t he number of\\nfeatures and feature sparseness can be reduced by matching s uch items with\\nregular expressions and converting them into distinguishe d tokens. Con-\\nsequently, effectiveness and classiﬁer speed are normally enhanced. Some-\\ntimes all numbers are converted into a single feature, but of ten some value\\ncan be had by distinguishing different kinds of numbers, suc h as four digit\\nnumbers (which are usually years) versus other cardinal num bers versus real\\nnumbers with a decimal point. Similar techniques can be appl ied to dates,\\nISBN numbers, sports game scores, and so on.\\nGoing in the other direction, it is often useful to increase t he number of fea-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 374}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP15.3 Issues in the classiﬁcation of text documents 339\\ntures by matching parts of words, and by matching selected mu ltiword pat-\\nterns that are particularly discriminative. Parts of words are often matched\\nby character k-gram features. Such features can be particularly good at pr o-\\nviding classiﬁcation clues for otherwise unknown words whe n the classiﬁer\\nis deployed. For instance, an unknown word ending in -rase is likely to be an\\nenzyme, even if it wasn’t seen in the training data. Good mult iword patterns\\nare often found by looking for distinctively common word pai rs (perhaps\\nusing a mutual information criterion between words, in a sim ilar way to\\nits use in Section 13.5.1 (page 272) for feature selection) and then using fea-\\nture selection methods evaluated against classes. They are useful when the\\ncomponents of a compound would themselves be misleading as c lassiﬁca-\\ntion cues. For instance, this would be the case if the keyword ethnic was\\nmost indicative of the categories food and arts, the keyword cleansing was\\nmost indicative of the category home , but the collocation ethnic cleansing in-\\nstead indicates the category world news . Some text classiﬁers also make use\\nof features from named entity recognizers (cf. page 195).\\nDo techniques like stemming and lowercasing (Section 2.2, page 22) help\\nfor text classiﬁcation? As always, the ultimate test is empi rical evaluations\\nconducted on an appropriate test collection. But it is never theless useful to\\nnote that such techniques have a more restricted chance of be ing useful for\\nclassiﬁcation. For IR, you often need to collapse forms of a w ord like oxy-\\ngenate and oxygenation , because the appearance of either in a document is a\\ngood clue that the document will be relevant to a query about oxygenation .\\nGiven copious training data, stemming necessarily deliver s no value for text\\nclassiﬁcation. If several forms that stem together have a si milar signal, the\\nparameters estimated for all of them will have similar weigh ts. Techniques\\nlike stemming help only in compensating for data sparseness . This can be\\na useful role (as noted at the start of this section), but ofte n different forms\\nof a word can convey signiﬁcantly different cues about the co rrect document\\nclassiﬁcation. Overly aggressive stemming can easily degr ade classiﬁcation\\nperformance.\\nDocument zones in text classiﬁcation\\nAs already discussed in Section 6.1, documents usually have zones, such as\\nmail message headers like the subject and author, or the titl e and keywords\\nof a research article. Text classiﬁers can usually gain from making use of\\nthese zones during training and classiﬁcation.\\nUpweighting document zones. In text classiﬁcation problems, you can fre-\\nquently get a nice boost to effectiveness by differentially weighting contri-\\nbutions from different document zones. Often, upweighting title words is\\nparticularly effective ( Cohen and Singer 1999 , p. 163). As a rule of thumb,', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 375}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP340 15 Support vector machines and machine learning on document s\\nit is often effective to double the weight of title words in te xt classiﬁcation\\nproblems. You can also get value from upweighting words from pieces of\\ntext that are not so much clearly deﬁned zones, but where neve rtheless evi-\\ndence from document structure or content suggests that they are important.\\nMurata et al. (2000 ) suggest that you can also get value (in an ad hoc retrieval\\ncontext) from upweighting the ﬁrst sentence of a (newswire) document.\\nSeparate feature spaces for document zones. There are two strategies that\\ncan be used for document zones. Above we upweighted words tha t appear\\nin certain zones. This means that we are using the same featur es (that is, pa-\\nrameters are “tied” across different zones), but we pay more attention to the PARAMETER TYING\\noccurrence of terms in particular zones. An alternative str ategy is to have a\\ncompletely separate set of features and corresponding para meters for words\\noccurring in different zones. This is in principle more powe rful: a word\\ncould usually indicate the topic Middle East when in the title but Commodities\\nwhen in the body of a document. But, in practice, tying parame ters is usu-\\nally more successful. Having separate feature sets means ha ving two or more\\ntimes as many parameters, many of which will be much more spar sely seen\\nin the training data, and hence with worse estimates, wherea s upweighting\\nhas no bad effects of this sort. Moreover, it is quite uncommo n for words to\\nhave different preferences when appearing in different zon es; it is mainly the\\nstrength of their vote that should be adjusted. Nevertheles s, ultimately this\\nis a contingent result, depending on the nature and quantity of the training\\ndata.\\nConnections to text summarization. In Section 8.7, we mentioned the ﬁeld\\nof text summarization, and how most work in that ﬁeld has adop ted the\\nlimited goal of extracting and assembling pieces of the orig inal text that are\\njudged to be central based on features of sentences that cons ider the sen-\\ntence’s position and content. Much of this work can be used to suggest zones\\nthat may be distinctively useful for text classiﬁcation. Fo r example Kołcz\\net al. (2000 ) consider a form of feature selection where you classify doc u-\\nments based only on words in certain zones. Based on text summ arization\\nresearch, they consider using (i) only the title, (ii) only t he ﬁrst paragraph,\\n(iii) only the paragraph with the most title words or keyword s, (iv) the ﬁrst\\ntwo paragraphs or the ﬁrst and last paragraph, or (v) all sent ences with a\\nminimum number of title words or keywords. In general, these positional\\nfeature selection methods produced as good results as mutua l information\\n(Section 13.5.1 ), and resulted in quite competitive classiﬁers. Ko et al. (2004 )\\nalso took inspiration from text summarization research to u pweight sen-\\ntences with either words from the title or words that are cent ral to the doc-\\nument’s content, leading to classiﬁcation accuracy gains o f almost 1%. This', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 376}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP15.4 Machine learning methods in ad hoc information retriev al 341\\npresumably works because most such sentences are somehow mo re central\\nto the concerns of the document.\\n?Exercise 15.4 [⋆⋆]\\nSpam email often makes use of various cloaking techniques to try to get through. One\\nmethod is to pad or substitute characters so as to defeat word -based text classiﬁers.\\nFor example, you see terms like the following in spam email:\\nRep1icaRolex bonmus Viiiaaaagra pi11z\\nPHARlbdMACY [LEV]i[IT]l[RA] se ∧xual ClAfLlS\\nDiscuss how you could engineer features that would largely d efeat this strategy.\\nExercise 15.5 [⋆⋆]\\nAnother strategy often used by purveyors of email spam is to f ollow the message\\nthey wish to send (such as buying a cheap stock or whatever) wi th a paragraph of\\ntext from another innocuous source (such as a news article). Why might this strategy\\nbe effective? How might it be addressed by a text classiﬁer?\\nExercise 15.6 [⋆]\\nWhat other kinds of features appear as if they would be useful in an email spam\\nclassiﬁer?\\n15.4 Machine learning methods in ad hoc information retriev al\\nRather than coming up with term and document weighting funct ions by\\nhand, as we primarily did in Chapter 6, we can view different sources of rele-\\nvance signal (cosine score, title match, etc.) as features i n a learning problem.\\nA classiﬁer that has been fed examples of relevant and nonrel evant docu-\\nments for each of a set of queries can then ﬁgure out the relati ve weights\\nof these signals. If we conﬁgure the problem so that there are pairs of a\\ndocument and a query which are assigned a relevance judgment ofrelevant\\nornonrelevant , then we can think of this problem too as a text classiﬁcation\\nproblem. Taking such a classiﬁcation approach is not necess arily best, and\\nwe present an alternative in Section 15.4.2 . Nevertheless, given the material\\nwe have covered, the simplest place to start is to approach th is problem as\\na classiﬁcation problem, by ordering the documents accordi ng to the conﬁ-\\ndence of a two-class classiﬁer in its relevance decision. An d this move is not\\npurely pedagogical; exactly this approach is sometimes use d in practice.\\n15.4.1 A simple example of machine-learned scoring\\nIn this section we generalize the methodology of Section 6.1.2 (page 113) to\\nmachine learning of the scoring function. In Section 6.1.2 we considered a\\ncase where we had to combine Boolean indicators of relevance ; here we con-\\nsider more general factors to further develop the notion of m achine-learned', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 377}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP342 15 Support vector machines and machine learning on document s\\nExample DocID Query Cosine score ω Judgment\\nΦ1 37linux operating system 0.032 3 relevant\\nΦ2 37penguin logo 0.02 4 nonrelevant\\nΦ3 238operatingsystem 0.043 2 relevant\\nΦ4 238runtimeenvironment 0.004 2 nonrelevant\\nΦ5 1741kernellayer 0.022 3 relevant\\nΦ6 2094devicedriver 0.03 2 relevant\\nΦ7 3191devicedriver 0.027 5 nonrelevant\\n········· ·········\\n◮Table 15.3 Training examples for machine-learned scoring.\\nrelevance. In particular, the factors we now consider go bey ond Boolean\\nfunctions of query term presence in document zones, as in Sec tion 6.1.2 .\\nWe develop the ideas in a setting where the scoring function i s a linear\\ncombination of two factors: (1) the vector space cosine simi larity between\\nquery and document and (2) the minimum window width ωwithin which\\nthe query terms lie. As we noted in Section 7.2.2 (page 144), query term\\nproximity is often very indicative of a document being on top ic, especially\\nwith longer documents and on the web. Among other things, thi s quantity\\ngives us an implementation of implicit phrases. Thus we have one factor that\\ndepends on the statistics of query terms in the document as a b ag of words,\\nand another that depends on proximity weighting. We conside r only two\\nfeatures in the development of the ideas because a two-featu re exposition\\nremains simple enough to visualize. The technique can be gen eralized to\\nmany more features.\\nAs in Section 6.1.2 , we are provided with a set of training examples , each\\nof which is a pair consisting of a query and a document, togeth er with a\\nrelevance judgment for that document on that query that is ei ther relevant or\\nnonrelevant . For each such example we can compute the vector space cosine\\nsimilarity, as well as the window width ω. The result is a training set as\\nshown in Table 15.3, which resembles Figure 6.5(page 115) from Section 6.1.2 .\\nHere, the two features (cosine score denoted αand window width ω) are\\nreal-valued predictors. If we once again quantify the judgm entrelevant as 1\\nand nonrelevant as 0, we seek a scoring function that combines the values of\\nthe features to generate a value that is (close to) 0 or 1. We wi sh this func-\\ntion to be in agreement with our set of training examples as fa r as possible.\\nWithout loss of generality, a linear classiﬁer will use a lin ear combination of\\nfeatures of the form\\nScore(d,q) =Score(α,ω) =aα+bω+c, (15.17)\\nwith the coefﬁcients a,b,cto be learned from the training data. While it is', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 378}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP15.4 Machine learning methods in ad hoc information retriev al 34302 3 4 5\\n0 . 0 50 . 0 2 5\\ncos\\ni n\\nescore\\n\\rT e r m p r o x i m i t y \\x18\\nRRR\\nRR\\nRR\\nRRRRNNNN\\nN\\nNNN\\nNN◮Figure 15.7 A collection of training examples. Each R denotes a training example\\nlabeled relevant , while each N is a training example labeled nonrelevant .\\npossible to formulate this as an error minimization problem as we did in\\nSection 6.1.2 , it is instructive to visualize the geometry of Equation ( 15.17 ).\\nThe examples in Table 15.3 can be plotted on a two-dimensional plane with\\naxes corresponding to the cosine score αand the window width ω. This is\\ndepicted in Figure 15.7.\\nIn this setting, the function Score(α,ω)from Equation ( 15.17 ) represents\\na plane “hanging above” Figure 15.7. Ideally this plane (in the direction\\nperpendicular to the page containing Figure 15.7) assumes values close to\\n1 above the points marked R, and values close to 0 above the poi nts marked\\nN. Since a plane is unlikely to assume only values close to 0 or 1 above the\\ntraining sample points, we make use of thresholding : given any query and\\ndocument for which we wish to determine relevance, we pick a v alue θand\\nifScore(α,ω)>θwe declare the document to be relevant , else we declare\\nthe document to be nonrelevant . As we know from Figure 14.8 (page 301),\\nall points that satisfy Score(α,ω) = θform a line (shown as a dashed line\\nin Figure 15.7) and we thus have a linear classiﬁer that separates relevant', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 379}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP344 15 Support vector machines and machine learning on document s\\nfrom nonrelevant instances. Geometrically, we can ﬁnd the s eparating line\\nas follows. Consider the line passing through the plane Score(α,ω)whose\\nheight is θabove the page containing Figure 15.7. Project this line down onto\\nFigure 15.7; this will be the dashed line in Figure 15.7. Then, any subse-\\nquent query/document pair that falls below the dashed line i n Figure 15.7 is\\ndeemed nonrelevant ; above the dashed line, relevant .\\nThus, the problem of making a binary relevant /nonrelevant judgment given\\ntraining examples as above turns into one of learning the das hed line in Fig-\\nure15.7 separating relevant training examples from the nonrelevant ones. Be-\\ning in the α-ωplane, this line can be written as a linear equation involvin g\\nαand ω, with two parameters (slope and intercept). The methods of l in-\\near classiﬁcation that we have already looked at in Chapters 13–15provide\\nmethods for choosing this line. Provided we can build a sufﬁc iently rich col-\\nlection of training samples, we can thus altogether avoid ha nd-tuning score\\nfunctions as in Section 7.2.3 (page 145). The bottleneck of course is the ability\\nto maintain a suitably representative set of training examp les, whose rele-\\nvance assessments must be made by experts.\\n15.4.2 Result ranking by machine learning\\nThe above ideas can be readily generalized to functions of ma ny more than\\ntwo variables. There are lots of other scores that are indica tive of the rel-\\nevance of a document to a query, including static quality (Pa geRank-style\\nmeasures, discussed in Chapter 21), document age, zone contributions, doc-\\nument length, and so on. Providing that these measures can be calculated\\nfor a training document collection with relevance judgment s, any number\\nof such measures can be used to train a machine learning class iﬁer. For in-\\nstance, we could train an SVM over binary relevance judgment s, and order\\ndocuments based on their probability of relevance, which is monotonic with\\nthe documents’ signed distance from the decision boundary.\\nHowever, approaching IR result ranking like this is not nece ssarily the\\nright way to think about the problem. Statisticians normall y ﬁrst divide\\nproblems into classiﬁcation problems (where a categorical variable is pre-\\ndicted) versus regression problems (where a real number is predicted). In REGRESSION\\nbetween is the specialized ﬁeld of ordinal regression where a ranking is pre- ORDINAL REGRESSION\\ndicted. Machine learning for ad hoc retrieval is most proper ly thought of as\\nan ordinal regression problem, where the goal is to rank a set of documents\\nfor a query, given training data of the same sort. This formul ation gives\\nsome additional power, since documents can be evaluated rel ative to other\\ncandidate documents for the same query, rather than having t o be mapped\\nto a global scale of goodness, while also weakening the probl em space, since\\njust a ranking is required rather than an absolute measure of relevance. Is-\\nsues of ranking are especially germane in web search, where t he ranking at', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 380}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP15.4 Machine learning methods in ad hoc information retriev al 345\\nthe very top of the results list is exceedingly important, wh ereas decisions\\nof relevance of a document to a query may be much less importan t. Such\\nwork can and has been pursued using the structural SVM framew ork which\\nwe mentioned in Section 15.2.2 , where the class being predicted is a ranking\\nof results for a query, but here we will present the slightly s impler ranking\\nSVM.\\nThe construction of a ranking SVM proceeds as follows. We begin with a RANKING SVM\\nset of judged queries. For each training query q, we have a set of documents\\nreturned in response to the query, which have been totally or dered by a per-\\nson for relevance to the query. We construct a vector of featu resψj=ψ(dj,q)\\nfor each document/query pair, using features such as those d iscussed in Sec-\\ntion 15.4.1 , and many more. For two documents diand dj, we then form the\\nvector of feature differences:\\nΦ(di,dj,q) =ψ(di,q)−ψ(dj,q) (15.18)\\nBy hypothesis, one of diand djhas been judged more relevant. If diis\\njudged more relevant than dj, denoted di≺dj(dishould precede djin the\\nresults ordering), then we will assign the vector Φ(di,dj,q)the class yijq=\\n+1; otherwise−1. The goal then is to build a classiﬁer which will return\\n⃗wTΦ(di,dj,q)>0 iff di≺dj (15.19)\\nThis SVM learning task is formalized in a manner much like the other exam-\\nples that we saw before:\\n(15.20) Find⃗w, and ξi,j≥0 such that:\\n•1\\n2⃗wT⃗w+C∑i,jξi,jis minimized\\n•and for all{Φ(di,dj,q):di≺dj},⃗wTΦ(di,dj,q)≥1−ξi,j\\nWe can leave out yijqin the statement of the constraint, since we only need\\nto consider the constraint for document pairs ordered in one direction, since\\n≺is antisymmetric. These constraints are then solved, as bef ore, to give\\na linear classiﬁer which can rank pairs of documents. This ap proach has\\nbeen used to build ranking functions which outperform stand ard hand-built\\nranking functions in IR evaluations on standard data sets; s ee the references\\nfor papers that present such results.\\nBoth of the methods that we have just looked at use a linear wei ghting\\nof document features that are indicators of relevance, as ha s most work in\\nthis area. It is therefore perhaps interesting to note that m uch of traditional\\nIR weighting involves nonlinear scaling of basic measurements (such as log-\\nweighting of term frequency, or idf). At the present time, ma chine learning is\\nvery good at producing optimal weights for features in a line ar combination', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 381}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP346 15 Support vector machines and machine learning on document s\\n(or other similar restricted model classes), but it is not go od at coming up\\nwith good nonlinear scalings of basic measurements. This ar ea remains the\\ndomain of human feature engineering.\\nThe idea of learning ranking functions has been around for a n umber of\\nyears, but it is only very recently that sufﬁcient machine le arning knowledge,\\ntraining document collections, and computational power ha ve come together\\nto make this method practical and exciting. It is thus too ear ly to write some-\\nthing deﬁnitive on machine learning approaches to ranking i n information\\nretrieval, but there is every reason to expect the use and imp ortance of ma-\\nchine learned ranking approaches to grow over time. While sk illed humans\\ncan do a very good job at deﬁning ranking functions by hand, ha nd tuning\\nis difﬁcult, and it has to be done again for each new document c ollection and\\nclass of users.\\n?Exercise 15.7\\nPlot the ﬁrst 7 rows of Table 15.3 in the α-ωplane to produce a ﬁgure like that in\\nFigure 15.7.\\nExercise 15.8\\nWrite down the equation of a line in the α-ωplane separating the Rs from the Ns.\\nExercise 15.9\\nGive a training example (consisting of values for α,ωand the relevance judgment)\\nthat when added to the training set makes it impossible to sep arate the R’s from the\\nN’s using a line in the α-ωplane.\\n15.5 References and further reading\\nThe somewhat quirky name support vector machine originates in the neu-\\nral networks literature, where learning algorithms were th ought of as ar-\\nchitectures, and often referred to as “machines”. The disti nctive element of\\nthis model is that the decision boundary to use is completely decided (“sup-\\nported”) by a few training data points, the support vectors.\\nFor a more detailed presentation of SVMs, a good, well-known article-\\nlength introduction is ( Burges 1998 ).Chen et al. (2005 ) introduce the more\\nrecent ν-SVM, which provides an alternative parameterization for d ealing\\nwith inseparable problems, whereby rather than specifying a penalty C, you\\nspecify a parameter νwhich bounds the number of examples which can ap-\\npear on the wrong side of the decision surface. There are now a lso several\\nbooks dedicated to SVMs, large margin learning, and kernels : (Cristianini\\nand Shawe-Taylor 2000 ) and ( Schölkopf and Smola 2001 ) are more math-\\nematically oriented, while ( Shawe-Taylor and Cristianini 2004 ) aims to be\\nmore practical. For the foundations by their originator, se e (Vapnik 1998 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 382}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP15.5 References and further reading 347\\nSome recent, more general books on statistical learning, su ch as ( Hastie et al.\\n2001 ) also give thorough coverage of SVMs.\\nThe construction of multiclass SVMs is discussed in ( Weston and Watkins\\n1999 ), (Crammer and Singer 2001 ), and ( Tsochantaridis et al. 2005 ). The last\\nreference provides an introduction to the general framewor k of structural\\nSVMs.\\nThe kernel trick was ﬁrst presented in ( Aizerman et al. 1964 ). For more\\nabout string kernels and other kernels for structured data, see ( Lodhi et al.\\n2002 ) and ( Gaertner et al. 2002 ). The Advances in Neural Information Pro-\\ncessing (NIPS) conferences have become the premier venue fo r theoretical\\nmachine learning work, such as on SVMs. Other venues such as S IGIR are\\nmuch stronger on experimental methodology and using text-s peciﬁc features\\nto improve classiﬁer effectiveness.\\nA recent comparison of most current machine learning classi ﬁers (though\\non problems rather different from typical text problems) ca n be found in\\n(Caruana and Niculescu-Mizil 2006 ). (Li and Yang 2003 ), discussed in Sec-\\ntion 13.6, is the most recent comparative evaluation of machine learn ing clas-\\nsiﬁers on text classiﬁcation. Older examinations of classi ﬁers on text prob-\\nlems can be found in ( Yang 1999 ,Yang and Liu 1999 ,Dumais et al. 1998 ).\\nJoachims (2002a ) presents his work on SVMs applied to text problems in de-\\ntail. Zhang and Oles (2001 ) present an insightful comparison of Naive Bayes,\\nregularized logistic regression and SVM classiﬁers.\\nJoachims (1999 ) discusses methods of making SVM learning practical over\\nlarge text data sets. Joachims (2006a ) improves on this work.\\nA number of approaches to hierarchical classiﬁcation have b een developed\\nin order to deal with the common situation where the classes t o be assigned\\nhave a natural hierarchical organization ( Koller and Sahami 1997 ,McCal-\\nlum et al. 1998 ,Weigend et al. 1999 ,Dumais and Chen 2000 ). In a recent\\nlarge study on scaling SVMs to the entire Yahoo! directory, Liu et al. (2005 )\\nconclude that hierarchical classiﬁcation noticeably if st ill modestly outper-\\nforms ﬂat classiﬁcation. Classiﬁer effectiveness remains limited by the very\\nsmall number of training documents for many classes. For a mo re general\\napproach that can be applied to modeling relations between c lasses, which\\nmay be arbitrary rather than simply the case of a hierarchy, s eeTsochan-\\ntaridis et al. (2005 ).\\nMoschitti and Basili (2004 ) investigate the use of complex nominals, proper\\nnouns and word senses as features in text classiﬁcation.\\nDietterich (2002 ) overviews ensemble methods for classiﬁer combination,\\nwhile Schapire (2003 ) focuses particularly on boosting, which is applied to\\ntext classiﬁcation in ( Schapire and Singer 2000 ).\\nChapelle et al. (2006 ) present an introduction to work in semi-supervised\\nmethods, including in particular chapters on using EM for se mi-supervised\\ntext classiﬁcation ( Nigam et al. 2006 ) and on transductive SVMs ( Joachims', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 383}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP348 15 Support vector machines and machine learning on document s\\n2006b ).Sindhwani and Keerthi (2006 ) present a more efﬁcient implementa-\\ntion of a transductive SVM for large data sets.\\nTong and Koller (2001 ) explore active learning with SVMs for text classi-\\nﬁcation; Baldridge and Osborne (2004 ) point out that examples selected for\\nannotation with one classiﬁer in an active learning context may be no better\\nthan random examples when used with another classiﬁer.\\nMachine learning approaches to ranking for ad hoc retrieval were pio-\\nneered in ( Wong et al. 1988 ), (Fuhr 1992 ), and ( Gey 1994 ). But limited training\\ndata and poor machine learning techniques meant that these p ieces of work\\nachieved only middling results, and hence they only had limi ted impact at\\nthe time.\\nTaylor et al. (2006 ) study using machine learning to tune the parameters\\nof the BM25 family of ranking functions (Section 11.4.3 , page 232) so as to\\nmaximize NDCG (Section 8.4, page 163). Machine learning approaches to\\nordinal regression appear in ( Herbrich et al. 2000 ) and ( Burges et al. 2005 ),\\nand are applied to clickstream data in ( Joachims 2002b ).Cao et al. (2006 )\\nstudy how to make this approach effective in IR, and Qin et al. (2007 ) suggest\\nan extension involving using multiple hyperplanes. Yue et al. (2007 ) study\\nhow to do ranking with a structural SVM approach, and in parti cular show\\nhow this construction can be effectively used to directly op timize for MAP\\n(Section 8.4, page 158), rather than using surrogate measures like accuracy or\\narea under the ROC curve. Geng et al. (2007 ) study feature selection for the\\nranking problem.\\nOther approaches to learning to rank have also been shown to b e effective\\nfor web search, such as ( Burges et al. 2005 ,Richardson et al. 2006 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 384}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 349\\n16 Flat clustering\\nClustering algorithms group a set of documents into subsets orclusters . The CLUSTER\\nalgorithms’ goal is to create clusters that are coherent int ernally, but clearly\\ndifferent from each other. In other words, documents within a cluster should\\nbe as similar as possible; and documents in one cluster shoul d be as dissimi-\\nlar as possible from documents in other clusters.\\n0.0 0.5 1.0 1.5 2.00.0 0.5 1.0 1.5 2.0 2.5\\n◮Figure 16.1 An example of a data set with a clear cluster structure.\\nClustering is the most common form of unsupervised learning . No super- UNSUPERVISED\\nLEARNING vision means that there is no human expert who has assigned do cuments\\nto classes. In clustering, it is the distribution and makeup of the data that\\nwill determine cluster membership. A simple example is Figu re16.1. It is\\nvisually clear that there are three distinct clusters of poi nts. This chapter and\\nChapter 17introduce algorithms that ﬁnd such clusters in an unsupervi sed\\nfashion.\\nThe difference between clustering and classiﬁcation may no t seem great\\nat ﬁrst. After all, in both cases we have a partition of a set of documents\\ninto groups. But as we will see the two problems are fundament ally differ-\\nent. Classiﬁcation is a form of supervised learning (Chapte r13, page 256):\\nour goal is to replicate a categorical distinction that a hum an supervisor im-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 385}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP350 16 Flat clustering\\nposes on the data. In unsupervised learning, of which cluste ring is the most\\nimportant example, we have no such teacher to guide us.\\nThe key input to a clustering algorithm is the distance measu re. In Fig-\\nure16.1, the distance measure is distance in the 2D plane. This measu re sug-\\ngests three different clusters in the ﬁgure. In document clu stering, the dis-\\ntance measure is often also Euclidean distance. Different d istance measures\\ngive rise to different clusterings. Thus, the distance meas ure is an important\\nmeans by which we can inﬂuence the outcome of clustering.\\nFlat clustering creates a ﬂat set of clusters without any explicit structure that FLAT CLUSTERING\\nwould relate clusters to each other. Hierarchical clustering creates a hierarchy\\nof clusters and will be covered in Chapter 17. Chapter 17also addresses the\\ndifﬁcult problem of labeling clusters automatically.\\nA second important distinction can be made between hard and s oft cluster-\\ning algorithms. Hard clustering computes a hard assignment – each document HARD CLUSTERING\\nis a member of exactly one cluster. The assignment of soft clustering algo- SOFT CLUSTERING\\nrithms issoft– a document’s assignment is a distribution over all cluster s.\\nIn a soft assignment, a document has fractional membership i n several clus-\\nters. Latent semantic indexing, a form of dimensionality re duction, is a soft\\nclustering algorithm (Chapter 18, page 417).\\nThis chapter motivates the use of clustering in information retrieval by\\nintroducing a number of applications (Section 16.1), deﬁnes the problem\\nwe are trying to solve in clustering (Section 16.2) and discusses measures\\nfor evaluating cluster quality (Section 16.3). It then describes two ﬂat clus-\\ntering algorithms, K-means (Section 16.4), a hard clustering algorithm, and\\nthe Expectation-Maximization (or EM) algorithm (Section 16.5), a soft clus-\\ntering algorithm. K-means is perhaps the most widely used ﬂat clustering\\nalgorithm due to its simplicity and efﬁciency. The EM algori thm is a gen-\\neralization of K-means and can be applied to a large variety of document\\nrepresentations and distributions.\\n16.1 Clustering in information retrieval\\nThe cluster hypothesis states the fundamental assumption we make when us- CLUSTER HYPOTHESIS\\ning clustering in information retrieval.\\nCluster hypothesis. Documents in the same cluster behave similarly\\nwith respect to relevance to information needs.\\nThe hypothesis states that if there is a document from a clust er that is rele-\\nvant to a search request, then it is likely that other documen ts from the same\\ncluster are also relevant. This is because clustering puts t ogether documents\\nthat share many terms. The cluster hypothesis essentially i s the contiguity', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 386}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP16.1 Clustering in information retrieval 351\\nApplication What is Beneﬁt Example\\nclustered?\\nSearch result clustering search\\nresultsmore effective information\\npresentation to userFigure 16.2\\nScatter-Gather (subsets of)\\ncollectionalternative user interface:\\n“search without typing”Figure 16.3\\nCollection clustering collection effective information pre-\\nsentation for exploratory\\nbrowsingMcKeown et al. (2002 ),\\nhttp://news.google.com\\nLanguage modeling collection increased precision and/or\\nrecallLiu and Croft (2004 )\\nCluster-based retrieval collection higher efﬁciency: faster\\nsearchSalton (1971a )\\n◮Table 16.1 Some applications of clustering in information retrieval.\\nhypothesis in Chapter 14(page 289). In both cases, we posit that similar\\ndocuments behave similarly with respect to relevance.\\nTable 16.1 shows some of the main applications of clustering in informa -\\ntion retrieval. They differ in the set of documents that they cluster – search\\nresults, collection or subsets of the collection – and the as pect of an informa-\\ntion retrieval system they try to improve – user experience, user interface,\\neffectiveness or efﬁciency of the search system. But they ar e all based on the\\nbasic assumption stated by the cluster hypothesis.\\nThe ﬁrst application mentioned in Table 16.1 issearch result clustering where SEARCH RESULT\\nCLUSTERING by search results we mean the documents that were returned in response to\\na query. The default presentation of search results in infor mation retrieval is\\na simple list. Users scan the list from top to bottom until the y have found\\nthe information they are looking for. Instead, search resul t clustering clus-\\nters the search results, so that similar documents appear to gether. It is often\\neasier to scan a few coherent groups than many individual doc uments. This\\nis particularly useful if a search term has different word se nses. The example\\nin Figure 16.2 isjaguar . Three frequent senses on the web refer to the car, the\\nanimal and an Apple operating system. The Clustered Results panel returned\\nby the Vivísimo search engine ( http://vivisimo.com ) can be a more effective user\\ninterface for understanding what is in the search results th an a simple list of\\ndocuments.\\nA better user interface is also the goal of Scatter-Gather , the second ap- SCATTER -GATHER\\nplication in Table 16.1. Scatter-Gather clusters the whole collection to get\\ngroups of documents that the user can select or gather . The selected groups\\nare merged and the resulting set is again clustered. This pro cess is repeated\\nuntil a cluster of interest is found. An example is shown in Fi gure 16.3.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 387}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP352 16 Flat clustering\\n◮Figure 16.2 Clustering of search results to improve recall. None of the t op hits\\ncover the animal sense of jaguar , but users can easily access it by clicking on the cat\\ncluster in the Clustered Results panel on the left (third arrow from the top).\\nAutomatically generated clusters like those in Figure 16.3 are not as neatly\\norganized as a manually constructed hierarchical tree like the Open Direc-\\ntory athttp://dmoz.org . Also, ﬁnding descriptive labels for clusters automati-\\ncally is a difﬁcult problem (Section 17.7, page 396). But cluster-based navi-\\ngation is an interesting alternative to keyword searching, the standard infor-\\nmation retrieval paradigm. This is especially true in scena rios where users\\nprefer browsing over searching because they are unsure abou t which search\\nterms to use.\\nAs an alternative to the user-mediated iterative clusterin g in Scatter-Gather,\\nwe can also compute a static hierarchical clustering of a col lection that is\\nnot inﬂuenced by user interactions (“Collection clusterin g” in Table 16.1).\\nGoogle News and its precursor, the Columbia NewsBlaster sys tem, are ex-\\namples of this approach. In the case of news, we need to freque ntly recom-\\npute the clustering to make sure that users can access the lat est breaking\\nstories. Clustering is well suited for access to a collectio n of news stories\\nsince news reading is not really search, but rather a process of selecting a\\nsubset of stories about recent events.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 388}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP16.1 Clustering in information retrieval 353\\n◮Figure 16.3 An example of a user session in Scatter-Gather. A collection of New\\nYork Times news stories is clustered (“scattered”) into eig ht clusters (top row). The\\nuser manually gathers three of these into a smaller collection International Stories and\\nperforms another scattering operation. This process repea ts until a small cluster with\\nrelevant documents is found (e.g., Trinidad ).\\nThe fourth application of clustering exploits the cluster h ypothesis directly\\nfor improving search results, based on a clustering of the en tire collection.\\nWe use a standard inverted index to identify an initial set of documents that\\nmatch the query, but we then add other documents from the same clusters\\neven if they have low similarity to the query. For example, if the query is car\\nand several car documents are taken from a cluster of automob ile documents,\\nthen we can add documents from this cluster that use terms oth er thancar\\n(automobile ,vehicle etc). This can increase recall since a group of documents\\nwith high mutual similarity is often relevant as a whole.\\nMore recently this idea has been used for language modeling. Equation ( 12.10 ),\\npage 245, showed that to avoid sparse data problems in the language mo d-\\neling approach to IR, the model of document dcan be interpolated with a', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 389}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP354 16 Flat clustering\\ncollection model. But the collection contains many documen ts with terms\\nuntypical of d. By replacing the collection model with a model derived from\\nd’s cluster, we get more accurate estimates of the occurrence probabilities of\\nterms in d.\\nClustering can also speed up search. As we saw in Section 6.3.2 (page 123)\\nsearch in the vector space model amounts to ﬁnding the neares t neighbors\\nto the query. The inverted index supports fast nearest-neig hbor search for\\nthe standard IR setting. However, sometimes we may not be abl e to use an\\ninverted index efﬁciently, e.g., in latent semantic indexi ng (Chapter 18). In\\nsuch cases, we could compute the similarity of the query to ev ery document,\\nbut this is slow. The cluster hypothesis offers an alternati ve: Find the clus-\\nters that are closest to the query and only consider document s from these\\nclusters. Within this much smaller set, we can compute simil arities exhaus-\\ntively and rank documents in the usual way. Since there are ma ny fewer\\nclusters than documents, ﬁnding the closest cluster is fast ; and since the doc-\\numents matching a query are all similar to each other, they te nd to be in\\nthe same clusters. While this algorithm is inexact, the expe cted decrease in\\nsearch quality is small. This is essentially the applicatio n of clustering that\\nwas covered in Section 7.1.6 (page 141).\\n?Exercise 16.1\\nDeﬁne two documents as similar if they have at least two prope r names like Clinton\\norSarkozy in common. Give an example of an information need and two docu ments,\\nfor which the cluster hypothesis does nothold for this notion of similarity.\\nExercise 16.2\\nMake up a simple one-dimensional example (i.e. points on a li ne) with two clusters\\nwhere the inexactness of cluster-based retrieval shows up. In your example, retriev-\\ning clusters close to the query should do worse than direct ne arest neighbor search.\\n16.2 Problem statement\\nWe can deﬁne the goal in hard ﬂat clustering as follows. Given (i) a set of\\ndocuments D={d1, . . . , dN}, (ii) a desired number of clusters K, and (iii)\\nanobjective function that evaluates the quality of a clustering, we want to OBJECTIVE FUNCTION\\ncompute an assignment γ:D→ { 1, . . . , K}that minimizes (or, in other\\ncases, maximizes) the objective function. In most cases, we also demand that\\nγis surjective, i.e., that none of the Kclusters is empty.\\nThe objective function is often deﬁned in terms of similarit y or distance\\nbetween documents. Below, we will see that the objective in K-means clus-\\ntering is to minimize the average distance between document s and their cen-\\ntroids or, equivalently, to maximize the similarity betwee n documents and\\ntheir centroids. The discussion of similarity measures and distance metrics', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 390}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP16.2 Problem statement 355\\nin Chapter 14(page 291) also applies to this chapter. As in Chapter 14, we use\\nboth similarity and distance to talk about relatedness betw een documents.\\nFor documents, the type of similarity we want is usually topi c similarity\\nor high values on the same dimensions in the vector space mode l. For exam-\\nple, documents about China have high values on dimensions li keChinese ,\\nBeijing , andMao whereas documents about the UK tend to have high values\\nforLondon ,Britain andQueen . We approximate topic similarity with cosine\\nsimilarity or Euclidean distance in vector space (Chapter 6). If we intend to\\ncapture similarity of a type other than topic, for example, s imilarity of lan-\\nguage, then a different representation may be appropriate. When computing\\ntopic similarity, stop words can be safely ignored, but they are important\\ncues for separating clusters of English (in which theoccurs frequently and la\\ninfrequently) and French documents (in which theoccurs infrequently and la\\nfrequently).\\nA note on terminology. An alternative deﬁnition of hard clustering is that\\na document can be a full member of more than one cluster. Partitional clus- PARTITIONAL\\nCLUSTERING tering always refers to a clustering where each document belongs to exactly\\none cluster. (But in a partitional hierarchical clustering (Chapter 17) all mem-\\nbers of a cluster are of course also members of its parent.) On the deﬁnition\\nof hard clustering that permits multiple membership, the di fference between\\nsoft clustering and hard clustering is that membership valu es in hard clus-\\ntering are either 0 or 1, whereas they can take on any non-nega tive value in\\nsoft clustering.\\nSome researchers distinguish between exhaustive clusterings that assign EXHAUSTIVE\\neach document to a cluster and non-exhaustive clusterings, in which some\\ndocuments will be assigned to no cluster. Non-exhaustive cl usterings in\\nwhich each document is a member of either no cluster or one clu ster are\\ncalled exclusive . We deﬁne clustering to be exhaustive in this book. EXCLUSIVE\\n16.2.1 Cardinality – the number of clusters\\nA difﬁcult issue in clustering is determining the number of c lusters or cardi- CARDINALITY\\nnality of a clustering, which we denote by K. Often Kis nothing more than\\na good guess based on experience or domain knowledge. But for K-means,\\nwe will also introduce a heuristic method for choosing Kand an attempt to\\nincorporate the selection of Kinto the objective function. Sometimes the ap-\\nplication puts constraints on the range of K. For example, the Scatter-Gather\\ninterface in Figure 16.3 could not display more than about K=10 clusters\\nper layer because of the size and resolution of computer moni tors in the early\\n1990s.\\nSince our goal is to optimize an objective function, cluster ing is essentially', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 391}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP356 16 Flat clustering\\na search problem. The brute force solution would be to enumer ate all pos-\\nsible clusterings and pick the best. However, there are expo nentially many\\npartitions, so this approach is not feasible.1For this reason, most ﬂat clus-\\ntering algorithms reﬁne an initial partitioning iterative ly. If the search starts\\nat an unfavorable initial point, we may miss the global optim um. Finding a\\ngood starting point is therefore another important problem we have to solve\\nin ﬂat clustering.\\n16.3 Evaluation of clustering\\nTypical objective functions in clustering formalize the go al of attaining high\\nintra-cluster similarity (documents within a cluster are s imilar) and low inter-\\ncluster similarity (documents from different clusters are dissimilar). This is\\naninternal criterion for the quality of a clustering. But good scores on an INTERNAL CRITERION\\nOF QUALITY internal criterion do not necessarily translate into good e ffectiveness in an\\napplication. An alternative to internal criteria is direct evaluation in the ap-\\nplication of interest. For search result clustering, we may want to measure\\nthe time it takes users to ﬁnd an answer with different cluste ring algorithms.\\nThis is the most direct evaluation, but it is expensive, espe cially if large user\\nstudies are necessary.\\nAs a surrogate for user judgments, we can use a set of classes i n an evalua-\\ntion benchmark or gold standard (see Section 8.5, page 164, and Section 13.6,\\npage 279). The gold standard is ideally produced by human judges with a\\ngood level of inter-judge agreement (see Chapter 8, page 152). We can then\\ncompute an external criterion that evaluates how well the clustering matches EXTERNAL CRITERION\\nOF QUALITY the gold standard classes. For example, we may want to say tha t the opti-\\nmal clustering of the search results for jaguar in Figure 16.2 consists of three\\nclasses corresponding to the three senses car,animal , and operating system .\\nIn this type of evaluation, we only use the partition provide d by the gold\\nstandard, not the class labels.\\nThis section introduces four external criteria of clusteri ng quality. Purity is\\na simple and transparent evaluation measure. Normalized mutual information\\ncan be information-theoretically interpreted. The Rand index penalizes both\\nfalse positive and false negative decisions during cluster ing. The F measure\\nin addition supports differential weighting of these two ty pes of errors.\\nTo compute purity , each cluster is assigned to the class which is most fre- PURITY\\nquent in the cluster, and then the accuracy of this assignmen t is measured\\nby counting the number of correctly assigned documents and d ividing by N.\\n1. An upper bound on the number of clusterings is KN/K!. The exact number of different\\npartitions of Ndocuments into Kclusters is the Stirling number of the second kind. See\\nhttp://mathworld.wolfram.com/StirlingNumberoftheSec ondKind.html orComtet (1974 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 392}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP16.3 Evaluation of clustering 357\\nx\\no\\nx xxx\\nox\\noo⋄o x\\n⋄⋄⋄\\nxcluster 1 cluster 2 cluster 3\\n◮Figure 16.4 Purity as an external evaluation criterion for cluster qual ity. Majority\\nclass and number of members of the majority class for the thre e clusters are: x, 5\\n(cluster 1); o, 4 (cluster 2); and ⋄, 3 (cluster 3). Purity is (1/17)×(5+4+3)≈0.71.\\npurity NMI RI F5\\nlower bound 0.0 0.0 0.0 0.0\\nmaximum 1 1 1 1\\nvalue for Figure 16.4 0.71 0.36 0.68 0.46\\n◮Table 16.2 The four external evaluation measures applied to the cluste ring in\\nFigure 16.4.\\nFormally:\\npurity (Ω,C) =1\\nN∑\\nkmax\\nj|ωk∩cj| (16.1)\\nwhere Ω={ω1,ω2, . . . , ωK}is the set of clusters and C={c1,c2, . . . , cJ}is\\nthe set of classes. We interpret ωkas the set of documents in ωkand cjas the\\nset of documents in cjin Equation ( 16.1).\\nWe present an example of how to compute purity in Figure 16.4.2Bad\\nclusterings have purity values close to 0, a perfect cluster ing has a purity of\\n1. Purity is compared with the other three measures discusse d in this chapter\\nin Table 16.2.\\nHigh purity is easy to achieve when the number of clusters is l arge – in\\nparticular, purity is 1 if each document gets its own cluster . Thus, we cannot\\nuse purity to trade off the quality of the clustering against the number of\\nclusters.\\nA measure that allows us to make this tradeoff is normalized mutual infor- NORMALIZED MUTUAL\\nINFORMATION\\n2. Recall our note of caution from Figure 14.2 (page 291) when looking at this and other 2D\\nﬁgures in this and the following chapter: these illustratio ns can be misleading because 2D pro-\\njections of length-normalized vectors distort similariti es and distances between points.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 393}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP358 16 Flat clustering\\nmation orNMI :\\nNMI(Ω,C) =I(Ω;C)\\n[H(Ω) +H(C)]/2(16.2)\\nIis mutual information (cf. Chapter 13, page 272):\\nI(Ω;C) = ∑\\nk∑\\njP(ωk∩cj)logP(ωk∩cj)\\nP(ωk)P(cj)(16.3)\\n=∑\\nk∑\\nj|ωk∩cj|\\nNlogN|ωk∩cj|\\n|ωk||cj|(16.4)\\nwhere P(ωk),P(cj), and P(ωk∩cj)are the probabilities of a document being\\nin cluster ωk, class cj, and in the intersection of ωkand cj, respectively. Equa-\\ntion ( 16.4) is equivalent to Equation ( 16.3) for maximum likelihood estimates\\nof the probabilities (i.e., the estimate of each probabilit y is the corresponding\\nrelative frequency).\\nHis entropy as deﬁned in Chapter 5(page 99):\\nH(Ω) =−∑\\nkP(ωk)logP(ωk) (16.5)\\n=−∑\\nk|ωk|\\nNlog|ωk|\\nN(16.6)\\nwhere, again, the second equation is based on maximum likeli hood estimates\\nof the probabilities.\\nI(Ω;C)in Equation ( 16.3) measures the amount of information by which\\nour knowledge about the classes increases when we are told wh at the clusters\\nare. The minimum of I(Ω;C)is 0 if the clustering is random with respect to\\nclass membership. In that case, knowing that a document is in a particular\\ncluster does not give us any new information about what its cl ass might be.\\nMaximum mutual information is reached for a clustering Ωexactthat perfectly\\nrecreates the classes – but also if clusters in Ωexactare further subdivided into\\nsmaller clusters (Exercise 16.7). In particular, a clustering with K=None-\\ndocument clusters has maximum MI. So MI has the same problem a s purity:\\nit does not penalize large cardinalities and thus does not fo rmalize our bias\\nthat, other things being equal, fewer clusters are better.\\nThe normalization by the denominator [H(Ω)+H(C)]/2 in Equation ( 16.2)\\nﬁxes this problem since entropy tends to increase with the nu mber of clus-\\nters. For example, H(Ω)reaches its maximum log NforK=N, which en-\\nsures that NMI is low for K=N. Because NMI is normalized, we can use\\nit to compare clusterings with different numbers of cluster s. The particular\\nform of the denominator is chosen because [H(Ω) +H(C)]/2 is a tight upper\\nbound on I(Ω;C)(Exercise 16.8). Thus, NMI is always a number between 0\\nand 1.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 394}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP16.3 Evaluation of clustering 359\\nAn alternative to this information-theoretic interpretat ion of clustering is\\nto view it as a series of decisions, one for each of the N(N−1)/2 pairs of\\ndocuments in the collection. We want to assign two documents to the same\\ncluster if and only if they are similar. A true positive (TP) d ecision assigns\\ntwo similar documents to the same cluster, a true negative (T N) decision as-\\nsigns two dissimilar documents to different clusters. Ther e are two types\\nof errors we can commit. A false positive (FP) decision assig ns two dissim-\\nilar documents to the same cluster. A false negative (FN) dec ision assigns\\ntwo similar documents to different clusters. The Rand index (RI) measures RAND INDEX\\nRI the percentage of decisions that are correct. That is, it is s imply accuracy\\n(Section 8.3, page 155).\\nRI=TP+TN\\nTP+FP+FN+TN\\nAs an example, we compute RI for Figure 16.4. We ﬁrst compute TP +FP.\\nThe three clusters contain 6, 6, and 5 points, respectively, so the total number\\nof “positives” or pairs of documents that are in the same clus ter is:\\nTP+FP=(\\n6\\n2)\\n+(\\n6\\n2)\\n+(\\n5\\n2)\\n=40\\nOf these, the x pairs in cluster 1, the o pairs in cluster 2, the ⋄pairs in cluster 3,\\nand the x pair in cluster 3 are true positives:\\nTP=(\\n5\\n2)\\n+(\\n4\\n2)\\n+(\\n3\\n2)\\n+(\\n2\\n2)\\n=20\\nThus, FP =40−20=20.\\nFN and TN are computed similarly, resulting in the following contingency\\ntable:\\nSame cluster Different clusters\\nSame class TP=20 FN =24\\nDifferent classes FP=20 TN =72\\nRI is then (20+72)/(20+20+24+72)≈0.68.\\nThe Rand index gives equal weight to false positives and fals e negatives.\\nSeparating similar documents is sometimes worse than putti ng pairs of dis-\\nsimilar documents in the same cluster. We can use the F measure (Section 8.3, FMEASURE\\npage 154) to penalize false negatives more strongly than false posit ives by\\nselecting a value β>1, thus giving more weight to recall.\\nP=TP\\nTP+FPR=TP\\nTP+FNFβ=(β2+1)PR\\nβ2P+R', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 395}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP360 16 Flat clustering\\nBased on the numbers in the contingency table, P=20/40 =0.5 and R=\\n20/44≈0.455. This gives us F1≈0.48 for β=1 and F5≈0.456 for β=5.\\nIn information retrieval, evaluating clustering with Fhas the advantage that\\nthe measure is already familiar to the research community.\\n?Exercise 16.3\\nReplace every point din Figure 16.4 with two identical copies of din the same class.\\n(i) Is it less difﬁcult, equally difﬁcult or more difﬁcult to cluster this set of 34 points\\nas opposed to the 17 points in Figure 16.4? (ii) Compute purity, NMI, RI, and F5for\\nthe clustering with 34 points. Which measures increase and w hich stay the same after\\ndoubling the number of points? (iii) Given your assessment i n (i) and the results in\\n(ii), which measures are best suited to compare the quality o f the two clusterings?\\n16.4 K-means\\nK-means is the most important ﬂat clustering algorithm. Its o bjective is to\\nminimize the average squared Euclidean distance (Chapter 6, page 131) of\\ndocuments from their cluster centers where a cluster center is deﬁned as the\\nmean or centroid ⃗µof the documents in a cluster ω: CENTROID\\n⃗µ(ω) =1\\n|ω|∑\\n⃗x∈ω⃗x\\nThe deﬁnition assumes that documents are represented as len gth-normalized\\nvectors in a real-valued space in the familiar way. We used ce ntroids for Roc-\\nchio classiﬁcation in Chapter 14(page 292). They play a similar role here.\\nThe ideal cluster in K-means is a sphere with the centroid as its center of\\ngravity. Ideally, the clusters should not overlap. Our desi derata for classes\\nin Rocchio classiﬁcation were the same. The difference is th at we have no la-\\nbeled training set in clustering for which we know which docu ments should\\nbe in the same cluster.\\nA measure of how well the centroids represent the members of t heir clus-\\nters is the residual sum of squares orRSS, the squared distance of each vector RESIDUAL SUM OF\\nSQUARES from its centroid summed over all vectors:\\nRSS k=∑\\n⃗x∈ωk|⃗x−⃗µ(ωk)|2\\nRSS=K\\n∑\\nk=1RSS k (16.7)\\nRSS is the objective function in K-means and our goal is to minimize it. Since\\nNis ﬁxed, minimizing RSS is equivalent to minimizing the aver age squared\\ndistance, a measure of how well centroids represent their do cuments.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 396}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP16.4 K-means 361\\nK-MEANS ({⃗x1, . . . ,⃗xN},K)\\n1(⃗s1,⃗s2, . . . ,⃗sK)←SELECT RANDOM SEEDS({⃗x1, . . . ,⃗xN},K)\\n2fork←1toK\\n3do⃗µk←⃗sk\\n4while stopping criterion has not been met\\n5do for k←1toK\\n6 doωk←{}\\n7 forn←1toN\\n8 doj←arg minj′|⃗µj′−⃗xn|\\n9 ωj←ωj∪{⃗xn}(reassignment of vectors)\\n10 fork←1toK\\n11 do⃗µk←1\\n|ωk|∑⃗x∈ωk⃗x (recomputation of centroids)\\n12 return{⃗µ1, . . . ,⃗µK}\\n◮Figure 16.5 The K-means algorithm. For most IR applications, the vectors\\n⃗xn∈RMshould be length-normalized. Alternative methods of seed s election and\\ninitialization are discussed on page 364.\\nThe ﬁrst step of K-means is to select as initial cluster centers Krandomly\\nselected documents, the seeds . The algorithm then moves the cluster centers SEED\\naround in space in order to minimize RSS. As shown in Figure 16.5, this is\\ndone iteratively by repeating two steps until a stopping cri terion is met: reas-\\nsigning documents to the cluster with the closest centroid; and recomputing\\neach centroid based on the current members of its cluster. Fi gure 16.6 shows\\nsnapshots from nine iterations of the K-means algorithm for a set of points.\\nThe “centroid” column of Table 17.2 (page 397) shows examples of centroids.\\nWe can apply one of the following termination conditions.\\n•A ﬁxed number of iterations Ihas been completed. This condition limits\\nthe runtime of the clustering algorithm, but in some cases th e quality of\\nthe clustering will be poor because of an insufﬁcient number of iterations.\\n•Assignment of documents to clusters (the partitioning func tion γ) does\\nnot change between iterations. Except for cases with a bad lo cal mini-\\nmum, this produces a good clustering, but runtimes may be una cceptably\\nlong.\\n•Centroids ⃗µkdo not change between iterations. This is equivalent to γnot\\nchanging (Exercise 16.5).\\n•Terminate when RSS falls below a threshold. This criterion e nsures that\\nthe clustering is of a desired quality after termination. In practice, we', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 397}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP362 16 Flat clustering\\n0 1 2 3 4 5 601234\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet\\n/Bullet/Bullet /Bullet\\n/Bullet/Bullet/Bullet\\n/Bullet/Bullet/Bullet/Bullet/Bullet\\n/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet /Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet××\\nselection of seeds0 1 2 3 4 5 601234\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet\\n/Bullet/Bullet /Bullet\\n/Bullet/Bullet/Bullet\\n/Bullet/Bullet/Bullet/Bullet/Bullet\\n/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet /Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet/Bullet\\n/Bullet××\\nassignment of documents (iter. 1)\\n0 1 2 3 4 5 601234\\n++\\n++\\n++\\n++\\n+++o\\no\\n+o +\\n+++\\n++ + +o\\n+\\n+o\\n++++ oo\\n+o\\n++o+\\no\\n××××\\nrecomputation/movement of ⃗µ’s (iter. 1)0 1 2 3 4 5 601234\\n++\\n++\\n++\\n++\\n++++\\n+\\n+o +\\n+++\\n+o + oo\\no\\no+\\no+o+ o+\\noo\\no+o+\\no××\\n⃗µ’s after convergence (iter. 9)\\n0 1 2 3 4 5 601234\\n..\\n..\\n..\\n..\\n....\\n.\\n.. .\\n...\\n.. . ..\\n.\\n..\\n.... ..\\n..\\n....\\n.\\nmovement of ⃗µ’s in 9 iterations\\n◮Figure 16.6 AK-means example for K=2 in R2. The position of the two cen-\\ntroids ( ⃗µ’s shown as X’s in the top four panels) converges after nine it erations.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 398}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP16.4 K-means 363\\nneed to combine it with a bound on the number of iterations to g uarantee\\ntermination.\\n•Terminate when the decrease in RSS falls below a threshold θ. For small θ,\\nthis indicates that we are close to convergence. Again, we ne ed to combine\\nit with a bound on the number of iterations to prevent very lon g runtimes.\\nWe now show that K-means converges by proving that RSS monotonically\\ndecreases in each iteration. We will use decrease in the meaning decrease or does\\nnot change in this section. First, RSS decreases in the reassignment st ep since\\neach vector is assigned to the closest centroid, so the dista nce it contributes\\nto RSS decreases. Second, it decreases in the recomputation step because the\\nnew centroid is the vector ⃗vfor which RSS kreaches its minimum.\\nRSS k(⃗v) = ∑\\n⃗x∈ωk|⃗v−⃗x|2=∑\\n⃗x∈ωkM\\n∑\\nm=1(vm−xm)2(16.8)\\n∂RSS k(⃗v)\\n∂vm=∑\\n⃗x∈ωk2(vm−xm) (16.9)\\nwhere xmand vmare the mthcomponents of their respective vectors. Setting\\nthe partial derivative to zero, we get:\\nvm=1\\n|ωk|∑\\n⃗x∈ωkxm (16.10)\\nwhich is the componentwise deﬁnition of the centroid. Thus, we minimize\\nRSS kwhen the old centroid is replaced with the new centroid. RSS, the sum\\nof the RSS k, must then also decrease during recomputation.\\nSince there is only a ﬁnite set of possible clusterings, a mon otonically de-\\ncreasing algorithm will eventually arrive at a (local) mini mum. Take care,\\nhowever, to break ties consistently, e.g., by assigning a do cument to the clus-\\nter with the lowest index if there are several equidistant ce ntroids. Other-\\nwise, the algorithm can cycle forever in a loop of clustering s that have the\\nsame cost.\\nWhile this proves the convergence of K-means, there is unfortunately no\\nguarantee that a global minimum in the objective function will be reached.\\nThis is a particular problem if a document set contains many outliers , doc- OUTLIER\\numents that are far from any other documents and therefore do not ﬁt well\\ninto any cluster. Frequently, if an outlier is chosen as an in itial seed, then no\\nother vector is assigned to it during subsequent iterations . Thus, we end up\\nwith a singleton cluster (a cluster with only one document) even though there SINGLETON CLUSTER\\nis probably a clustering with lower RSS. Figure 16.7 shows an example of a\\nsuboptimal clustering resulting from a bad choice of initia l seeds.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 399}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP364 16 Flat clustering\\n0 1 2 3 40123\\n××\\n××\\n××d1d2 d3\\nd4d5 d6\\n◮Figure 16.7 The outcome of clustering in K-means depends on the initial seeds.\\nFor seeds d2and d5,K-means converges to {{d1,d2,d3},{d4,d5,d6}}, a suboptimal\\nclustering. For seeds d2and d3, it converges to{{d1,d2,d4,d5},{d3,d6}}, the global\\noptimum for K=2.\\nAnother type of suboptimal clustering that frequently occu rs is one with\\nempty clusters (Exercise 16.11 ).\\nEffective heuristics for seed selection include (i) exclud ing outliers from\\nthe seed set; (ii) trying out multiple starting points and ch oosing the cluster-\\ning with lowest cost; and (iii) obtaining seeds from another method such as\\nhierarchical clustering. Since deterministic hierarchic al clustering methods\\nare more predictable than K-means, a hierarchical clustering of a small ran-\\ndom sample of size iK(e.g., for i=5 or i=10) often provides good seeds\\n(see the description of the Buckshot algorithm, Chapter 17, page 399).\\nOther initialization methods compute seeds that are not sel ected from the\\nvectors to be clustered. A robust method that works well for a large variety\\nof document distributions is to select i(e.g., i=10) random vectors for each\\ncluster and use their centroid as the seed for this cluster. S ee Section 16.6 for\\nmore sophisticated initializations.\\nWhat is the time complexity of K-means? Most of the time is spent on com-\\nputing vector distances. One such operation costs Θ(M). The reassignment\\nstep computes KN distances, so its overall complexity is Θ(KNM ). In the\\nrecomputation step, each vector gets added to a centroid onc e, so the com-\\nplexity of this step is Θ(NM). For a ﬁxed number of iterations I, the overall\\ncomplexity is therefore Θ(IKNM ). Thus, K-means is linear in all relevant\\nfactors: iterations, number of clusters, number of vectors and dimensionality\\nof the space. This means that K-means is more efﬁcient than the hierarchical\\nalgorithms in Chapter 17. We had to ﬁx the number of iterations I, which can\\nbe tricky in practice. But in most cases, K-means quickly reaches either com-\\nplete convergence or a clustering that is close to convergen ce. In the latter\\ncase, a few documents would switch membership if further ite rations were\\ncomputed, but this has a small effect on the overall quality o f the clustering.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 400}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP16.4 K-means 365\\nThere is one subtlety in the preceding argument. Even a linea r algorithm\\ncan be quite slow if one of the arguments of Θ(. . .)is large, and Musually is\\nlarge. High dimensionality is not a problem for computing th e distance be-\\ntween two documents. Their vectors are sparse, so that only a small fraction\\nof the theoretically possible Mcomponentwise differences need to be com-\\nputed. Centroids, however, are dense since they pool all ter ms that occur in\\nany of the documents of their clusters. As a result, distance computations are\\ntime consuming in a naive implementation of K-means. However, there are\\nsimple and effective heuristics for making centroid-docum ent similarities as\\nfast to compute as document-document similarities. Trunca ting centroids to\\nthe most signiﬁcant kterms (e.g., k=1000) hardly decreases cluster quality\\nwhile achieving a signiﬁcant speedup of the reassignment st ep (see refer-\\nences in Section 16.6).\\nThe same efﬁciency problem is addressed by K-medoids , a variant of K- K-MEDOIDS\\nmeans that computes medoids instead of centroids as cluster centers. We\\ndeﬁne the medoid of a cluster as the document vector that is closest to the MEDOID\\ncentroid. Since medoids are sparse document vectors, dista nce computations\\nare fast.\\n✄16.4.1 Cluster cardinality in K-means\\nWe stated in Section 16.2 that the number of clusters Kis an input to most ﬂat\\nclustering algorithms. What do we do if we cannot come up with a plausible\\nguess for K?\\nA naive approach would be to select the optimal value of Kaccording to\\nthe objective function, namely the value of Kthat minimizes RSS. Deﬁning\\nRSS min(K)as the minimal RSS of all clusterings with Kclusters, we observe\\nthat RSS min(K)is a monotonically decreasing function in K(Exercise 16.13 ),\\nwhich reaches its minimum 0 for K=Nwhere Nis the number of doc-\\numents. We would end up with each document being in its own clu ster.\\nClearly, this is not an optimal clustering.\\nA heuristic method that gets around this problem is to estima te RSS min(K)\\nas follows. We ﬁrst perform i(e.g., i=10) clusterings with Kclusters (each\\nwith a different initialization) and compute the RSS of each . Then we take the\\nminimum of the iRSS values. We denote this minimum by ˆRSS min(K). Now\\nwe can inspect the values ˆRSS min(K)asKincreases and ﬁnd the “knee” in the\\ncurve – the point where successive decreases in ˆRSS minbecome noticeably\\nsmaller. There are two such points in Figure 16.8, one at K=4, where the\\ngradient ﬂattens slightly, and a clearer ﬂattening at K=9. This is typical:\\nthere is seldom a single best number of clusters. We still nee d to employ an\\nexternal constraint to choose from a number of possible valu es of K(4 and 9\\nin this case).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 401}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP366 16 Flat clustering\\n2 4 6 8 101750 1800 1850 1900 1950\\nnumber of clustersresidual sum of squares\\n◮Figure 16.8 Estimated minimal residual sum of squares as a function of th e num-\\nber of clusters in K-means. In this clustering of 1203 Reuters-RCV1 documents, there\\nare two points where the ˆRSS mincurve ﬂattens: at 4 clusters and at 9 clusters. The\\ndocuments were selected from the categories China ,Germany ,Russia and Sports , so\\ntheK=4 clustering is closest to the Reuters classiﬁcation.\\nA second type of criterion for cluster cardinality imposes a penalty for each\\nnew cluster – where conceptually we start with a single clust er containing all\\ndocuments and then search for the optimal number of clusters Kby succes-\\nsively incrementing Kby one. To determine the cluster cardinality in this\\nway, we create a generalized objective function that combin es two elements:\\ndistortion , a measure of how much documents deviate from the prototype o f DISTORTION\\ntheir clusters (e.g., RSS for K-means); and a measure of model complexity . We MODEL COMPLEXITY\\ninterpret a clustering here as a model of the data. Model comp lexity in clus-\\ntering is usually the number of clusters or a function thereo f. For K-means,\\nwe then get this selection criterion for K:\\nK=arg min\\nK[RSS min(K) +λK] (16.11)\\nwhere λis a weighting factor. A large value of λfavors solutions with few\\nclusters. For λ=0, there is no penalty for more clusters and K=Nis the\\nbest solution.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 402}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP16.4 K-means 367\\nThe obvious difﬁculty with Equation ( 16.11 ) is that we need to determine\\nλ. Unless this is easier than determining Kdirectly, then we are back to\\nsquare one. In some cases, we can choose values of λthat have worked well\\nfor similar data sets in the past. For example, if we periodic ally cluster news\\nstories from a newswire, there is likely to be a ﬁxed value of λthat gives us\\nthe right Kin each successive clustering. In this application, we woul d not\\nbe able to determine Kbased on past experience since Kchanges.\\nA theoretical justiﬁcation for Equation ( 16.11 ) is the Akaike Information Cri- AKAIKE INFORMATION\\nCRITERION terion or AIC, an information-theoretic measure that trades off di stortion\\nagainst model complexity. The general form of AIC is:\\nAIC: K=arg min\\nK[−2L(K) +2q(K)] (16.12)\\nwhere−L(K), the negative maximum log-likelihood of the data for Kclus-\\nters, is a measure of distortion and q(K), the number of parameters of a\\nmodel with Kclusters, is a measure of model complexity. We will not at-\\ntempt to derive the AIC here, but it is easy to understand intu itively. The\\nﬁrst property of a good model of the data is that each data poin t is modeled\\nwell by the model. This is the goal of low distortion. But mode ls should\\nalso be small (i.e., have low model complexity) since a model that merely\\ndescribes the data (and therefore has zero distortion) is wo rthless. AIC pro-\\nvides a theoretical justiﬁcation for one particular way of w eighting these two\\nfactors, distortion and model complexity, when selecting a model.\\nForK-means, the AIC can be stated as follows:\\nAIC: K=arg min\\nK[RSS min(K) +2MK] (16.13)\\nEquation ( 16.13 ) is a special case of Equation ( 16.11 ) for λ=2M.\\nTo derive Equation ( 16.13 ) from Equation ( 16.12 ) observe that q(K) =KM\\ninK-means since each element of the Kcentroids is a parameter that can be\\nvaried independently; and that L(K) =−(1/2)RSS min(K)(modulo a con-\\nstant) if we view the model underlying K-means as a Gaussian mixture with\\nhard assignment, uniform cluster priors and identical sphe rical covariance\\nmatrices (see Exercise 16.19 ).\\nThe derivation of AIC is based on a number of assumptions, e.g ., that the\\ndata are independent and identically distributed. These as sumptions are\\nonly approximately true for data sets in information retrie val. As a conse-\\nquence, the AIC can rarely be applied without modiﬁcation in text clustering.\\nIn Figure 16.8, the dimensionality of the vector space is M≈50,000. Thus,\\n2MK>50,000 dominates the smaller RSS-based term ( ˆRSS min(1)<5000,\\nnot shown in the ﬁgure) and the minimum of the expression is re ached for\\nK=1. But as we know, K=4 (corresponding to the four classes China ,', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 403}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP368 16 Flat clustering\\nGermany ,Russia and Sports ) is a better choice than K=1. In practice, Equa-\\ntion ( 16.11 ) is often more useful than Equation ( 16.13 ) – with the caveat that\\nwe need to come up with an estimate for λ.\\n?Exercise 16.4\\nWhy are documents that do not use the same term for the concept carlikely to end\\nup in the same cluster in K-means clustering?\\nExercise 16.5\\nTwo of the possible termination conditions for K-means were (1) assignment does not\\nchange, (2) centroids do not change (page 361). Do these two conditions imply each\\nother?\\n✄16.5 Model-based clustering\\nIn this section, we describe a generalization of K-means, the EM algorithm.\\nIt can be applied to a larger variety of document representat ions and distri-\\nbutions than K-means.\\nInK-means, we attempt to ﬁnd centroids that are good representa tives. We\\ncan view the set of Kcentroids as a model that generates the data. Generating\\na document in this model consists of ﬁrst picking a centroid a t random and\\nthen adding some noise. If the noise is normally distributed , this procedure\\nwill result in clusters of spherical shape. Model-based clustering assumes that MODEL -BASED\\nCLUSTERING the data were generated by a model and tries to recover the ori ginal model\\nfrom the data. The model that we recover from the data then deﬁ nes clusters\\nand an assignment of documents to clusters.\\nA commonly used criterion for estimating the model paramete rs is maxi-\\nmum likelihood. In K-means, the quantity exp (−RSS)is proportional to the\\nlikelihood that a particular model (i.e., a set of centroids ) generated the data.\\nForK-means, maximum likelihood and minimal RSS are equivalent c riteria.\\nWe denote the model parameters by Θ. InK-means, Θ={⃗µ1, . . . ,⃗µK}.\\nMore generally, the maximum likelihood criterion is to sele ct the parame-\\nters Θthat maximize the log-likelihood of generating the data D:\\nΘ=arg max\\nΘL(D|Θ) =arg max\\nΘlogN\\n∏\\nn=1P(dn|Θ) =arg max\\nΘN\\n∑\\nn=1logP(dn|Θ)\\nL(D|Θ)is the objective function that measures the goodness of the c luster-\\ning. Given two clusterings with the same number of clusters, we prefer the\\none with higher L(D|Θ).\\nThis is the same approach we took in Chapter 12(page 237) for language\\nmodeling and in Section 13.1 (page 265) for text classiﬁcation. In text clas-\\nsiﬁcation, we chose the class that maximizes the likelihood of generating a\\nparticular document. Here, we choose the clustering Θthat maximizes the', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 404}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP16.5 Model-based clustering 369\\nlikelihood of generating a given set of documents. Once we ha veΘ, we can\\ncompute an assignment probability P(d|ωk;Θ)for each document-cluster\\npair. This set of assignment probabilities deﬁnes a soft clu stering.\\nAn example of a soft assignment is that a document about Chine se cars\\nmay have a fractional membership of 0.5 in each of the two clus ters China\\nand automobiles , reﬂecting the fact that both topics are pertinent. A hard cl us-\\ntering like K-means cannot model this simultaneous relevance to two topi cs.\\nModel-based clustering provides a framework for incorpora ting our know-\\nledge about a domain. K-means and the hierarchical algorithms in Chap-\\nter17make fairly rigid assumptions about the data. For example, c lusters\\ninK-means are assumed to be spheres. Model-based clustering of fers more\\nﬂexibility. The clustering model can be adapted to what we kn ow about\\nthe underlying distribution of the data, be it Bernoulli (as in the example\\nin Table 16.3), Gaussian with non-spherical variance (another model tha t is\\nimportant in document clustering) or a member of a different family.\\nA commonly used algorithm for model-based clustering is the Expectation- EXPECTATION -\\nMAXIMIZATION\\nALGORITHMMaximization algorithm orEM algorithm . EM clustering is an iterative algo-\\nrithm that maximizes L(D|Θ). EM can be applied to many different types of\\nprobabilistic modeling. We will work with a mixture of multi variate Bernoulli\\ndistributions here, the distribution we know from Section 11.3 (page 222) and\\nSection 13.3 (page 263):\\nP(d|ωk;Θ) =(\\n∏\\ntm∈dqmk)(\\n∏\\ntm/∈d(1−qmk))\\n(16.14)\\nwhere Θ={Θ1, . . . , ΘK},Θk= (αk,q1k, . . . , qMk), and qmk=P(Um=1|ωk)\\nare the parameters of the model.3P(Um=1|ωk)is the probability that a\\ndocument from cluster ωkcontains term tm. The probability αkis the prior of\\ncluster ωk: the probability that a document dis in ωkif we have no informa-\\ntion about d.\\nThe mixture model then is:\\nP(d|Θ) =K\\n∑\\nk=1αk(\\n∏\\ntm∈dqmk)(\\n∏\\ntm/∈d(1−qmk))\\n(16.15)\\nIn this model, we generate a document by ﬁrst picking a cluste rkwith prob-\\nability αkand then generating the terms of the document according to th e\\nparameters qmk. Recall that the document representation of the multivaria te\\nBernoulli is a vector of MBoolean values (and not a real-valued vector).\\n3.Umis the random variable we deﬁned in Section 13.3 (page 266) for the Bernoulli Naive Bayes\\nmodel. It takes the values 1 (term tmis present in the document) and 0 (term tmis absent in the\\ndocument).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 405}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP370 16 Flat clustering\\nHow do we use EM to infer the parameters of the clustering from the data?\\nThat is, how do we choose parameters Θthat maximize L(D|Θ)? EM is simi-\\nlar to K-means in that it alternates between an expectation step , corresponding EXPECTATION STEP\\nto reassignment, and a maximization step , corresponding to recomputation of MAXIMIZATION STEP\\nthe parameters of the model. The parameters of K-means are the centroids,\\nthe parameters of the instance of EM in this section are the αkand qmk.\\nThe maximization step recomputes the conditional paramete rsqmkand the\\npriors αkas follows:\\nMaximization step: qmk=∑N\\nn=1rnkI(tm∈dn)\\n∑N\\nn=1rnkαk=∑N\\nn=1rnk\\nN(16.16)\\nwhere I(tm∈dn) = 1 if tm∈dnand 0 otherwise and rnkis the soft as-\\nsignment of document dnto cluster kas computed in the preceding iteration.\\n(We’ll address the issue of initialization in a moment.) The se are the max-\\nimum likelihood estimates for the parameters of the multiva riate Bernoulli\\nfrom Table 13.3 (page 268) except that documents are assigned fractionally to\\nclusters here. These maximum likelihood estimates maximiz e the likelihood\\nof the data given the model.\\nThe expectation step computes the soft assignment of docume nts to clus-\\nters given the current parameters qmkand αk:\\nExpectation step :rnk=αk(∏tm∈dnqmk)(∏tm/∈dn(1−qmk))\\n∑K\\nk=1αk(∏tm∈dnqmk)(∏tm/∈dn(1−qmk))(16.17)\\nThis expectation step applies Equations ( 16.14 ) and ( 16.15 ) to computing the\\nlikelihood that ωkgenerated document dn. It is the classiﬁcation procedure\\nfor the multivariate Bernoulli in Table 13.3. Thus, the expectation step is\\nnothing else but Bernoulli Naive Bayes classiﬁcation (incl uding normaliza-\\ntion, i.e. dividing by the denominator, to get a probability distribution over\\nclusters).\\nWe clustered a set of 11 documents into two clusters using EM i n Ta-\\nble16.3. After convergence in iteration 25, the ﬁrst 5 documents are assigned\\nto cluster 1 ( ri,1=1.00) and the last 6 to cluster 2 ( ri,1=0.00). Somewhat\\natypically, the ﬁnal assignment is a hard assignment here. E M usually con-\\nverges to a soft assignment. In iteration 25, the prior α1for cluster 1 is\\n5/11≈0.45 because 5 of the 11 documents are in cluster 1. Some terms\\nare quickly associated with one cluster because the initial assignment can\\n“spread” to them unambiguously. For example, membership in cluster 2\\nspreads from document 7 to document 8 in the ﬁrst iteration be cause they\\nsharesugar (r8,1=0 in iteration 1). For parameters of terms occurring\\nin ambiguous contexts, convergence takes longer. Seed docu ments 6 and 7', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 406}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP16.5 Model-based clustering 371\\n(a) docID document text docID document text\\n1 hot chocolate cocoa beans 7 sweet sugar\\n2 cocoa ghana africa 8 sugar cane brazil\\n3 beans harvest ghana 9 sweet sugar beet\\n4 cocoa butter 10 sweet cake icing\\n5 butter trufﬂes 11 cake black forest\\n6 sweet chocolate\\n(b) Parameter Iteration of clustering\\n0 1 2 3 4 5 15 25\\nα1 0.50 0.45 0.53 0.57 0.58 0.54 0.45\\nr1,1 1.00 1.00 1.00 1.00 1.00 1.00 1.00\\nr2,1 0.50 0.79 0.99 1.00 1.00 1.00 1.00\\nr3,1 0.50 0.84 1.00 1.00 1.00 1.00 1.00\\nr4,1 0.50 0.75 0.94 1.00 1.00 1.00 1.00\\nr5,1 0.50 0.52 0.66 0.91 1.00 1.00 1.00\\nr6,1 1.00 1.00 1.00 1.00 1.00 1.00 0.83 0.00\\nr7,1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\\nr8,1 0.00 0.00 0.00 0.00 0.00 0.00 0.00\\nr9,1 0.00 0.00 0.00 0.00 0.00 0.00 0.00\\nr10,1 0.50 0.40 0.14 0.01 0.00 0.00 0.00\\nr11,1 0.50 0.57 0.58 0.41 0.07 0.00 0.00\\nqafrica,1 0.000 0.100 0.134 0.158 0.158 0.169 0.200\\nqafrica,2 0.000 0.083 0.042 0.001 0.000 0.000 0.000\\nqbrazil,1 0.000 0.000 0.000 0.000 0.000 0.000 0.000\\nqbrazil,2 0.000 0.167 0.195 0.213 0.214 0.196 0.167\\nqcocoa,1 0.000 0.400 0.432 0.465 0.474 0.508 0.600\\nqcocoa,2 0.000 0.167 0.090 0.014 0.001 0.000 0.000\\nqsugar,1 0.000 0.000 0.000 0.000 0.000 0.000 0.000\\nqsugar,2 1.000 0.500 0.585 0.640 0.642 0.589 0.500\\nqsweet,1 1.000 0.300 0.238 0.180 0.159 0.153 0.000\\nqsweet,2 1.000 0.417 0.507 0.610 0.640 0.608 0.667\\n◮Table 16.3 The EM clustering algorithm. The table shows a set of documen ts\\n(a) and parameter values for selected iterations during EM c lustering (b). Parameters\\nshown are prior α1, soft assignment scores rn,1(both omitted for cluster 2), and lexical\\nparameters qm,kfor a few terms. The authors initially assigned document 6 to clus-\\nter 1 and document 7 to cluster 2 (iteration 0). EM converges a fter 25 iterations. For\\nsmoothing, the rnkin Equation ( 16.16 ) were replaced with rnk+ǫwhere ǫ=0.0001.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 407}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP372 16 Flat clustering\\nboth contain sweet . As a result, it takes 25 iterations for the term to be unam-\\nbiguously associated with cluster 2. ( qsweet,1 =0 in iteration 25.)\\nFinding good seeds is even more critical for EM than for K-means. EM is\\nprone to get stuck in local optima if the seeds are not chosen w ell. This is a\\ngeneral problem that also occurs in other applications of EM .4Therefore, as\\nwith K-means, the initial assignment of documents to clusters is o ften com-\\nputed by a different algorithm. For example, a hard K-means clustering may\\nprovide the initial assignment, which EM can then “soften up .”\\n?Exercise 16.6\\nWe saw above that the time complexity of K-means is Θ(IKNM ). What is the time\\ncomplexity of EM?\\n16.6 References and further reading\\nBerkhin (2006b ) gives a general up-to-date survey of clustering methods wi th\\nspecial attention to scalability. The classic reference fo r clustering in pat-\\ntern recognition, covering both K-means and EM, is ( Duda et al. 2000 ).Ras-\\nmussen (1992 ) introduces clustering from an information retrieval pers pec-\\ntive. Anderberg (1973 ) provides a general introduction to clustering for ap-\\nplications. In addition to Euclidean distance and cosine si milarity, Kullback-\\nLeibler divergence is often used in clustering as a measure o f how (dis)similar\\ndocuments and clusters are ( Xu and Croft 1999 ,Muresan and Harper 2004 ,\\nKurland and Lee 2004 ).\\nThe cluster hypothesis is due to Jardine and van Rijsbergen (1971 ) who\\nstate it as follows: Associations between documents convey information about t he\\nrelevance of documents to requests. Salton (1971a ;1975 ),Croft (1978 ),Voorhees\\n(1985a ),Can and Ozkarahan (1990 ),Cacheda et al. (2003 ),Can et al. (2004 ),\\nSingitham et al. (2004 ) and Altingövde et al. (2008 ) investigate the efﬁciency\\nand effectiveness of cluster-based retrieval. While some o f these studies\\nshow improvements in effectiveness, efﬁciency or both, the re is no consensus\\nthat cluster-based retrieval works well consistently acro ss scenarios. Cluster-\\nbased language modeling was pioneered by Liu and Croft (2004 ).\\nThere is good evidence that clustering of search results imp roves user ex-\\nperience and search result quality ( Hearst and Pedersen 1996 ,Zamir and Et-\\nzioni 1999 ,Tombros et al. 2002 ,Käki 2005 ,Toda and Kataoka 2005 ), although\\nnot as much as search result structuring based on carefully e dited category\\nhierarchies ( Hearst 2006 ). The Scatter-Gather interface for browsing collec-\\ntions was presented by Cutting et al. (1992 ). A theoretical framework for an-\\n4. For example, this problem is common when EM is used to estim ate parameters of hidden\\nMarkov models, probabilistic grammars, and machine transl ation models in natural language\\nprocessing ( Manning and Schütze 1999 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 408}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP16.6 References and further reading 373\\nalyzing the properties of Scatter/Gather and other informa tion seeking user\\ninterfaces is presented by Pirolli (2007 ).Schütze and Silverstein (1997 ) eval-\\nuate LSI (Chapter 18) and truncated representations of centroids for efﬁcient\\nK-means clustering.\\nThe Columbia NewsBlaster system ( McKeown et al. 2002 ), a forerunner to\\nthe now much more famous and reﬁned Google News ( http://news.google.com ),\\nused hierarchical clustering (Chapter 17) to give two levels of news topic\\ngranularity. See Hatzivassiloglou et al. (2000 ) for details, and Chen and Lin\\n(2000 ) and Radev et al. (2001 ) for related systems. Other applications of\\nclustering in information retrieval are duplicate detecti on (Yang and Callan\\n(2006 ), Section 19.6, page 438), novelty detection (see references in Section 17.9,\\npage 399) and metadata discovery on the semantic web ( Alonso et al. 2006 ).\\nThe discussion of external evaluation measures is partiall y based on Strehl\\n(2002 ).Dom (2002 ) proposes a measure Q0that is better motivated theoret-\\nically than NMI. Q0is the number of bits needed to transmit class member-\\nships assuming cluster memberships are known. The Rand inde x is due to\\nRand (1971 ).Hubert and Arabie (1985 ) propose an adjusted Rand index that ADJUSTED RAND INDEX\\nranges between−1 and 1 and is 0 if there is only chance agreement between\\nclusters and classes (similar to κin Chapter 8, page 165).Basu et al. (2004 ) ar-\\ngue that the three evaluation measures NMI, Rand index and F m easure give\\nvery similar results. Stein et al. (2003 ) propose expected edge density as an in-\\nternal measure and give evidence that it is a good predictor o f the quality of a\\nclustering. Kleinberg (2002 ) and Meil˘ a (2005 ) present axiomatic frameworks\\nfor comparing clusterings.\\nAuthors that are often credited with the invention of the K-means algo-\\nrithm include Lloyd (1982 ) (ﬁrst distributed in 1957), Ball (1965 ),MacQueen\\n(1967 ), and Hartigan and Wong (1979 ).Arthur and Vassilvitskii (2006 ) in-\\nvestigate the worst-case complexity of K-means. Bradley and Fayyad (1998 ),\\nPelleg and Moore (1999 ) and Davidson and Satyanarayana (2003 ) investi-\\ngate the convergence properties of K-means empirically and how it depends\\non initial seed selection. Dhillon and Modha (2001 ) compare K-means clus-\\nters with SVD-based clusters (Chapter 18). The K-medoid algorithm was\\npresented by Kaufman and Rousseeuw (1990 ). The EM algorithm was orig-\\ninally introduced by Dempster et al. (1977 ). An in-depth treatment of EM is\\n(McLachlan and Krishnan 1996 ). See Section 18.5 (page 417) for publications\\non latent analysis, which can also be viewed as soft clusteri ng.\\nAIC is due to Akaike (1974 ) (see also Burnham and Anderson (2002 )). An\\nalternative to AIC is BIC, which can be motivated as a Bayesia n model se-\\nlection procedure ( Schwarz 1978 ).Fraley and Raftery (1998 ) show how to\\nchoose an optimal number of clusters based on BIC. An applica tion of BIC to\\nK-means is ( Pelleg and Moore 2000 ).Hamerly and Elkan (2003 ) propose an\\nalternative to BIC that performs better in their experiment s. Another inﬂu-\\nential Bayesian approach for determining the number of clus ters (simultane-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 409}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP374 16 Flat clustering\\nously with cluster assignment) is described by Cheeseman and Stutz (1996 ).\\nTwo methods for determining cardinality without external c riteria are pre-\\nsented by Tibshirani et al. (2001 ).\\nWe only have space here for classical completely unsupervis ed clustering.\\nAn important current topic of research is how to use prior kno wledge to\\nguide clustering (e.g., Ji and Xu (2006 )) and how to incorporate interactive\\nfeedback during clustering (e.g., Huang and Mitchell (2006 )).Fayyad et al.\\n(1998 ) propose an initialization for EM clustering. For algorith ms that can\\ncluster very large data sets in one scan through the data see Bradley et al.\\n(1998 ).\\nThe applications in Table 16.1 all cluster documents. Other information re-\\ntrieval applications cluster words (e.g., Crouch 1988 ), contexts of words (e.g.,\\nSchütze and Pedersen 1995 ) or words and documents simultaneously (e.g.,\\nTishby and Slonim 2000 ,Dhillon 2001 ,Zha et al. 2001 ). Simultaneous clus-\\ntering of words and documents is an example of co-clustering orbiclustering . CO-CLUSTERING\\n16.7 Exercises\\n?Exercise 16.7\\nLetΩbe a clustering that exactly reproduces a class structure Cand Ω′a clustering\\nthat further subdivides some clusters in Ω. Show that I(Ω;C) =I(Ω′;C).\\nExercise 16.8\\nShow that I(Ω;C)≤[H(Ω) +H(C)]/2.\\nExercise 16.9\\nMutual information is symmetric in the sense that its value d oes not change if the\\nroles of clusters and classes are switched: I(Ω;C) = I(C;Ω). Which of the other\\nthree evaluation measures are symmetric in this sense?\\nExercise 16.10\\nCompute RSS for the two clusterings in Figure 16.7.\\nExercise 16.11\\n(i) Give an example of a set of points and three initial centro ids (which need not be\\nmembers of the set of points) for which 3-means converges to a clustering with an\\nempty cluster. (ii) Can a clustering with an empty cluster be the global optimum with\\nrespect to RSS?\\nExercise 16.12\\nDownload Reuters-21578. Discard documents that do not occu r in one of the 10\\nclasses acquisitions ,corn,crude ,earn,grain ,interest ,money-fx ,ship,trade , and wheat .\\nDiscard documents that occur in two of these 10 classes. (i) C ompute a K-means clus-\\ntering of this subset into 10 clusters. There are a number of s oftware packages that\\nimplement K-means, such as WEKA ( Witten and Frank 2005 ) and R ( R Development\\nCore Team 2005 ). (ii) Compute purity, normalized mutual information, F1and RI for', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 410}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP16.7 Exercises 375\\nthe clustering with respect to the 10 classes. (iii) Compile a confusion matrix (Ta-\\nble14.5, page 308) for the 10 classes and 10 clusters. Identify classes that gi ve rise to\\nfalse positives and false negatives.\\nExercise 16.13\\nProve that RSS min(K)is monotonically decreasing in K.\\nExercise 16.14\\nThere is a soft version of K-means that computes the fractional membership of a doc-\\nument in a cluster as a monotonically decreasing function of the distance ∆from its\\ncentroid, e.g., as e−∆. Modify reassignment and recomputation steps of hard K-means\\nfor this soft version.\\nExercise 16.15\\nIn the last iteration in Table 16.3, document 6 is in cluster 2 even though it was the\\ninitial seed for cluster 1. Why does the document change memb ership?\\nExercise 16.16\\nThe values of the parameters qmkin iteration 25 in Table 16.3 are rounded. What are\\nthe exact values that EM will converge to?\\nExercise 16.17\\nPerform a K-means clustering for the documents in Table 16.3. After how many\\niterations does K-means converge? Compare the result with the EM clustering i n\\nTable 16.3 and discuss the differences.\\nExercise 16.18 [⋆ ⋆ ⋆ ]\\nModify the expectation and maximization steps of EM for a Gau ssian mixture. The\\nmaximization step computes the maximum likelihood paramet er estimates αk,⃗µk,\\nand Σkfor each of the clusters. The expectation step computes for e ach vector a soft\\nassignment to clusters (Gaussians) based on their current p arameters. Write down\\nthe equations for Gaussian mixtures corresponding to Equat ions ( 16.16 ) and ( 16.17 ).\\nExercise 16.19 [⋆ ⋆ ⋆ ]\\nShow that K-means can be viewed as the limiting case of EM for Gaussian mi xtures\\nif variance is very small and all covariances are 0.\\nExercise 16.20 [⋆ ⋆ ⋆ ]\\nThewithin-point scatter of a clustering is deﬁned as ∑k1\\n2∑⃗xi∈ωk∑⃗xj∈ωk|⃗xi−⃗xj|2. Show WITHIN -POINT\\nSCATTERthat minimizing RSS and minimizing within-point scatter ar e equivalent.\\nExercise 16.21 [⋆ ⋆ ⋆ ]\\nDerive an AIC criterion for the multivariate Bernoulli mixt ure model from Equa-\\ntion ( 16.12 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 411}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 412}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 377\\n17 Hierarchical clustering\\nFlat clustering is efﬁcient and conceptually simple, but as we saw in Chap-\\nter16it has a number of drawbacks. The algorithms introduced in Ch ap-\\nter16return a ﬂat unstructured set of clusters, require a prespec iﬁed num-\\nber of clusters as input and are nondeterministic. Hierarchical clustering (or HIERARCHICAL\\nCLUSTERING hierarchic clustering ) outputs a hierarchy, a structure that is more informative\\nthan the unstructured set of clusters returned by ﬂat cluste ring.1Hierarchical\\nclustering does not require us to prespecify the number of cl usters and most\\nhierarchical algorithms that have been used in IR are determ inistic. These ad-\\nvantages of hierarchical clustering come at the cost of lowe r efﬁciency. The\\nmost common hierarchical clustering algorithms have a comp lexity that is at\\nleast quadratic in the number of documents compared to the li near complex-\\nity of K-means and EM (cf. Section 16.4, page 364).\\nThis chapter ﬁrst introduces agglomerative hierarchical clustering (Section 17.1)\\nand presents four different agglomerative algorithms, in S ections 17.2–17.4,\\nwhich differ in the similarity measures they employ: single -link, complete-\\nlink, group-average, and centroid similarity. We then disc uss the optimality\\nconditions of hierarchical clustering in Section 17.5. Section 17.6 introduces\\ntop-down (or divisive ) hierarchical clustering. Section 17.7 looks at labeling\\nclusters automatically, a problem that must be solved whene ver humans in-\\nteract with the output of clustering. We discuss implementa tion issues in\\nSection 17.8. Section 17.9 provides pointers to further reading, including ref-\\nerences to soft hierarchical clustering, which we do not cov er in this book.\\nThere are few differences between the applications of ﬂat an d hierarchi-\\ncal clustering in information retrieval. In particular, hi erarchical clustering\\nis appropriate for any of the applications shown in Table 16.1 (page 351; see\\nalso Section 16.6, page 372). In fact, the example we gave for collection clus-\\ntering is hierarchical. In general, we select ﬂat clusterin g when efﬁciency\\nis important and hierarchical clustering when one of the pot ential problems\\n1. In this chapter, we only consider hierarchies that are bin ary trees like the one shown in Fig-\\nure17.1 – but hierarchical clustering can be easily extended to othe r types of trees.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 413}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP378 17 Hierarchical clustering\\nof ﬂat clustering (not enough structure, predetermined num ber of clusters,\\nnon-determinism) is a concern. In addition, many researche rs believe that hi-\\nerarchical clustering produces better clusters than ﬂat cl ustering. However,\\nthere is no consensus on this issue (see references in Sectio n17.9).\\n17.1 Hierarchical agglomerative clustering\\nHierarchical clustering algorithms are either top-down or bottom-up. Bottom-\\nup algorithms treat each document as a singleton cluster at t he outset and\\nthen successively merge (or agglomerate ) pairs of clusters until all clusters\\nhave been merged into a single cluster that contains all docu ments. Bottom-\\nup hierarchical clustering is therefore called hierarchical agglomerative cluster- HIERARCHICAL\\nAGGLOMERATIVE\\nCLUSTERINGingorHAC . Top-down clustering requires a method for splitting a clus ter.\\nHACIt proceeds by splitting clusters recursively until indivi dual documents are\\nreached. See Section 17.6. HAC is more frequently used in IR than top-down\\nclustering and is the main subject of this chapter.\\nBefore looking at speciﬁc similarity measures used in HAC in Sections\\n17.2–17.4, we ﬁrst introduce a method for depicting hierarchical clus terings\\ngraphically, discuss a few key properties of HACs and presen t a simple algo-\\nrithm for computing an HAC.\\nAn HAC clustering is typically visualized as a dendrogram as shown in DENDROGRAM\\nFigure 17.1. Each merge is represented by a horizontal line. The y-coord inate\\nof the horizontal line is the similarity of the two clusters t hat were merged,\\nwhere documents are viewed as singleton clusters. We call th is similarity the\\ncombination similarity of the merged cluster. For example, the combination COMBINATION\\nSIMILARITY similarity of the cluster consisting of Lloyd’s CEO questioned and Lloyd’s chief\\n/ U.S. grilling in Figure 17.1 is≈0.56. We deﬁne the combination similarity\\nof a singleton cluster as its document’s self-similarity (w hich is 1.0 for cosine\\nsimilarity).\\nBy moving up from the bottom layer to the top node, a dendrogra m al-\\nlows us to reconstruct the history of merges that resulted in the depicted\\nclustering. For example, we see that the two documents entit ledWar hero\\nColin Powell were merged ﬁrst in Figure 17.1 and that the last merge added\\nAg trade reform to a cluster consisting of the other 29 documents.\\nA fundamental assumption in HAC is that the merge operation i smono- MONOTONICITY\\ntonic . Monotonic means that if s1,s2, . . . , sK−1are the combination similarities\\nof the successive merges of an HAC, then s1≥s2≥. . .≥sK−1holds. A non-\\nmonotonic hierarchical clustering contains at least one inversion s i<si+1INVERSION\\nand contradicts the fundamental assumption that we chose th e best merge\\navailable at each step. We will see an example of an inversion in Figure 17.12 .\\nHierarchical clustering does not require a prespeciﬁed num ber of clusters.\\nHowever, in some applications we want a partition of disjoin t clusters just as', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 414}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP17.1 Hierarchical agglomerative clustering 3791.0 0.8 0.6 0.4 0.2 0.0\\nAg trade reform.\\nBack−to−school spending is up\\nLloyd’s CEO questioned\\nLloyd’s chief / U.S. grilling\\nViag stays positive\\nChrysler / Latin America\\nOhio Blue Cross\\nJapanese prime minister / Mexico\\nCompuServe reports loss\\nSprint / Internet access service\\nPlanet Hollywood\\nTrocadero: tripling of revenues\\nGerman unions split\\nWar hero Colin Powell\\nWar hero Colin Powell\\nOil prices slip\\nChains may raise prices\\nClinton signs law\\nLawsuit against tobacco companies\\nsuits against tobacco firms\\nIndiana tobacco lawsuit\\nMost active stocks\\nMexican markets\\nHog prices tumble\\nNYSE closing averages\\nBritish FTSE index\\nFed holds interest rates steady\\nFed to keep interest rates steady\\nFed keeps interest rates steady\\nFed keeps interest rates steady\\n◮Figure 17.1 A dendrogram of a single-link clustering of 30 documents fro m\\nReuters-RCV1. Two possible cuts of the dendrogram are shown : at 0.4 into 24 clusters\\nand at 0.1 into 12 clusters.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 415}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP380 17 Hierarchical clustering\\nin ﬂat clustering. In those cases, the hierarchy needs to be c ut at some point.\\nA number of criteria can be used to determine the cutting poin t:\\n•Cut at a prespeciﬁed level of similarity. For example, we cut the dendro-\\ngram at 0.4 if we want clusters with a minimum combination sim ilarity\\nof 0.4. In Figure 17.1, cutting the diagram at y=0.4 yields 24 clusters\\n(grouping only documents with high similarity together) an d cutting it at\\ny=0.1 yields 12 clusters (one large ﬁnancial news cluster and 1 1 smaller\\nclusters).\\n•Cut the dendrogram where the gap between two successive comb ination\\nsimilarities is largest. Such large gaps arguably indicate “natural” clus-\\nterings. Adding one more cluster decreases the quality of th e clustering\\nsigniﬁcantly, so cutting before this steep decrease occurs is desirable. This\\nstrategy is analogous to looking for the knee in the K-means graph in Fig-\\nure16.8 (page 366).\\n•Apply Equation ( 16.11 ) (page 366):\\nK=arg min\\nK′[RSS(K′) +λK′]\\nwhere K′refers to the cut of the hierarchy that results in K′clusters, RSS is\\nthe residual sum of squares and λis a penalty for each additional cluster.\\nInstead of RSS, another measure of distortion can be used.\\n•As in ﬂat clustering, we can also prespecify the number of clu sters Kand\\nselect the cutting point that produces Kclusters.\\nA simple, naive HAC algorithm is shown in Figure 17.2. We ﬁrst compute\\ntheN×Nsimilarity matrix C. The algorithm then executes N−1 steps\\nof merging the currently most similar clusters. In each iter ation, the two\\nmost similar clusters are merged and the rows and columns of t he merged\\ncluster iinCare updated.2The clustering is stored as a list of merges in\\nA.Iindicates which clusters are still available to be merged. T he function\\nSIM(i,m,j)computes the similarity of cluster jwith the merge of clusters i\\nand m. For some HAC algorithms, SIM(i,m,j)is simply a function of C[j][i]\\nand C[j][m], for example, the maximum of these two values for single-lin k.\\nWe will now reﬁne this algorithm for the different similarit y measures\\nof single-link and complete-link clustering (Section 17.2) and group-average\\nand centroid clustering (Sections 17.3 and 17.4). The merge criteria of these\\nfour variants of HAC are shown in Figure 17.3.\\n2. We assume that we use a deterministic method for breaking t ies, such as always choose the\\nmerge that is the ﬁrst cluster with respect to a total orderin g of the subsets of the document set\\nD.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 416}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP17.1 Hierarchical agglomerative clustering 381\\nSIMPLE HAC(d1, . . . , dN)\\n1forn←1toN\\n2do for i←1toN\\n3 doC[n][i]←SIM(dn,di)\\n4 I[n]←1(keeps track of active clusters)\\n5A←[](assembles clustering as a sequence of merges)\\n6fork←1toN−1\\n7do⟨i,m⟩← arg max{⟨i,m⟩:i̸=m∧I[i]=1∧I[m]=1}C[i][m]\\n8 A.APPEND (⟨i,m⟩)(store merge)\\n9 forj←1toN\\n10 doC[i][j]←SIM(i,m,j)\\n11 C[j][i]←SIM(i,m,j)\\n12 I[m]←0(deactivate cluster)\\n13 return A\\n◮Figure 17.2 A simple, but inefﬁcient HAC algorithm.\\n/Bullet\\n/Bullet\\n/Bullet/Bullet\\n(a) single-link: maximum similarity/Bullet\\n/Bullet\\n/Bullet/Bullet\\n(b) complete-link: minimum similarity\\n/Bullet\\n/Bullet\\n/Bullet/Bullet\\n(c) centroid: average inter-similarity/Bullet\\n/Bullet\\n/Bullet/Bullet\\n(d) group-average: average of all similarities\\n◮Figure 17.3 The different notions of cluster similarity used by the four HAC al-\\ngorithms. An inter-similarity is a similarity between two documents from different\\nclusters.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 417}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP382 17 Hierarchical clustering\\n0 1 2 3 40123\\n×d5\\n×d6\\n×d7\\n×d8×d1\\n×d2\\n×d3\\n×d4\\n0 1 2 3 40123\\n×d5\\n×d6\\n×d7\\n×d8×d1\\n×d2\\n×d3\\n×d4\\n◮Figure 17.4 A single-link (left) and complete-link (right) clustering of eight doc-\\numents. The ellipses correspond to successive clustering s tages. Left: The single-link\\nsimilarity of the two upper two-point clusters is the simila rity of d2and d3(solid\\nline), which is greater than the single-link similarity of t he two left two-point clusters\\n(dashed line). Right: The complete-link similarity of the t wo upper two-point clusters\\nis the similarity of d1and d4(dashed line), which is smaller than the complete-link\\nsimilarity of the two left two-point clusters (solid line).\\n17.2 Single-link and complete-link clustering\\nInsingle-link clustering orsingle-linkage clustering , the similarity of two clus- SINGLE -LINK\\nCLUSTERING ters is the similarity of their most similar members (see Figure 17.3, (a))3. This\\nsingle-link merge criterion is local. We pay attention solely to the area where\\nthe two clusters come closest to each other. Other, more dist ant parts of the\\ncluster and the clusters’ overall structure are not taken in to account.\\nIncomplete-link clustering orcomplete-linkage clustering , the similarity of two COMPLETE -LINK\\nCLUSTERING clusters is the similarity of their most dissimilar members (see Figure 17.3, (b)).\\nThis is equivalent to choosing the cluster pair whose merge h as the smallest\\ndiameter. This complete-link merge criterion is non-local ; the entire structure\\nof the clustering can inﬂuence merge decisions. This result s in a preference\\nfor compact clusters with small diameters over long, stragg ly clusters, but\\nalso causes sensitivity to outliers. A single document far f rom the center can\\nincrease diameters of candidate merge clusters dramatical ly and completely\\nchange the ﬁnal clustering.\\nFigure 17.4 depicts a single-link and a complete-link clustering of eig ht\\ndocuments. The ﬁrst four steps, each producing a cluster con sisting of a pair\\nof two documents, are identical. Then single-link clusteri ng joins the up-\\nper two pairs (and after that the lower two pairs) because on t he maximum-\\nsimilarity deﬁnition of cluster similarity, those two clus ters are closest. Complete-\\n3. Throughout this chapter, we equate similarity with proxi mity in 2D depictions of clustering.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 418}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP17.2 Single-link and complete-link clustering 3831.0 0.8 0.6 0.4 0.2 0.0\\nNYSE closing averages\\nHog prices tumble\\nOil prices slip\\nAg trade reform.\\nChrysler / Latin America\\nJapanese prime minister / Mexico\\nFed holds interest rates steady\\nFed to keep interest rates steady\\nFed keeps interest rates steady\\nFed keeps interest rates steady\\nMexican markets\\nBritish FTSE index\\nWar hero Colin Powell\\nWar hero Colin Powell\\nLloyd’s CEO questioned\\nLloyd’s chief / U.S. grilling\\nOhio Blue Cross\\nLawsuit against tobacco companies\\nsuits against tobacco firms\\nIndiana tobacco lawsuit\\nViag stays positive\\nMost active stocks\\nCompuServe reports loss\\nSprint / Internet access service\\nPlanet Hollywood\\nTrocadero: tripling of revenues\\nBack−to−school spending is up\\nGerman unions split\\nChains may raise prices\\nClinton signs law\\n◮Figure 17.5 A dendrogram of a complete-link clustering. The same 30 docu ments\\nwere clustered with single-link clustering in Figure 17.1.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 419}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP384 17 Hierarchical clustering\\n× × × × × ×× × × × × ×\\n◮Figure 17.6 Chaining in single-link clustering. The local criterion in single-link\\nclustering can cause undesirable elongated clusters.\\nlink clustering joins the left two pairs (and then the right t wo pairs) because\\nthose are the closest pairs according to the minimum-simila rity deﬁnition of\\ncluster similarity.4\\nFigure 17.1 is an example of a single-link clustering of a set of document s\\nand Figure 17.5 is the complete-link clustering of the same set. When cuttin g\\nthe last merge in Figure 17.5, we obtain two clusters of similar size (doc-\\numents 1–16, from NYSE closing averages toLloyd’s chief / U.S. grilling , and\\ndocuments 17–30, from Ohio Blue Cross toClinton signs law ). There is no cut\\nof the dendrogram in Figure 17.1 that would give us an equally balanced\\nclustering.\\nBoth single-link and complete-link clustering have graph- theoretic inter-\\npretations. Deﬁne skto be the combination similarity of the two clusters\\nmerged in step k, and G(sk)the graph that links all data points with a similar-\\nity of at least sk. Then the clusters after step kin single-link clustering are the\\nconnected components of G(sk)and the clusters after step kin complete-link\\nclustering are maximal cliques of G(sk). Aconnected component is a maximal CONNECTED\\nCOMPONENT set of connected points such that there is a path connecting e ach pair. A clique\\nCLIQUEis a set of points that are completely linked with each other.\\nThese graph-theoretic interpretations motivate the terms single-link and\\ncomplete-link clustering. Single-link clusters at step kare maximal sets of\\npoints that are linked via at least one link (a single link) of similarity s≥sk;\\ncomplete-link clusters at step kare maximal sets of points that are completely\\nlinked with each other via links of similarity s≥sk.\\nSingle-link and complete-link clustering reduce the asses sment of cluster\\nquality to a single similarity between a pair of documents: t he two most sim-\\nilar documents in single-link clustering and the two most di ssimilar docu-\\nments in complete-link clustering. A measurement based on o ne pair cannot\\nfully reﬂect the distribution of documents in a cluster. It i s therefore not sur-\\nprising that both algorithms often produce undesirable clu sters. Single-link\\nclustering can produce straggling clusters as shown in Figu re17.6. Since the\\nmerge criterion is strictly local, a chain of points can be ex tended for long\\n4. If you are bothered by the possibility of ties, assume that d1has coordinates (1+ǫ, 3−ǫ)and\\nthat all other points have integer coordinates.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 420}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP17.2 Single-link and complete-link clustering 385\\n0 1 2 3 4 5 6 701×d1\\n×d2\\n×d3\\n×d4\\n×d5\\n◮Figure 17.7 Outliers in complete-link clustering. The ﬁve documents ha ve\\nthe x-coordinates 1 +2ǫ, 4, 5+2ǫ, 6 and 7−ǫ. Complete-link clustering cre-\\nates the two clusters shown as ellipses. The most intuitive t wo-cluster cluster-\\ning is{{d1},{d2,d3,d4,d5}}, but in complete-link clustering, the outlier d1splits\\n{d2,d3,d4,d5}as shown.\\ndistances without regard to the overall shape of the emergin g cluster. This\\neffect is called chaining . CHAINING\\nThe chaining effect is also apparent in Figure 17.1. The last eleven merges\\nof the single-link clustering (those above the 0.1 line) add on single docu-\\nments or pairs of documents, corresponding to a chain. The co mplete-link\\nclustering in Figure 17.5 avoids this problem. Documents are split into two\\ngroups of roughly equal size when we cut the dendrogram at the last merge.\\nIn general, this is a more useful organization of the data tha n a clustering\\nwith chains.\\nHowever, complete-link clustering suffers from a differen t problem. It\\npays too much attention to outliers, points that do not ﬁt wel l into the global\\nstructure of the cluster. In the example in Figure 17.7 the four documents\\nd2,d3,d4,d5are split because of the outlier d1at the left edge (Exercise 17.1).\\nComplete-link clustering does not ﬁnd the most intuitive cl uster structure in\\nthis example.\\n17.2.1 Time complexity of HAC\\nThe complexity of the naive HAC algorithm in Figure 17.2 isΘ(N3)because\\nwe exhaustively scan the N×Nmatrix Cfor the largest similarity in each of\\nN−1 iterations.\\nFor the four HAC methods discussed in this chapter a more efﬁc ient algo-\\nrithm is the priority-queue algorithm shown in Figure 17.8. Its time complex-\\nity is Θ(N2logN). The rows C[k]of the N×Nsimilarity matrix Care sorted\\nin decreasing order of similarity in the priority queues P.P[k].MAX () then\\nreturns the cluster in P[k]that currently has the highest similarity with ωk,\\nwhere we use ωkto denote the kthcluster as in Chapter 16. After creating the\\nmerged cluster of ωk1and ωk2,ωk1is used as its representative. The function\\nSIMcomputes the similarity function for potential merge pairs : largest simi-\\nlarity for single-link, smallest similarity for complete- link, average similarity\\nfor GAAC (Section 17.3), and centroid similarity for centroid clustering (Sec-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 421}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP386 17 Hierarchical clustering\\nEFFICIENT HAC(⃗d1, . . . ,⃗dN)\\n1forn←1toN\\n2do for i←1toN\\n3 doC[n][i].sim←⃗dn·⃗di\\n4 C[n][i].index←i\\n5 I[n]←1\\n6 P[n]←priority queue for C[n]sorted on sim\\n7 P[n].DELETE (C[n][n])(don’t want self-similarities)\\n8A←[]\\n9fork←1toN−1\\n10 dok1←arg max{k:I[k]=1}P[k].MAX().sim\\n11 k2←P[k1].MAX().index\\n12 A.APPEND (⟨k1,k2⟩)\\n13 I[k2]←0\\n14 P[k1]←[]\\n15 for each iwith I[i] =1∧i̸=k1\\n16 doP[i].DELETE (C[i][k1])\\n17 P[i].DELETE (C[i][k2])\\n18 C[i][k1].sim←SIM(i,k1,k2)\\n19 P[i].INSERT (C[i][k1])\\n20 C[k1][i].sim←SIM(i,k1,k2)\\n21 P[k1].INSERT (C[k1][i])\\n22 return A\\nclustering algorithm SIM(i,k1,k2)\\nsingle-link max(SIM(i,k1),SIM(i,k2))\\ncomplete-link min(SIM(i,k1),SIM(i,k2))\\ncentroid (1\\nNm⃗vm)·(1\\nNi⃗vi)\\ngroup-average1\\n(Nm+Ni)(Nm+Ni−1)[(⃗vm+⃗vi)2−(Nm+Ni)]\\ncompute C[5]1 2 3 4 5\\n0.2 0.8 0.6 0.4 1.0\\ncreate P[5](by sorting)2 3 4 1\\n0.8 0.6 0.4 0.2\\nmerge 2 and 3, update\\nsimilarity of 2, delete 32 4 1\\n0.3 0.4 0.2\\ndelete and reinsert 24 2 1\\n0.4 0.3 0.2\\n◮Figure 17.8 The priority-queue algorithm for HAC. Top: The algorithm. C enter:\\nFour different similarity measures. Bottom: An example for processing steps 6 and\\n16–19. This is a made up example showing P[5]for a 5×5 matrix C.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 422}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP17.2 Single-link and complete-link clustering 387\\nSINGLE LINKCLUSTERING (d1, . . . , dN)\\n1forn←1toN\\n2do for i←1toN\\n3 doC[n][i].sim←SIM(dn,di)\\n4 C[n][i].index←i\\n5 I[n]←n\\n6 NBM[n]←arg maxX∈{C[n][i]:n̸=i}X.sim\\n7A←[]\\n8forn←1toN−1\\n9doi1←arg max{i:I[i]=i}NBM[i].sim\\n10 i2←I[NBM[i1].index ]\\n11 A.APPEND (⟨i1,i2⟩)\\n12 fori←1toN\\n13 do if I[i] =i∧i̸=i1∧i̸=i2\\n14 then C[i1][i].sim←C[i][i1].sim←max(C[i1][i].sim, C[i2][i].sim)\\n15 ifI[i] =i2\\n16 then I[i]←i1\\n17 NBM[i1]←arg maxX∈{C[i1][i]:I[i]=i∧i̸=i1}X.sim\\n18 return A\\n◮Figure 17.9 Single-link clustering algorithm using an NBM array. After merging\\ntwo clusters i1and i2, the ﬁrst one ( i1) represents the merged cluster. If I[i] =i, then i\\nis the representative of its current cluster. If I[i]̸=i, then ihas been merged into the\\ncluster represented by I[i]and will therefore be ignored when updating NBM[i1].\\ntion 17.4). We give an example of how a row of Cis processed (Figure 17.8,\\nbottom panel). The loop in lines 1–7 is Θ(N2)and the loop in lines 9–21 is\\nΘ(N2logN)for an implementation of priority queues that supports dele tion\\nand insertion in Θ(logN). The overall complexity of the algorithm is there-\\nfore Θ(N2logN). In the deﬁnition of the function SIM,⃗vmand⃗viare the\\nvector sums of ωk1∪ωk2and ωi, respectively, and Nmand Niare the number\\nof documents in ωk1∪ωk2and ωi, respectively.\\nThe argument of E FFICIENT HAC in Figure 17.8 is a set of vectors (as op-\\nposed to a set of generic documents) because GAAC and centroi d clustering\\n(Sections 17.3 and 17.4) require vectors as input. The complete-link version\\nof E FFICIENT HAC can also be applied to documents that are not represented\\nas vectors.\\nFor single-link, we can introduce a next-best-merge array ( NBM) as a fur-\\nther optimization as shown in Figure 17.9. NBM keeps track of what the best\\nmerge is for each cluster. Each of the two top level for-loops in Figure 17.9\\nareΘ(N2), thus the overall complexity of single-link clustering is Θ(N2).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 423}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP388 17 Hierarchical clustering\\n0 1 2 3 4 5 6 7 8 9 1001×d1\\n×d2\\n×d3\\n×d4\\n◮Figure 17.10 Complete-link clustering is not best-merge persistent. At ﬁrst, d2is\\nthe best-merge cluster for d3. But after merging d1and d2,d4becomes d3’s best-merge\\ncandidate. In a best-merge persistent algorithm like singl e-link, d3’s best-merge clus-\\nter would be{d1,d2}.\\nCan we also speed up the other three HAC algorithms with an NBM ar-\\nray? We cannot because only single-link clustering is best-merge persistent . BEST -MERGE\\nPERSISTENCE Suppose that the best merge cluster for ωkisωjin single-link clustering.\\nThen after merging ωjwith a third cluster ωi̸=ωk, the merge of ωiand ωj\\nwill be ωk’s best merge cluster (Exercise 17.6). In other words, the best-merge\\ncandidate for the merged cluster is one of the two best-merge candidates of\\nits components in single-link clustering. This means that Ccan be updated\\ninΘ(N)in each iteration – by taking a simple max of two values on line 14\\nin Figure 17.9 for each of the remaining ≤Nclusters.\\nFigure 17.10 demonstrates that best-merge persistence does not hold for\\ncomplete-link clustering, which means that we cannot use an NBM array to\\nspeed up clustering. After merging d3’s best merge candidate d2with cluster\\nd1, an unrelated cluster d4becomes the best merge candidate for d3. This is\\nbecause the complete-link merge criterion is non-local and can be affected by\\npoints at a great distance from the area where two merge candi dates meet.\\nIn practice, the efﬁciency penalty of the Θ(N2logN)algorithm is small\\ncompared with the Θ(N2)single-link algorithm since computing the similar-\\nity between two documents (e.g., as a dot product) is an order of magnitude\\nslower than comparing two scalars in sorting. All four HAC al gorithms in\\nthis chapter are Θ(N2)with respect to similarity computations. So the differ-\\nence in complexity is rarely a concern in practice when choos ing one of the\\nalgorithms.\\n?Exercise 17.1\\nShow that complete-link clustering creates the two-cluste r clustering depicted in Fig-\\nure17.7.\\n17.3 Group-average agglomerative clustering\\nGroup-average agglomerative clustering orGAAC (see Figure 17.3, (d)) evaluates GROUP -AVERAGE\\nAGGLOMERATIVE\\nCLUSTERINGcluster quality based on allsimilarities between documents, thus avoiding\\nthe pitfalls of the single-link and complete-link criteria , which equate cluster', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 424}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP17.3 Group-average agglomerative clustering 389\\nsimilarity with the similarity of a single pair of documents . GAAC is also\\ncalled group-average clustering and average-link clustering . GAAC computes\\nthe average similarity SIM-GAof all pairs of documents, including pairs from\\nthe same cluster. But self-similarities are not included in the average:\\nSIM-GA(ωi,ωj) =1\\n(Ni+Nj)(Ni+Nj−1)∑\\ndm∈ωi∪ωj∑\\ndn∈ωi∪ωj,dn̸=dm⃗dm·⃗dn (17.1)\\nwhere ⃗dis the length-normalized vector of document d,·denotes the dot\\nproduct, and Niand Njare the number of documents in ωiand ωj, respec-\\ntively.\\nThe motivation for GAAC is that our goal in selecting two clus ters ωi\\nand ωjas the next merge in HAC is that the resulting merge cluster ωk=\\nωi∪ωjshould be coherent. To judge the coherence of ωk, we need to look\\nat all document-document similarities within ωk, including those that occur\\nwithin ωiand those that occur within ωj.\\nWe can compute the measure SIM-GAefﬁciently because the sum of indi-\\nvidual vector similarities is equal to the similarities of t heir sums:\\n∑\\ndm∈ωi∑\\ndn∈ωj(⃗dm·⃗dn) = ( ∑\\ndm∈ωi⃗dm)·(∑\\ndn∈ωj⃗dn) (17.2)\\nWith ( 17.2), we have:\\nSIM-GA(ωi,ωj) =1\\n(Ni+Nj)(Ni+Nj−1)[(∑\\ndm∈ωi∪ωj⃗dm)2−(Ni+Nj)] (17.3)\\nThe term (Ni+Nj)on the right is the sum of Ni+Njself-similarities of value\\n1.0. With this trick we can compute cluster similarity in con stant time (as-\\nsuming we have available the two vector sums ∑dm∈ωi⃗dmand ∑dm∈ωj⃗dm)\\ninstead of in Θ(NiNj). This is important because we need to be able to com-\\npute the function SIM on lines 18 and 20 in E FFICIENT HAC (Figure 17.8)\\nin constant time for efﬁcient implementations of GAAC. Note that for two\\nsingleton clusters, Equation ( 17.3) is equivalent to the dot product.\\nEquation ( 17.2) relies on the distributivity of the dot product with respec t\\nto vector addition. Since this is crucial for the efﬁcient co mputation of a\\nGAAC clustering, the method cannot be easily applied to repr esentations of\\ndocuments that are not real-valued vectors. Also, Equation (17.2) only holds\\nfor the dot product. While many algorithms introduced in thi s book have\\nnear-equivalent descriptions in terms of dot product, cosi ne similarity and\\nEuclidean distance (cf. Section 14.1, page 291), Equation ( 17.2) can only be\\nexpressed using the dot product. This is a fundamental diffe rence between\\nsingle-link/complete-link clustering and GAAC. The ﬁrst t wo only require a', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 425}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP390 17 Hierarchical clustering\\nsquare matrix of similarities as input and do not care how the se similarities\\nwere computed.\\nTo summarize, GAAC requires (i) documents represented as ve ctors, (ii)\\nlength normalization of vectors, so that self-similaritie s are 1.0, and (iii) the\\ndot product as the measure of similarity between vectors and sums of vec-\\ntors.\\nThe merge algorithms for GAAC and complete-link clustering are the same\\nexcept that we use Equation ( 17.3) as similarity function in Figure 17.8. There-\\nfore, the overall time complexity of GAAC is the same as for co mplete-link\\nclustering: Θ(N2logN). Like complete-link clustering, GAAC is not best-\\nmerge persistent (Exercise 17.6). This means that there is no Θ(N2)algorithm\\nfor GAAC that would be analogous to the Θ(N2)algorithm for single-link in\\nFigure 17.9.\\nWe can also deﬁne group-average similarity as including sel f-similarities:\\nSIM-GA′(ωi,ωj) =1\\n(Ni+Nj)2(∑\\ndm∈ωi∪ωj⃗dm)2=1\\nNi+Nj∑\\ndm∈ωi∪ωj[⃗dm·⃗µ(ωi∪ωj)] (17.4)\\nwhere the centroid ⃗µ(ω)is deﬁned as in Equation ( 14.1) (page 292). This\\ndeﬁnition is equivalent to the intuitive deﬁnition of clust er quality as average\\nsimilarity of documents ⃗dmto the cluster’s centroid ⃗µ.\\nSelf-similarities are always equal to 1.0, the maximum poss ible value for\\nlength-normalized vectors. The proportion of self-simila rities in Equation ( 17.4)\\nisi/i2=1/ifor a cluster of size i. This gives an unfair advantage to small\\nclusters since they will have proportionally more self-sim ilarities. For two\\ndocuments d1,d2with a similarity s, we have SIM-GA′(d1,d2) = ( 1+s)/2.\\nIn contrast, SIM-GA(d1,d2) = s≤(1+s)/2. This similarity SIM-GA(d1,d2)\\nof two documents is the same as in single-link, complete-lin k and centroid\\nclustering. We prefer the deﬁnition in Equation ( 17.3), which excludes self-\\nsimilarities from the average, because we do not want to pena lize large clus-\\nters for their smaller proportion of self-similarities and because we want a\\nconsistent similarity value sfor document pairs in all four HAC algorithms.\\n?Exercise 17.2\\nApply group-average clustering to the points in Figures 17.6 and 17.7. Map them onto\\nthe surface of the unit sphere in a three-dimensional space t o get length-normalized\\nvectors. Is the group-average clustering different from th e single-link and complete-\\nlink clusterings?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 426}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP17.4 Centroid clustering 391\\n0 1 2 3 4 5 6 7012345×d1\\n×d2×d3\\n×d4\\n×d5×d6/Bullet/Circle\\nµ1/Bullet/Circleµ3/Bullet/Circleµ2\\n◮Figure 17.11 Three iterations of centroid clustering. Each iteration me rges the\\ntwo clusters whose centroids are closest.\\n17.4 Centroid clustering\\nIn centroid clustering, the similarity of two clusters is de ﬁned as the similar-\\nity of their centroids:\\nSIM-CENT(ωi,ωj) = ⃗µ(ωi)·⃗µ(ωj) (17.5)\\n= (1\\nNi∑\\ndm∈ωi⃗dm)·(1\\nNj∑\\ndn∈ωj⃗dn)\\n=1\\nNiNj∑\\ndm∈ωi∑\\ndn∈ωj⃗dm·⃗dn (17.6)\\nEquation ( 17.5) is centroid similarity. Equation ( 17.6) shows that centroid\\nsimilarity is equivalent to average similarity of all pairs of documents from\\ndifferent clusters. Thus, the difference between GAAC and centroid cl ustering\\nis that GAAC considers all pairs of documents in computing av erage pair-\\nwise similarity (Figure 17.3, (d)) whereas centroid clustering excludes pairs\\nfrom the same cluster (Figure 17.3, (c)).\\nFigure 17.11 shows the ﬁrst three steps of a centroid clustering. The ﬁrst\\ntwo iterations form the clusters {d5,d6}with centroid µ1and{d1,d2}with\\ncentroid µ2because the pairs ⟨d5,d6⟩and⟨d1,d2⟩have the highest centroid\\nsimilarities. In the third iteration, the highest centroid similarity is between\\nµ1and d4producing the cluster {d4,d5,d6}with centroid µ3.\\nLike GAAC, centroid clustering is not best-merge persisten t and therefore\\nΘ(N2logN)(Exercise 17.6).\\nIn contrast to the other three HAC algorithms, centroid clus tering is not\\nmonotonic. So-called inversions can occur: Similarity can increase during INVERSION', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 427}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP392 17 Hierarchical clustering\\n0 1 2 3 4 5012345\\n× ××\\n/Bullet/Circled1 d2d3\\n−4\\n−3\\n−2\\n−1\\n0\\nd1 d2 d3\\n◮Figure 17.12 Centroid clustering is not monotonic. The documents d1at(1+ǫ, 1),\\nd2at(5, 1), and d3at(3, 1+2√\\n3)are almost equidistant, with d1and d2closer to\\neach other than to d3. The non-monotonic inversion in the hierarchical clusteri ng\\nof the three points appears as an intersecting merge line in t he dendrogram. The\\nintersection is circled.\\nclustering as in the example in Figure 17.12 , where we deﬁne similarity as\\nnegative distance. In the ﬁrst merge, the similarity of d1and d2is−(4−ǫ). In\\nthe second merge, the similarity of the centroid of d1and d2(the circle) and d3\\nis≈− cos(π/6)×4=−√\\n3/2×4≈− 3.46>−(4−ǫ). This is an example\\nof an inversion: similarity increases in this sequence of two clustering steps.\\nIn a monotonic HAC algorithm, similarity is monotonically decreasing from\\niteration to iteration.\\nIncreasing similarity in a series of HAC clustering steps co ntradicts the\\nfundamental assumption that small clusters are more cohere nt than large\\nclusters. An inversion in a dendrogram shows up as a horizont al merge line\\nthat is lower than the previous merge line. All merge lines in Figures 17.1\\nand 17.5 are higher than their predecessors because single-link and complete-\\nlink clustering are monotonic clustering algorithms.\\nDespite its non-monotonicity, centroid clustering is ofte n used because its\\nsimilarity measure – the similarity of two centroids – is con ceptually simpler\\nthan the average of all pairwise similarities in GAAC. Figur e17.11 is all one\\nneeds to understand centroid clustering. There is no equall y simple graph\\nthat would explain how GAAC works.\\n?Exercise 17.3\\nFor a ﬁxed set of Ndocuments there are up to N2distinct similarities between clusters\\nin single-link and complete-link clustering. How many dist inct cluster similarities are\\nthere in GAAC and centroid clustering?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 428}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP17.5 Optimality of HAC 393\\n✄17.5 Optimality of HAC\\nTo state the optimality conditions of hierarchical cluster ing precisely, we ﬁrst\\ndeﬁne the combination similarity COMB -SIMof a clustering Ω={ω1, . . . , ωK}\\nas the smallest combination similarity of any of its Kclusters:\\nCOMB -SIM({ω1, . . . , ωK}) =min\\nkCOMB -SIM(ωk)\\nRecall that the combination similarity of a cluster ωthat was created as the\\nmerge of ω1and ω2is the similarity of ω1and ω2(page 378).\\nWe then deﬁne Ω={ω1, . . . , ωK}to be optimal if all clusterings Ω′with k OPTIMAL CLUSTERING\\nclusters, k≤K, have lower combination similarities:\\n|Ω′|≤| Ω|⇒ COMB -SIM(Ω′)≤COMB -SIM(Ω)\\nFigure 17.12 shows that centroid clustering is not optimal. The cluster-\\ning{{d1,d2},{d3}}(for K=2) has combination similarity −(4−ǫ)and\\n{{d1,d2,d3}}(for K=1) has combination similarity -3.46. So the cluster-\\ning{{d1,d2},{d3}}produced in the ﬁrst merge is not optimal since there is\\na clustering with fewer clusters ( {{d1,d2,d3}}) that has higher combination\\nsimilarity. Centroid clustering is not optimal because inv ersions can occur.\\nThe above deﬁnition of optimality would be of limited use if i t was only\\napplicable to a clustering together with its merge history. However, we can\\nshow (Exercise 17.4) that combination similarity for the three non-inversion COMBINATION\\nSIMILARITY algorithms can be read off from the cluster without knowing i ts history. These\\ndirect deﬁnitions of combination similarity are as follows .\\nsingle-link The combination similarity of a cluster ωis the smallest similar-\\nity of any bipartition of the cluster, where the similarity o f a bipartition is\\nthe largest similarity between any two documents from the tw o parts:\\nCOMB -SIM(ω) = min\\n{ω′:ω′⊂ω}max\\ndi∈ω′max\\ndj∈ω−ω′SIM(di,dj)\\nwhere each⟨ω′,ω−ω′⟩is a bipartition of ω.\\ncomplete-link The combination similarity of a cluster ωis the smallest sim-\\nilarity of any two points in ω: min di∈ωmin dj∈ωSIM(di,dj).\\nGAAC The combination similarity of a cluster ωis the average of all pair-\\nwise similarities in ω(where self-similarities are not included in the aver-\\nage): Equation ( 17.3).\\nIf we use these deﬁnitions of combination similarity, then o ptimality is a\\nproperty of a set of clusters and not of a process that produce s a set of clus-\\nters.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 429}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP394 17 Hierarchical clustering\\nWe can now prove the optimality of single-link clustering by induction\\nover the number of clusters K. We will give a proof for the case where no two\\npairs of documents have the same similarity, but it can easil y be extended to\\nthe case with ties.\\nThe inductive basis of the proof is that a clustering with K=Nclusters has\\ncombination similarity 1.0, which is the largest value poss ible. The induc-\\ntion hypothesis is that a single-link clustering ΩKwith Kclusters is optimal:\\nCOMB -SIM(ΩK)≥COMB -SIM(Ω′\\nK)for all Ω′\\nK. Assume for contradiction that\\nthe clustering ΩK−1we obtain by merging the two most similar clusters in\\nΩKis not optimal and that instead a different sequence of merge sΩ′\\nK,Ω′\\nK−1\\nleads to the optimal clustering with K−1 clusters. We can write the as-\\nsumption that Ω′\\nK−1is optimal and that ΩK−1is not as COMB -SIM(Ω′\\nK−1)>\\nCOMB -SIM(ΩK−1).\\nCase 1: The two documents linked by s=COMB -SIM(Ω′\\nK−1)are in the\\nsame cluster in ΩK. They can only be in the same cluster if a merge with sim-\\nilarity smaller than shas occurred in the merge sequence producing ΩK. This\\nimplies s>COMB -SIM(ΩK). Thus, COMB -SIM(Ω′\\nK−1) =s>COMB -SIM(ΩK)>\\nCOMB -SIM(Ω′\\nK)>COMB -SIM(Ω′\\nK−1). Contradiction.\\nCase 2: The two documents linked by s=COMB -SIM(Ω′\\nK−1)are not in\\nthe same cluster in ΩK. But s=COMB -SIM(Ω′\\nK−1)>COMB -SIM(ΩK−1), so\\nthe single-link merging rule should have merged these two cl usters when\\nprocessing ΩK. Contradiction.\\nThus, ΩK−1is optimal.\\nIn contrast to single-link clustering, complete-link clus tering and GAAC\\nare not optimal as this example shows:\\n× × × × 1 3 3\\nd1 d2 d3 d4\\nBoth algorithms merge the two points with distance 1 ( d2and d3) ﬁrst and\\nthus cannot ﬁnd the two-cluster clustering {{d1,d2},{d3,d4}}. But{{d1,d2},{d3,d4}}\\nis optimal on the optimality criteria of complete-link clus tering and GAAC.\\nHowever, the merge criteria of complete-link clustering an d GAAC ap-\\nproximate the desideratum of approximate sphericity bette r than the merge\\ncriterion of single-link clustering. In many applications , we want spheri-\\ncal clusters. Thus, even though single-link clustering may seem preferable at\\nﬁrst because of its optimality, it is optimal with respect to the wrong criterion\\nin many document clustering applications.\\nTable 17.1 summarizes the properties of the four HAC algorithms intro-\\nduced in this chapter. We recommend GAAC for document cluste ring be-\\ncause it is generally the method that produces the clusterin g with the best', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 430}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP17.6 Divisive clustering 395\\nmethod combination similarity time compl. optimal? comment\\nsingle-link max inter-similarity of any 2 docs Θ(N2) yes chaining effect\\ncomplete-link min inter-similarity of any 2 docs Θ(N2logN)no sensitive to outliers\\ngroup-average average of all sims Θ(N2logN)nobest choice for\\nmost applications\\ncentroid average inter-similarity Θ(N2logN)no inversions can occur\\n◮Table 17.1 Comparison of HAC algorithms.\\nproperties for applications. It does not suffer from chaini ng, from sensitivity\\nto outliers and from inversions.\\nThere are two exceptions to this recommendation. First, for non-vector\\nrepresentations, GAAC is not applicable and clustering sho uld typically be\\nperformed with the complete-link method.\\nSecond, in some applications the purpose of clustering is no t to create a\\ncomplete hierarchy or exhaustive partition of the entire do cument set. For\\ninstance, ﬁrst story detection ornovelty detection is the task of detecting the ﬁrst FIRST STORY\\nDETECTION occurrence of an event in a stream of news stories. One approa ch to this task\\nis to ﬁnd a tight cluster within the documents that were sent a cross the wire\\nin a short period of time and are dissimilar from all previous documents. For\\nexample, the documents sent over the wire in the minutes afte r the World\\nTrade Center attack on September 11, 2001 form such a cluster . Variations of\\nsingle-link clustering can do well on this task since it is th e structure of small\\nparts of the vector space – and not global structure – that is i mportant in this\\ncase.\\nSimilarly, we will describe an approach to duplicate detect ion on the web\\nin Section 19.6 (page 440) where single-link clustering is used in the guise of\\nthe union-ﬁnd algorithm. Again, the decision whether a grou p of documents\\nare duplicates of each other is not inﬂuenced by documents th at are located\\nfar away and single-link clustering is a good choice for dupl icate detection.\\n?Exercise 17.4\\nShow the equivalence of the two deﬁnitions of combination si milarity: the process\\ndeﬁnition on page 378and the static deﬁnition on page 393.\\n17.6 Divisive clustering\\nSo far we have only looked at agglomerative clustering, but a cluster hierar-\\nchy can also be generated top-down. This variant of hierarch ical clustering\\nis called top-down clustering ordivisive clustering . We start at the top with all TOP-DOWN\\nCLUSTERING documents in one cluster. The cluster is split using a ﬂat clu stering algo-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 431}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP396 17 Hierarchical clustering\\nrithm. This procedure is applied recursively until each doc ument is in its\\nown singleton cluster.\\nTop-down clustering is conceptually more complex than bott om-up clus-\\ntering since we need a second, ﬂat clustering algorithm as a “ subroutine”. It\\nhas the advantage of being more efﬁcient if we do not generate a complete\\nhierarchy all the way down to individual document leaves. Fo r a ﬁxed num-\\nber of top levels, using an efﬁcient ﬂat algorithm like K-means, top-down\\nalgorithms are linear in the number of documents and cluster s. So they run\\nmuch faster than HAC algorithms, which are at least quadrati c.\\nThere is evidence that divisive algorithms produce more acc urate hierar-\\nchies than bottom-up algorithms in some circumstances. See the references\\non bisecting K-means in Section 17.9. Bottom-up methods make cluster-\\ning decisions based on local patterns without initially tak ing into account\\nthe global distribution. These early decisions cannot be un done. Top-down\\nclustering beneﬁts from complete information about the glo bal distribution\\nwhen making top-level partitioning decisions.\\n17.7 Cluster labeling\\nIn many applications of ﬂat clustering and hierarchical clu stering, particu-\\nlarly in analysis tasks and in user interfaces (see applicat ions in Table 16.1,\\npage 351), human users interact with clusters. In such settings, we m ust label\\nclusters, so that users can see what a cluster is about.\\nDifferential cluster labeling selects cluster labels by comparing the distribu- DIFFERENTIAL CLUSTER\\nLABELING tion of terms in one cluster with that of other clusters. The f eature selection\\nmethods we introduced in Section 13.5 (page 271) can all be used for differen-\\ntial cluster labeling.5In particular, mutual information (MI) (Section 13.5.1 ,\\npage 272) or, equivalently, information gain and the χ2-test (Section 13.5.2 ,\\npage 275) will identify cluster labels that characterize one cluste r in contrast\\nto other clusters. A combination of a differential test with a penalty for rare\\nterms often gives the best labeling results because rare ter ms are not neces-\\nsarily representative of the cluster as a whole.\\nWe apply three labeling methods to a K-means clustering in Table 17.2. In\\nthis example, there is almost no difference between MI and χ2. We therefore\\nomit the latter.\\nCluster-internal labeling computes a label that solely depends on the cluster CLUSTER -INTERNAL\\nLABELING itself, not on other clusters. Labeling a cluster with the ti tle of the document\\nclosest to the centroid is one cluster-internal method. Tit les are easier to read\\nthan a list of terms. A full title can also contain important c ontext that didn’t\\nmake it into the top 10 terms selected by MI. On the web, anchor text can\\n5. Selecting the most frequent terms is a non-differential f eature selection technique we dis-\\ncussed in Section 13.5. It can also be used for labeling clusters.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 432}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP17.7 Cluster labeling 397\\nlabeling method\\n# docs centroid mutual information title\\n4 622oil plant mexico pro-\\nduction crude power\\n000 reﬁnery gas bpdplant oil production\\nbarrels crude bpd\\nmexico dolly capacity\\npetroleumMEXICO: Hurri-\\ncane Dolly heads for\\nMexico coast\\n9 1017police security russian\\npeople military peace\\nkilled told grozny\\ncourtpolice killed military\\nsecurity peace told\\ntroops forces rebels\\npeopleRUSSIA: Russia’s\\nLebed meets rebel\\nchief in Chechnya\\n10 125900 000 tonnes traders\\nfutures wheat prices\\ncents september\\ntonnedelivery traders\\nfutures tonne tonnes\\ndesk wheat prices 000\\n00USA: Export Business\\n- Grain/oilseeds com-\\nplex\\n◮Table 17.2 Automatically computed cluster labels. This is for three of ten clusters\\n(4, 9, and 10) in a K-means clustering of the ﬁrst 10,000 documents in Reuters-R CV1.\\nThe last three columns show cluster summaries computed by th ree labeling methods:\\nmost highly weighted terms in centroid (centroid), mutual i nformation, and the title\\nof the document closest to the centroid of the cluster (title ). Terms selected by only\\none of the ﬁrst two methods are in bold.\\nplay a role similar to a title since the anchor text pointing t o a page can serve\\nas a concise summary of its contents.\\nIn Table 17.2, the title for cluster 9 suggests that many of its documents a re\\nabout the Chechnya conﬂict, a fact the MI terms do not reveal. However, a\\nsingle document is unlikely to be representative of all docu ments in a cluster.\\nAn example is cluster 4, whose selected title is misleading. The main topic of\\nthe cluster is oil. Articles about hurricane Dolly only ende d up in this cluster\\nbecause of its effect on oil prices.\\nWe can also use a list of terms with high weights in the centroi d of the clus-\\nter as a label. Such highly weighted terms (or, even better, p hrases, especially\\nnoun phrases) are often more representative of the cluster t han a few titles\\ncan be, even if they are not ﬁltered for distinctiveness as in the differential\\nmethods. However, a list of phrases takes more time to digest for users than\\na well crafted title.\\nCluster-internal methods are efﬁcient, but they fail to dis tinguish terms\\nthat are frequent in the collection as a whole from those that are frequent only\\nin the cluster. Terms like year orTuesday may be among the most frequent in\\na cluster, but they are not helpful in understanding the cont ents of a cluster\\nwith a speciﬁc topic like oil.\\nIn Table 17.2, the centroid method selects a few more uninformative terms\\n(000,court ,cents ,september ) than MI ( forces ,desk ), but most of the terms se-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 433}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP398 17 Hierarchical clustering\\nlected by either method are good descriptors. We get a good se nse of the\\ndocuments in a cluster from scanning the selected terms.\\nFor hierarchical clustering, additional complications ar ise in cluster label-\\ning. Not only do we need to distinguish an internal node in the tree from\\nits siblings, but also from its parent and its children. Docu ments in child\\nnodes are by deﬁnition also members of their parent node, so w e cannot use\\na naive differential method to ﬁnd labels that distinguish t he parent from\\nits children. However, more complex criteria, based on a com bination of\\noverall collection frequency and prevalence in a given clus ter, can determine\\nwhether a term is a more informative label for a child node or a parent node\\n(see Section 17.9).\\n17.8 Implementation notes\\nMost problems that require the computation of a large number of dot prod-\\nucts beneﬁt from an inverted index. This is also the case for H AC clustering.\\nComputational savings due to the inverted index are large if there are many\\nzero similarities – either because many documents do not sha re any terms or\\nbecause an aggressive stop list is used.\\nIn low dimensions, more aggressive optimizations are possi ble that make\\nthe computation of most pairwise similarities unnecessary (Exercise 17.10 ).\\nHowever, no such algorithms are known in higher dimensions. We encoun-\\ntered the same problem in kNN classiﬁcation (see Section 14.7, page 314).\\nWhen using GAAC on a large document set in high dimensions, we have\\nto take care to avoid dense centroids. For dense centroids, c lustering can\\ntake time Θ(MN2logN)where Mis the size of the vocabulary, whereas\\ncomplete-link clustering is Θ(MaveN2logN)where Maveis the average size\\nof the vocabulary of a document. So for large vocabularies co mplete-link\\nclustering can be more efﬁcient than an unoptimized impleme ntation of GAAC.\\nWe discussed this problem in the context of K-means clustering in Chap-\\nter16(page 365) and suggested two solutions: truncating centroids (keepi ng\\nonly highly weighted terms) and representing clusters by me ans of sparse\\nmedoids instead of dense centroids. These optimizations ca n also be applied\\nto GAAC and centroid clustering.\\nEven with these optimizations, HAC algorithms are all Θ(N2)orΘ(N2logN)\\nand therefore infeasible for large sets of 1,000,000 or more documents. For\\nsuch large sets, HAC can only be used in combination with a ﬂat clustering\\nalgorithm like K-means. Recall that K-means requires a set of seeds as initial-\\nization (Figure 16.5, page 361). If these seeds are badly chosen, then the re-\\nsulting clustering will be of poor quality. We can employ an H AC algorithm\\nto compute seeds of high quality. If the HAC algorithm is appl ied to a docu-\\nment subset of size√\\nN, then the overall runtime of K-means cum HAC seed', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 434}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP17.9 References and further reading 399\\ngeneration is Θ(N). This is because the application of a quadratic algorithm\\nto a sample of size√\\nNhas an overall complexity of Θ(N). An appropriate\\nadjustment can be made for an Θ(N2logN)algorithm to guarantee linear-\\nity. This algorithm is referred to as the Buckshot algorithm . It combines the BUCKSHOT\\nALGORITHM determinism and higher reliability of HAC with the efﬁcienc y of K-means.\\n17.9 References and further reading\\nAn excellent general review of clustering is ( Jain et al. 1999 ). Early references\\nfor speciﬁc HAC algorithms are ( King 1967 ) (single-link), ( Sneath and Sokal\\n1973 ) (complete-link, GAAC) and ( Lance and Williams 1967 ) (discussing a\\nlarge variety of hierarchical clustering algorithms). The single-link algorithm\\nin Figure 17.9 is similar to Kruskal’s algorithm for constructing a minimum KRUSKAL ’S\\nALGORITHM spanning tree. A graph-theoretical proof of the correctnes s of Kruskal’s al-\\ngorithm (which is analogous to the proof in Section 17.5) is provided by Cor-\\nmen et al. (1990 , Theorem 23.1). See Exercise 17.5 for the connection between\\nminimum spanning trees and single-link clusterings.\\nIt is often claimed that hierarchical clustering algorithm s produce better\\nclusterings than ﬂat algorithms ( Jain and Dubes (1988 , p. 140), Cutting et al.\\n(1992 ),Larsen and Aone (1999 )) although more recently there have been ex-\\nperimental results suggesting the opposite ( Zhao and Karypis 2002 ). Even\\nwithout a consensus on average behavior, there is no doubt th at results of\\nEM and K-means are highly variable since they will often converge to a local\\noptimum of poor quality. The HAC algorithms we have presente d here are\\ndeterministic and thus more predictable.\\nThe complexity of complete-link, group-average and centro id clustering\\nis sometimes given as Θ(N2)(Day and Edelsbrunner 1984 ,Voorhees 1985b ,\\nMurtagh 1983 ) because a document similarity computation is an order of\\nmagnitude more expensive than a simple comparison, the main operation\\nexecuted in the merging steps after the N×Nsimilarity matrix has been\\ncomputed.\\nThe centroid algorithm described here is due to Voorhees (1985b ). Voorhees\\nrecommends complete-link and centroid clustering over sin gle-link for a re-\\ntrieval application. The Buckshot algorithm was originall y published by Cut-\\nting et al. (1993 ).Allan et al. (1998 ) apply single-link clustering to ﬁrst story\\ndetection.\\nAn important HAC technique not discussed here is Ward’s method (Ward WARD ’S METHOD\\nJr. 1963 ,El-Hamdouchi and Willett 1986 ), also called minimum variance clus-\\ntering . In each step, it selects the merge with the smallest RSS (Cha pter 16,\\npage 360). The merge criterion in Ward’s method (a function of all ind ividual\\ndistances from the centroid) is closely related to the merge criterion in GAAC\\n(a function of all individual similarities to the centroid) .', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 435}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP400 17 Hierarchical clustering\\nDespite its importance for making the results of clustering useful, compar-\\natively little work has been done on labeling clusters. Popescul and Ungar\\n(2000 ) obtain good results with a combination of χ2and collection frequency\\nof a term. Glover et al. (2002b ) use information gain for labeling clusters of\\nweb pages. Stein and zu Eissen ’s approach is ontology-based ( 2004 ). The\\nmore complex problem of labeling nodes in a hierarchy (which requires dis-\\ntinguishing more general labels for parents from more speci ﬁc labels for chil-\\ndren) is tackled by Glover et al. (2002a ) and Treeratpituk and Callan (2006 ).\\nSome clustering algorithms attempt to ﬁnd a set of labels ﬁrs t and then build\\n(often overlapping) clusters around the labels, thereby av oiding the problem\\nof labeling altogether ( Zamir and Etzioni 1999 ,Käki 2005 ,Osi´ nski and Weiss\\n2005 ). We know of no comprehensive study that compares the qualit y of\\nsuch “label-based” clustering to the clustering algorithm s discussed in this\\nchapter and in Chapter 16. In principle, work on multi-document summa-\\nrization ( McKeown and Radev 1995 ) is also applicable to cluster labeling, but\\nmulti-document summaries are usually longer than the short text fragments\\nneeded when labeling clusters (cf. Section 8.7, page 170). Presenting clusters\\nin a way that users can understand is a UI problem. We recommen d read-\\ning ( Baeza-Yates and Ribeiro-Neto 1999 , ch. 10) for an introduction to user\\ninterfaces in IR.\\nAn example of an efﬁcient divisive algorithm is bisecting K-means ( Stein-\\nbach et al. 2000 ).Spectral clustering algorithms ( Kannan et al. 2000 ,Dhillon SPECTRAL CLUSTERING\\n2001 ,Zha et al. 2001 ,Ng et al. 2001a ), including principal direction divisive\\npartitioning (PDDP) (whose bisecting decisions are based on SVD, see Chap -\\nter18) (Boley 1998 ,Savaresi and Boley 2004 ), are computationally more ex-\\npensive than bisecting K-means, but have the advantage of being determin-\\nistic.\\nUnlike K-means and EM, most hierarchical clustering algorithms do n ot\\nhave a probabilistic interpretation. Model-based hierarc hical clustering ( Vaithyanathan\\nand Dom 2000 ,Kamvar et al. 2002 ,Castro et al. 2004 ) is an exception.\\nThe evaluation methodology described in Section 16.3 (page 356) is also\\napplicable to hierarchical clustering. Specialized evalu ation measures for hi-\\nerarchies are discussed by Fowlkes and Mallows (1983 ),Larsen and Aone\\n(1999 ) and Sahoo et al. (2006 ).\\nThe R environment ( R Development Core Team 2005 ) offers good support\\nfor hierarchical clustering. The R function hclust implements single-link,\\ncomplete-link, group-average, and centroid clustering; a nd Ward’s method.\\nAnother option provided is median clustering which represents each cluster\\nby its medoid (cf. k-medoids in Chapter 16, page 365). Support for cluster-\\ning vectors in high-dimensional spaces is provided by the so ftware package\\nCLUTO ( http://glaros.dtc.umn.edu/gkhome/views/cluto ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 436}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP17.10 Exercises 401\\n17.10 Exercises\\n?Exercise 17.5\\nA single-link clustering can also be computed from the minimum spanning tree of a MINIMUM SPANNING\\nTREE graph. The minimum spanning tree connects the vertices of a g raph at the smallest\\npossible cost, where cost is deﬁned as the sum over all edges o f the graph. In our\\ncase the cost of an edge is the distance between two documents . Show that if ∆k−1>\\n∆k>. . .>∆1are the costs of the edges of a minimum spanning tree, then the se\\nedges correspond to the k−1 merges in constructing a single-link clustering.\\nExercise 17.6\\nShow that single-link clustering is best-merge persistent and that GAAC and centroid\\nclustering are not best-merge persistent.\\nExercise 17.7\\na.Consider running 2-means clustering on a collection with do cuments from two\\ndifferent languages. What result would you expect?\\nb.Would you expect the same result when running an HAC algorith m?\\nExercise 17.8\\nDownload Reuters-21578. Keep only documents that are in the classes crude ,inter-\\nest, and grain . Discard documents that are members of more than one of these three\\nclasses. Compute a (i) single-link, (ii) complete-link, (i ii) GAAC, (iv) centroid cluster-\\ning of the documents. (v) Cut each dendrogram at the second br anch from the top to\\nobtain K=3 clusters. Compute the Rand index for each of the 4 clusterin gs. Which\\nclustering method performs best?\\nExercise 17.9\\nSuppose a run of HAC ﬁnds the clustering with K=7 to have the highest value on\\nsome prechosen goodness measure of clustering. Have we foun d the highest-value\\nclustering among all clusterings with K=7?\\nExercise 17.10\\nConsider the task of producing a single-link clustering of Npoints on a line:\\n× × × × × × × × × ×\\nShow that we only need to compute a total of about Nsimilarities. What is the overall\\ncomplexity of single-link clustering for a set of points on a line?\\nExercise 17.11\\nProve that single-link, complete-link, and group-average clustering are monotonic in\\nthe sense deﬁned on page 378.\\nExercise 17.12\\nForNpoints, there are≤NKdifferent ﬂat clusterings into Kclusters (Section 16.2,\\npage 356). What is the number of different hierarchical clusterings (or dendrograms)\\nofNdocuments? Are there more ﬂat clusterings or more hierarchi cal clusterings for\\ngiven Kand N?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 437}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 438}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 403\\n18Matrix decompositions and latent\\nsemantic indexing\\nOn page 123we introduced the notion of a term-document matrix: anM×N\\nmatrix C, each of whose rows represents a term and each of whose column s\\nrepresents a document in the collection. Even for a collecti on of modest size,\\nthe term-document matrix Cis likely to have several tens of thousands of\\nrows and columns. In Section 18.1.1 we ﬁrst develop a class of operations\\nfrom linear algebra, known as matrix decomposition . In Section 18.2 we use a\\nspecial form of matrix decomposition to construct a low-rank approximation\\nto the term-document matrix. In Section 18.3 we examine the application\\nof such low-rank approximations to indexing and retrieving documents, a\\ntechnique referred to as latent semantic indexing . While latent semantic in-\\ndexing has not been established as a signiﬁcant force in scor ing and ranking\\nfor information retrieval, it remains an intriguing approa ch to clustering in a\\nnumber of domains including for collections of text documen ts (Section 16.6,\\npage 372). Understanding its full potential remains an area of activ e research.\\nReaders who do not require a refresher on linear algebra may s kip Sec-\\ntion 18.1, although Example 18.1 is especially recommended as it highlights\\na property of eigenvalues that we exploit later in the chapte r.\\n18.1 Linear algebra review\\nWe brieﬂy review some necessary background in linear algebr a. Let Cbe\\nanM×Nmatrix with real-valued entries; for a term-document matri x, all\\nentries are in fact non-negative. The rank of a matrix is the number of linearly RANK\\nindependent rows (or columns) in it; thus, r ank(C)≤min{M,N}. A square\\nr×rmatrix all of whose off-diagonal entries are zero is called a diagonal\\nmatrix ; its rank is equal to the number of non-zero diagonal entries . If all\\nrdiagonal entries of such a diagonal matrix are 1, it is called the identity\\nmatrix of dimension rand represented by Ir.\\nFor a square M×Mmatrix Cand a vector ⃗xthat is not all zeros, the values', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 439}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP404 18 Matrix decompositions and latent semantic indexing\\nofλsatisfying\\nC⃗x=λ⃗x (18.1)\\nare called the eigenvalues ofC. The N-vector ⃗xsatisfying Equation ( 18.1) EIGENVALUE\\nfor an eigenvalue λis the corresponding right eigenvector . The eigenvector\\ncorresponding to the eigenvalue of largest magnitude is cal led the principal\\neigenvector. In a similar fashion, the left eigenvectors ofCare the M-vectors y\\nsuch that\\n⃗yTC=λ⃗yT. (18.2)\\nThe number of non-zero eigenvalues of Cis at most rank (C).\\nThe eigenvalues of a matrix are found by solving the characteristic equation ,\\nwhich is obtained by rewriting Equation ( 18.1) in the form (C−λIM)⃗x=0.\\nThe eigenvalues of Care then the solutions of |(C−λIM)|=0, where|S|\\ndenotes the determinant of a square matrix S. The equation|(C−λIM)|=0\\nis an Mth order polynomial equation in λand can have at most Mroots,\\nwhich are the eigenvalues of C. These eigenvalues can in general be complex,\\neven if all entries of Care real.\\nWe now examine some further properties of eigenvalues and ei genvectors,\\nto set up the central idea of singular value decompositions i n Section 18.2 be-\\nlow. First, we look at the relationship between matrix-vect or multiplication\\nand eigenvalues.\\n✎Example 18.1: Consider the matrix\\nS=\\uf8eb\\n\\uf8ed30 0 0\\n0 20 0\\n0 0 1\\uf8f6\\n\\uf8f8.\\nClearly the matrix has rank 3, and has 3 non-zero eigenvalues λ1=30,λ2=20 and\\nλ3=1, with the three corresponding eigenvectors\\n⃗x1=\\uf8eb\\n\\uf8ed1\\n0\\n0\\uf8f6\\n\\uf8f8,⃗x2=\\uf8eb\\n\\uf8ed0\\n1\\n0\\uf8f6\\n\\uf8f8and⃗x3=\\uf8eb\\n\\uf8ed0\\n0\\n1\\uf8f6\\n\\uf8f8.\\nFor each of the eigenvectors, multiplication by Sacts as if we were multiplying the\\neigenvector by a multiple of the identity matrix; the multip le is different for each\\neigenvector. Now, consider an arbitrary vector, such as ⃗v=\\uf8eb\\n\\uf8ed2\\n4\\n6\\uf8f6\\n\\uf8f8. We can always\\nexpress ⃗vas a linear combination of the three eigenvectors of S; in the current example\\nwe have\\n⃗v=\\uf8eb\\n\\uf8ed2\\n4\\n6\\uf8f6\\n\\uf8f8=2⃗x1+4⃗x2+6⃗x3.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 440}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP18.1 Linear algebra review 405\\nSuppose we multiply ⃗vbyS:\\nS⃗v= S(2⃗x1+4⃗x2+6⃗x3)\\n= 2S⃗x1+4S⃗x2+6S⃗x3\\n= 2λ1⃗x1+4λ2⃗x2+6λ3⃗x3\\n= 60⃗x1+80⃗x2+6⃗x3. (18.3)\\nExample 18.1 shows that even though ⃗vis an arbitrary vector, the effect of\\nmultiplication by Sis determined by the eigenvalues and eigenvectors of S.\\nFurthermore, it is intuitively apparent from Equation ( 18.3) that the product\\nS⃗vis relatively unaffected by terms arising from the small eig envalues of S;\\nin our example, since λ3=1, the contribution of the third term on the right\\nhand side of Equation ( 18.3) is small. In fact, if we were to completely ignore\\nthe contribution in Equation ( 18.3) from the third eigenvector corresponding\\ntoλ3=1, then the product S⃗vwould be computed to be\\uf8eb\\n\\uf8ed60\\n80\\n0\\uf8f6\\n\\uf8f8rather than\\nthe correct product which is\\uf8eb\\n\\uf8ed60\\n80\\n6\\uf8f6\\n\\uf8f8; these two vectors are relatively close\\nto each other by any of various metrics one could apply (such a s the length\\nof their vector difference).\\nThis suggests that the effect of small eigenvalues (and thei r eigenvectors)\\non a matrix-vector product is small. We will carry forward th is intuition\\nwhen studying matrix decompositions and low-rank approxim ations in Sec-\\ntion 18.2. Before doing so, we examine the eigenvectors and eigenvalu es of\\nspecial forms of matrices that will be of particular interes t to us.\\nFor a symmetric matrix S, the eigenvectors corresponding to distinct eigen-\\nvalues are orthogonal . Further, if Sis both real and symmetric, the eigenvalues\\nare all real.\\n✎Example 18.2: Consider the real, symmetric matrix\\nS=(2 1\\n1 2)\\n. (18.4)\\nFrom the characteristic equation |S−λI|=0, we have the quadratic (2−λ)2−1=\\n0, whose solutions yield the eigenvalues 3 and 1. The corresp onding eigenvectors(1\\n−1)\\nand(1\\n1)\\nare orthogonal.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 441}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP406 18 Matrix decompositions and latent semantic indexing\\n18.1.1 Matrix decompositions\\nIn this section we examine ways in which a square matrix can be factored\\ninto the product of matrices derived from its eigenvectors; we refer to this\\nprocess as matrix decomposition . Matrix decompositions similar to the ones MATRIX\\nDECOMPOSITION in this section will form the basis of our principal text-ana lysis technique\\nin Section 18.3, where we will look at decompositions of non-square term-\\ndocument matrices. The square decompositions in this secti on are simpler\\nand can be treated with sufﬁcient mathematical rigor to help the reader un-\\nderstand how such decompositions work. The detailed mathem atical deriva-\\ntion of the more complex decompositions in Section 18.2 are beyond the\\nscope of this book.\\nWe begin by giving two theorems on the decomposition of a squa re ma-\\ntrix into the product of three matrices of a special form. The ﬁrst of these,\\nTheorem 18.1, gives the basic factorization of a square real-valued matr ix\\ninto three factors. The second, Theorem 18.2, applies to square symmetric\\nmatrices and is the basis of the singular value decompositio n described in\\nTheorem 18.3.\\nTheorem 18.1. (Matrix diagonalization theorem) Let S be a square real-valued\\nM×M matrix with M linearly independent eigenvectors. Then the re exists an\\neigen decomposition EIGEN DECOMPOSITION\\nS=UΛU−1, (18.5)\\nwhere the columns of U are the eigenvectors of S and Λis a diagonal matrix whose\\ndiagonal entries are the eigenvalues of S in decreasing orde r\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8edλ1\\nλ2\\n···\\nλM\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8,λi≥λi+1. (18.6)\\nIf the eigenvalues are distinct, then this decomposition is unique.\\nTo understand how Theorem 18.1 works, we note that Uhas the eigenvec-\\ntors of Sas columns\\nU=(⃗u1⃗u2···⃗uM). (18.7)\\nThen we have\\nSU=S(⃗u1⃗u2···⃗uM)\\n=(λ1⃗u1λ2⃗u2···λM⃗uM)\\n=(⃗u1⃗u2···⃗uM)\\uf8eb\\n\\uf8ec\\uf8ec\\uf8edλ1\\nλ2\\n···\\nλM\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 442}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP18.2 Term-document matrices and singular value decomposit ions 407\\nThus, we have SU=UΛ, orS=UΛU−1.\\nWe next state a closely related decomposition of a symmetric square matrix\\ninto the product of matrices derived from its eigenvectors. This will pave the\\nway for the development of our main tool for text analysis, th e singular value\\ndecomposition (Section 18.2).\\nTheorem 18.2. (Symmetric diagonalization theorem) Let S be a square, sym-\\nmetric real-valued M ×M matrix with M linearly independent eigenvectors. Then\\nthere exists a symmetric diagonal decomposition SYMMETRIC DIAGONAL\\nDECOMPOSITION\\nS=QΛQT, (18.8)\\nwhere the columns of Q are the orthogonal and normalized (uni t length, real) eigen-\\nvectors of S, and Λis the diagonal matrix whose entries are the eigenvalues of S .\\nFurther, all entries of Q are real and we have Q−1=QT.\\nWe will build on this symmetric diagonal decomposition to bu ild low-rank\\napproximations to term-document matrices.\\n?Exercise 18.1\\nWhat is the rank of the 3 ×3 diagonal matrix below?\\n\\uf8eb\\n\\uf8ed1 1 0\\n0 1 1\\n1 2 1\\uf8f6\\n\\uf8f8\\nExercise 18.2\\nShow that λ=2 is an eigenvalue of\\nC=(6−2\\n4 0)\\n.\\nFind the corresponding eigenvector.\\nExercise 18.3\\nCompute the unique eigen decomposition of the 2 ×2 matrix in ( 18.4).\\n18.2 Term-document matrices and singular value decomposit ions\\nThe decompositions we have been studying thus far apply to sq uare matri-\\nces. However, the matrix we are interested in is the M×Nterm-document\\nmatrix Cwhere (barring a rare coincidence) M̸=N; furthermore, Cis very\\nunlikely to be symmetric. To this end we ﬁrst describe an exte nsion of the\\nsymmetric diagonal decomposition known as the singular value decomposi- SINGULAR VALUE\\nDECOMPOSITION tion. We then show in Section 18.3 how this can be used to construct an ap-\\nproximate version of C. It is beyond the scope of this book to develop a full', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 443}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP408 18 Matrix decompositions and latent semantic indexing\\ntreatment of the mathematics underlying singular value dec ompositions; fol-\\nlowing the statement of Theorem 18.3 we relate the singular value decompo-\\nsition to the symmetric diagonal decompositions from Secti on18.1.1 . Given SYMMETRIC DIAGONAL\\nDECOMPOSITION C, let Ube the M×Mmatrix whose columns are the orthogonal eigenvec-\\ntors of CCT, and Vbe the N×Nmatrix whose columns are the orthogonal\\neigenvectors of CTC. Denote by CTthe transpose of a matrix C.\\nTheorem 18.3. Let r be the rank of the M ×N matrix C. Then, there is a singular-\\nvalue decomposition (SVD for short) of C of the form SVD\\nC=UΣVT, (18.9)\\nwhere\\n1.The eigenvalues λ1, . . . , λrof CCTare the same as the eigenvalues of CTC;\\n2.For1≤i≤r, let σi=√λi, with λi≥λi+1. Then the M×N matrix Σis\\ncomposed by setting Σii=σifor1≤i≤r, and zero otherwise.\\nThe values σiare referred to as the singular values ofC. It is instructive to\\nexamine the relationship of Theorem 18.3 to Theorem 18.2; we do this rather\\nthan derive the general proof of Theorem 18.3, which is beyond the scope of\\nthis book.\\nBy multiplying Equation ( 18.9) by its transposed version, we have\\nCCT=UΣVTVΣUT=UΣ2UT. (18.10)\\nNote now that in Equation ( 18.10 ), the left-hand side is a square symmetric\\nmatrix real-valued matrix, and the right-hand side represe nts its symmetric\\ndiagonal decomposition as in Theorem 18.2. What does the left-hand side\\nCCTrepresent? It is a square matrix with a row and a column corres pond-\\ning to each of the Mterms. The entry (i,j)in the matrix is a measure of the\\noverlap between the ith and jth terms, based on their co-occurrence in docu-\\nments. The precise mathematical meaning depends on the mann er in which\\nCis constructed based on term weighting. Consider the case wh ereCis the\\nterm-document incidence matrix of page 3, illustrated in Figure 1.1. Then the\\nentry(i,j)inCCTis the number of documents in which both term iand term\\njoccur.\\nWhen writing down the numerical values of the SVD, it is conve ntional\\nto represent Σas an r×rmatrix with the singular values on the diagonals,\\nsince all its entries outside this sub-matrix are zeros. Acc ordingly, it is con-\\nventional to omit the rightmost M−rcolumns of Ucorresponding to these\\nomitted rows of Σ; likewise the rightmost N−rcolumns of Vare omitted\\nsince they correspond in VTto the rows that will be multiplied by the N−r\\ncolumns of zeros in Σ. This written form of the SVD is sometimes known', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 444}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP18.2 Term-document matrices and singular value decomposit ions 409\\nrrrrr\\nrrrrr\\nrrrrr\\nrrrrr\\nrrrrr\\nrrrrr\\nrrrrr\\nrrrrr r\\nr\\nr\\nrrr\\nrrr\\nrrr\\nC = U Σ VT\\nrrr\\nrrr\\nrrr\\nrrr\\nrrr\\nrrr\\nrrr\\nrrr r\\nr\\nr\\nrrrrr\\nrrrrr\\nrrrrr\\nrrrrr\\nrrrrr\\n◮Figure 18.1 Illustration of the singular-value decomposition. In this schematic\\nillustration of ( 18.9), we see two cases illustrated. In the top half of the ﬁgure, w e\\nhave a matrix Cfor which M>N. The lower half illustrates the case M<N.\\nas the reduced SVD ortruncated SVD and we will encounter it again in Ex- REDUCED SVD\\nTRUNCATED SVD ercise 18.9. Henceforth, our numerical examples and exercises will use this\\nreduced form.\\n✎Example 18.3: We now illustrate the singular-value decomposition of a 4 ×2 ma-\\ntrix of rank 2; the singular values are Σ11=2.236 and Σ22=1.\\nC=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed1−1\\n0 1\\n1 0\\n−1 1\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed−0.632 0.000\\n0.316−0.707\\n−0.316−0.707\\n0.632 0.000\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8(\\n2.236 0.000\\n0.000 1.000)(−0.707 0.707\\n−0.707−0.707)\\n. (18.11)\\nAs with the matrix decompositions deﬁned in Section 18.1.1 , the singu-\\nlar value decomposition of a matrix can be computed by a varie ty of algo-\\nrithms, many of which have been publicly available software implementa-\\ntions; pointers to these are given in Section 18.5.\\n?Exercise 18.4\\nLet\\nC=\\uf8eb\\n\\uf8ed1 1\\n0 1\\n1 0\\uf8f6\\n\\uf8f8 (18.12)\\nbe the term-document incidence matrix for a collection. Com pute the co-occurrence\\nmatrix CCT. What is the interpretation of the diagonal entries of CCTwhen Cis a\\nterm-document incidence matrix?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 445}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP410 18 Matrix decompositions and latent semantic indexing\\nExercise 18.5\\nVerify that the SVD of the matrix in Equation ( 18.12 ) is\\nU=\\uf8eb\\n\\uf8ed−0.816 0.000\\n−0.408−0.707\\n−0.408 0.707\\uf8f6\\n\\uf8f8,Σ=(1.732 0.000\\n0.000 1.000)\\nand VT=(−0.707−0.707\\n0.707−0.707)\\n, (18.13)\\nby verifying all of the properties in the statement of Theore m18.3.\\nExercise 18.6\\nSuppose that Cis a binary term-document incidence matrix. What do the entr ies of\\nCTCrepresent?\\nExercise 18.7\\nLet\\nC=\\uf8eb\\n\\uf8ed0 2 1\\n0 3 0\\n2 1 0\\uf8f6\\n\\uf8f8 (18.14)\\nbe a term-document matrix whose entries are term frequencie s; thus term 1 occurs 2\\ntimes in document 2 and once in document 3. Compute CCT; observe that its entries\\nare largest where two terms have their most frequent occurre nces together in the same\\ndocument.\\n18.3 Low-rank approximations\\nWe next state a matrix approximation problem that at ﬁrst see ms to have\\nlittle to do with information retrieval. We describe a solut ion to this matrix\\nproblem using singular-value decompositions, then develo p its application\\nto information retrieval.\\nGiven an M×Nmatrix Cand a positive integer k, we wish to ﬁnd an\\nM×Nmatrix Ckof rank at most k, so as to minimize the Frobenius norm of FROBENIUS NORM\\nthe matrix difference X=C−Ck, deﬁned to be\\n∥X∥F=\\ued6a\\ued6b\\ued6b√M\\n∑\\ni=1N\\n∑\\nj=1X2\\nij. (18.15)\\nThus, the Frobenius norm of Xmeasures the discrepancy between Ckand C;\\nour goal is to ﬁnd a matrix Ckthat minimizes this discrepancy, while con-\\nstraining Ckto have rank at most k. If ris the rank of C, clearly Cr=C\\nand the Frobenius norm of the discrepancy is zero in this case . When kis far\\nsmaller than r, we refer to Ckas a low-rank approximation . LOW -RANK\\nAPPROXIMATION The singular value decomposition can be used to solve the low -rank ma-\\ntrix approximation problem. We then derive from it an applic ation to ap-\\nproximating term-document matrices. We invoke the followi ng three-step\\nprocedure to this end:', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 446}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP18.3 Low-rank approximations 411\\nCk = U Σk VT\\nrrr\\nrrr\\nrrr\\nrrr\\nrrr\\nrrr\\nrrr\\nrrr r\\nr\\nr\\nrrrrr\\nrrrrr\\nrrrrr\\nrrrrr\\nrrrrr\\n◮Figure 18.2 Illustration of low rank approximation using the singular- value de-\\ncomposition. The dashed boxes indicate the matrix entries a ffected by “zeroing out”\\nthe smallest singular values.\\n1.Given C, construct its SVD in the form shown in ( 18.9); thus, C=UΣVT.\\n2.Derive from Σthe matrix Σkformed by replacing by zeros the r−ksmall-\\nest singular values on the diagonal of Σ.\\n3.Compute and output Ck=UΣkVTas the rank- kapproximation to C.\\nThe rank of Ckis at most k: this follows from the fact that Σkhas at most\\nknon-zero values. Next, we recall the intuition of Example 18.1: the effect\\nof small eigenvalues on matrix products is small. Thus, it se ems plausible\\nthat replacing these small eigenvalues by zero will not subs tantially alter the\\nproduct, leaving it “close” to C. The following theorem due to Eckart and\\nYoung tells us that, in fact, this procedure yields the matri x of rank kwith\\nthe lowest possible Frobenius error.\\nTheorem 18.4.\\nmin\\nZ|rank(Z)=k∥C−Z∥F=∥C−Ck∥F=σk+1. (18.16)\\nRecalling that the singular values are in decreasing order σ1≥σ2≥··· ,\\nwe learn from Theorem 18.4 that Ckis the best rank- kapproximation to C,\\nincurring an error (measured by the Frobenius norm of C−Ck) equal to σk+1.\\nThus the larger kis, the smaller this error (and in particular, for k=r, the\\nerror is zero since Σr=Σ; provided r<M,N, then σr+1=0 and thus\\nCr=C).\\nTo derive further insight into why the process of truncating the smallest\\nr−ksingular values in Σhelps generate a rank- kapproximation of low error,\\nwe examine the form of Ck:\\nCk=UΣkVT(18.17)', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 447}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP412 18 Matrix decompositions and latent semantic indexing\\n=U\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edσ10 0 0 0\\n0··· 0 0 0\\n0 0 σk0 0\\n0 0 0 0 0\\n0 0 0 0···\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8VT(18.18)\\n=k\\n∑\\ni=1σi⃗ui⃗vT\\ni, (18.19)\\nwhere ⃗uiand⃗viare the ith columns of Uand V, respectively. Thus, ⃗ui⃗vT\\niis\\na rank-1 matrix, so that we have just expressed Ckas the sum of krank-1\\nmatrices each weighted by a singular value. As iincreases, the contribution\\nof the rank-1 matrix ⃗ui⃗vT\\niis weighted by a sequence of shrinking singular\\nvalues σi.\\n?Exercise 18.8\\nCompute a rank 1 approximation C1to the matrix Cin Example 18.12 , using the SVD\\nas in Exercise 18.13 . What is the Frobenius norm of the error of this approximatio n?\\nExercise 18.9\\nConsider now the computation in Exercise 18.8. Following the schematic in Fig-\\nure18.2, notice that for a rank 1 approximation we have σ1being a scalar. Denote\\nbyU1the ﬁrst column of Uand by V1the ﬁrst column of V. Show that the rank-1\\napproximation to Ccan then be written as U1σ1VT\\n1=σ1U1VT\\n1.\\nExercise 18.10\\nExercise 18.9 can be generalized to rank kapproximations: we let U′\\nkand V′\\nkdenote\\nthe “reduced” matrices formed by retaining only the ﬁrst kcolumns of Uand V,\\nrespectively. Thus U′\\nkis an M×kmatrix while V′T\\nkis ak×Nmatrix. Then, we have\\nCk=U′\\nkΣ′\\nkV′T\\nk, (18.20)\\nwhere Σ′\\nkis the square k×ksubmatrix of Σkwith the singular values σ1, . . . , σkon\\nthe diagonal. The primary advantage of using ( 18.20 ) is to eliminate a lot of redun-\\ndant columns of zeros in Uand V, thereby explicitly eliminating multiplication by\\ncolumns that do not affect the low-rank approximation; this version of the SVD is\\nsometimes known as the reduced SVD or truncated SVD and is a co mputationally\\nsimpler representation from which to compute the low rank ap proximation.\\nFor the matrix Cin Example 18.3, write down both Σ2and Σ′\\n2.\\n18.4 Latent semantic indexing\\nWe now discuss the approximation of a term-document matrix Cby one of\\nlower rank using the SVD. The low-rank approximation to Cyields a new\\nrepresentation for each document in the collection. We will cast queries', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 448}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP18.4 Latent semantic indexing 413\\ninto this low-rank representation as well, enabling us to co mpute query-\\ndocument similarity scores in this low-rank representatio n. This process is\\nknown as latent semantic indexing (generally abbreviated LSI). LATENT SEMANTIC\\nINDEXING But ﬁrst, we motivate such an approximation. Recall the vect or space rep-\\nresentation of documents and queries introduced in Section 6.3(page 120).\\nThis vector space representation enjoys a number of advanta ges including\\nthe uniform treatment of queries and documents as vectors, t he induced\\nscore computation based on cosine similarity, the ability t o weight differ-\\nent terms differently, and its extension beyond document re trieval to such\\napplications as clustering and classiﬁcation. The vector s pace representa-\\ntion suffers, however, from its inability to cope with two cl assic problems\\narising in natural languages: synonymy and polysemy . Synonymy refers to a\\ncase where two different words (say carandautomobile ) have the same mean-\\ning. Because the vector space representation fails to captu re the relationship\\nbetween synonymous terms such as carandautomobile – according each a\\nseparate dimension in the vector space. Consequently the co mputed simi-\\nlarity⃗q·⃗dbetween a query ⃗q(say,car) and a document ⃗dcontaining both car\\nandautomobile underestimates the true similarity that a user would percei ve.\\nPolysemy on the other hand refers to the case where a term such ascharge\\nhas multiple meanings, so that the computed similarity ⃗q·⃗doverestimates\\nthe similarity that a user would perceive. Could we use the co -occurrences\\nof terms (whether, for instance, charge occurs in a document containing steed\\nversus in a document containing electron ) to capture the latent semantic as-\\nsociations of terms and alleviate these problems?\\nEven for a collection of modest size, the term-document matr ixCis likely\\nto have several tens of thousand of rows and columns, and a ran k in the\\ntens of thousands as well. In latent semantic indexing (some times referred\\nto as latent semantic analysis (LSA) ), we use the SVD to construct a low-rank LSA\\napproximation Ckto the term-document matrix, for a value of kthat is far\\nsmaller than the original rank of C. In the experimental work cited later\\nin this section, kis generally chosen to be in the low hundreds. We thus\\nmap each row/column (respectively corresponding to a term/ document) to\\nak-dimensional space; this space is deﬁned by the kprincipal eigenvectors\\n(corresponding to the largest eigenvalues) of CCTand CTC. Note that the\\nmatrix Ckis itself still an M×Nmatrix, irrespective of k.\\nNext, we use the new k-dimensional LSI representation as we did the orig-\\ninal representation – to compute similarities between vect ors. A query vector\\n⃗qis mapped into its representation in the LSI space by the tran sformation\\n⃗qk=Σ−1\\nkUT\\nk⃗q. (18.21)\\nNow, we may use cosine similarities as in Section 6.3.1 (page 120) to com-\\npute the similarity between a query and a document, between t wo docu-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 449}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP414 18 Matrix decompositions and latent semantic indexing\\nments, or between two terms. Note especially that Equation ( 18.21 ) does not\\nin any way depend on ⃗qbeing a query; it is simply a vector in the space of\\nterms. This means that if we have an LSI representation of a co llection of\\ndocuments, a new document not in the collection can be “folde d in” to this\\nrepresentation using Equation ( 18.21 ). This allows us to incrementally add\\ndocuments to an LSI representation. Of course, such increme ntal addition\\nfails to capture the co-occurrences of the newly added docum ents (and even\\nignores any new terms they contain). As such, the quality of t he LSI rep-\\nresentation will degrade as more documents are added and wil l eventually\\nrequire a recomputation of the LSI representation.\\nThe ﬁdelity of the approximation of CktoCleads us to hope that the rel-\\native values of cosine similarities are preserved: if a quer y is close to a doc-\\nument in the original space, it remains relatively close in t hek-dimensional\\nspace. But this in itself is not sufﬁciently interesting, es pecially given that\\nthe sparse query vector ⃗qturns into a dense query vector ⃗qkin the low-\\ndimensional space. This has a signiﬁcant computational cos t, when com-\\npared with the cost of processing ⃗qin its native form.\\n✎Example 18.4: Consider the term-document matrix C=\\nd1d2d3d4d5d6\\nship 1 0 1 0 0 0\\nboat 0 1 0 0 0 0\\nocean 1 1 0 0 0 0\\nvoyage 1 0 0 1 1 0\\ntrip 0 0 0 1 0 1\\nIts singular value decomposition is the product of three mat rices as below. First we\\nhave Uwhich in this example is:\\n1 2 3 4 5\\nship−0.44−0.30 0.57 0.58 0.25\\nboat−0.13−0.33−0.59 0.00 0.73\\nocean−0.48−0.51−0.37 0.00−0.61\\nvoyage−0.70 0.35 0.15 −0.58 0.16\\ntrip−0.26 0.65−0.41 0.58−0.09\\nWhen applying the SVD to a term-document matrix, Uis known as the SVD term\\nmatrix . The singular values are Σ=\\n2.16 0.00 0.00 0.00 0.00\\n0.00 1.59 0.00 0.00 0.00\\n0.00 0.00 1.28 0.00 0.00\\n0.00 0.00 0.00 1.00 0.00\\n0.00 0.00 0.00 0.00 0.39\\nFinally we have VT, which in the context of a term-document matrix is known as\\ntheSVD document matrix :', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 450}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP18.4 Latent semantic indexing 415\\nd1 d2 d3 d4 d5 d6\\n1−0.75−0.28−0.20−0.45−0.33−0.12\\n2−0.29−0.53−0.19 0.63 0.22 0.41\\n3 0.28−0.75 0.45−0.20 0.12−0.33\\n4 0.00 0.00 0.58 0.00 −0.58 0.58\\n5−0.53 0.29 0.63 0.19 0.41 −0.22\\nBy “zeroing out” all but the two largest singular values of Σ, we obtain Σ2=\\n2.16 0.00 0.00 0.00 0.00\\n0.00 1.59 0.00 0.00 0.00\\n0.00 0.00 0.00 0.00 0.00\\n0.00 0.00 0.00 0.00 0.00\\n0.00 0.00 0.00 0.00 0.00\\nFrom this, we compute C2=\\nd1 d2 d3 d4 d5 d6\\n1−1.62−0.60−0.44−0.97−0.70−0.26\\n2−0.46−0.84−0.30 1.00 0.35 0.65\\n3 0.00 0.00 0.00 0.00 0.00 0.00\\n4 0.00 0.00 0.00 0.00 0.00 0.00\\n5 0.00 0.00 0.00 0.00 0.00 0.00\\nNotice that the low-rank approximation, unlike the origina l matrix C, can have\\nnegative entries.\\nExamination of C2and Σ2in Example 18.4 shows that the last 3 rows of\\neach of these matrices are populated entirely by zeros. This suggests that\\nthe SVD product UΣVTin Equation ( 18.18 ) can be carried out with only two\\nrows in the representations of Σ2and VT; we may then replace these matrices\\nby their truncated versions Σ′\\n2and(V′)T. For instance, the truncated SVD\\ndocument matrix (V′)Tin this example is:\\nd1 d2 d3 d4 d5 d6\\n1−1.62−0.60−0.44−0.97−0.70−0.26\\n2−0.46−0.84−0.30 1.00 0.35 0.65\\nFigure 18.3 illustrates the documents in (V′)Tin two dimensions. Note\\nalso that C2is dense relative to C.\\nWe may in general view the low-rank approximation of CbyCkas a con-\\nstrained optimization problem: subject to the constraint that Ckhave rank at\\nmost k, we seek a representation of the terms and documents compris ingC\\nwith low Frobenius norm for the error C−Ck. When forced to squeeze the\\nterms/documents down to a k-dimensional space, the SVD should bring to-\\ngether terms with similar co-occurrences. This intuition s uggests, then, that\\nnot only should retrieval quality not suffer too much from th e dimension\\nreduction, but in fact may improve .', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 451}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP416 18 Matrix decompositions and latent semantic indexing\\n−0.5−1.0−1.50.51.0\\n−0.5\\n−1.0dim 2\\ndim 1\\n×\\nd1\\n×\\nd2×d3×\\nd4\\n×\\nd5×d6\\n◮Figure 18.3 The documents of Example 18.4 reduced to two dimensions in (V′)T.\\nDumais (1993 ) and Dumais (1995 ) conducted experiments with LSI on\\nTREC documents and tasks, using the commonly-used Lanczos a lgorithm\\nto compute the SVD. At the time of their work in the early 1990’ s, the LSI\\ncomputation on tens of thousands of documents took approxim ately a day\\non one machine. On these experiments, they achieved precisi on at or above\\nthat of the median TREC participant. On about 20% of TREC topi cs their\\nsystem was the top scorer, and reportedly slightly better on average than\\nstandard vector spaces for LSI at about 350 dimensions. Here are some con-\\nclusions on LSI ﬁrst suggested by their work, and subsequent ly veriﬁed by\\nmany other experiments.\\n•The computational cost of the SVD is signiﬁcant; at the time o f this writ-\\ning, we know of no successful experiment with over one millio n docu-\\nments. This has been the biggest obstacle to the widespread a doption to\\nLSI. One approach to this obstacle is to build the LSI represe ntation on a\\nrandomly sampled subset of the documents in the collection, following\\nwhich the remaining documents are “folded in” as detailed wi th Equa-\\ntion ( 18.21 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 452}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP18.5 References and further reading 417\\n•As we reduce k, recall tends to increase, as expected.\\n•Most surprisingly, a value of kin the low hundreds can actually increase\\nprecision on some query benchmarks. This appears to suggest that for a\\nsuitable value of k, LSI addresses some of the challenges of synonymy.\\n•LSI works best in applications where there is little overlap between queries\\nand documents.\\nThe experiments also documented some modes where LSI failed to match\\nthe effectiveness of more traditional indexes and score com putations. Most\\nnotably (and perhaps obviously), LSI shares two basic drawb acks of vector\\nspace retrieval: there is no good way of expressing negation s (ﬁnd docu-\\nments that contain german but notshepherd ), and no way of enforcing Boolean\\nconditions.\\nLSI can be viewed as soft clustering by interpreting each dimension of the SOFT CLUSTERING\\nreduced space as a cluster and the value that a document has on that dimen-\\nsion as its fractional membership in that cluster.\\n18.5 References and further reading\\nStrang (1986 ) provides an excellent introductory overview of matrix dec om-\\npositions including the singular value decomposition. The orem 18.4 is due\\ntoEckart and Young (1936 ). The connection between information retrieval\\nand low-rank approximations of the term-document matrix wa s introduced\\ninDeerwester et al. (1990 ), with a subsequent survey of results in Berry\\net al. (1995 ).Dumais (1993 ) and Dumais (1995 ) describe experiments on\\nTREC benchmarks giving evidence that at least on some benchm arks, LSI\\ncan produce better precision and recall than standard vecto r-space retrieval.\\nhttp://www.cs.utk.edu/˜berry/lsi++/ andhttp://lsi.argreenhouse.com/lsi/LSIpapers.html\\noffer comprehensive pointers to the literature and softwar e of LSI. Schütze\\nand Silverstein (1997 ) evaluate LSI and truncated representations of cen-\\ntroids for efﬁcient K-means clustering (Section 16.4).Bast and Majumdar\\n(2005 ) detail the role of the reduced dimension kin LSI and how different\\npairs of terms get coalesced together at differing values of k. Applications of\\nLSI to cross-language information retrieval (where documents in two or more CROSS -LANGUAGE\\nINFORMATION\\nRETRIEVALdifferent languages are indexed, and a query posed in one lan guage is ex-\\npected to retrieve documents in other languages) are develo ped in Berry and\\nYoung (1995 ) and Littman et al. (1998 ). LSI (referred to as LSA in more gen-\\neral settings) has been applied to host of other problems in c omputer science\\nranging from memory modeling to computer vision.\\nHofmann (1999a ;b) provides an initial probabilistic extension of the basic\\nlatent semantic indexing technique. A more satisfactory fo rmal basis for a', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 453}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP418 18 Matrix decompositions and latent semantic indexing\\nDocID Document text\\n1 hello\\n2 open house\\n3 mi casa\\n4 hola Profesor\\n5 hola y bienvenido\\n6 hello and welcome\\n◮Figure 18.4 Documents for Exercise 18.11 .\\nSpanish English\\nmi my\\ncasa house\\nhola hello\\nprofesor professor\\ny and\\nbienvenido welcome\\n◮Figure 18.5 Glossary for Exercise 18.11 .\\nprobabilistic latent variable model for dimensionality re duction is the Latent\\nDirichlet Allocation (LDA) model ( Blei et al. 2003 ), which is generative and\\nassigns probabilities to documents outside of the training set. This model is\\nextended to a hierarchical clustering by Rosen-Zvi et al. (2004 ).Wei and Croft\\n(2006 ) present the ﬁrst large scale evaluation of LDA, ﬁnding it to signiﬁ-\\ncantly outperform the query likelihood model of Section 12.2 (page 242), but\\nto not perform quite as well as the relevance model mentioned in Section 12.4\\n(page 250) – but the latter does additional per-query processing unli ke LDA.\\nTeh et al. (2006 ) generalize further by presenting Hierarchical Dirichlet Pro-\\ncesses, a probabilistic model which allows a group (for us, a document) to\\nbe drawn from an inﬁnite mixture of latent topics, while stil l allowing these\\ntopics to be shared across documents.\\n?Exercise 18.11\\nAssume you have a set of documents each of which is in either En glish or in Spanish.\\nThe collection is given in Figure 18.4.\\nFigure 18.5 gives a glossary relating the Spanish and English words abov e for your\\nown information. This glossary is NOT available to the retri eval system:\\n1.Construct the appropriate term-document matrix Cto use for a collection con-\\nsisting of these documents. For simplicity, use raw term fre quencies rather than\\nnormalized tf-idf weights. Make sure to clearly label the di mensions of your ma-\\ntrix.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 454}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP18.5 References and further reading 419\\n2.Write down the matrices U2,Σ′\\n2and V2and from these derive the rank 2 approxi-\\nmation C2.\\n3.State succinctly what the (i,j)entry in the matrix CTCrepresents.\\n4.State succinctly what the (i,j)entry in the matrix CT\\n2C2represents, and why it\\ndiffers from that in CTC.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 455}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 456}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 421\\n19 Web search basics\\nIn this and the following two chapters, we consider web searc h engines. Sec-\\ntions 19.1–19.4 provide some background and history to help the reader ap-\\npreciate the forces that conspire to make the Web chaotic, fa st-changing and\\n(from the standpoint of information retrieval) very differ ent from the “tradi-\\ntional” collections studied thus far in this book. Sections 19.5–19.6 deal with\\nestimating the number of documents indexed by web search eng ines, and the\\nelimination of duplicate documents in web indexes, respect ively. These two\\nlatter sections serve as background material for the follow ing two chapters.\\n19.1 Background and history\\nThe Web is unprecedented in many ways: unprecedented in scal e, unprece-\\ndented in the almost-complete lack of coordination in its cr eation, and un-\\nprecedented in the diversity of backgrounds and motives of i ts participants.\\nEach of these contributes to making web search different – an d generally far\\nharder – than searching “traditional” documents.\\nThe invention of hypertext, envisioned by Vannevar Bush in t he 1940’s and\\nﬁrst realized in working systems in the 1970’s, signiﬁcantl y precedes the for-\\nmation of the World Wide Web (which we will simply refer to as t he Web), in\\nthe 1990’s. Web usage has shown tremendous growth to the poin t where it\\nnow claims a good fraction of humanity as participants, by re lying on a sim-\\nple, open client-server design: (1) the server communicate s with the client\\nvia a protocol (the http or hypertext transfer protocol) that is lightweight and HTTP\\nsimple, asynchronously carrying a variety of payloads (tex t, images and –\\nover time – richer media such as audio and video ﬁles) encoded in a sim-\\nple markup language called HTML (for hypertext markup language); (2) the HTML\\nclient – generally a browser , an application within a graphical user environ-\\nment – can ignore what it does not understand. Each of these se emingly\\ninnocuous features has contributed enormously to the growt h of the Web, so\\nit is worthwhile to examine them further.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 457}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP422 19 Web search basics\\nThe basic operation is as follows: a client (such as a browser ) sends an http\\nrequest to aweb server . The browser speciﬁes a URL (forUniversal Resource Lo- URL\\ncator ) such ashttp://www.stanford.edu/home/atoz/contact.html .\\nIn this example URL, the string http refers to the protocol to be used for\\ntransmitting the data. The string www.stanford.edu is known as the do-\\nmain and speciﬁes the root of a hierarchy of web pages (typically m irroring a\\nﬁlesystem hierarchy underlying the web server). In this exa mple,/home/atoz/contact.html\\nis a path in this hierarchy with a ﬁle contact.html that contains the infor-\\nmation to be returned by the web server at www.stanford.edu in response\\nto this request. The HTML-encoded ﬁle contact.html holds the hyper-\\nlinks and the content (in this instance, contact informatio n for Stanford Uni-\\nversity), as well as formatting rules for rendering this con tent in a browser.\\nSuch an http request thus allows us to fetch the content of a pa ge, some-\\nthing that will prove to be useful to us for crawling and index ing documents\\n(Chapter 20).\\nThe designers of the ﬁrst browsers made it easy to view the HTM L markup\\ntags on the content of a URL. This simple convenience allowed new users to\\ncreate their own HTML content without extensive training or experience;\\nrather, they learned from example content that they liked. A s they did so, a\\nsecond feature of browsers supported the rapid proliferati on of web content\\ncreation and usage: browsers ignored what they did not under stand. This\\ndid not, as one might fear, lead to the creation of numerous in compatible\\ndialects of HTML. What it did promote was amateur content cre ators who\\ncould freely experiment with and learn from their newly crea ted web pages\\nwithout fear that a simple syntax error would “bring the syst em down.” Pub-\\nlishing on the Web became a mass activity that was not limited to a few\\ntrained programmers, but rather open to tens and eventually hundreds of\\nmillions of individuals. For most users and for most informa tion needs, the\\nWeb quickly became the best way to supply and consume informa tion on\\neverything from rare ailments to subway schedules.\\nThe mass publishing of information on the Web is essentially useless un-\\nless this wealth of information can be discovered and consum ed by other\\nusers. Early attempts at making web information “discovera ble” fell into two\\nbroad categories: (1) full-text index search engines such a s Altavista, Excite\\nand Infoseek and (2) taxonomies populated with web pages in c ategories,\\nsuch as Yahoo! The former presented the user with a keyword se arch in-\\nterface supported by inverted indexes and ranking mechanis ms building on\\nthose introduced in earlier chapters. The latter allowed th e user to browse\\nthrough a hierarchical tree of category labels. While this i s at ﬁrst blush a\\nconvenient and intuitive metaphor for ﬁnding web pages, it h as a number of\\ndrawbacks: ﬁrst, accurately classifying web pages into tax onomy tree nodes\\nis for the most part a manual editorial process, which is difﬁ cult to scale\\nwith the size of the Web. Arguably, we only need to have “high- quality”', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 458}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP19.2 Web characteristics 423\\nweb pages in the taxonomy, with only the best web pages for eac h category.\\nHowever, just discovering these and classifying them accur ately and consis-\\ntently into the taxonomy entails signiﬁcant human effort. F urthermore, in\\norder for a user to effectively discover web pages classiﬁed into the nodes of\\nthe taxonomy tree, the user’s idea of what sub-tree(s) to see k for a particu-\\nlar topic should match that of the editors performing the cla ssiﬁcation. This\\nquickly becomes challenging as the size of the taxonomy grow s; the Yahoo!\\ntaxonomy tree surpassed 1000 distinct nodes fairly early on . Given these\\nchallenges, the popularity of taxonomies declined over tim e, even though\\nvariants (such as About.com and the Open Directory Project) sprang up with\\nsubject-matter experts collecting and annotating web page s for each cate-\\ngory.\\nThe ﬁrst generation of web search engines transported class ical search\\ntechniques such as those in the preceding chapters to the web domain, focus-\\ning on the challenge of scale. The earliest web search engine s had to contend\\nwith indexes containing tens of millions of documents, whic h was a few or-\\nders of magnitude larger than any prior information retriev al system in the\\npublic domain. Indexing, query serving and ranking at this s cale required\\nthe harnessing together of tens of machines to create highly available sys-\\ntems, again at scales not witnessed hitherto in a consumer-f acing search ap-\\nplication. The ﬁrst generation of web search engines was lar gely successful\\nat solving these challenges while continually indexing a si gniﬁcant fraction\\nof the Web, all the while serving queries with sub-second res ponse times.\\nHowever, the quality and relevance of web search results lef t much to be\\ndesired owing to the idiosyncrasies of content creation on t he Web that we\\ndiscuss in Section 19.2. This necessitated the invention of new ranking and\\nspam-ﬁghting techniques in order to ensure the quality of th e search results.\\nWhile classical information retrieval techniques (such as those covered ear-\\nlier in this book) continue to be necessary for web search, th ey are not by\\nany means sufﬁcient. A key aspect (developed further in Chap ter21) is that\\nwhereas classical techniques measure the relevance of a doc ument to a query,\\nthere remains a need to gauge the authoritativeness of a document based on\\ncues such as which website hosts it.\\n19.2 Web characteristics\\nThe essential feature that led to the explosive growth of the web – decentral-\\nized content publishing with essentially no central contro l of authorship –\\nturned out to be the biggest challenge for web search engines in their quest to\\nindex and retrieve this content. Web page authors created co ntent in dozens\\nof (natural) languages and thousands of dialects, thus dema nding many dif-\\nferent forms of stemming and other linguistic operations. B ecause publish-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 459}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP424 19 Web search basics\\ning was now open to tens of millions, web pages exhibited hete rogeneity at a\\ndaunting scale, in many crucial aspects. First, content-cr eation was no longer\\nthe privy of editorially-trained writers; while this repre sented a tremendous\\ndemocratization of content creation, it also resulted in a t remendous varia-\\ntion in grammar and style (and in many cases, no recognizable grammar or\\nstyle). Indeed, web publishing in a sense unleashed the best and worst of\\ndesktop publishing on a planetary scale, so that pages quick ly became rid-\\ndled with wild variations in colors, fonts and structure. So me web pages,\\nincluding the professionally created home pages of some lar ge corporations,\\nconsisted entirely of images (which, when clicked, led to ri cher textual con-\\ntent) – and therefore, no indexable text.\\nWhat about the substance of the text in web pages? The democra tization\\nof content creation on the web meant a new level of granularit y in opinion on\\nvirtually any subject. This meant that the web contained tru th, lies, contra-\\ndictions and suppositions on a grand scale. This gives rise t o the question:\\nwhich web pages does one trust? In a simplistic approach, one might argue\\nthat some publishers are trustworthy and others not – beggin g the question\\nof how a search engine is to assign such a measure of trust to ea ch website\\nor web page. In Chapter 21we will examine approaches to understanding\\nthis question. More subtly, there may be no universal, user- independent no-\\ntion of trust; a web page whose contents are trustworthy to on e user may\\nnot be so to another. In traditional (non-web) publishing th is is not an issue:\\nusers self-select sources they ﬁnd trustworthy. Thus one re ader may ﬁnd\\nthe reporting of The New York Times to be reliable, while another may prefer\\nThe Wall Street Journal . But when a search engine is the only viable means\\nfor a user to become aware of (let alone select) most content, this challenge\\nbecomes signiﬁcant.\\nWhile the question “how big is the Web?” has no easy answer (se e Sec-\\ntion 19.5), the question “how many web pages are in a search engine’s in dex”\\nis more precise, although, even this question has issues. By the end of 1995,\\nAltavista reported that it had crawled and indexed approxim ately 30 million\\nstatic web pages . Static web pages are those whose content does not vary from STATIC WEB PAGES\\none request for that page to the next. For this purpose, a prof essor who man-\\nually updates his home page every week is considered to have a static web\\npage, but an airport’s ﬂight status page is considered to be d ynamic. Dy-\\nnamic pages are typically mechanically generated by an appl ication server\\nin response to a query to a database, as show in Figure 19.1. One sign of\\nsuch a page is that the URL has the character \"?\" in it. Since th e number\\nof static web pages was believed to be doubling every few mont hs in 1995,\\nearly web search engines such as Altavista had to constantly add hardware\\nand bandwidth for crawling and indexing web pages.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 460}),\n",
              " Document(page_content=\"Online edition (c)\\n2009 Cambridge UP19.2 Web characteristics 425\\n◮Figure 19.1 A dynamically generated web page. The browser sends a reques t for\\nﬂight information on ﬂight AA129 to the web application, tha t fetches the informa-\\ntion from back-end databases then creates a dynamic web page that it returns to the\\nbrowser.\\n&%'$\\n&%'$\\n-anchor\\n◮Figure 19.2 Two nodes of the web graph joined by a link.\\n19.2.1 The web graph\\nWe can view the static Web consisting of static HTML pages tog ether with\\nthe hyperlinks between them as a directed graph in which each web page is\\na node and each hyperlink a directed edge.\\nFigure 19.2 shows two nodes A and B from the web graph, each corre-\\nsponding to a web page, with a hyperlink from A to B. We refer to the set of\\nall such nodes and directed edges as the web graph. Figure 19.2 also shows\\nthat (as is the case with most links on web pages) there is some text surround-\\ning the origin of the hyperlink on page A. This text is general ly encapsulated\\nin thehref attribute of the <a> (for anchor) tag that encodes the hyperl ink\\nin the HTML code of page A, and is referred to as anchor text . As one might ANCHOR TEXT\\nsuspect, this directed graph is not strongly connected : there are pairs of pages\\nsuch that one cannot proceed from one page of the pair to the ot her by follow-\\ning hyperlinks. We refer to the hyperlinks into a page as in-links and those IN-LINKS\\nout of a page as out-links . The number of in-links to a page (also known as OUT -LINKS\\nitsin-degree ) has averaged from roughly 8 to 15, in a range of studies. We\\nsimilarly deﬁne the out-degree of a web page to be the number o f links out\", metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 461}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP426 19 Web search basics\\n◮Figure 19.3 A sample small web graph. In this example we have six pages lab eled\\nA-F. Page B has in-degree 3 and out-degree 1. This example gra ph is not strongly\\nconnected: there is no path from any of pages B-F to page A.\\nof it. These notions are represented in Figure 19.3.\\nThere is ample evidence that these links are not randomly dis tributed; for\\none thing, the distribution of the number of links into a web p age does not\\nfollow the Poisson distribution one would expect if every we b page were\\nto pick the destinations of its links uniformly at random. Ra ther, this dis-\\ntribution is widely reported to be a power law , in which the total number of POWER LAW\\nweb pages with in-degree iis proportional to 1/ iα; the value of αtypically\\nreported by studies is 2.1.1Furthermore, several studies have suggested that\\nthe directed graph connecting web pages has a bowtie shape: there are three BOWTIE\\nmajor categories of web pages that are sometimes referred to as IN, OUT\\nand SCC. A web surfer can pass from any page in IN to any page in S CC, by\\nfollowing hyperlinks. Likewise, a surfer can pass from page in SCC to any\\npage in OUT. Finally, the surfer can surf from any page in SCC t o any other\\npage in SCC. However, it is not possible to pass from a page in S CC to any\\npage in IN, or from a page in OUT to a page in SCC (or, consequent ly, IN).\\nNotably, in several studies IN and OUT are roughly equal in si ze, whereas\\n1. Cf. Zipf’s law of the distribution of words in text in Chapt er5(page 90), which is a power\\nlaw with α=1.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 462}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP19.2 Web characteristics 427\\n◮Figure 19.4 The bowtie structure of the Web. Here we show one tube and thre e\\ntendrils.\\nSCC is somewhat larger; most web pages fall into one of these t hree sets. The\\nremaining pages form into tubes that are small sets of pages outside SCC that\\nlead directly from IN to OUT, and tendrils that either lead nowhere from IN,\\nor from nowhere to OUT. Figure 19.4 illustrates this structure of the Web.\\n19.2.2 Spam\\nEarly in the history of web search, it became clear that web se arch engines\\nwere an important means for connecting advertisers to prosp ective buyers.\\nA user searching for maui golf real estate is not merely seeking news or en-\\ntertainment on the subject of housing on golf courses on the i sland of Maui,\\nbut instead likely to be seeking to purchase such a property. Sellers of such\\nproperty and their agents, therefore, have a strong incenti ve to create web\\npages that rank highly on this query. In a search engine whose scoring was\\nbased on term frequencies, a web page with numerous repetiti ons ofmauigolf\\nreal estate would rank highly. This led to the ﬁrst generation of spam , which SPAM\\n(in the context of web search) is the manipulation of web page content for\\nthe purpose of appearing high up in search results for select ed keywords.\\nTo avoid irritating users with these repetitions, sophisti cated spammers re-\\nsorted to such tricks as rendering these repeated terms in th e same color as\\nthe background. Despite these words being consequently inv isible to the hu-\\nman user, a search engine indexer would parse the invisible w ords out of', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 463}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP428 19 Web search basics\\n◮Figure 19.5 Cloaking as used by spammers.\\nthe HTML representation of the web page and index these words as being\\npresent in the page.\\nAt its root, spam stems from the heterogeneity of motives in c ontent cre-\\nation on the Web. In particular, many web content creators ha ve commercial\\nmotives and therefore stand to gain from manipulating searc h engine results.\\nYou might argue that this is no different from a company that u ses large fonts\\nto list its phone numbers in the yellow pages; but this genera lly costs the\\ncompany more and is thus a fairer mechanism. A more apt analog y, perhaps,\\nis the use of company names beginning with a long string of A’s to be listed\\nearly in a yellow pages category. In fact, the yellow pages’ m odel of com-\\npanies paying for larger/darker fonts has been replicated i n web search: in\\nmany search engines, it is possible to pay to have one’s web pa ge included\\nin the search engine’s index – a model known as paid inclusion . Different PAID INCLUSION\\nsearch engines have different policies on whether to allow p aid inclusion,\\nand whether such a payment has any effect on ranking in search results.\\nSearch engines soon became sophisticated enough in their sp am detection\\nto screen out a large number of repetitions of particular key words. Spam-\\nmers responded with a richer set of spam techniques, the best known of\\nwhich we now describe. The ﬁrst of these techniques is cloaking , shown in\\nFigure 19.5. Here, the spammer’s web server returns different pages dep end-\\ning on whether the http request comes from a web search engine ’s crawler\\n(the part of the search engine that gathers web pages, to be de scribed in\\nChapter 20), or from a human user’s browser. The former causes the web\\npage to be indexed by the search engine under misleading keyw ords. When\\nthe user searches for these keywords and elects to view the pa ge, he receives\\na web page that has altogether different content than that in dexed by the\\nsearch engine. Such deception of search indexers is unknown in the tra-\\nditional world of information retrieval; it stems from the f act that the rela-\\ntionship between page publishers and web search engines is n ot completely\\ncollaborative.\\nAdoorway page contains text and metadata carefully chosen to rank highly', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 464}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP19.3 Advertising as the economic model 429\\non selected search keywords. When a browser requests the doo rway page, it\\nis redirected to a page containing content of a more commerci al nature. More\\ncomplex spamming techniques involve manipulation of the me tadata related\\nto a page including (for reasons we will see in Chapter 21) the links into a\\nweb page. Given that spamming is inherently an economically motivated\\nactivity, there has sprung around it an industry of Search Engine Optimizers , SEARCH ENGINE\\nOPTIMIZERS or SEOs to provide consultancy services for clients who seek to have their\\nweb pages rank highly on selected keywords. Web search engin es frown on\\nthis business of attempting to decipher and adapt to their pr oprietary rank-\\ning techniques and indeed announce policies on forms of SEO b ehavior they\\ndo not tolerate (and have been known to shut down search reque sts from cer-\\ntain SEOs for violation of these). Inevitably, the parrying between such SEOs\\n(who gradually infer features of each web search engine’s ra nking methods)\\nand the web search engines (who adapt in response) is an unend ing struggle;\\nindeed, the research sub-area of adversarial information retrieval has sprung up ADVERSARIAL\\nINFORMATION\\nRETRIEVALaround this battle. To combat spammers who manipulate the te xt of their\\nweb pages is the exploitation of the link structure of the Web – a technique\\nknown as link analysis . The ﬁrst web search engine known to apply link anal-\\nysis on a large scale (to be detailed in Chapter 21) was Google, although all\\nweb search engines currently make use of it (and correspondi ngly, spam-\\nmers now invest considerable effort in subverting it – this i s known as link LINK SPAM\\nspam ).\\n?Exercise 19.1\\nIf the number of pages with in-degree iis proportional to 1/ i2.1, what is the probabil-\\nity that a randomly chosen web page has in-degree 1?\\nExercise 19.2\\nIf the number of pages with in-degree iis proportional to 1/ i2.1, what is the average\\nin-degree of a web page?\\nExercise 19.3\\nIf the number of pages with in-degree iis proportional to 1/ i2.1, then as the largest\\nin-degree goes to inﬁnity, does the fraction of pages with in -degree igrow, stay the\\nsame, or diminish? How would your answer change for values of the exponent other\\nthan 2.1?\\nExercise 19.4\\nThe average in-degree of all nodes in a snapshot of the web gra ph is 9. What can we\\nsay about the average out-degree of all nodes in this snapsho t?\\n19.3 Advertising as the economic model\\nEarly in the history of the Web, companies used graphical ban ner advertise-\\nments on web pages at popular websites (news and entertainme nt sites such\\nas MSN, America Online, Yahoo! and CNN). The primary purpose of these\\nadvertisements was branding : to convey to the viewer a positive feeling about', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 465}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP430 19 Web search basics\\nthe brand of the company placing the advertisement. Typical ly these adver-\\ntisements are priced on a cost per mil (CPM ) basis: the cost to the company of CPM\\nhaving its banner advertisement displayed 1000 times. Some websites struck\\ncontracts with their advertisers in which an advertisement was priced not by\\nthe number of times it is displayed (also known as impressions ), but rather\\nby the number of times it was clicked on by the user. This pricing model is\\nknown as the cost per click (CPC ) model. In such cases, clicking on the adver- CPC\\ntisement leads the user to a web page set up by the advertiser, where the user\\nis induced to make a purchase. Here the goal of the advertisem ent is not so\\nmuch brand promotion as to induce a transaction. This distin ction between\\nbrand and transaction-oriented advertising was already wi dely recognized\\nin the context of conventional media such as broadcast and pr int. The inter-\\nactivity of the web allowed the CPC billing model – clicks cou ld be metered\\nand monitored by the website and billed to the advertiser.\\nThe pioneer in this direction was a company named Goto, which changed\\nits name to Overture prior to eventual acquisition by Yahoo! Goto was not,\\nin the traditional sense, a search engine; rather, for every query term qit ac-\\ncepted bidsfrom companies who wanted their web page shown on the query\\nq. In response to the query q, Goto would return the pages of all advertisers\\nwho bid for q, ordered by their bids. Furthermore, when the user clicked\\non one of the returned results, the corresponding advertise r would make a\\npayment to Goto (in the initial implementation, this paymen t equaled the\\nadvertiser’s bid for q).\\nSeveral aspects of Goto’s model are worth highlighting. Fir st, a user typing\\nthe query qinto Goto’s search interface was actively expressing an int erest\\nand intent related to the query q. For instance, a user typing golfclubs is more\\nlikely to be imminently purchasing a set than one who is simpl y browsing\\nnews on golf. Second, Goto only got compensated when a user ac tually ex-\\npressed interest in an advertisement – as evinced by the user clicking the ad-\\nvertisement. Taken together, these created a powerful mech anism by which\\nto connect advertisers to consumers, quickly raising the an nual revenues of\\nGoto/Overture into hundreds of millions of dollars. This st yle of search en-\\ngine came to be known variously as sponsored search orsearch advertising . SPONSORED SEARCH\\nSEARCH ADVERTISING Given these two kinds of search engines – the “pure” search en gines such\\nas Google and Altavista, versus the sponsored search engine s – the logi-\\ncal next step was to combine them into a single user experienc e. Current\\nsearch engines follow precisely this model: they provide pu re search results\\n(generally known as algorithmic search results) as the primary response to a ALGORITHMIC SEARCH\\nuser’s search, together with sponsored search results disp layed separately\\nand distinctively to the right of the algorithmic results. T his is shown in Fig-\\nure19.6. Retrieving sponsored search results and ranking them in re sponse\\nto a query has now become considerably more sophisticated th an the sim-\\nple Goto scheme; the process entails a blending of ideas from information', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 466}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP19.3 Advertising as the economic model 431\\n◮Figure 19.6 Search advertising triggered by query keywords. Here the qu eryA320\\nreturns algorithmic search results about the Airbus aircra ft, together with advertise-\\nments for various non-aircraft goods numbered A320, that ad vertisers seek to market\\nto those querying on this query. The lack of advertisements f or the aircraft reﬂects the\\nfact that few marketers attempt to sell A320 aircraft on the w eb.\\nretrieval and microeconomics, and is beyond the scope of thi s book. For\\nadvertisers, understanding how search engines do this rank ing and how to\\nallocate marketing campaign budgets to different keywords and to different\\nsponsored search engines has become a profession known as search engine SEARCH ENGINE\\nMARKETING marketing (SEM).\\nThe inherently economic motives underlying sponsored sear ch give rise\\nto attempts by some participants to subvert the system to the ir advantage.\\nThis can take many forms, one of which is known as click spam . There is CLICK SPAM\\ncurrently no universally accepted deﬁnition of click spam. It refers (as the\\nname suggests) to clicks on sponsored search results that ar e not from bona\\nﬁde search users. For instance, a devious advertiser may att empt to exhaust\\nthe advertising budget of a competitor by clicking repeated ly (through the\\nuse of a robotic click generator) on that competitor’s spons ored search ad-\\nvertisements. Search engines face the challenge of discern ing which of the\\nclicks they observe are part of a pattern of click spam, to avo id charging their\\nadvertiser clients for such clicks.\\n?Exercise 19.5\\nThe Goto method ranked advertisements matching a query by bid: the highest-bidding\\nadvertiser got the top position, the second-highest the nex t, and so on. What can go\\nwrong with this when the highest-bidding advertiser places an advertisement that is\\nirrelevant to the query? Why might an advertiser with an irre levant advertisement\\nbid high in this manner?\\nExercise 19.6\\nSuppose that, in addition to bids, we had for each advertiser their click-through rate :\\nthe ratio of the historical number of times users click on the ir advertisement to the\\nnumber of times the advertisement was shown. Suggest a modiﬁ cation of the Goto\\nscheme that exploits this data to avoid the problem in Exerci se19.5 above.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 467}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP432 19 Web search basics\\n19.4 The search user experience\\nIt is crucial that we understand the users of web search as wel l. This is\\nagain a signiﬁcant change from traditional information ret rieval, where users\\nwere typically professionals with at least some training in the art of phrasing\\nqueries over a well-authored collection whose style and str ucture they un-\\nderstood well. In contrast, web search users tend to not know (or care) about\\nthe heterogeneity of web content, the syntax of query langua ges and the art\\nof phrasing queries; indeed, a mainstream tool (as web searc h has come to\\nbecome) should not place such onerous demands on billions of people. A\\nrange of studies has concluded that the average number of key words in a\\nweb search is somewhere between 2 and 3. Syntax operators (Bo olean con-\\nnectives, wildcards, etc.) are seldom used, again a result o f the composition\\nof the audience – “normal” people, not information scientis ts.\\nIt is clear that the more user trafﬁc a web search engine can at tract, the\\nmore revenue it stands to earn from sponsored search. How do s earch en-\\ngines differentiate themselves and grow their trafﬁc? Here Google identiﬁed\\ntwo principles that helped it grow at the expense of its compe titors: (1) a\\nfocus on relevance, speciﬁcally precision rather than reca ll in the ﬁrst few re-\\nsults; (2) a user experience that is lightweight, meaning th at both the search\\nquery page and the search results page are uncluttered and al most entirely\\ntextual, with very few graphical elements. The effect of the ﬁrst was simply\\nto save users time in locating the information they sought. T he effect of the\\nsecond is to provide a user experience that is extremely resp onsive, or at any\\nrate not bottlenecked by the time to load the search query or r esults page.\\n19.4.1 User query needs\\nThere appear to be three broad categories into which common w eb search\\nqueries can be grouped: (i) informational, (ii) navigation al and (iii) transac-\\ntional. We now explain these categories; it should be clear t hat some queries\\nwill fall in more than one of these categories, while others w ill fall outside\\nthem.\\nInformational queries seek general information on a broad topic, such as INFORMATIONAL\\nQUERIES leukemia or Provence. There is typically not a single web pag e that con-\\ntains all the information sought; indeed, users with inform ational queries\\ntypically try to assimilate information from multiple web p ages.\\nNavigational queries seek the website or home page of a single entity that the NAVIGATIONAL\\nQUERIES user has in mind, say Lufthansa airlines . In such cases, the user’s expectation\\nis that the very ﬁrst search result should be the home page of L ufthansa.\\nThe user is not interested in a plethora of documents contain ing the term\\nLufthansa ; for such a user, the best measure of user satisfaction is pre cision at\\n1.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 468}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP19.5 Index size and estimation 433\\nAtransactional query is one that is a prelude to the user performing a trans- TRANSACTIONAL\\nQUERY action on the Web – such as purchasing a product, downloading a ﬁle or\\nmaking a reservation. In such cases, the search engine shoul d return results\\nlisting services that provide form interfaces for such tran sactions.\\nDiscerning which of these categories a query falls into can b e challeng-\\ning. The category not only governs the algorithmic search re sults, but the\\nsuitability of the query for sponsored search results (sinc e the query may re-\\nveal an intent to purchase). For navigational queries, some have argued that\\nthe search engine should return only a single result or even t he target web\\npage directly. Nevertheless, web search engines have histo rically engaged in\\na battle of bragging rights over which one indexes more web pa ges. Does\\nthe user really care? Perhaps not, but the media does highlig ht estimates\\n(often statistically indefensible) of the sizes of various search engines. Users\\nare inﬂuenced by these reports and thus, search engines do ha ve to pay at-\\ntention to how their index sizes compare to competitors’. Fo r informational\\n(and to a lesser extent, transactional) queries, the user do es care about the\\ncomprehensiveness of the search engine.\\nFigure 19.7 shows a composite picture of a web search engine including\\nthe crawler, as well as both the web page and advertisement in dexes. The\\nportion of the ﬁgure under the curved dashed line is internal to the search\\nengine.\\n19.5 Index size and estimation\\nTo a ﬁrst approximation, comprehensiveness grows with inde x size, although\\nit does matter which speciﬁc pages a search engine indexes – s ome pages are\\nmore informative than others. It is also difﬁcult to reason a bout the fraction\\nof the Web indexed by a search engine, because there is an inﬁn ite number of\\ndynamic web pages; for instance, http://www.yahoo.com/any_string\\nreturns a valid HTML page rather than an error, politely info rming the user\\nthat there is no such page at Yahoo! Such a \"soft 404 error\" is o nly one exam-\\nple of many ways in which web servers can generate an inﬁnite n umber of\\nvalid web pages. Indeed, some of these are malicious spider t raps devised\\nto cause a search engine’s crawler (the component that syste matically gath-\\ners web pages for the search engine’s index, described in Cha pter 20) to stay\\nwithin a spammer’s website and index many pages from that sit e.\\nWe could ask the following better-deﬁned question: given tw o search en-\\ngines, what are the relative sizes of their indexes? Even thi s question turns\\nout to be imprecise, because:\\n1.In response to queries a search engine can return web pages wh ose con-\\ntents it has not (fully or even partially) indexed. For one th ing, search\\nengines generally index only the ﬁrst few thousand words in a web page.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 469}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP434 19 Web search basicsT h e W e b\\nA d i n d e x e s\\nW e b c r a w l e rI n d e x e r\\nI n d e x e s\\nS e a r c h\\nU s e r\\n◮Figure 19.7 The various components of a web search engine.\\nIn some cases, a search engine is aware of a page pthat is linked to by pages\\nit has indexed, but has not indexed pitself. As we will see in Chapter 21,\\nit is still possible to meaningfully return pin search results.\\n2.Search engines generally organize their indexes in various tiers and parti-\\ntions, not all of which are examined on every search (recall t iered indexes\\nfrom Section 7.2.1 ). For instance, a web page deep inside a website may be\\nindexed but not retrieved on general web searches; it is howe ver retrieved\\nas a result on a search that a user has explicitly restricted t o that website\\n(such site-speciﬁc search is offered by most web search engi nes).\\nThus, search engine indexes include multiple classes of ind exed pages, so\\nthat there is no single measure of index size. These issues no twithstanding,\\na number of techniques have been devised for crude estimates of the ratio of\\nthe index sizes of two search engines, E1and E2. The basic hypothesis under-\\nlying these techniques is that each search engine indexes a f raction of the Web\\nchosen independently and uniformly at random. This involve s some ques-\\ntionable assumptions: ﬁrst, that there is a ﬁnite size for th e Web from which\\neach search engine chooses a subset, and second, that each en gine chooses\\nan independent, uniformly chosen subset. As will be clear fr om the discus-\\nsion of crawling in Chapter 20, this is far from true. However, if we begin', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 470}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP19.5 Index size and estimation 435\\nwith these assumptions, then we can invoke a classical estim ation technique\\nknown as the capture-recapture method . CAPTURE -RECAPTURE\\nMETHOD Suppose that we could pick a random page from the index of E1and test\\nwhether it is in E2’s index and symmetrically, test whether a random page\\nfrom E2is in E1. These experiments give us fractions xand ysuch that our\\nestimate is that a fraction xof the pages in E1are in E2, while a fraction yof\\nthe pages in E2are in E1. Then, letting|Ei|denote the size of the index of\\nsearch engine Ei, we have\\nx|E1|≈y|E2|,\\nfrom which we have the form we will use\\n|E1|\\n|E2|≈y\\nx. (19.1)\\nIf our assumption about E1and E2being independent and uniform random\\nsubsets of the Web were true, and our sampling process unbias ed, then Equa-\\ntion ( 19.1) should give us an unbiased estimator for |E1|/|E2|. We distinguish\\nbetween two scenarios here. Either the measurement is perfo rmed by some-\\none with access to the index of one of the search engines (say a n employee of\\nE1), or the measurement is performed by an independent party wi th no ac-\\ncess to the innards of either search engine. In the former cas e, we can simply\\npick a random document from one index. The latter case is more challeng-\\ning; by picking a random page from one search engine from outside the search\\nengine , then verify whether the random page is present in the other s earch\\nengine.\\nTo implement the sampling phase, we might generate a random p age from\\nthe entire (idealized, ﬁnite) Web and test it for presence in each search engine.\\nUnfortunately, picking a web page uniformly at random is a di fﬁcult prob-\\nlem. We brieﬂy outline several attempts to achieve such a sam ple, pointing\\nout the biases inherent to each; following this we describe i n some detail one\\ntechnique that much research has built on.\\n1.Random searches: Begin with a search log of web searches; send a random\\nsearch from this log to E1and a random page from the results. Since such\\nlogs are not widely available outside a search engine, one im plementation\\nis to trap all search queries going out of a work group (say sci entists in a\\nresearch center) that agrees to have all its searches logged . This approach\\nhas a number of issues, including the bias from the types of se arches made\\nby the work group. Further, a random document from the result s of such\\na random search to E1is not the same as a random document from E1.\\n2.Random IP addresses: A second approach is to generate random IP ad-\\ndresses and send a request to a web server residing at the rand om ad-\\ndress, collecting all pages at that server. The biases here i nclude the fact', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 471}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP436 19 Web search basics\\nthat many hosts might share one IP (due to a practice known as v irtual\\nhosting) or not accept http requests from the host where the e xperiment\\nis conducted. Furthermore, this technique is more likely to hit one of the\\nmany sites with few pages, skewing the document probabiliti es; we may\\nbe able to correct for this effect if we understand the distri bution of the\\nnumber of pages on websites.\\n3.Random walks: If the web graph were a strongly connected directed graph,\\nwe could run a random walk starting at an arbitrary web page. T his\\nwalk would converge to a steady state distribution (see Chap ter21, Sec-\\ntion 21.2.1 for more background material on this), from which we could in\\nprinciple pick a web page with a ﬁxed probability. This metho d, too has\\na number of biases. First, the Web is not strongly connected s o that, even\\nwith various corrective rules, it is difﬁcult to argue that w e can reach a\\nsteady state distribution starting from any page. Second, t he time it takes\\nfor the random walk to settle into this steady state is unknow n and could\\nexceed the length of the experiment.\\nClearly each of these approaches is far from perfect. We now d escribe a\\nfourth sampling approach, random queries . This approach is noteworthy for\\ntwo reasons: it has been successfully built upon for a series of increasingly\\nreﬁned estimates, and conversely it has turned out to be the a pproach most\\nlikely to be misinterpreted and carelessly implemented, le ading to mislead-\\ning measurements. The idea is to pick a page (almost) uniform ly at random\\nfrom a search engine’s index by posing a random query to it. It should be\\nclear that picking a set of random terms from (say) Webster’s dictionary is\\nnot a good way of implementing this idea. For one thing, not al l vocabulary\\nterms occur equally often, so this approach will not result i n documents be-\\ning chosen uniformly at random from the search engine. For an other, there\\nare a great many terms in web documents that do not occur in a st andard\\ndictionary such as Webster’s. To address the problem of voca bulary terms\\nnot in a standard dictionary, we begin by amassing a sample we b dictionary.\\nThis could be done by crawling a limited portion of the Web, or by crawling a\\nmanually-assembled representative subset of the Web such a s Yahoo! (as was\\ndone in the earliest experiments with this method). Conside r a conjunctive\\nquery with two or more randomly chosen words from this dictio nary.\\nOperationally, we proceed as follows: we use a random conjun ctive query\\nonE1and pick from the top 100 returned results a page pat random. We\\nthen test pfor presence in E2by choosing 6-8 low-frequency terms in pand\\nusing them in a conjunctive query for E2. We can improve the estimate by\\nrepeating the experiment a large number of times. Both the sa mpling process\\nand the testing process have a number of issues.\\n1.Our sample is biased towards longer documents.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 472}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP19.6 Near-duplicates and shingling 437\\n2.Picking from the top 100 results of E1induces a bias from the ranking\\nalgorithm of E1. Picking from all the results of E1makes the experiment\\nslower. This is particularly so because most web search engi nes put up\\ndefenses against excessive robotic querying.\\n3.During the checking phase, a number of additional biases are introduced:\\nfor instance, E2may not handle 8-word conjunctive queries properly.\\n4.Either E1orE2may refuse to respond to the test queries, treating them as\\nrobotic spam rather than as bona ﬁde queries.\\n5.There could be operational problems like connection time-o uts.\\nA sequence of research has built on this basic paradigm to eli minate some\\nof these issues; there is no perfect solution yet, but the lev el of sophistica-\\ntion in statistics for understanding the biases is increasi ng. The main idea\\nis to address biases by estimating, for each document, the ma gnitude of the\\nbias. From this, standard statistical sampling methods can generate unbi-\\nased samples. In the checking phase, the newer work moves awa y from\\nconjunctive queries to phrase and other queries that appear to be better-\\nbehaved. Finally, newer experiments use other sampling met hods besides\\nrandom queries. The best known of these is document random walk sampling ,\\nin which a document is chosen by a random walk on a virtual grap h de-\\nrived from documents. In this graph, nodes are documents; tw o documents\\nare connected by an edge if they share two or more words in comm on. The\\ngraph is never instantiated; rather, a random walk on it can b e performed by\\nmoving from a document dto another by picking a pair of keywords in d,\\nrunning a query on a search engine and picking a random docume nt from\\nthe results. Details may be found in the references in Sectio n19.7.\\n?Exercise 19.7\\nTwo web search engines A and B each generate a large number of p ages uniformly at\\nrandom from their indexes. 30% of A’s pages are present in B’s index, while 50% of\\nB’s pages are present in A’s index. What is the number of pages in A’s index relative\\nto B’s?\\n19.6 Near-duplicates and shingling\\nOne aspect we have ignored in the discussion of index size in S ection 19.5 is\\nduplication : the Web contains multiple copies of the same content. By som e\\nestimates, as many as 40% of the pages on the Web are duplicate s of other\\npages. Many of these are legitimate copies; for instance, ce rtain information\\nrepositories are mirrored simply to provide redundancy and access reliabil-\\nity. Search engines try to avoid indexing multiple copies of the same content,\\nto keep down storage and processing overheads.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 473}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP438 19 Web search basics\\nThe simplest approach to detecting duplicates is to compute , for each web\\npage, a ﬁngerprint that is a succinct (say 64-bit) digest of the characters on th at\\npage. Then, whenever the ﬁngerprints of two web pages are equ al, we test\\nwhether the pages themselves are equal and if so declare one o f them to be a\\nduplicate copy of the other. This simplistic approach fails to capture a crucial\\nand widespread phenomenon on the Web: near duplication . In many cases,\\nthe contents of one web page are identical to those of another except for a\\nfew characters – say, a notation showing the date and time at w hich the page\\nwas last modiﬁed. Even in such cases, we want to be able to decl are the two\\npages to be close enough that we only index one copy. Short of e xhaustively\\ncomparing all pairs of web pages, an infeasible task at the sc ale of billions of\\npages, how can we detect and ﬁlter out such near duplicates?\\nWe now describe a solution to the problem of detecting near-d uplicate web\\npages. The answer lies in a technique known as shingling . Given a positive SHINGLING\\ninteger kand a sequence of terms in a document d, deﬁne the k-shingles of\\ndto be the set of all consecutive sequences of kterms in d. As an example,\\nconsider the following text: aroseisaroseisarose . The 4-shingles for this text\\n(k=4 is a typical value used in the detection of near-duplicate w eb pages)\\narea rose is a ,rose is a rose andis a rose is . The ﬁrst two of these shingles\\neach occur twice in the text. Intuitively, two documents are near duplicates if\\nthe sets of shingles generated from them are nearly the same. We now make\\nthis intuition precise, then develop a method for efﬁcientl y computing and\\ncomparing the sets of shingles for all web pages.\\nLetS(dj)denote the set of shingles of document dj. Recall the Jaccard\\ncoefﬁcient from page 61, which measures the degree of overlap between\\nthe sets S(d1)and S(d2)as|S(d1)∩S(d2)|/|S(d1)∪S(d2)|; denote this by\\nJ(S(d1),S(d2)). Our test for near duplication between d1and d2is to com-\\npute this Jaccard coefﬁcient; if it exceeds a preset thresho ld (say, 0.9), we\\ndeclare them near duplicates and eliminate one from indexin g. However,\\nthis does not appear to have simpliﬁed matters: we still have to compute\\nJaccard coefﬁcients pairwise.\\nTo avoid this, we use a form of hashing. First, we map every shi ngle into\\na hash value over a large space, say 64 bits. For j=1, 2, let H(dj)be the\\ncorresponding set of 64-bit hash values derived from S(dj). We now invoke\\nthe following trick to detect document pairs whose sets H()have large Jac-\\ncard overlaps. Let πbe a random permutation from the 64-bit integers to the\\n64-bit integers. Denote by Π(dj)the set of permuted hash values in H(dj);\\nthus for each h∈H(dj), there is a corresponding value π(h)∈Π(dj).\\nLetxπ\\njbe the smallest integer in Π(dj). Then\\nTheorem 19.1.\\nJ(S(d1),S(d2)) = P(xπ\\n1=xπ\\n2).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 474}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP19.6 Near-duplicates and shingling 439\\n----\\n----\\n0000\\n0000\\n264−1264−1264−1264−1\\n264−1264−1264−1264−1\\nDocument 1 Document 2H(d1) H(d2)\\nu\\n1u\\n1u\\n2u\\n2u\\n3u\\n3u\\n4u\\n4\\nH(d1)and Π(d1) H(d2)and Π(d2)\\nu u u u u u u u3 3 1 1 4 4 2 2\\n3 3 1 1 4 4 2 2\\n3 3Π(d1) Π(d2)\\nxπ\\n1xπ\\n2\\n◮Figure 19.8 Illustration of shingle sketches. We see two documents goin g through\\nfour stages of shingle sketch computation. In the ﬁrst step ( top row), we apply a 64-bit\\nhash to each shingle from each document to obtain H(d1)and H(d2)(circles). Next,\\nwe apply a random permutation Πto permute H(d1)and H(d2), obtaining Π(d1)\\nand Π(d2)(squares). The third row shows only Π(d1)and Π(d2), while the bottom\\nrow shows the minimum values xπ\\n1and xπ\\n2for each document.\\nProof. We give the proof in a slightly more general setting: conside r a family\\nof sets whose elements are drawn from a common universe. View the sets\\nas columns of a matrix A, with one row for each element in the universe.\\nThe element aij=1 if element iis present in the set Sjthat the jth column\\nrepresents.\\nLetΠbe a random permutation of the rows of A; denote by Π(Sj)the\\ncolumn that results from applying Πto the jth column. Finally, let xπ\\njbe the\\nindex of the ﬁrst row in which the column Π(Sj)has a 1. We then prove that\\nfor any two columns j1,j2,\\nP(xπ\\nj1=xπ\\nj2) = J(Sj1,Sj2).\\nIf we can prove this, the theorem follows.\\nConsider two columns j1,j2as shown in Figure 19.9. The ordered pairs of\\nentries of Sj1and Sj2partition the rows into four types: those with 0’s in both\\nof these columns, those with a 0 in Sj1and a 1 in Sj2, those with a 1 in Sj1\\nand a 0 in Sj2, and ﬁnally those with 1’s in both of these columns. Indeed,\\nthe ﬁrst four rows of Figure 19.9 exemplify all of these four types of rows.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 475}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP440 19 Web search basics\\nSj1Sj2\\n0 1\\n1 0\\n1 1\\n0 0\\n1 1\\n0 1\\n◮Figure 19.9 Two sets Sj1and Sj2; their Jaccard coefﬁcient is 2/5.\\nDenote by C00the number of rows with 0’s in both columns, C01the second,\\nC10the third and C11the fourth. Then,\\nJ(Sj1,Sj2) =C11\\nC01+C10+C11. (19.2)\\nTo complete the proof by showing that the right-hand side of E quation ( 19.2)\\nequals P(xπ\\nj1=xπ\\nj2), consider scanning columns j1,j2in increasing row in-\\ndex until the ﬁrst non-zero entry is found in either column. B ecause Πis a\\nrandom permutation, the probability that this smallest row has a 1 in both\\ncolumns is exactly the right-hand side of Equation ( 19.2).\\nThus, our test for the Jaccard coefﬁcient of the shingle sets is probabilis-\\ntic: we compare the computed values xπ\\nifrom different documents. If a pair\\ncoincides, we have candidate near duplicates. Repeat the pr ocess indepen-\\ndently for 200 random permutations π(a choice suggested in the literature).\\nCall the set of the 200 resulting values of xπ\\nithesketch ψ(di)ofdi. We can\\nthen estimate the Jaccard coefﬁcient for any pair of documen tsdi,djto be\\n|ψi∩ψj|/200; if this exceeds a preset threshold, we declare that diand djare\\nsimilar.\\nHow can we quickly compute |ψi∩ψj|/200 for all pairs i,j? Indeed, how\\ndo we represent all pairs of documents that are similar, with out incurring\\na blowup that is quadratic in the number of documents? First, we use ﬁn-\\ngerprints to remove all but one copy of identical documents. We may also\\nremove common HTML tags and integers from the shingle comput ation, to\\neliminate shingles that occur very commonly in documents wi thout telling\\nus anything about duplication. Next we use a union-ﬁnd algorithm to create\\nclusters that contain documents that are similar. To do this , we must accom-\\nplish a crucial step: going from the set of sketches to the set of pairs i,jsuch\\nthat diand djare similar.\\nTo this end, we compute the number of shingles in common for an y pair of\\ndocuments whose sketches have any members in common. We begi n with\\nthe list <xπ\\ni,di>sorted by xπ\\nipairs. For each xπ\\ni, we can now generate', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 476}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP19.7 References and further reading 441\\nall pairs i,jfor which xπ\\niis present in both their sketches. From these we\\ncan compute, for each pair i,jwith non-zero sketch overlap, a count of the\\nnumber of xπ\\nivalues they have in common. By applying a preset threshold,\\nwe know which pairs i,jhave heavily overlapping sketches. For instance, if\\nthe threshold were 80%, we would need the count to be at least 1 60 for any\\ni,j. As we identify such pairs, we run the union-ﬁnd to group docu ments\\ninto near-duplicate “syntactic clusters”. This is essenti ally a variant of the\\nsingle-link clustering algorithm introduced in Section 17.2 (page 382).\\nOne ﬁnal trick cuts down the space needed in the computation o f|ψi∩\\nψj|/200 for pairs i,j, which in principle could still demand space quadratic\\nin the number of documents. To remove from consideration tho se pairs i,j\\nwhose sketches have few shingles in common, we preprocess th e sketch for\\neach document as follows: sort the xπ\\niin the sketch, then shingle this sorted\\nsequence to generate a set of super-shingles for each document. If two docu-\\nments have a super-shingle in common, we proceed to compute t he precise\\nvalue of|ψi∩ψj|/200. This again is a heuristic but can be highly effective\\nin cutting down the number of i,jpairs for which we accumulate the sketch\\noverlap counts.\\n?Exercise 19.8\\nWeb search engines A and B each crawl a random subset of the sam e size of the Web.\\nSome of the pages crawled are duplicates – exact textual copi es of each other at dif-\\nferent URLs. Assume that duplicates are distributed unifor mly amongst the pages\\ncrawled by A and B. Further, assume that a duplicate is a page t hat has exactly two\\ncopies – no pages have more than two copies. A indexes pages wi thout duplicate\\nelimination whereas B indexes only one copy of each duplicat e page. The two ran-\\ndom subsets have the same size before duplicate elimination . If, 45% of A’s indexed\\nURLs are present in B’s index, while 50% of B’s indexed URLs ar e present in A’s\\nindex, what fraction of the Web consists of pages that do not h ave a duplicate?\\nExercise 19.9\\nInstead of using the process depicted in Figure 19.8, consider instead the following\\nprocess for estimating the Jaccard coefﬁcient of the overla p between two sets S1and\\nS2. We pick a random subset of the elements of the universe from w hich S1and S2\\nare drawn; this corresponds to picking a random subset of the rows of the matrix Ain\\nthe proof. We exhaustively compute the Jaccard coefﬁcient o f these random subsets.\\nWhy is this estimate an unbiased estimator of the Jaccard coe fﬁcient for S1and S2?\\nExercise 19.10\\nExplain why this estimator would be very difﬁcult to use in pr actice.\\n19.7 References and further reading\\nBush (1945 ) foreshadowed the Web when he described an information man-\\nagement system that he called memex .Berners-Lee et al. (1992 ) describes\\none of the earliest incarnations of the Web. Kumar et al. (2000 ) and Broder', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 477}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP442 19 Web search basics\\net al. (2000 ) provide comprehensive studies of the Web as a graph. The use\\nof anchor text was ﬁrst described in McBryan (1994 ). The taxonomy of web\\nqueries in Section 19.4 is due to Broder (2002 ). The observation of the power\\nlaw with exponent 2.1 in Section 19.2.1 appeared in Kumar et al. (1999 ).\\nChakrabarti (2002 ) is a good reference for many aspects of web search and\\nanalysis.\\nThe estimation of web search index sizes has a long history of develop-\\nment covered by Bharat and Broder (1998 ),Lawrence and Giles (1998 ),Rus-\\nmevichientong et al. (2001 ),Lawrence and Giles (1999 ),Henzinger et al. (2000 ),\\nBar-Yossef and Gurevich (2006 ). The state of the art is Bar-Yossef and Gure-\\nvich (2006 ), including several of the bias-removal techniques mentio ned at\\nthe end of Section 19.5. Shingling was introduced by Broder et al. (1997 ) and\\nused for detecting websites (rather than simply pages) that are identical by\\nBharat et al. (2000 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 478}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 443\\n20 Web crawling and indexes\\n20.1 Overview\\nWeb crawling is the process by which we gather pages from the W eb, in\\norder to index them and support a search engine. The objectiv e of crawling\\nis to quickly and efﬁciently gather as many useful web pages a s possible,\\ntogether with the link structure that interconnects them. I n Chapter 19we\\nstudied the complexities of the Web stemming from its creati on by millions of\\nuncoordinated individuals. In this chapter we study the res ulting difﬁculties\\nfor crawling the Web. The focus of this chapter is the compone nt shown in\\nFigure 19.7 asweb crawler ; it is sometimes referred to as a spider . WEB CRAWLER\\nSPIDER The goal of this chapter is not to describe how to build the cra wler for\\na full-scale commercial web search engine. We focus instead on a range of\\nissues that are generic to crawling from the student project scale to substan-\\ntial research projects. We begin (Section 20.1.1 ) by listing desiderata for web\\ncrawlers, and then discuss in Section 20.2 how each of these issues is ad-\\ndressed. The remainder of this chapter describes the archit ecture and some\\nimplementation details for a distributed web crawler that s atisﬁes these fea-\\ntures. Section 20.3 discusses distributing indexes across many machines for\\na web-scale implementation.\\n20.1.1 Features a crawler must provide\\nWe list the desiderata for web crawlers in two categories: fe atures that web\\ncrawlers must provide, followed by features they should provide.\\nRobustness: The Web contains servers that create spider traps , which are gen-\\nerators of web pages that mislead crawlers into getting stuc k fetching an\\ninﬁnite number of pages in a particular domain. Crawlers mus t be de-\\nsigned to be resilient to such traps. Not all such traps are ma licious; some\\nare the inadvertent side-effect of faulty website developm ent.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 479}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP444 20 Web crawling and indexes\\nPoliteness: Web servers have both implicit and explicit policies regula ting\\nthe rate at which a crawler can visit them. These politeness p olicies must\\nbe respected.\\n20.1.2 Features a crawler should provide\\nDistributed: The crawler should have the ability to execute in a distribut ed\\nfashion across multiple machines.\\nScalable: The crawler architecture should permit scaling up the crawl rate\\nby adding extra machines and bandwidth.\\nPerformance and efﬁciency: The crawl system should make efﬁcient use of\\nvarious system resources including processor, storage and network band-\\nwidth.\\nQuality: Given that a signiﬁcant fraction of all web pages are of poor u til-\\nity for serving user query needs, the crawler should be biase d towards\\nfetching “useful” pages ﬁrst.\\nFreshness: In many applications, the crawler should operate in continu ous\\nmode: it should obtain fresh copies of previously fetched pa ges. A search\\nengine crawler, for instance, can thus ensure that the searc h engine’s index\\ncontains a fairly current representation of each indexed we b page. For\\nsuch continuous crawling, a crawler should be able to crawl a page with\\na frequency that approximates the rate of change of that page .\\nExtensible: Crawlers should be designed to be extensible in many ways –\\nto cope with new data formats, new fetch protocols, and so on. This de-\\nmands that the crawler architecture be modular.\\n20.2 Crawling\\nThe basic operation of any hypertext crawler (whether for th e Web, an in-\\ntranet or other hypertext document collection) is as follow s. The crawler\\nbegins with one or more URLs that constitute a seed set . It picks a URL from\\nthis seed set, then fetches the web page at that URL. The fetch ed page is then\\nparsed, to extract both the text and the links from the page (e ach of which\\npoints to another URL). The extracted text is fed to a text ind exer (described\\nin Chapters 4and 5). The extracted links (URLs) are then added to a URL\\nfrontier , which at all times consists of URLs whose corresponding pag es have\\nyet to be fetched by the crawler. Initially, the URL frontier contains the seed\\nset; as pages are fetched, the corresponding URLs are delete d from the URL\\nfrontier. The entire process may be viewed as traversing the web graph (see', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 480}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP20.2 Crawling 445\\nChapter 19). In continuous crawling, the URL of a fetched page is added\\nback to the frontier for fetching again in the future.\\nThis seemingly simple recursive traversal of the web graph i s complicated\\nby the many demands on a practical web crawling system: the cr awler has to\\nbe distributed, scalable, efﬁcient, polite, robust and ext ensible while fetching\\npages of high quality. We examine the effects of each of these issues. Our\\ntreatment follows the design of the Mercator crawler that has formed the ba- MERCATOR\\nsis of a number of research and commercial crawlers. As a refe rence point,\\nfetching a billion pages (a small fraction of the static Web a t present) in a\\nmonth-long crawl requires fetching several hundred pages e ach second. We\\nwill see how to use a multi-threaded design to address severa l bottlenecks in\\nthe overall crawler system in order to attain this fetch rate .\\nBefore proceeding to this detailed description, we reitera te for readers who\\nmay attempt to build crawlers of some basic properties any no n-professional\\ncrawler should satisfy:\\n1.Only one connection should be open to any given host at a time.\\n2.A waiting time of a few seconds should occur between successi ve requests\\nto a host.\\n3.Politeness restrictions detailed in Section 20.2.1 should be obeyed.\\n20.2.1 Crawler architecture\\nThe simple scheme outlined above for crawling demands sever al modules\\nthat ﬁt together as shown in Figure 20.1.\\n1.The URL frontier, containing URLs yet to be fetched in the cur rent crawl\\n(in the case of continuous crawling, a URL may have been fetch ed previ-\\nously but is back in the frontier for re-fetching). We descri be this further\\nin Section 20.2.3 .\\n2.ADNS resolution module that determines the web server from which to\\nfetch the page speciﬁed by a URL. We describe this further in S ection 20.2.2 .\\n3.A fetch module that uses the http protocol to retrieve the web page at a\\nURL.\\n4.A parsing module that extracts the text and set of links from a fetched web\\npage.\\n5.A duplicate elimination module that determines whether an e xtracted\\nlink is already in the URL frontier or has recently been fetch ed.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 481}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP446 20 Web crawling and indexes\\nwww\\nFetchDNS\\nParse\\nURL FrontierContent\\nSeen?\\x13\\n\\x12\\x10\\n\\x11\\x12\\x11Doc\\nFP’s\\x13\\n\\x12\\x10\\n\\x11\\x12\\x11robots\\ntemplates\\x13\\n\\x12\\x10\\n\\x11\\x12\\x11URL\\nset\\nURL\\nFilterDup\\nURL\\nElim-\\x1b\\n-\\n6\\x1b-\\n?6\\n- - -\\n\\x1b6?6?6?\\n◮Figure 20.1 The basic crawler architecture.\\nCrawling is performed by anywhere from one to potentially hu ndreds of\\nthreads, each of which loops through the logical cycle in Fig ure20.1. These\\nthreads may be run in a single process, or be partitioned amon gst multiple\\nprocesses running at different nodes of a distributed syste m. We begin by\\nassuming that the URL frontier is in place and non-empty and d efer our de-\\nscription of the implementation of the URL frontier to Secti on20.2.3 . We\\nfollow the progress of a single URL through the cycle of being fetched, pass-\\ning through various checks and ﬁlters, then ﬁnally (for cont inuous crawling)\\nbeing returned to the URL frontier.\\nA crawler thread begins by taking a URL from the frontier and f etching\\nthe web page at that URL, generally using the http protocol. T he fetched\\npage is then written into a temporary store, where a number of operations\\nare performed on it. Next, the page is parsed and the text as we ll as the\\nlinks in it are extracted. The text (with any tag information – e.g., terms in\\nboldface) is passed on to the indexer. Link information incl uding anchor text\\nis also passed on to the indexer for use in ranking in ways that are described\\nin Chapter 21. In addition, each extracted link goes through a series of te sts\\nto determine whether the link should be added to the URL front ier.\\nFirst, the thread tests whether a web page with the same conte nt has al-\\nready been seen at another URL. The simplest implementation for this would\\nuse a simple ﬁngerprint such as a checksum (placed in a store l abeled \"Doc\\nFP’s\" in Figure 20.1). A more sophisticated test would use shingles instead', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 482}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP20.2 Crawling 447\\nof ﬁngerprints, as described in Chapter 19.\\nNext, a URL ﬁlter is used to determine whether the extracted URL should\\nbe excluded from the frontier based on one of several tests. F or instance, the\\ncrawl may seek to exclude certain domains (say, all .com URLs ) – in this case\\nthe test would simply ﬁlter out the URL if it were from the .com domain.\\nA similar test could be inclusive rather than exclusive. Man y hosts on the\\nWeb place certain portions of their websites off-limits to c rawling, under a\\nstandard known as the Robots Exclusion Protocol . This is done by placing a ROBOTS EXCLUSION\\nPROTOCOL ﬁle with the name robots.txt at the root of the URL hierarchy a t the site. Here\\nis an example robots.txt ﬁle that speciﬁes that no robot shou ld visit any URL\\nwhose position in the ﬁle hierarchy starts with /yoursite/temp/ , except for the\\nrobot called “searchengine”.\\nUser-agent:*\\nDisallow:/yoursite/temp/\\nUser-agent:searchengine\\nDisallow:\\nThe robots.txt ﬁle must be fetched from a website in order to t est whether\\nthe URL under consideration passes the robot restrictions, and can there-\\nfore be added to the URL frontier. Rather than fetch it afresh for testing on\\neach URL to be added to the frontier, a cache can be used to obta in a re-\\ncently fetched copy of the ﬁle for the host. This is especiall y important since\\nmany of the links extracted from a page fall within the host fr om which the\\npage was fetched and therefore can be tested against the host ’s robots.txt\\nﬁle. Thus, by performing the ﬁltering during the link extrac tion process, we\\nwould have especially high locality in the stream of hosts th at we need to test\\nfor robots.txt ﬁles, leading to high cache hit rates. Unfort unately, this runs\\nafoul of webmasters’ politeness expectations. A URL (parti cularly one refer-\\nring to a low-quality or rarely changing document) may be in t he frontier for\\ndays or even weeks. If we were to perform the robots ﬁltering before adding\\nsuch a URL to the frontier, its robots.txt ﬁle could have chan ged by the time\\nthe URL is dequeued from the frontier and fetched. We must con sequently\\nperform robots-ﬁltering immediately before attempting to fetch a web page.\\nAs it turns out, maintaining a cache of robots.txt ﬁles is sti ll highly effective;\\nthere is sufﬁcient locality even in the stream of URLs dequeu ed from the URL\\nfrontier.\\nNext, a URL should be normalized in the following sense: often the HTML URL NORMALIZATION\\nencoding of a link from a web page pindicates the target of that link relative\\nto the page p. Thus, there is a relative link encoded thus in the HTML of the\\npageen.wikipedia.org/wiki/Main_Page :', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 483}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP448 20 Web crawling and indexes\\n<a href=\"/wiki/Wikipedia:General_disclaimer\" title=\"W ikipedia:General\\ndisclaimer\">Disclaimers</a>\\npoints to the URL http://en.wikipedia.org/wiki/Wikipedia:General_disc laimer .\\nFinally, the URL is checked for duplicate elimination: if th e URL is already\\nin the frontier or (in the case of a non-continuous crawl) alr eady crawled,\\nwe do not add it to the frontier. When the URL is added to the fro ntier, it is\\nassigned a priority based on which it is eventually removed f rom the frontier\\nfor fetching. The details of this priority queuing are in Sec tion 20.2.3 .\\nCertain housekeeping tasks are typically performed by a ded icated thread.\\nThis thread is generally quiescent except that it wakes up on ce every few\\nseconds to log crawl progress statistics (URLs crawled, fro ntier size, etc.),\\ndecide whether to terminate the crawl, or (once every few hou rs of crawling)\\ncheckpoint the crawl. In checkpointing, a snapshot of the cr awler’s state (say,\\nthe URL frontier) is committed to disk. In the event of a catas trophic crawler\\nfailure, the crawl is restarted from the most recent checkpo int.\\nDistributing the crawler\\nWe have mentioned that the threads in a crawler could run unde r different\\nprocesses, each at a different node of a distributed crawlin g system. Such\\ndistribution is essential for scaling; it can also be of use i n a geographically\\ndistributed crawler system where each node crawls hosts “ne ar” it. Parti-\\ntioning the hosts being crawled amongst the crawler nodes ca n be done by\\na hash function, or by some more speciﬁcally tailored policy . For instance,\\nwe may locate a crawler node in Europe to focus on European dom ains, al-\\nthough this is not dependable for several reasons – the route s that packets\\ntake through the internet do not always reﬂect geographic pr oximity, and in\\nany case the domain of a host does not always reﬂect its physic al location.\\nHow do the various nodes of a distributed crawler communicat e and share\\nURLs? The idea is to replicate the ﬂow of Figure 20.1 at each node, with one\\nessential difference: following the URL ﬁlter, we use a host splitter to dispatch\\neach surviving URL to the crawler node responsible for the UR L; thus the set\\nof hosts being crawled is partitioned among the nodes. This m odiﬁed ﬂow is\\nshown in Figure 20.2. The output of the host splitter goes into the Duplicate\\nURL Eliminator block of each other node in the distributed sy stem.\\nThe “Content Seen?” module in the distributed architecture of Figure 20.2\\nis, however, complicated by several factors:\\n1.Unlike the URL frontier and the duplicate elimination modul e, document\\nﬁngerprints/shingles cannot be partitioned based on host n ame. There is\\nnothing preventing the same (or highly similar) content fro m appearing\\non different web servers. Consequently, the set of ﬁngerpri nts/shingles\\nmust be partitioned across the nodes based on some property o f the ﬁn-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 484}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP20.2 Crawling 449\\nwww\\nFetchDNS\\nParse\\nURL FrontierContent\\nSeen?\\x13\\n\\x12\\x10\\n\\x11\\x12\\x11Doc\\nFP’s\\x13\\n\\x12\\x10\\n\\x11\\x12\\x11URL\\nset\\nURL\\nFilterHost\\nsplitterTo\\nother\\nnodes\\nFrom\\nother\\nnodesDup\\nURL\\nElim-\\x1b\\n-\\n6\\x1b-\\n?6\\n- - - -\\n\\x1b6?6?666\\n---\\n◮Figure 20.2 Distributing the basic crawl architecture.\\ngerprint/shingle (say by taking the ﬁngerprint modulo the n umber of\\nnodes). The result of this locality-mismatch is that most “C ontent Seen?”\\ntests result in a remote procedure call (although it is possi ble to batch\\nlookup requests).\\n2.There is very little locality in the stream of document ﬁnger prints/shingles.\\nThus, caching popular ﬁngerprints does not help (since ther e are no pop-\\nular ﬁngerprints).\\n3.Documents change over time and so, in the context of continuo us crawl-\\ning, we must be able to delete their outdated ﬁngerprints/sh ingles from\\nthe content-seen set(s). In order to do so, it is necessary to save the ﬁnger-\\nprint/shingle of the document in the URL frontier, along wit h the URL\\nitself.\\n20.2.2 DNS resolution\\nEach web server (and indeed any host connected to the interne t) has a unique\\nIP address : a sequence of four bytes generally represented as four inte gers IPADDRESS\\nseparated by dots; for instance 207.142.131.248 is the nume rical IP address as-\\nsociated with the host www.wikipedia.org. Given a URL such a swww.wikipedia.org\\nin textual form, translating it to an IP address (in this case , 207.142.131.248) is', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 485}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP450 20 Web crawling and indexes\\na process known as DNS resolution or DNS lookup; here DNS stands for Do- DNS RESOLUTION\\nmain Name Service . During DNS resolution, the program that wishes to per-\\nform this translation (in our case, a component of the web cra wler) contacts a\\nDNS server that returns the translated IP address. (In practice the ent ire trans- DNS SERVER\\nlation may not occur at a single DNS server; rather, the DNS se rver contacted\\ninitially may recursively call upon other DNS servers to com plete the transla-\\ntion.) For a more complex URL such as en.wikipedia.org/wiki/Domain_Name_System ,\\nthe crawler component responsible for DNS resolution extra cts the host name\\n– in this case en.wikipedia.org – and looks up the IP address f or the host\\nen.wikipedia.org.\\nDNS resolution is a well-known bottleneck in web crawling. D ue to the\\ndistributed nature of the Domain Name Service, DNS resoluti on may entail\\nmultiple requests and round-trips across the internet, req uiring seconds and\\nsometimes even longer. Right away, this puts in jeopardy our goal of fetching\\nseveral hundred documents a second. A standard remedy is to i ntroduce\\ncaching: URLs for which we have recently performed DNS looku ps are likely\\nto be found in the DNS cache, avoiding the need to go to the DNS s ervers\\non the internet. However, obeying politeness constraints ( see Section 20.2.3 )\\nlimits the of cache hit rate.\\nThere is another important difﬁculty in DNS resolution; the lookup imple-\\nmentations in standard libraries (likely to be used by anyon e developing a\\ncrawler) are generally synchronous. This means that once a r equest is made\\nto the Domain Name Service, other crawler threads at that nod e are blocked\\nuntil the ﬁrst request is completed. To circumvent this, mos t web crawlers\\nimplement their own DNS resolver as a component of the crawle r. Thread\\niexecuting the resolver code sends a message to the DNS server and then\\nperforms a timed wait: it resumes either when being signaled by another\\nthread or when a set time quantum expires. A single, separate DNS thread\\nlistens on the standard DNS port (port 53) for incoming respo nse packets\\nfrom the name service. Upon receiving a response, it signals the appropriate\\ncrawler thread (in this case, i) and hands it the response packet if ihas not\\nyet resumed because its time quantum has expired. A crawler t hread that re-\\nsumes because its wait time quantum has expired retries for a ﬁxed number\\nof attempts, sending out a new message to the DNS server and pe rforming\\na timed wait each time; the designers of Mercator recommend o f the order\\nof ﬁve attempts. The time quantum of the wait increases expon entially with\\neach of these attempts; Mercator started with one second and ended with\\nroughly 90 seconds, in consideration of the fact that there a re host names\\nthat take tens of seconds to resolve.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 486}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP20.2 Crawling 451\\n20.2.3 The URL frontier\\nThe URL frontier at a node is given a URL by its crawl process (o r by the\\nhost splitter of another crawl process). It maintains the UR Ls in the frontier\\nand regurgitates them in some order whenever a crawler threa d seeks a URL.\\nTwo important considerations govern the order in which URLs are returned\\nby the frontier. First, high-quality pages that change freq uently should be\\nprioritized for frequent crawling. Thus, the priority of a p age should be a\\nfunction of both its change rate and its quality (using some r easonable quality\\nestimate). The combination is necessary because a large num ber of spam\\npages change completely on every fetch.\\nThe second consideration is politeness: we must avoid repea ted fetch re-\\nquests to a host within a short time span. The likelihood of th is is exacerbated\\nbecause of a form of locality of reference: many URLs link to o ther URLs at\\nthe same host. As a result, a URL frontier implemented as a sim ple priority\\nqueue might result in a burst of fetch requests to a host. This might occur\\neven if we were to constrain the crawler so that at most one thr ead could\\nfetch from any single host at any time. A common heuristic is t o insert a\\ngap between successive fetch requests to a host that is an ord er of magnitude\\nlarger than the time taken for the most recent fetch from that host.\\nFigure 20.3 shows a polite and prioritizing implementation of a URL fron -\\ntier. Its goals are to ensure that (i) only one connection is o pen at a time to any\\nhost; (ii) a waiting time of a few seconds occurs between succ essive requests\\nto a host and (iii) high-priority pages are crawled preferen tially.\\nThe two major sub-modules are a set of F front queues in the upper por-\\ntion of the ﬁgure, and a set of B back queues in the lower part; all of these are\\nFIFO queues. The front queues implement the prioritization , while the back\\nqueues implement politeness. In the ﬂow of a URL added to the f rontier as\\nit makes its way through the front and back queues, a prioritizer ﬁrst assigns\\nto the URL an integer priority ibetween 1 and Fbased on its fetch history\\n(taking into account the rate at which the web page at this URL has changed\\nbetween previous crawls). For instance, a document that has exhibited fre-\\nquent change would be assigned a higher priority. Other heur istics could be\\napplication-dependent and explicit – for instance, URLs fr om news services\\nmay always be assigned the highest priority. Now that it has b een assigned\\npriority i, the URL is now appended to the ith of the front queues.\\nEach of the Bback queues maintains the following invariants: (i) it is no n-\\nempty while the crawl is in progress and (ii) it only contains URLs from a\\nsingle host1. An auxiliary table T(Figure 20.4) is used to maintain the map-\\nping from hosts to back queues. Whenever a back-queue is empt y and is\\nbeing re-ﬁlled from a front-queue, table Tmust be updated accordingly.\\n1. The number of hosts is assumed to far exceed B.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 487}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP452 20 Web crawling and indexes\\nBack queue\\nselector-\\x1bBiased front queue selector\\nBack queue routerPrioritizer\\nr r r rBback queues\\nSingle host on eachr r r\\nrrFfront queues1 2 F\\n1 2 B\\n?XXXXXXXXXXXXzXXXXXXXXXXXXz\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x189\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x189\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x189XXXXXXXXXXXXXXz\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10)\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10)PPPPPPPPPPPq?\\nHHHHHHHHHHHjHHHHHHHHHHHj\\x08\\x08\\x08\\x08\\x08\\x08\\x08\\x08\\x08\\x08\\x08\\x19\\n@\\n@@\\x00\\n\\x00\\x00Heap\\n◮Figure 20.3 The URL frontier. URLs extracted from already crawled pages ﬂow in\\nat the top of the ﬁgure. A crawl thread requesting a URL extrac ts it from the bottom of\\nthe ﬁgure. En route, a URL ﬂows through one of several front queues that manage its\\npriority for crawling, followed by one of several back queues that manage the crawler’s\\npoliteness.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 488}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP20.2 Crawling 453\\nHost Back queue\\nstanford.edu 23\\nmicrosoft.com 47\\nacm.org 12\\n◮Figure 20.4 Example of an auxiliary hosts-to-back queues table.\\nIn addition, we maintain a heap with one entry for each back qu eue, the\\nentry being the earliest time teat which the host corresponding to that queue\\ncan be contacted again.\\nA crawler thread requesting a URL from the frontier extracts the root of\\nthis heap and (if necessary) waits until the corresponding t ime entry te. It\\nthen takes the URL uat the head of the back queue jcorresponding to the\\nextracted heap root, and proceeds to fetch the URL u. After fetching u, the\\ncalling thread checks whether jis empty. If so, it picks a front queue and\\nextracts from its head a URL v. The choice of front queue is biased (usually\\nby a random process) towards queues of higher priority, ensu ring that URLs\\nof high priority ﬂow more quickly into the back queues. We exa mine vto\\ncheck whether there is already a back queue holding URLs from its host.\\nIf so, vis added to that queue and we reach back to the front queues to\\nﬁnd another candidate URL for insertion into the now-empty q ueue j. This\\nprocess continues until jis non-empty again. In any case, the thread inserts\\na heap entry for jwith a new earliest time tebased on the properties of the\\nURL in jthat was last fetched (such as when its host was last contacte d as\\nwell as the time taken for the last fetch), then continues wit h its processing.\\nFor instance, the new entry tecould be the current time plus ten times the\\nlast fetch time.\\nThe number of front queues, together with the policy of assig ning priori-\\nties and picking queues, determines the priority propertie s we wish to build\\ninto the system. The number of back queues governs the extent to which we\\ncan keep all crawl threads busy while respecting politeness . The designers\\nof Mercator recommend a rough rule of three times as many back queues as\\ncrawler threads.\\nOn a Web-scale crawl, the URL frontier may grow to the point wh ere it\\ndemands more memory at a node than is available. The solution is to let\\nmost of the URL frontier reside on disk. A portion of each queu e is kept in\\nmemory, with more brought in from disk as it is drained in memo ry.\\n?Exercise 20.1\\nWhy is it better to partition hosts (rather than individual U RLs) between the nodes of\\na distributed crawl system?\\nExercise 20.2\\nWhy should the host splitter precede the Duplicate URL Elimi nator?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 489}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP454 20 Web crawling and indexes\\nExercise 20.3 [⋆ ⋆ ⋆ ]\\nIn the preceding discussion we encountered two recommended “hard constants” –\\nthe increment on tebeingten times the last fetch time, and the number of back\\nqueues beingthree times the number of crawl threads. How are these two constant s\\nrelated?\\n20.3 Distributing indexes\\nIn Section 4.4we described distributed indexing. We now consider the dist ri-\\nbution of the index across a large computer cluster2that supports querying.\\nTwo obvious alternative index implementations suggest the mselves: parti- TERM PARTITIONING\\ntioning by terms , also known as global index organization, and partitioning by DOCUMENT\\nPARTITIONING documents , also know as local index organization. In the former, the di ction-\\nary of index terms is partitioned into subsets, each subset r esiding at a node.\\nAlong with the terms at a node, we keep the postings for those t erms. A\\nquery is routed to the nodes corresponding to its query terms . In principle,\\nthis allows greater concurrency since a stream of queries wi th different query\\nterms would hit different sets of machines.\\nIn practice, partitioning indexes by vocabulary terms turn s out to be non-\\ntrivial. Multi-word queries require the sending of long pos tings lists between\\nsets of nodes for merging, and the cost of this can outweigh th e greater con-\\ncurrency. Load balancing the partition is governed not by an a priori analysis\\nof relative term frequencies, but rather by the distributio n of query terms\\nand their co-occurrences, which can drift with time or exhib it sudden bursts.\\nAchieving good partitions is a function of the co-occurrenc es of query terms\\nand entails the clustering of terms to optimize objectives t hat are not easy to\\nquantify. Finally, this strategy makes implementation of d ynamic indexing\\nmore difﬁcult.\\nA more common implementation is to partition by documents: e ach node\\ncontains the index for a subset of all documents. Each query i s distributed to\\nall nodes, with the results from various nodes being merged b efore presenta-\\ntion to the user. This strategy trades more local disk seeks f or less inter-node\\ncommunication. One difﬁculty in this approach is that globa l statistics used\\nin scoring – such as idf – must be computed across the entire do cument col-\\nlection even though the index at any single node only contain s a subset of\\nthe documents. These are computed by distributed “backgrou nd” processes\\nthat periodically refresh the node indexes with fresh globa l statistics.\\nHow do we decide the partition of documents to nodes? Based on our de-\\nvelopment of the crawler architecture in Section 20.2.1 , one simple approach\\nwould be to assign all pages from a host to a single node. This p artitioning\\n2. Please note the different usage of “clusters” elsewhere i n this book, in the sense of Chapters\\n16and 17.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 490}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP20.4 Connectivity servers 455\\ncould follow the partitioning of hosts to crawler nodes. A da nger of such\\npartitioning is that on many queries, a preponderance of the results would\\ncome from documents at a small number of hosts (and hence a sma ll number\\nof index nodes).\\nA hash of each URL into the space of index nodes results in a mor e uni-\\nform distribution of query-time computation across nodes. At query time,\\nthe query is broadcast to each of the nodes, with the top kresults from each\\nnode being merged to ﬁnd the top kdocuments for the query. A common\\nimplementation heuristic is to partition the document coll ection into indexes\\nof documents that are more likely to score highly on most quer ies (using,\\nfor instance, techniques in Chapter 21) and low-scoring indexes with the re-\\nmaining documents. We only search the low-scoring indexes w hen there are\\ntoo few matches in the high-scoring indexes, as described in Section 7.2.1 .\\n20.4 Connectivity servers\\nFor reasons to become clearer in Chapter 21, web search engines require a\\nconnectivity server that supports fast connectivity queries on the web graph. CONNECTIVITY SERVER\\nCONNECTIVITY\\nQUERIESTypical connectivity queries are which URLs link to a given URL? and which\\nURLs does a given URL link to? To this end, we wish to store mappings in\\nmemory from URL to out-links, and from URL to in-links. Appli cations in-\\nclude crawl control, web graph analysis, sophisticated cra wl optimization\\nand link analysis (to be covered in Chapter 21).\\nSuppose that the Web had four billion pages, each with ten lin ks to other\\npages. In the simplest form, we would require 32 bits or 4 byte s to specify\\neach end (source and destination) of each link, requiring a t otal of\\n4×109×10×8=3.2×1011\\nbytes of memory. Some basic properties of the web graph can be exploited to\\nuse well under 10% of this memory requirement. At ﬁrst sight, we appear to\\nhave a data compression problem – which is amenable to a varie ty of stan-\\ndard solutions. However, our goal is not to simply compress t he web graph\\nto ﬁt into memory; we must do so in a way that efﬁciently suppor ts connec-\\ntivity queries; this challenge is reminiscent of index comp ression (Chapter 5).\\nWe assume that each web page is represented by a unique intege r; the\\nspeciﬁc scheme used to assign these integers is described be low. We build\\nanadjacency table that resembles an inverted index: it has a row for each web\\npage, with the rows ordered by the corresponding integers. T he row for any\\npage pcontains a sorted list of integers, each corresponding to a w eb page\\nthat links to p. This table permits us to respond to queries of the form which\\npages link to p? In similar fashion we build a table whose entries are the page s\\nlinked to by p.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 491}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP456 20 Web crawling and indexes\\n1:www.stanford.edu/alchemy\\n2:www.stanford.edu/biology\\n3:www.stanford.edu/biology/plant\\n4:www.stanford.edu/biology/plant/copyright\\n5:www.stanford.edu/biology/plant/people\\n6:www.stanford.edu/chemistry\\n◮Figure 20.5 A lexicographically ordered set of URLs.\\nThis table representation cuts the space taken by the naive r epresentation\\n(in which we explicitly represent each link by its two end poi nts, each a 32-bit\\ninteger) by 50%. Our description below will focus on the tabl e for the links\\nfrom each page; it should be clear that the techniques apply just a s well to\\nthe table of links to each page. To further reduce the storage for the table, we\\nexploit several ideas:\\n1.Similarity between lists: Many rows of the table have many en tries in\\ncommon. Thus, if we explicitly represent a prototype row for several\\nsimilar rows, the remainder can be succinctly expressed in t erms of the\\nprototypical row.\\n2.Locality: many links from a page go to “nearby” pages – pages o n the\\nsame host, for instance. This suggests that in encoding the d estination of\\na link, we can often use small integers and thereby save space .\\n3.We use gap encodings in sorted lists: rather than store the de stination of\\neach link, we store the offset from the previous entry in the r ow.\\nWe now develop each of these techniques.\\nIn a lexicographic ordering of all URLs, we treat each URL as an alphanu-\\nmeric string and sort these strings. Figure 20.5 shows a segment of this sorted\\norder. For a true lexicographic sort of web pages, the domain name part of\\nthe URL should be inverted, so that www.stanford.edu becomes edu.stanford.www ,\\nbut this is not necessary here since we are mainly concerned w ith links local\\nto a single host.\\nTo each URL, we assign its position in this ordering as the uni que identi-\\nfying integer. Figure 20.6 shows an example of such a numbering and the\\nresulting table. In this example sequence, www.stanford.edu/biology\\nis assigned the integer 2 since it is second in the sequence.\\nWe next exploit a property that stems from the way most websit es are\\nstructured to get similarity and locality. Most websites ha ve a template with\\na set of links from each page in the site to a ﬁxed set of pages on the site (such', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 492}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP20.4 Connectivity servers 457\\n1:1, 2,4,8, 16,32,64\\n2:1, 4,9,16, 25,36,49,64\\n3:1, 2,3,5, 8,13, 21,34,55,89, 144\\n4:1, 4,8,16, 25,36,49,64\\n◮Figure 20.6 A four-row segment of the table of links.\\nas its copyright notice, terms of use, and so on). In this case , the rows cor-\\nresponding to pages in a website will have many table entries in common.\\nMoreover, under the lexicographic ordering of URLs, it is ve ry likely that the\\npages from a website appear as contiguous rows in the table.\\nWe adopt the following strategy: we walk down the table, enco ding each\\ntable row in terms of the seven preceding rows. In the example of Figure 20.6,\\nwe could encode the fourth row as “the same as the row at offset 2 (mean-\\ning, two rows earlier in the table), with 9 replaced by 8”. Thi s requires the\\nspeciﬁcation of the offset, the integer(s) dropped (in this case 9) and the in-\\nteger(s) added (in this case 8). The use of only the seven prec eding rows has\\ntwo advantages: (i) the offset can be expressed with only 3 bi ts; this choice\\nis optimized empirically (the reason for seven and not eight preceding rows\\nis the subject of Exercise 20.4) and (ii) ﬁxing the maximum offset to a small\\nvalue like seven avoids having to perform an expensive searc h among many\\ncandidate prototypes in terms of which to express the curren t row.\\nWhat if none of the preceding seven rows is a good prototype fo r express-\\ning the current row? This would happen, for instance, at each boundary\\nbetween different websites as we walk down the rows of the tab le. In this\\ncase we simply express the row as starting from the empty set a nd “adding\\nin” each integer in that row. By using gap encodings to store t he gaps (rather\\nthan the actual integers) in each row, and encoding these gap s tightly based\\non the distribution of their values, we obtain further space reduction. In ex-\\nperiments mentioned in Section 20.5, the series of techniques outlined here\\nappears to use as few as 3 bits per link, on average – a dramatic reduction\\nfrom the 64 required in the naive representation.\\nWhile these ideas give us a representation of sizable web gra phs that com-\\nfortably ﬁt in memory, we still need to support connectivity queries. What\\nis entailed in retrieving from this representation the set o f links from a page?\\nFirst, we need an index lookup from (a hash of) the URL to its ro w number\\nin the table. Next, we need to reconstruct these entries, whi ch may be en-\\ncoded in terms of entries in other rows. This entails followi ng the offsets to\\nreconstruct these other rows – a process that in principle co uld lead through\\nmany levels of indirection. In practice however, this does n ot happen very\\noften. A heuristic for controlling this can be introduced in to the construc-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 493}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP458 20 Web crawling and indexes\\ntion of the table: when examining the preceding seven rows as candidates\\nfrom which to model the current row, we demand a threshold of s imilarity\\nbetween the current row and the candidate prototype. This th reshold must\\nbe chosen with care. If the threshold is set too high, we seldo m use proto-\\ntypes and express many rows afresh. If the threshold is too lo w, most rows\\nget expressed in terms of prototypes, so that at query time th e reconstruction\\nof a row leads to many levels of indirection through precedin g prototypes.\\n?Exercise 20.4\\nWe noted that expressing a row in terms of one of seven precedi ng rows allowed us\\nto use no more than three bits to specify which of the precedin g rows we are using\\nas prototype. Why seven and not eight preceding rows? (Hint: consider the case when\\nnone of the preceding seven rows is a good prototype.)\\nExercise 20.5\\nWe noted that for the scheme in Section 20.4, decoding the links incident on a URL\\ncould result in many levels of indirection. Construct an exa mple in which the number\\nof levels of indirection grows linearly with the number of UR Ls.\\n20.5 References and further reading\\nThe ﬁrst web crawler appears to be Matthew Gray’s Wanderer, w ritten in the\\nspring of 1993. The Mercator crawler is due to Najork and Heyd on (Najork\\nand Heydon 2001 ;2002 ); the treatment in this chapter follows their work.\\nOther classic early descriptions of web crawling include Burner (1997 ),Brin\\nand Page (1998 ),Cho et al. (1998 ) and the creators of the Webbase system\\nat Stanford ( Hirai et al. 2000 ).Cho and Garcia-Molina (2002 ) give a taxon-\\nomy and comparative study of different modes of communicati on between\\nthe nodes of a distributed crawler. The Robots Exclusion Pro tocol standard\\nis described at http://www.robotstxt.org/wc/exclusion.html .Boldi et al. (2002 ) and\\nShkapenyuk and Suel (2002 ) provide more recent details of implementing\\nlarge-scale distributed web crawlers.\\nOur discussion of DNS resolution (Section 20.2.2 ) uses the current conven-\\ntion for internet addresses, known as IPv4 (for Internet Pro tocol version 4) –\\neach IP address is a sequence of four bytes. In the future, the convention for\\naddresses (collectively known as the internet address space ) is likely to use a\\nnew standard known as IPv6 ( http://www.ipv6.org/ ).\\nTomasic and Garcia-Molina (1993 ) and Jeong and Omiecinski (1995 ) are\\nkey early papers evaluating term partitioning versus docum ent partitioning\\nfor distributed indexes. Document partitioning is found to be superior, at\\nleast when the distribution of terms is skewed, as it typical ly is in practice.\\nThis result has generally been conﬁrmed in more recent work ( MacFarlane\\net al. 2000 ). But the outcome depends on the details of the distributed s ystem;', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 494}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP20.5 References and further reading 459\\nat least one thread of work has reached the opposite conclusi on (Ribeiro-\\nNeto and Barbosa 1998 ,Badue et al. 2001 ).Sornil (2001 ) argues for a par-\\ntitioning scheme that is a hybrid between term and document p artitioning.\\nBarroso et al. (2003 ) describe the distribution methods used at Google. The\\nﬁrst implementation of a connectivity server was described byBharat et al.\\n(1998 ). The scheme discussed in this chapter, currently believed to be the\\nbest published scheme (achieving as few as 3 bits per link for encoding), is\\ndescribed in a series of papers by Boldi and Vigna (2004a ;b).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 495}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 496}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 461\\n21 Link analysis\\nThe analysis of hyperlinks and the graph structure of the Web has been in-\\nstrumental in the development of web search. In this chapter we focus on the\\nuse of hyperlinks for ranking web search results. Such link a nalysis is one\\nof many factors considered by web search engines in computin g a compos-\\nite score for a web page on any given query. We begin by reviewi ng some\\nbasics of the Web as a graph in Section 21.1, then proceed to the technical\\ndevelopment of the elements of link analysis for ranking.\\nLink analysis for web search has intellectual antecedents i n the ﬁeld of cita-\\ntion analysis, aspects of which overlap with an area known as bibliometrics.\\nThese disciplines seek to quantify the inﬂuence of scholarl y articles by ana-\\nlyzing the pattern of citations amongst them. Much as citati ons represent the\\nconferral of authority from a scholarly article to others, l ink analysis on the\\nWeb treats hyperlinks from a web page to another as a conferra l of authority.\\nClearly, not every citation or hyperlink implies such autho rity conferral; for\\nthis reason, simply measuring the quality of a web page by the number of\\nin-links (citations from other pages) is not robust enough. For instance, one\\nmay contrive to set up multiple web pages pointing to a target web page,\\nwith the intent of artiﬁcially boosting the latter’s tally o f in-links. This phe-\\nnomenon is referred to as link spam. Nevertheless, the pheno menon of ci-\\ntation is prevalent and dependable enough that it is feasibl e for web search\\nengines to derive useful signals for ranking from more sophi sticated link\\nanalysis. Link analysis also proves to be a useful indicator of what page(s)\\nto crawl next while crawling the web; this is done by using lin k analysis to\\nguide the priority assignment in the front queues of Chapter 20.\\nSection 21.1 develops the basic ideas underlying the use of the web graph\\nin link analysis. Sections 21.2 and 21.3 then develop two distinct methods for\\nlink analysis, PageRank and HITS.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 497}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP462 21 Link analysis\\n21.1 The Web as a graph\\nRecall the notion of the web graph from Section 19.2.1 and particularly Fig-\\nure19.2. Our study of link analysis builds on two intuitions:\\n1.The anchor text pointing to page B is a good description of pag e B.\\n2.The hyperlink from A to B represents an endorsement of page B, by the\\ncreator of page A. This is not always the case; for instance, m any links\\namongst pages within a single website stem from the user of a c ommon\\ntemplate. For instance, most corporate websites have a poin ter from ev-\\nery page to a page containing a copyright notice – this is clea rly not an\\nendorsement. Accordingly, implementations of link analys is algorithms\\nwill typical discount such “internal” links.\\n21.1.1 Anchor text and the web graph\\nThe following fragment of HTML code from a web page shows a hyp erlink\\npointing to the home page of the Journal of the ACM:\\n<ahref=\"http://www.acm.org/jacm/\">Journalof theACM.< /a>\\nIn this case, the link points to the page http://www.acm.org/jacm/ and\\nthe anchor text is Journal of the ACM. Clearly, in this example the anchor is de-\\nscriptive of the target page. But then the target page (B = http://www.acm.org/jacm/ )\\nitself contains the same description as well as considerabl e additional infor-\\nmation on the journal. So what use is the anchor text?\\nThe Web is full of instances where the page B does not provide a n accu-\\nrate description of itself. In many cases this is a matter of h ow the publish-\\ners of page B choose to present themselves; this is especiall y common with\\ncorporate web pages, where a web presence is a marketing stat ement. For\\nexample, at the time of the writing of this book the home page o f the IBM\\ncorporation (http://www.ibm.com ) did not contain the term computer any-\\nwhere in its HTML code, despite the fact that IBM is widely vie wed as the\\nworld’s largest computer maker. Similarly, the HTML code fo r the home\\npage of Yahoo! (http://www.yahoo.com ) does not at this time contain the\\nwordportal .\\nThus, there is often a gap between the terms in a web page, and h ow web\\nusers would describe that web page. Consequently, web searc hers need not\\nuse the terms in a page to query for it. In addition, many web pa ges are rich\\nin graphics and images, and/or embed their text in these imag es; in such\\ncases, the HTML parsing performed when crawling will not ext ract text that\\nis useful for indexing these pages. The “standard IR” approa ch to this would\\nbe to use the methods outlined in Chapter 9and Section 12.4. The insight', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 498}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP21.1 The Web as a graph 463\\nbehind anchor text is that such methods can be supplanted by a nchor text,\\nthereby tapping the power of the community of web page author s.\\nThe fact that the anchors of many hyperlinks pointing to http://www.ibm.com\\ninclude the word computer can be exploited by web search engines. For in-\\nstance, the anchor text terms can be included as terms under w hich to index\\nthe target web page. Thus, the postings for the term computer would include\\nthe documenthttp://www.ibm.com and that for the term portal would in-\\nclude the document http://www.yahoo.com , using a special indicator to\\nshow that these terms occur as anchor (rather than in-page) t ext. As with\\nin-page terms, anchor text terms are generally weighted bas ed on frequency,\\nwith a penalty for terms that occur very often (the most commo n terms in an-\\nchor text across the Web are Click andhere, using methods very similar to idf).\\nThe actual weighting of terms is determined by machine-lear ned scoring, as\\nin Section 15.4.1 ; current web search engines appear to assign a substantial\\nweighting to anchor text terms.\\nThe use of anchor text has some interesting side-effects. Se arching for big\\nblue on most web search engines returns the home page of the IBM cor pora-\\ntion as the top hit; this is consistent with the popular nickn ame that many\\npeople use to refer to IBM. On the other hand, there have been ( and con-\\ntinue to be) many instances where derogatory anchor text suc h asevil empire\\nleads to somewhat unexpected results on querying for these t erms on web\\nsearch engines. This phenomenon has been exploited in orche strated cam-\\npaigns against speciﬁc sites. Such orchestrated anchor tex t may be a form\\nof spamming, since a website can create misleading anchor te xt pointing to\\nitself, to boost its ranking on selected query terms. Detect ing and combating\\nsuch systematic abuse of anchor text is another form of spam d etection that\\nweb search engines perform.\\nThe window of text surrounding anchor text (sometimes refer red to as ex-\\ntended anchor text ) is often usable in the same manner as anchor text itself;\\nconsider for instance the fragment of web text thereis gooddiscussion\\nofvedicscripture<a>here</a> . This has been considered in a num-\\nber of settings and the useful width of this window has been st udied; see\\nSection 21.4 for references.\\n?Exercise 21.1\\nIs it always possible to follow directed edges (hyperlinks) in the web graph from any\\nnode (web page) to any other? Why or why not?\\nExercise 21.2\\nFind an instance of misleading anchor-text on the Web.\\nExercise 21.3\\nGiven the collection of anchor-text phrases for a web page x, suggest a heuristic for\\nchoosing one term or phrase from this collection that is most descriptive of x.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 499}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP464 21 Link analysis\\n\\x12\\x11\\x13\\x10\\nA\\n\\x12\\x11\\x13\\x10C\\x12\\x11\\x13\\x10\\nB\\n\\x12\\x11\\x13\\x10D-\\x00\\x00\\x12\\n@\\n@R\\n◮Figure 21.1 The random surfer at node A proceeds with probability 1/3 to e ach\\nof B, C and D.\\nExercise 21.4\\nDoes your heuristic in the previous exercise take into accou nt a single domain D\\nrepeating anchor text for xfrom multiple pages in D?\\n21.2 PageRank\\nWe now focus on scoring and ranking measures derived from the link struc-\\nture alone. Our ﬁrst technique for link analysis assigns to e very node in\\nthe web graph a numerical score between 0 and 1, known as its PageRank . PAGE RANK\\nThe PageRank of a node will depend on the link structure of the web graph.\\nGiven a query, a web search engine computes a composite score for each\\nweb page that combines hundreds of features such as cosine si milarity (Sec-\\ntion 6.3) and term proximity (Section 7.2.2 ), together with the PageRank score.\\nThis composite score, developed using the methods of Sectio n15.4.1 , is used\\nto provide a ranked list of results for the query.\\nConsider a random surfer who begins at a web page (a node of the web\\ngraph) and executes a random walk on the Web as follows. At eac h time\\nstep, the surfer proceeds from his current page A to a randoml y chosen web\\npage that A hyperlinks to. Figure 21.1 shows the surfer at a node A, out of\\nwhich there are three hyperlinks to nodes B, C and D; the surfe r proceeds at\\nthe next time step to one of these three nodes, with equal prob abilities 1/3.\\nAs the surfer proceeds in this random walk from node to node, h e visits\\nsome nodes more often than others; intuitively, these are no des with many\\nlinks coming in from other frequently visited nodes. The ide a behind Page-\\nRank is that pages visited more often in this walk are more imp ortant.\\nWhat if the current location of the surfer, the node A, has no o ut-links?\\nTo address this we introduce an additional operation for our random surfer:\\ntheteleport operation. In the teleport operation the surfer jumps from a node TELEPORT\\nto any other node in the web graph. This could happen because h e types', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 500}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP21.2 PageRank 465\\nan address into the URL bar of his browser. The destination of a teleport\\noperation is modeled as being chosen uniformly at random fro m all web\\npages. In other words, if Nis the total number of nodes in the web graph1,\\nthe teleport operation takes the surfer to each node with pro bability 1/ N.\\nThe surfer would also teleport to his present position with p robability 1/ N.\\nIn assigning a PageRank score to each node of the web graph, we use the\\nteleport operation in two ways: (1) When at a node with no out- links, the\\nsurfer invokes the teleport operation. (2) At any node that h as outgoing links,\\nthe surfer invokes the teleport operation with probability 0<α<1 and the\\nstandard random walk (follow an out-link chosen uniformly a t random as in\\nFigure 21.1) with probability 1 −α, where αis a ﬁxed parameter chosen in\\nadvance. Typically, αmight be 0.1.\\nIn Section 21.2.1 , we will use the theory of Markov chains to argue that\\nwhen the surfer follows this combined process (random walk p lus teleport)\\nhe visits each node vof the web graph a ﬁxed fraction of the time π(v)that\\ndepends on (1) the structure of the web graph and (2) the value ofα. We call\\nthis value π(v)the PageRank of vand will show how to compute this value\\nin Section 21.2.2 .\\n21.2.1 Markov chains\\nA Markov chain is a discrete-time stochastic process: a process that occurs in\\na series of time-steps in each of which a random choice is made . A Markov\\nchain consists of N states . Each web page will correspond to a state in the\\nMarkov chain we will formulate.\\nA Markov chain is characterized by an N×N transition probability matrix P\\neach of whose entries is in the interval [0, 1]; the entries in each row of Padd\\nup to 1. The Markov chain can be in one of the Nstates at any given time-\\nstep; then, the entry Pijtells us the probability that the state at the next time-\\nstep is j, conditioned on the current state being i. Each entry Pijis known as a\\ntransition probability and depends only on the current stat ei; this is known\\nas the Markov property. Thus, by the Markov property,\\n∀i,j,Pij∈[0, 1]\\nand\\n∀i,N\\n∑\\nj=1Pij=1. (21.1)\\nA matrix with non-negative entries that satisﬁes Equation ( 21.1) is known\\nas a stochastic matrix . A key property of a stochastic matrix is that it has a STOCHASTIC MATRIX\\nprincipal left eigenvector corresponding to its largest eigenvalue, which is 1. PRINCIPAL LEFT\\nEIGENVECTOR\\n1. This is consistent with our usage of Nfor the number of documents in the collection.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 501}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP466 21 Link analysis\\n\\x16\\x15\\x17\\x14\\nA\\x16\\x15\\x17\\x14\\nB\\x16\\x15\\x17\\x14\\nC-1-0.5\\n\\x1b\\n0.5\\x1b\\n1\\n◮Figure 21.2 A simple Markov chain with three states; the numbers on the li nks\\nindicate the transition probabilities.\\nIn a Markov chain, the probability distribution of next stat es for a Markov\\nchain depends only on the current state, and not on how the Mar kov chain\\narrived at the current state. Figure 21.2 shows a simple Markov chain with\\nthree states. From the middle state A, we proceed with (equal ) probabilities\\nof 0.5 to either B or C. From either B or C, we proceed with proba bility 1 to\\nA. The transition probability matrix of this Markov chain is then\\n\\uf8eb\\n\\uf8ed0 0.5 0.5\\n1 0 0\\n1 0 0\\uf8f6\\n\\uf8f8\\nA Markov chain’s probability distribution over its states m ay be viewed as\\naprobability vector : a vector all of whose entries are in the interval [0, 1], and PROBABILITY VECTOR\\nthe entries add up to 1. An N-dimensional probability vector each of whose\\ncomponents corresponds to one of the Nstates of a Markov chain can be\\nviewed as a probability distribution over its states. For ou r simple Markov\\nchain of Figure 21.2, the probability vector would have 3 components that\\nsum to 1.\\nWe can view a random surfer on the web graph as a Markov chain, w ith\\none state for each web page, and each transition probability representing the\\nprobability of moving from one web page to another. The telep ort operation\\ncontributes to these transition probabilities. The adjace ncy matrix Aof the\\nweb graph is deﬁned as follows: if there is a hyperlink from pa geito page\\nj, then Aij=1, otherwise Aij=0. We can readily derive the transition\\nprobability matrix Pfor our Markov chain from the N×Nmatrix A:\\n1.If a row of Ahas no 1’s, then replace each element by 1/N. For all other\\nrows proceed as follows.\\n2.Divide each 1 in Aby the number of 1’s in its row. Thus, if there is a row\\nwith three 1’s, then each of them is replaced by 1/3.\\n3.Multiply the resulting matrix by 1 −α.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 502}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP21.2 PageRank 467\\n4.Add α/Nto every entry of the resulting matrix, to obtain P.\\nWe can depict the probability distribution of the surfer’s p osition at any\\ntime by a probability vector ⃗x. At t=0 the surfer may begin at a state whose\\ncorresponding entry in ⃗xis 1 while all others are zero. By deﬁnition, the\\nsurfer’s distribution at t=1 is given by the probability vector ⃗xP; att=2\\nby(⃗xP)P=⃗xP2, and so on. We will detail this process in Section 21.2.2 . We\\ncan thus compute the surfer’s distribution over the states a t any time, given\\nonly the initial distribution and the transition probabili ty matrix P.\\nIf a Markov chain is allowed to run for many time steps, each st ate is vis-\\nited at a (different) frequency that depends on the structur e of the Markov\\nchain. In our running analogy, the surfer visits certain web pages (say, pop-\\nular news home pages) more often than other pages. We now make this in-\\ntuition precise, establishing conditions under which such the visit frequency\\nconverges to ﬁxed, steady-state quantity. Following this, we set the Page-\\nRank of each node vto this steady-state visit frequency and show how it can\\nbe computed.\\nDeﬁnition: A Markov chain is said to be ergodic if there exists a positive ERGODIC MARKOV\\nCHAIN integer T0such that for all pairs of states i,jin the Markov chain, if it is\\nstarted at time 0 in state ithen for all t>T0, the probability of being in state\\njat time tis greater than 0.\\nFor a Markov chain to be ergodic, two technical conditions ar e required\\nof its states and the non-zero transition probabilities; th ese conditions are\\nknown as irreducibility and aperiodicity . Informally, the ﬁrst ensures that there\\nis a sequence of transitions of non-zero probability from an y state to any\\nother, while the latter ensures that the states are not parti tioned into sets\\nsuch that all state transitions occur cyclically from one se t to another.\\nTheorem 21.1. For any ergodic Markov chain, there is a unique steady-state prob- STEADY -STATE\\nability vector ⃗πthat is the principal left eigenvector of P, such that if η(i,t)is the\\nnumber of visits to state i in t steps, then\\nlim\\nt→∞η(i,t)\\nt=π(i),\\nwhere π(i)>0is the steady-state probability for state i.\\nIt follows from Theorem 21.1 that the random walk with teleporting re-\\nsults in a unique distribution of steady-state probabiliti es over the states of\\nthe induced Markov chain. This steady-state probability fo r a state is the\\nPageRank of the corresponding web page.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 503}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP468 21 Link analysis\\n21.2.2 The PageRank computation\\nHow do we compute PageRank values? Recall the deﬁnition of a l eft eigen-\\nvector from Equation 18.2; the left eigenvectors of the transition probability\\nmatrix PareN-vectors ⃗πsuch that\\n⃗πP=λ⃗π. (21.2)\\nThe Nentries in the principal eigenvector ⃗πare the steady-state proba-\\nbilities of the random walk with teleporting, and thus the Pa geRank values\\nfor the corresponding web pages. We may interpret Equation ( 21.2) as fol-\\nlows: if ⃗πis the probability distribution of the surfer across the web pages,\\nhe remains in the steady-state distribution ⃗π. Given that ⃗πis the steady-state\\ndistribution, we have that πP=1π, so 1 is an eigenvalue of P . Thus if we\\nwere to compute the principal left eigenvector of the matrix P— the one with\\neigenvalue 1 — we would have computed the PageRank values.\\nThere are many algorithms available for computing left eige nvectors; the\\nreferences at the end of Chapter 18and the present chapter are a guide to\\nthese. We give here a rather elementary method, sometimes kn own as power\\niteration . If⃗xis the initial distribution over the states, then the distri bution at\\ntime tis⃗xPt. As tgrows large, we would expect that the distribution ⃗xPt2\\nis very similar to the distribution ⃗xPt+1, since for large twe would expect\\nthe Markov chain to attain its steady state. By Theorem 21.1 this is indepen-\\ndent of the initial distribution ⃗x. The power iteration method simulates the\\nsurfer’s walk: begin at a state and run the walk for a large num ber of steps\\nt, keeping track of the visit frequencies for each of the state s. After a large\\nnumber of steps t, these frequencies “settle down” so that the variation in th e\\ncomputed frequencies is below some predetermined threshol d. We declare\\nthese tabulated frequencies to be the PageRank values.\\nWe consider the web graph in Exercise 21.6 with α=0.5. The transition\\nprobability matrix of the surfer’s walk with teleportation is then\\nP=\\uf8eb\\n\\uf8ed1/6 2/3 1/6\\n5/12 1/6 5/12\\n1/6 2/3 1/6\\uf8f6\\n\\uf8f8. (21.3)\\nImagine that the surfer starts in state 1, corresponding to t he initial proba-\\nbility distribution vector ⃗x0= (1 0 0). Then, after one step the distribution\\nis\\n⃗x0P=(\\n1/6 2/3 1/6)=⃗x1. (21.4)\\n2. Note that Ptrepresents Praised to the tth power, not the transpose of Pwhich is denoted PT.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 504}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP21.2 PageRank 469\\n⃗x0 1 0 0\\n⃗x1 1/6 2/3 1/6\\n⃗x2 1/3 1/3 1/3\\n⃗x3 1/4 1/2 1/4\\n⃗x4 7/24 5/12 7/24\\n. . .·········\\n⃗x 5/18 4/9 5/18\\n◮Figure 21.3 The sequence of probability vectors.\\nAfter two steps it is\\n⃗x1P=(1/6 2/3 1/6)\\uf8eb\\n\\uf8ed1/6 2/3 1/6\\n5/12 1/6 5/12\\n1/6 2/3 1/6\\uf8f6\\n\\uf8f8=(1/3 1/3 1/3)=⃗x2. (21.5)\\nContinuing in this fashion gives a sequence of probability v ectors as shown\\nin Figure 21.3.\\nContinuing for several steps, we see that the distribution c onverges to the\\nsteady state of ⃗x= (5/18 4/9 5/18 ). In this simple example, we may\\ndirectly calculate this steady-state probability distrib ution by observing the\\nsymmetry of the Markov chain: states 1 and 3 are symmetric, as evident from\\nthe fact that the ﬁrst and third rows of the transition probab ility matrix in\\nEquation ( 21.3) are identical. Postulating, then, that they both have the s ame\\nsteady-state probability and denoting this probability by p, we know that the\\nsteady-state distribution is of the form ⃗π= (p1−2p p). Now, using the\\nidentity ⃗π=⃗πP, we solve a simple linear equation to obtain p=5/18 and\\nconsequently, ⃗π= (5/18 4/9 5/18 ).\\nThe PageRank values of pages (and the implicit ordering amon gst them)\\nare independent of any query a user might pose; PageRank is th us a query-\\nindependent measure of the static quality of each web page (r ecall such static\\nquality measures from Section 7.1.4 ). On the other hand, the relative order-\\ning of pages should, intuitively, depend on the query being s erved. For this\\nreason, search engines use static quality measures such as P ageRank as just\\none of many factors in scoring a web page on a query. Indeed, th e relative\\ncontribution of PageRank to the overall score may again be de termined by\\nmachine-learned scoring as in Section 15.4.1 .', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 505}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP470 21 Link analysis\\nd0\\nd2 d1\\nd5\\nd3 d6\\nd4car benz\\nfordgm\\nhonda\\njaguar\\njag\\ncatleopard\\ntiger\\njaguar\\nlioncheetah\\nspeed\\n◮Figure 21.4 A small web graph. Arcs are annotated with the word that occur s in\\nthe anchor text of the corresponding link.\\n✎Example 21.1: Consider the graph in Figure 21.4. For a teleportation rate of 0.14\\nits (stochastic) transition probability matrix is:\\n0.02 0.02 0.88 0.02 0.02 0.02 0.02\\n0.02 0.45 0.45 0.02 0.02 0.02 0.02\\n0.31 0.02 0.31 0.31 0.02 0.02 0.02\\n0.02 0.02 0.02 0.45 0.45 0.02 0.02\\n0.02 0.02 0.02 0.02 0.02 0.02 0.88\\n0.02 0.02 0.02 0.02 0.02 0.45 0.45\\n0.02 0.02 0.02 0.31 0.31 0.02 0.31\\nThe PageRank vector of this matrix is:\\n⃗x= (0.05 0.04 0.11 0.25 0.21 0.04 0.31 ) (21.6)\\nObserve that in Figure 21.4,q2,q3,q4and q6are the nodes with at least two in-links.\\nOf these, q2has the lowest PageRank since the random walk tends to drift o ut of the\\ntop part of the graph – the walker can only return there throug h teleportation.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 506}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP21.2 PageRank 471\\n21.2.3 Topic-speciﬁc PageRank\\nThus far we have discussed the PageRank computation with a te leport op-\\neration in which the surfer jumps to a random web page chosen u niformly\\nat random. We now consider teleporting to a random web page ch osen non-\\nuniformly . In doing so, we are able to derive PageRank values tailored t o\\nparticular interests. For instance, a sports aﬁcionado mig ht wish that pages\\non sports be ranked higher than non-sports pages. Suppose th at web pages\\non sports are “near” one another in the web graph. Then, a rand om surfer\\nwho frequently ﬁnds himself on random sports pages is likely (in the course\\nof the random walk) to spend most of his time at sports pages, s o that the\\nsteady-state distribution of sports pages is boosted.\\nSuppose our random surfer, endowed with a teleport operatio n as before,\\nteleports to a random web page on the topic of sports instead of teleporting to a\\nuniformly chosen random web page. We will not focus on how we c ollect all\\nweb pages on the topic of sports; in fact, we only need a non-ze ro subset Sof\\nsports-related web pages, so that the teleport operation is feasible. This may\\nbe obtained, for instance, from a manually built directory o f sports pages\\nsuch as the open directory project ( http://www.dmoz.org/ ) or that of Yahoo.\\nProvided the set Sof sports-related pages is non-empty, it follows that\\nthere is a non-empty set of web pages Y⊇Sover which the random walk\\nhas a steady-state distribution; let us denote this sports PageRank distribution\\nby⃗πs. For web pages not in Y, we set the PageRank values to zero. We call\\n⃗πsthetopic-speciﬁc PageRank for sports. TOPIC -SPECIFIC\\nPAGE RANK We do not demand that teleporting takes the random surfer to a uniformly\\nchosen sports page; the distribution over teleporting targ etsScould in fact\\nbe arbitrary.\\nIn like manner we can envision topic-speciﬁc PageRank distr ibutions for\\neach of several topics such as science, religion, politics a nd so on. Each of\\nthese distributions assigns to each web page a PageRank valu e in the interval\\n[0, 1). For a user interested in only a single topic from among these topics,\\nwe may invoke the corresponding PageRank distribution when scoring and\\nranking search results. This gives us the potential of consi dering settings in\\nwhich the search engine knows what topic a user is interested in. This may\\nhappen because users either explicitly register their inte rests, or because the\\nsystem learns by observing each user’s behavior over time.\\nBut what if a user is known to have a mixture of interests from m ultiple\\ntopics? For instance, a user may have an interest mixture (or proﬁle ) that is\\n60% sports and 40% politics; can we compute a personalized PageRank for this PERSONALIZED\\nPAGE RANK user? At ﬁrst glance, this appears daunting: how could we pos sibly compute\\na different PageRank distribution for each user proﬁle (wit h, potentially, in-\\nﬁnitely many possible proﬁles)? We can in fact address this p rovided we\\nassume that an individual’s interests can be well-approxim ated as a linear', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 507}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP472 21 Link analysis\\n◮Figure 21.5 Topic-speciﬁc PageRank. In this example we consider a user w hose\\ninterests are 60% sports and 40% politics. If the teleportat ion probability is 10%, this\\nuser is modeled as teleporting 6% to sports pages and 4% to pol itics pages.\\ncombination of a small number of topic page distributions. A user with this\\nmixture of interests could teleport as follows: determine ﬁ rst whether to tele-\\nport to the set Sof known sports pages, or to the set of known politics pages.\\nThis choice is made at random, choosing sports pages 60% of th e time and\\npolitics pages 40% of the time. Once we choose that a particul ar teleport step\\nis to (say) a random sports page, we choose a web page in Suniformly at\\nrandom to teleport to. This in turn leads to an ergodic Markov chain with a\\nsteady-state distribution that is personalized to this use r’s preferences over\\ntopics (see Exercise 21.16 ).\\nWhile this idea has intuitive appeal, its implementation ap pears cumber-\\nsome: it seems to demand that for each user, we compute a trans ition prob-', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 508}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP21.2 PageRank 473\\nability matrix and compute its steady-state distribution. We are rescued by\\nthe fact that the evolution of the probability distribution over the states of\\na Markov chain can be viewed as a linear system. In Exercise 21.16 we will\\nshow that it is not necessary to compute a PageRank vector for every distinct\\ncombination of user interests over topics; the personalize d PageRank vector\\nfor any user can be expressed as a linear combination of the un derlying topic-\\nspeciﬁc PageRanks. For instance, the personalized PageRan k vector for the\\nuser whose interests are 60% sports and 40% politics can be co mputed as\\n0.6⃗πs+0.4⃗πp, (21.7)\\nwhere ⃗πsand⃗πpare the topic-speciﬁc PageRank vectors for sports and for\\npolitics, respectively.\\n?Exercise 21.5\\nWrite down the transition probability matrix for the exampl e in Figure 21.2.\\nExercise 21.6\\nConsider a web graph with three nodes 1, 2 and 3. The links are a s follows: 1→\\n2, 3→2, 2→1, 2→3. Write down the transition probability matrices for the su rfer’s\\nwalk with teleporting, for the following three values of the teleport probability: (a)\\nα=0; (b) α=0.5 and (c) α=1.\\nExercise 21.7\\nA user of a browser can, in addition to clicking a hyperlink on the page xhe is cur-\\nrently browsing, use the back button to go back to the page from which he arrived at\\nx. Can such a user of back buttons be modeled as a Markov chain? H ow would we\\nmodel repeated invocations of the back button?\\nExercise 21.8\\nConsider a Markov chain with three states A, B and C, and trans ition probabilities as\\nfollows. From state A, the next state is B with probability 1. From B, the next state is\\neither A with probability pA, or state C with probability 1 −pA. From C the next state\\nis A with probability 1. For what values of pA∈[0, 1]is this Markov chain ergodic?\\nExercise 21.9\\nShow that for any directed graph, the Markov chain induced by a random walk with\\nthe teleport operation is ergodic.\\nExercise 21.10\\nShow that the PageRank of every page is at least α/N. What does this imply about\\nthe difference in PageRank values (over the various pages) a sαbecomes close to 1?\\nExercise 21.11\\nFor the data in Example 21.1, write a small routine or use a scientiﬁc calculator to\\ncompute the PageRank values stated in Equation ( 21.6).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 509}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP474 21 Link analysis\\nExercise 21.12\\nSuppose that the web graph is stored on disk as an adjacency li st, in such a way that\\nyou may only query for the out-neighbors of pages in the order in which they are\\nstored. You cannot load the graph in main memory but you may do multiple reads\\nover the full graph. Write the algorithm for computing the Pa geRank in this setting.\\nExercise 21.13\\nRecall the sets Sand Yintroduced near the beginning of Section 21.2.3 . How does the\\nsetYrelate to S?\\nExercise 21.14\\nIs the set Yalways the set of all web pages? Why or why not?\\nExercise 21.15 [⋆ ⋆ ⋆ ]\\nIs the sports PageRank of any page in Sat least as large as its PageRank?\\nExercise 21.16 [⋆ ⋆ ⋆ ]\\nConsider a setting where we have two topic-speciﬁc PageRank values for each web\\npage: a sports PageRank ⃗πs, and a politics PageRank ⃗πp. Let αbe the (common)\\nteleportation probability used in computing both sets of to pic-speciﬁc PageRanks.\\nForq∈[0, 1], consider a user whose interest proﬁle is divided between a f raction qin\\nsports and a fraction 1 −qin politics. Show that the user’s personalized PageRank is\\nthe steady-state distribution of a random walk in which – on a teleport step – the walk\\nteleports to a sports page with probability qand to a politics page with probability\\n1−q.\\nExercise 21.17\\nShow that the Markov chain corresponding to the walk in Exerc ise21.16 is ergodic\\nand hence the user’s personalized PageRank can be obtained b y computing the steady-\\nstate distribution of this Markov chain.\\nExercise 21.18\\nShow that in the steady-state distribution of Exercise 21.17 , the steady-state probabil-\\nity for any web page iequals qπs(i) + ( 1−q)πp(i).\\n21.3 Hubs and Authorities\\nWe now develop a scheme in which, given a query, every web page is as-\\nsigned twoscores. One is called its hub score and the other its authority score . HUB SCORE\\nAUTHORITY SCORE For any query, we compute two ranked lists of results rather t han one. The\\nranking of one list is induced by the hub scores and that of the other by the\\nauthority scores.\\nThis approach stems from a particular insight into the creat ion of web\\npages, that there are two primary kinds of web pages useful as results for\\nbroad-topic searches . By a broad topic search we mean an informational query\\nsuch as \"I wish to learn about leukemia \". There are authoritative sources of\\ninformation on the topic; in this case, the National Cancer I nstitute’s page on', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 510}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP21.3 Hubs and Authorities 475\\nleukemia would be such a page. We will call such pages authorities ; in the\\ncomputation we are about to describe, they are the pages that will emerge\\nwith high authority scores.\\nOn the other hand, there are many pages on the Web that are hand -compiled\\nlists of links to authoritative web pages on a speciﬁc topic. These hubpages\\nare not in themselves authoritative sources of topic-speci ﬁc information, but\\nrather compilations that someone with an interest in the top ic has spent time\\nputting together. The approach we will take, then, is to use t hese hub pages\\nto discover the authority pages. In the computation we now de velop, these\\nhub pages are the pages that will emerge with high hub scores.\\nA good hub page is one that points to many good authorities; a g ood au-\\nthority page is one that is pointed to by many good hub pages. W e thus\\nappear to have a circular deﬁnition of hubs and authorities; we will turn this\\ninto an iterative computation. Suppose that we have a subset of the web con-\\ntaining good hub and authority pages, together with the hype rlinks amongst\\nthem. We will iteratively compute a hub score and an authorit y score for ev-\\nery web page in this subset, deferring the discussion of how w e pick this\\nsubset until Section 21.3.1 .\\nFor a web page vin our subset of the web, we use h(v)to denote its hub\\nscore and a(v)its authority score. Initially, we set h(v) = a(v) = 1 for all\\nnodes v. We also denote by v↦→ythe existence of a hyperlink from vto\\ny. The core of the iterative algorithm is a pair of updates to th e hub and au-\\nthority scores of all pages given by Equation 21.8, which capture the intuitive\\nnotions that good hubs point to good authorities and that goo d authorities\\nare pointed to by good hubs.\\nh(v)←∑\\nv↦→ya(y) (21.8)\\na(v)←∑\\ny↦→vh(y).\\nThus, the ﬁrst line of Equation ( 21.8) sets the hub score of page vto the sum\\nof the authority scores of the pages it links to. In other word s, if vlinks to\\npages with high authority scores, its hub score increases. T he second line\\nplays the reverse role; if page vis linked to by good hubs, its authority score\\nincreases.\\nWhat happens as we perform these updates iteratively, recom puting hub\\nscores, then new authority scores based on the recomputed hu b scores, and\\nso on? Let us recast the equations Equation ( 21.8) into matrix-vector form.\\nLet⃗hand⃗adenote the vectors of all hub and all authority scores respec tively,\\nfor the pages in our subset of the web graph. Let Adenote the adjacency\\nmatrix of the subset of the web graph that we are dealing with: Ais a square\\nmatrix with one row and one column for each page in the subset. The entry', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 511}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP476 21 Link analysis\\nAijis 1 if there is a hyperlink from page ito page j, and 0 otherwise. Then,\\nwe may write Equation ( 21.8)\\n⃗h← A⃗a (21.9)\\n⃗a← AT⃗h,\\nwhere ATdenotes the transpose of the matrix A. Now the right hand side of\\neach line of Equation ( 21.9) is a vector that is the left hand side of the other\\nline of Equation ( 21.9). Substituting these into one another, we may rewrite\\nEquation ( 21.9) as\\n⃗h← AAT⃗h (21.10)\\n⃗a← ATA⃗a.\\nNow, Equation ( 21.10 ) bears an uncanny resemblance to a pair of eigenvector\\nequations (Section 18.1); indeed, if we replace the ←symbols by =symbols\\nand introduce the (unknown) eigenvalue, the ﬁrst line of Equ ation ( 21.10 )\\nbecomes the equation for the eigenvectors of AAT, while the second becomes\\nthe equation for the eigenvectors of ATA:\\n⃗h= ( 1/λh)AAT⃗h\\n⃗a= ( 1/λa)ATA⃗a. (21.11)\\nHere we have used λhto denote the eigenvalue of AATand λato denote the\\neigenvalue of ATA.\\nThis leads to some key consequences:\\n1.The iterative updates in Equation ( 21.8) (or equivalently, Equation ( 21.9)),\\nif scaled by the appropriate eigenvalues, are equivalent to the power iter-\\nation method for computing the eigenvectors of AATand ATA. Provided\\nthat the principal eigenvalue of AATis unique, the iteratively computed\\nentries of ⃗hand⃗asettle into unique steady-state values determined by the\\nentries of Aand hence the link structure of the graph.\\n2.In computing these eigenvector entries, we are not restrict ed to using the\\npower iteration method; indeed, we could use any fast method for com-\\nputing the principal eigenvector of a stochastic matrix.\\nThe resulting computation thus takes the following form:\\n1.Assemble the target subset of web pages, form the graph induc ed by their\\nhyperlinks and compute AATand ATA.\\n2.Compute the principal eigenvectors of AATand ATAto form the vector\\nof hub scores ⃗hand authority scores ⃗a.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 512}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP21.3 Hubs and Authorities 477\\n3.Output the top-scoring hubs and the top-scoring authoritie s.\\nThis method of link analysis is known as HITS , which is an acronym for HITS\\nHyperlink-Induced Topic Search .\\n✎Example 21.2: Assuming the query jaguar and double-weighting of links whose\\nanchors contain the query word, the matrix Afor Figure 21.4 is as follows:\\n0 0 1 0 0 0 0\\n0 1 1 0 0 0 0\\n1 0 1 2 0 0 0\\n0 0 0 1 1 0 0\\n0 0 0 0 0 0 1\\n0 0 0 0 0 1 1\\n0 0 0 2 1 0 1\\nThe hub and authority vectors are:\\n⃗h= (0.03 0.04 0.33 0.18 0.04 0.04 0.35 )\\n⃗a= (0.10 0.01 0.12 0.47 0.16 0.01 0.13 )\\nHere, q3is the main authority – two hubs ( q2and q6) are pointing to it via highly\\nweighted jaguar links.\\nSince the iterative updates captured the intuition of good h ubs and good\\nauthorities, the high-scoring pages we output would give us good hubs and\\nauthorities from the target subset of web pages. In Section 21.3.1 we describe\\nthe remaining detail: how do we gather a target subset of web p ages around\\na topic such as leukemia ?\\n21.3.1 Choosing the subset of the Web\\nIn assembling a subset of web pages around a topic such as leukemia , we must\\ncope with the fact that good authority pages may not contain t he speciﬁc\\nquery term leukemia . This is especially true, as we noted in Section 21.1.1 ,\\nwhen an authority page uses its web presence to project a cert ain market-\\ning image. For instance, many pages on the IBM website are aut horitative\\nsources of information on computer hardware, even though th ese pages may\\nnot contain the term computer orhardware . However, a hub compiling com-\\nputer hardware resources is likely to use these terms and als o link to the\\nrelevant pages on the IBM website.\\nBuilding on these observations, the following procedure ha s been sug-\\ngested for compiling the subset of the Web for which to comput e hub and\\nauthority scores.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 513}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP478 21 Link analysis\\n1.Given a query (say leukemia ), use a text index to get all pages containing\\nleukemia . Call this the root set of pages.\\n2.Build the base set of pages, to include the root set as well as any page that\\neither links to a page in the root set, or is linked to by a page i n the root\\nset.\\nWe then use the base set for computing hub and authority score s. The base\\nset is constructed in this manner for three reasons:\\n1.A good authority page may not contain the query text (such as computer\\nhardware ).\\n2.If the text query manages to capture a good hub page vhin the root set,\\nthen the inclusion of all pages linked to by any page in the roo t set will\\ncapture all the good authorities linked to by vhin the base set.\\n3.Conversely, if the text query manages to capture a good autho rity page\\nvain the root set, then the inclusion of pages which point to vawill bring\\nother good hubs into the base set. In other words, the “expans ion” of\\nthe root set into the base set enriches the common pool of good hubs and\\nauthorities.\\nRunning HITS across a variety of queries reveals some intere sting insights\\nabout link analysis. Frequently, the documents that emerge as top hubs and\\nauthorities include languages other than the language of th e query. These\\npages were presumably drawn into the base set, following the assembly of\\nthe root set. Thus, some elements of cross-language retrieval (where a query\\nin one language retrieves documents in another) are evident here; interest-\\ningly, this cross-language effect resulted purely from lin k analysis, with no\\nlinguistic translation taking place.\\nWe conclude this section with some notes on implementing thi s algorithm.\\nThe root set consists of all pages matching the text query; in fact, implemen-\\ntations (see the references in Section 21.4) suggest that it sufﬁces to use 200 or\\nso web pages for the root set, rather than all pages matching t he text query.\\nAny algorithm for computing eigenvectors may be used for com puting the\\nhub/authority score vector. In fact, we need not compute the exact values\\nof these scores; it sufﬁces to know the relative values of the scores so that\\nwe may identify the top hubs and authorities. To this end, it i s possible that\\na small number of iterations of the power iteration method yi elds the rela-\\ntive ordering of the top hubs and authorities. Experiments h ave suggested\\nthat in practice, about ﬁve iterations of Equation ( 21.8) yield fairly good re-\\nsults. Moreover, since the link structure of the web graph is fairly sparse\\n(the average web page links to about ten others), we do not per form these as\\nmatrix-vector products but rather as additive updates as in Equation ( 21.8).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 514}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP21.3 Hubs and Authorities 479\\n◮Figure 21.6 A sample run of HITS on the query japan elementary schools .\\nFigure 21.6 shows the results of running HITS on the query japan elemen-\\ntaryschools . The ﬁgure shows the top hubs and authorities; each row lists the\\ntitle tag from the corresponding HTML page. Because the resulting string\\nis not necessarily in Latin characters, the resulting print is (in many cases)\\na string of gibberish. Each of these corresponds to a web page that does\\nnot use Latin characters, in this case very likely pages in Ja panese. There\\nalso appear to be pages in other non-English languages, whic h seems sur-\\nprising given that the query string is in English. In fact, th is result is em-\\nblematic of the functioning of HITS – following the assembly of the root set,\\nthe (English) query string is ignored. The base set is likely to contain pages\\nin other languages, for instance if an English-language hub page links to\\nthe Japanese-language home pages of Japanese elementary sc hools. Because\\nthe subsequent computation of the top hubs and authorities i s entirely link-\\nbased, some of these non-English pages will appear among the top hubs and\\nauthorities.\\n?Exercise 21.19\\nIf all the hub and authority scores are initialized to 1, what is the hub/authority score\\nof a node after one iteration?', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 515}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP480 21 Link analysis\\nExercise 21.20\\nHow would you interpret the entries of the matrices AATand ATA? What is the\\nconnection to the co-occurrence matrix CCTin Chapter 18?\\nExercise 21.21\\nWhat are the principal eigenvalues of AATand ATA?\\nd1 d2\\nd3\\n◮Figure 21.7 Web graph for Exercise 21.22 .\\nExercise 21.22\\nFor the web graph in Figure 21.7, compute PageRank, hub and authority scores for\\neach of the three pages. Also give the relative ordering of th e 3 nodes for each of these\\nscores, indicating any ties.\\nPageRank: Assume that at each step of the PageRank random wal k, we teleport to a\\nrandom page with probability 0.1, with a uniform distributi on over which particular\\npage we teleport to.\\nHubs/Authorities: Normalize the hub (authority) scores so that the maximum hub\\n(authority) score is 1.\\nHint 1: Using symmetries to simplify and solving with linear equations might be\\neasier than using iterative methods.\\nHint 2: Provide the relative ordering (indicating any ties) of the three nodes for each\\nof the three scoring measures.\\n21.4 References and further reading\\nGarﬁeld (1955 ) is seminal in the science of citation analysis. This was bui lt\\non by Pinski and Narin (1976 ) to develop a journal inﬂuence weight , whose\\ndeﬁnition is remarkably similar to that of the PageRank meas ure.\\nThe use of anchor text as an aid to searching and ranking stems from the\\nwork of McBryan (1994 ). Extended anchor-text was implicit in his work, with\\nsystematic experiments reported in Chakrabarti et al. (1998 ).\\nKemeny and Snell (1976 ) is a classic text on Markov chains. The PageRank\\nmeasure was developed in Brin and Page (1998 ) and in Page et al. (1998 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 516}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP21.4 References and further reading 481\\nA number of methods for the fast computation of PageRank valu es are sur-\\nveyed in Berkhin (2005 ) and in Langville and Meyer (2006 ); the former also\\ndetails how the PageRank eigenvector solution may be viewed as solving a\\nlinear system, leading to one way of solving Exercise 21.16 . The effect of the\\nteleport probability αhas been studied by Baeza-Yates et al. (2005 ) and by\\nBoldi et al. (2005 ). Topic-speciﬁc PageRank and variants were developed in\\nHaveliwala (2002 ),Haveliwala (2003 ) and in Jeh and Widom (2003 ).Berkhin\\n(2006a ) develops an alternate view of topic-speciﬁc PageRank.\\nNg et al. (2001b ) suggests that the PageRank score assignment is more ro-\\nbust than HITS in the sense that scores are less sensitive to s mall changes in\\ngraph topology. However, it has also been noted that the tele port operation\\ncontributes signiﬁcantly to PageRank’s robustness in this sense. Both Page-\\nRank and HITS can be “spammed” by the orchestrated insertion of links into\\nthe web graph; indeed, the Web is known to have such link farms that col- LINK FARMS\\nlude to increase the score assigned to certain pages by vario us link analysis\\nalgorithms.\\nThe HITS algorithm is due to Kleinberg (1999 ).Chakrabarti et al. (1998 ) de-\\nveloped variants that weighted links in the iterative compu tation based on\\nthe presence of query terms in the pages being linked and comp ared these\\nto results from several web search engines. Bharat and Henzinger (1998 ) fur-\\nther developed these and other heuristics, showing that cer tain combinations\\noutperformed the basic HITS algorithm. Borodin et al. (2001 ) provides a sys-\\ntematic study of several variants of the HITS algorithm. Ng et al. (2001b )\\nintroduces a notion of stability for link analysis, arguing that small changes\\nto link topology should not lead to signiﬁcant changes in the ranked list of\\nresults for a query. Numerous other variants of HITS have bee n developed\\nby a number of authors, the best know of which is perhaps SALSA (Lempel\\nand Moran 2000 ).', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 517}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 518}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPDRAFT!©April1, 2009CambridgeUniversityPress. Feedbackwelcome . 483\\nBibliography\\nWe use the following abbreviated journal and conference nam es in the bibliography:\\nCACM Communications of the Association for Computing Machinery .\\nIP&M Information Processing and Management.\\nIRInformation Retrieval.\\nJACM Journal of the Association for Computing Machinery.\\nJASIS Journal of the American Society for Information Science.\\nJASIST Journal of the American Society for Information Science and Technology.\\nJMLR Journal of Machine Learning Research.\\nTOIS ACM Transactions on Information Systems.\\nProc. ACL Proceedings of the Annual Meeting of the Association for Com putational\\nLinguistics. Available from: http://www.aclweb.org/anthology-index/\\nProc. CIKM Proceedings of the ACM CIKM Conference on Information and Kn ow-\\nledge Management. ACM Press.\\nProc. ECIR Proceedings of the European Conference on Information Retr ieval.\\nProc. ECML Proceedings of the European Conference on Machine Learning .\\nProc. ICML Proceedings of the International Conference on Machine Lea rning.\\nProc. IJCAI Proceedings of the International Joint Conference on Artiﬁ cial Intelli-\\ngence.\\nProc. INEX Proceedings of the Initiative for the Evaluation of XML Retr ieval.\\nProc. KDD Proceedings of the ACM SIGKDD International Conference on K now-\\nledge Discovery and Data Mining.\\nProc. NIPS Proceedings of the Neural Information Processing Systems C onference.\\nProc. PODS Proceedings of the ACM Conference on Principles of Database Systems.\\nProc. SDAIR Proceedings of the Annual Symposium on Document Analysis an d In-\\nformation Retrieval.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 519}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP484 Bibliography\\nProc. SIGIR Proceedings of the Annual International ACM/SIGIR Confere nce on\\nResearch and Development in Information Retrieval. Availa ble from: http://www.sigir.org/proceedings/Proc-\\nBrowse.html\\nProc. SPIRE Proceedings of the Symposium on String Processing and Infor mation\\nRetrieval.\\nProc. TREC Proceedings of the Text Retrieval Conference.\\nProc. UAI Proceedings of the Conference on Uncertainty in Artiﬁcial I ntelligence.\\nProc. VLDB Proceedings of the Very Large Data Bases Conference.\\nProc. WWW Proceedings of the International World Wide Web Conference .\\nAberer, Karl. 2001. P-Grid: A self-organizing access struc ture for P2P information\\nsystems. In Proc. International Conference on Cooperative Informatio n Systems , pp.\\n179–194. Springer. xxxiv ,519\\nAizerman, Mark A., Emmanuel M. Braverman, and Lev I. Rozonoé r. 1964. Theoret-\\nical foundations of the potential function method in patter n recognition learning.\\nAutomation and Remote Control 25:821–837. 347,519,520,530\\nAkaike, Hirotugu. 1974. A new look at the statistical model i dentiﬁcation. IEEE\\nTransactions on automatic control 19(6):716–723. 373,519\\nAllan, James. 2005. HARD track overview in TREC 2005: High ac curacy retrieval\\nfrom documents. In Proc. TREC .174,519\\nAllan, James, Ron Papka, and Victor Lavrenko. 1998. On-line new event\\ndetection and tracking. In Proc. SIGIR , pp. 37–45. ACM Press. DOI:\\ndoi.acm.org/10.1145/290941.290954 .399,519,526,528\\nAllwein, Erin L., Robert E. Schapire, and Yoram Singer. 2000 . Reducing multiclass\\nto binary: A unifying approach for margin classiﬁers. JMLR 1:113–141. URL:\\nwww.jmlr.org/papers/volume1/allwein00a/allwein00a.p df.315,519,530,531\\nAlonso, Omar, Sandeepan Banerjee, and Mark Drake. 2006. GIO : A semantic web\\napplication using the information grid framework. In Proc. WWW , pp. 857–858.\\nACM Press. DOI:doi.acm.org/10.1145/1135777.1135913 .373,519,522\\nAltingövde, Ismail Sengör, Engin Demir, Fazli Can, and Özgü r Ulusoy. 2008. In-\\ncremental cluster-based retrieval using compressed clust er-skipping inverted ﬁles.\\nTOIS . To appear. 372\\nAltingövde, Ismail Sengör, Rifat Ozcan, Huseyin Cagdas Oca lan, Fazli Can, and\\nÖzgür Ulusoy. 2007. Large-scale cluster-based retrieval e xperiments on Turkish\\ntexts. In Proc. SIGIR , pp. 891–892. ACM Press. 519,521,528,532\\nAmer-Yahia, Sihem, Chavdar Botev, Jochen Dörre, and Jayave l Shanmugasundaram.\\n2006. XQuery full-text extensions explained. IBM Systems Journal 45(2):335–352.\\n217,519,520,522,530\\nAmer-Yahia, Sihem, Pat Case, Thomas Rölleke, Jayavel Shanm ugasundaram, and\\nGerhard Weikum. 2005. Report on the DB/IR panel at SIGMOD 200 5.SIGMOD\\nRecord 34(4):71–74. DOI:doi.acm.org/10.1145/1107499.1107514 .217,519,521,530,532', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 520}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 485\\nAmer-Yahia, Sihem, and Mounia Lalmas. 2006. XML search: Lan guages, INEX and\\nscoring. SIGMOD Record 35(4):16–23. DOI:doi.acm.org/10.1145/1228268.1228271 .\\n217,519,526\\nAnagnostopoulos, Aris, Andrei Z. Broder, and Kunal Punera. 2006. Effective and\\nefﬁcient classiﬁcation on a search-engine model. In Proc. CIKM , pp. 208–217. ACM\\nPress. DOI:doi.acm.org/10.1145/1183614.1183648 .315,519,520,529\\nAnderberg, Michael R. 1973. Cluster analysis for applications . Academic Press. 372,519\\nAndoni, Alexandr, Mayur Datar, Nicole Immorlica, Piotr Ind yk, and Vahab Mirrokni.\\n2006. Locality-sensitive hashing using stable distributi ons. In Nearest Neighbor\\nMethods in Learning and Vision: Theory and Practice . MIT Press. 314,519,522,524,\\n527\\nAnh, Vo Ngoc, Owen de Kretser, and Alistair Moffat. 2001. Vec tor-space ranking with\\neffective early termination. In Proc. SIGIR , pp. 35–42. ACM Press. 149,519,526,527\\nAnh, Vo Ngoc, and Alistair Moffat. 2005. Inverted index com-\\npression using word-aligned binary codes. IR 8(1):151–166. DOI:\\ndx.doi.org/10.1023/B:INRT.0000048490.99518.5c .106,519,527\\nAnh, Vo Ngoc, and Alistair Moffat. 2006a. Improved word-ali gned binary compres-\\nsion for text indexing. IEEE Transactions on Knowledge and Data Engineering 18(6):\\n857–861. 106,519,527\\nAnh, Vo Ngoc, and Alistair Moffat. 2006b. Pruned query evalu ation us-\\ning pre-computed impacts. In Proc. SIGIR , pp. 372–379. ACM Press. DOI:\\ndoi.acm.org/10.1145/1148170.1148235 .149,519,527\\nAnh, Vo Ngoc, and Alistair Moffat. 2006c. Structured index o rganizations for high-\\nthroughput text querying. In Proc. SPIRE , pp. 304–315. Springer. 149,519,527\\nApté, Chidanand, Fred Damerau, and Sholom M. Weiss. 1994. Au tomated learning\\nof decision rules for text categorization. TOIS 12(1):233–251. 286,519,522,532\\nArthur, David, and Sergei Vassilvitskii. 2006. How slow is t hek-means method? In\\nProc. ACM Symposium on Computational Geometry , pp. 144–153. 373,519,532\\nArvola, Paavo, Marko Junkkari, and Jaana Kekäläinen. 2005. Generalized contextual-\\nization method for XML information retrieval. In Proc. CIKM , pp. 20–27. 216,519,\\n525\\nAslam, Javed A., and Emine Yilmaz. 2005. A geometric interpr etation and analysis\\nof R-precision. In Proc. CIKM , pp. 664–671. ACM Press. 174,519,533\\nAult, Thomas Galen, and Yiming Yang. 2002. Information ﬁlte ring in TREC-9 and\\nTDT-3: A comparative analysis. IR5(2-3):159–187. 315,519,533\\nBadue, Claudine Santos, Ricardo A. Baeza-Yates, Berthier R ibeiro-Neto, and Nivio\\nZiviani. 2001. Distributed query processing using partiti oned inverted ﬁles. In\\nProc. SPIRE , pp. 10–20. 459,519,529,533\\nBaeza-Yates, Ricardo, Paolo Boldi, and Carlos Castillo. 20 05. The choice of a damp-\\ning function for propagating importance in link-based rank ing. Technical report,\\nDipartimento di Scienze dell’Informazione, Università de gli Studi di Milano. 481,\\n519,520,521', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 521}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP486 Bibliography\\nBaeza-Yates, Ricardo, and Berthier Ribeiro-Neto. 1999. Modern Information Retrieval .\\nAddison Wesley. xxxiv ,84,105,175,400,519,529\\nBahle, Dirk, Hugh E. Williams, and Justin Zobel. 2002. Efﬁci ent phrase querying with\\nan auxiliary index. In Proc. SIGIR , pp. 215–221. ACM Press. 47,519,533\\nBaldridge, Jason, and Miles Osborne. 2004. Active learning and the total cost of\\nannotation. In Proc. Empirical Methods in Natural Language Processing , pp. 9–16. 348,\\n519,528\\nBall, G. H. 1965. Data analysis in the social sciences: What a bout the details? In Proc.\\nFall Joint Computer Conference , pp. 533–560. Spartan Books. 373,519\\nBanko, Michele, and Eric Brill. 2001. Scaling to very very la rge corpora for natural\\nlanguage disambiguation. In Proc. ACL .337,519,520\\nBar-Ilan, Judit, and Tatyana Gutman. 2005. How do search eng ines respond to some\\nnon-English queries? Journal of Information Science 31(1):13–28. 46,519,523\\nBar-Yossef, Ziv, and Maxim Gurevich. 2006. Random sampling from a\\nsearch engine’s index. In Proc. WWW , pp. 367–376. ACM Press. DOI:\\ndoi.acm.org/10.1145/1135777.1135833 .442,519,523\\nBarroso, Luiz André, Jeffrey Dean, and Urs Hölzle. 2003. Web search for\\na planet: The Google cluster architecture. IEEE Micro 23(2):22–28. DOI:\\ndx.doi.org/10.1109/MM.2003.1196112 .459,519,522,524\\nBartell, Brian Theodore. 1994. Optimizing ranking functions: A connectionist approach to\\nadaptive information retrieval . PhD thesis, University of California at San Diego, La\\nJolla, CA. 150,519\\nBartell, Brian T., Garrison W. Cottrell, and Richard K. Bele w. 1998. Optimizing sim-\\nilarity using multi-query relevance feedback. JASIS 49(8):742–761. 150,519,520,\\n521\\nBarzilay, Regina, and Michael Elhadad. 1997. Using lexical chains for text summa-\\nrization. In Workshop on Intelligent Scalable Text Summarization , pp. 10–17. 174,520,\\n522\\nBast, Holger, and Debapriyo Majumdar. 2005. Why spectral re trieval works. In Proc.\\nSIGIR , pp. 11–18. ACM Press. DOI:doi.acm.org/10.1145/1076034.1076040 .417,520,\\n527\\nBasu, Sugato, Arindam Banerjee, and Raymond J. Mooney. 2004 . Active semi-\\nsupervision for pairwise constrained clustering. In Proc. SIAM International Con-\\nference on Data Mining , pp. 333–344. 373,519,520,528\\nBeesley, Kenneth R. 1998. Language identiﬁer: A computer pr ogram for automatic\\nnatural-language identiﬁcation of on-line text. In Languages at Crossroads: Proc.\\nAnnual Conference of the American Translators Association , pp. 47–54. 46,520\\nBeesley, Kenneth R., and Lauri Karttunen. 2003. Finite State Morphology . CSLI Publi-\\ncations. 46,520,525\\nBennett, Paul N. 2000. Assessing the calibration of naive Ba yes’ posterior estimates.\\nTechnical Report CMU-CS-00-155, School of Computer Scienc e, Carnegie Mellon\\nUniversity. 286,520', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 522}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 487\\nBerger, Adam, and John Lafferty. 1999. Information retriev al as statistical translation.\\nInProc. SIGIR , pp. 222–229. ACM Press. 251,252,520,526\\nBerkhin, Pavel. 2005. A survey on pagerank computing. Internet Mathematics 2(1):\\n73–120. 481,520\\nBerkhin, Pavel. 2006a. Bookmark-coloring algorithm for pe rsonalized pagerank com-\\nputing. Internet Mathematics 3(1):41–62. 481,520\\nBerkhin, Pavel. 2006b. A survey of clustering data mining te chniques. In Jacob Kogan,\\nCharles Nicholas, and Marc Teboulle (eds.), Grouping Multidimensional Data: Recent\\nAdvances in Clustering , pp. 25–71. Springer. 372,520\\nBerners-Lee, Tim, Robert Cailliau, Jean-Francois Groff, a nd Bernd Pollermann.\\n1992. World-Wide Web: The information universe. Electronic Networking: Re-\\nsearch, Applications and Policy 1(2):74–82. URL:citeseer.ist.psu.edu/article/berners-\\nlee92worldwide.html .441,520,521,523,529\\nBerry, Michael, and Paul Young. 1995. Using latent semantic indexing for multilan-\\nguage information retrieval. Computers and the Humanities 29(6):413–429. 417,520,\\n533\\nBerry, Michael W., Susan T. Dumais, and Gavin W. O’Brien. 199 5. Using linear algebra\\nfor intelligent information retrieval. SIAM Review 37(4):573–595. 417,520,522,528\\nBetsi, Stamatina, Mounia Lalmas, Anastasios Tombros, and T heodora Tsikrika. 2006.\\nUser expectations from XML element retrieval. In Proc. SIGIR , pp. 611–612. ACM\\nPress. 217,520,526,531,532\\nBharat, Krishna, and Andrei Broder. 1998. A technique for me asuring the relative size\\nand overlap of public web search engines. Computer Networks and ISDN Systems 30\\n(1-7):379–388. DOI:dx.doi.org/10.1016/S0169-7552(98)00127-5 .442,520\\nBharat, Krishna, Andrei Broder, Monika Henzinger, Puneet K umar, and Suresh\\nVenkatasubramanian. 1998. The connectivity server: Fast a ccess to linkage in-\\nformation on the web. In Proc. WWW , pp. 469–477. 459,520,524,526,532\\nBharat, Krishna, Andrei Z. Broder, Jeffrey Dean, and Monika Rauch Henzinger. 2000.\\nA comparison of techniques to ﬁnd mirrored hosts on the WWW. JASIS 51(12):\\n1114–1122. URL:citeseer.ist.psu.edu/bharat99comparison.html .442,520,522,524\\nBharat, Krishna, and Monika R. Henzinger. 1998. Improved al gorithms for topic\\ndistillation in a hyperlinked environment. In Proc. SIGIR , pp. 104–111. ACM Press.\\nURL:citeseer.ist.psu.edu/bharat98improved.html .481,520,524\\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning . Springer. 315,\\n520\\nBlair, David C., and M. E. Maron. 1985. An evaluation of retri eval effectiveness for a\\nfull-text document-retrieval system. CACM 28(3):289–299. 193,520,527\\nBlanco, Roi, and Alvaro Barreiro. 2006. TSP and cluster-bas ed solutions to the reas-\\nsignment of document identiﬁers. IR9(4):499–517. 106,519,520\\nBlanco, Roi, and Alvaro Barreiro. 2007. Boosting static pru ning of inverted ﬁles. In\\nProc. SIGIR . ACM Press. 105,519,520', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 523}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP488 Bibliography\\nBlandford, Dan, and Guy Blelloch. 2002. Index compression t hrough document re-\\nordering. In Proc. Data Compression Conference , p. 342. IEEE Computer Society. 106,\\n520\\nBlei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. Lat ent Dirichlet allocation.\\nJMLR 3:993–1022. 418,520,525,528\\nBoldi, Paolo, Bruno Codenotti, Massimo Santini, and Sebast iano Vigna. 2002. Ubi-\\ncrawler: A scalable fully distributed web crawler. In Proc. Australian World Wide\\nWeb Conference .URL:citeseer.ist.psu.edu/article/boldi03ubicrawler.html .458,520,521,\\n530,532\\nBoldi, Paolo, Massimo Santini, and Sebastiano Vigna. 2005. PageRank as a function\\nof the damping factor. In Proc. WWW .URL:citeseer.ist.psu.edu/boldi05pagerank.html .\\n481,520,530,532\\nBoldi, Paolo, and Sebastiano Vigna. 2004a. Codes for the Wor ld-Wide Web. Internet\\nMathematics 2(4):405–427. 459,520,532\\nBoldi, Paolo, and Sebastiano Vigna. 2004b. The WebGraph fra mework I: Compression\\ntechniques. In Proc. WWW , pp. 595–601. ACM Press. 459,520,532\\nBoldi, Paolo, and Sebastiano Vigna. 2005. Compressed perfe ct embedded skip lists\\nfor quick inverted-index lookups. In Proc. SPIRE . Springer. 46,520,532\\nBoley, Daniel. 1998. Principal direction divisive partiti oning. Data Mining and Know-\\nledge Discovery 2(4):325–344. DOI:dx.doi.org/10.1023/A:1009740529316 .400,520\\nBorodin, Allan, Gareth O. Roberts, Jeffrey S. Rosenthal, an d Panayiotis Tsaparas.\\n2001. Finding authorities and hubs from link structures on t he World Wide Web.\\nInProc. WWW , pp. 415–429. 481,520,529,530,532\\nBourne, Charles P ., and Donald F. Ford. 1961. A study of metho ds for sys-\\ntematically abbreviating English words and names. JACM 8(4):538–552. DOI:\\ndoi.acm.org/10.1145/321088.321094 .65,520,523\\nBradley, Paul S., and Usama M. Fayyad. 1998. Reﬁning initial points for K-means\\nclustering. In Proc. ICML , pp. 91–99. 373,520,522\\nBradley, Paul S., Usama M. Fayyad, and Cory Reina. 1998. Scal ing clustering algo-\\nrithms to large databases. In Proc. KDD , pp. 9–15. 374,520,522,529\\nBrill, Eric, and Robert C. Moore. 2000. An improved error mod el for noisy channel\\nspelling correction. In Proc. ACL , pp. 286–293. 65,520,528\\nBrin, Sergey, and Lawrence Page. 1998. The anatomy of a large -scale hypertextual\\nweb search engine. In Proc. WWW , pp. 107–117. 149,458,480,520,528\\nBrisaboa, Nieves R., Antonio Fariña, Gonzalo Navarro, and J osé R. Paramá. 2007.\\nLightweight natural language text compression. IR10(1):1–33. 107,520,522,528\\nBroder, Andrei. 2002. A taxonomy of web search. SIGIR Forum 36(2):3–10. DOI:\\ndoi.acm.org/10.1145/792550.792552 .442,520\\nBroder, Andrei, S. Ravi Kumar, Farzin Maghoul, Prabhakar Ra ghavan, Sridhar Ra-\\njagopalan, Raymie Stata, Andrew Tomkins, and Janet Wiener. 2000. Graph struc-\\nture in the web. Computer Networks 33(1):309–320. 441,520,526,527,529,531,\\n532', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 524}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 489\\nBroder, Andrei Z., Steven C. Glassman, Mark S. Manasse, and G eoffrey Zweig. 1997.\\nSyntactic clustering of the web. In Proc. WWW , pp. 391–404. 442,520,523,527,533\\nBrown, Eric W. 1995. Execution Performance Issues in Full-Text Information Ret rieval .\\nPhD thesis, University of Massachusetts, Amherst. 149,520\\nBuckley, Chris, James Allan, and Gerard Salton. 1994a. Auto matic routing and ad-hoc\\nretrieval using SMART: TREC 2. In Proc. TREC , pp. 45–55. 314,519,520,530\\nBuckley, Chris, and Gerard Salton. 1995. Optimization of re levance feedback weights.\\nInProc. SIGIR , pp. 351–357. ACM Press. DOI:doi.acm.org/10.1145/215206.215383 .\\n315,520,530\\nBuckley, Chris, Gerard Salton, and James Allan. 1994b. The e ffect of adding relevance\\ninformation in a relevance feedback environment. In Proc. SIGIR , pp. 292–300.\\nACM Press. 185,194,314,519,520,530\\nBuckley, Chris, Amit Singhal, and Mandar Mitra. 1995. New re trieval approaches\\nusing SMART: TREC 4. In Proc. TREC .187,520,527,531\\nBuckley, Chris, and Ellen M. Voorhees. 2000. Evaluating eva luation measure stability.\\nInProc. SIGIR , pp. 33–40. 173,174,520,532\\nBurges, Chris, Tal Shaked, Erin Renshaw, Ari Lazier, Matt De eds, Nicole Hamilton,\\nand Greg Hullender. 2005. Learning to rank using gradient de scent. In Proc. ICML .\\n348,520,522,523,524,526,529,530\\nBurges, Christopher J. C. 1998. A tutorial on support vector machines for pattern\\nrecognition. Data Mining and Knowledge Discovery 2(2):121–167. 346,520\\nBurner, Mike. 1997. Crawling towards eternity: Building an archive of the World\\nWide Web. Web Techniques Magazine 2(5). 458,520\\nBurnham, Kenneth P ., and David Anderson. 2002. Model Selection and Multi-Model\\nInference . Springer. 373,519,521\\nBush, Vannevar. 1945. As we may think. The Atlantic Monthly . URL:\\nwww.theatlantic.com/doc/194507/bush .17,441,521\\nBüttcher, Stefan, and Charles L. A. Clarke. 2005a. Indexing time vs. query time:\\nTrade-offs in dynamic information retrieval systems. In Proc. CIKM , pp. 317–318.\\nACM Press. DOI:doi.acm.org/10.1145/1099554.1099645 .84,521\\nBüttcher, Stefan, and Charles L. A. Clarke. 2005b. A securit y model for full-\\ntext ﬁle system search in multi-user environments. In Proc. FAST . URL:\\nwww.usenix.org/events/fast05/tech/buettcher.html .84,521\\nBüttcher, Stefan, and Charles L. A. Clarke. 2006. A document -centric approach to\\nstatic index pruning in text retrieval systems. In Proc. CIKM , pp. 182–189. DOI:\\ndoi.acm.org/10.1145/1183614.1183644 .105,521\\nBüttcher, Stefan, Charles L. A. Clarke, and Brad Lushman. 20 06. Hybrid index main-\\ntenance for growing text collections. In Proc. SIGIR , pp. 356–363. ACM Press. DOI:\\ndoi.acm.org/10.1145/1148170.1148233 .84,521,527\\nCacheda, Fidel, Victor Carneiro, Carmen Guerrero, and Ánge l Viña. 2003. Optimiza-\\ntion of restricted searches in web directories using hybrid data structures. In Proc.\\nECIR , pp. 436–451. 372,521,523,532', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 525}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP490 Bibliography\\nCallan, Jamie. 2000. Distributed information retrieval. I n W. Bruce Croft (ed.), Ad-\\nvances in information retrieval , pp. 127–150. Kluwer. 84,521\\nCan, Fazli, Ismail Sengör Altingövde, and Engin Demir. 2004 . Efﬁciency and effec-\\ntiveness of query processing in cluster-based retrieval. Information Systems 29(8):\\n697–717. DOI:dx.doi.org/10.1016/S0306-4379(03)00062-0 .372,519,521,522\\nCan, Fazli, and Esen A. Ozkarahan. 1990. Concepts and effect iveness of the cover-\\ncoefﬁcient-based clustering methodology for text databas es.ACM Trans. Database\\nSyst. 15(4):483–517. 372,521,528\\nCao, Guihong, Jian-Yun Nie, and Jing Bai. 2005. Integrating word relationships into\\nlanguage models. In Proc. SIGIR , pp. 298–305. ACM Press. 252,519,521,528\\nCao, Yunbo, Jun Xu, Tie-Yan Liu, Hang Li, Yalou Huang, and Hsi ao-Wuen Hon. 2006.\\nAdapting Ranking SVM to document retrieval. In Proc. SIGIR . ACM Press. 348,521,\\n524,526,533\\nCarbonell, Jaime, and Jade Goldstein. 1998. The use of MMR, d iversity-based rerank-\\ning for reordering documents and producing summaries. In Proc. SIGIR , pp. 335–\\n336. ACM Press. DOI:doi.acm.org/10.1145/290941.291025 .167,521,523\\nCarletta, Jean. 1996. Assessing agreement on classiﬁcatio n tasks: The kappa statistic.\\nComputational Linguistics 22:249–254. 174,521\\nCarmel, David, Doron Cohen, Ronald Fagin, Eitan Farchi, Mic hael Herscovici,\\nYoelle S. Maarek, and Aya Soffer. 2001. Static index pruning for infor-\\nmation retrieval systems. In Proc. SIGIR , pp. 43–50. ACM Press. DOI:\\ndoi.acm.org/10.1145/383952.383958 .105,149,521,522,524,527,531\\nCarmel, David, Yoelle S. Maarek, Matan Mandelbrod, Yosi Mas s, and Aya Soffer.\\n2003. Searching XML documents via XML fragments. In Proc. SIGIR , pp. 151–158.\\nACM Press. DOI:doi.acm.org/10.1145/860435.860464 .216,521,527,531\\nCaruana, Rich, and Alexandru Niculescu-Mizil. 2006. An emp irical comparison of\\nsupervised learning algorithms. In Proc. ICML .347,521,528\\nCastro, R. M., M. J. Coates, and R. D. Nowak. 2004. Likelihood based hierarchical\\nclustering. IEEE Transactions in Signal Processing 52(8):2308–2321. 400,521,528\\nCavnar, William B., and John M. Trenkle. 1994. N-gram-based text categorization. In\\nProc. SDAIR , pp. 161–175. 46,521,532\\nChakrabarti, Soumen. 2002. Mining the Web: Analysis of Hypertext and Semi Structured\\nData . Morgan Kaufmann. 442,521\\nChakrabarti, Soumen, Byron Dom, David Gibson, Jon Kleinber g, Prabhakar Ragha-\\nvan, and Sridhar Rajagopalan. 1998. Automatic resource lis t compilation by\\nanalyzing hyperlink structure and associated text. In Proc. WWW .URL:cite-\\nseer.ist.psu.edu/chakrabarti98automatic.html .480,481,521,522,523,525,529\\nChapelle, Olivier, Bernhard Schölkopf, and Alexander Zien (eds.). 2006. Semi-\\nSupervised Learning . MIT Press. 347,500,507,521,533\\nChaudhuri, Surajit, Gautam Das, Vagelis Hristidis, and Ger hard Weikum.\\n2006. Probabilistic information retrieval approach for ra nking of database\\nquery results. ACM Transactions on Database Systems 31(3):1134–1168. DOI:\\ndoi.acm.org/10.1145/1166074.1166085 .217,521,522,524,532', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 526}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 491\\nCheeseman, Peter, and John Stutz. 1996. Bayesian classiﬁca tion (AutoClass): Theory\\nand results. In Advances in Knowledge Discovery and Data Mining , pp. 153–180. MIT\\nPress. 374,521,531\\nChen, Hsin-Hsi, and Chuan-Jie Lin. 2000. A multilingual new s summarizer. In Proc.\\nCOLING , pp. 159–165. 373,521,526\\nChen, Pai-Hsuen, Chih-Jen Lin, and Bernhard Schölkopf. 200 5. A tutorial on ν-\\nsupport vector machines. Applied Stochastic Models in Business and Industry 21:\\n111–136. 346,521,526,530\\nChiaramella, Yves, Philippe Mulhem, and Franck Fourel. 199 6. A model for multime-\\ndia information retrieval. Technical Report 4-96, Univers ity of Glasgow. 216,521,\\n523,528\\nChierichetti, Flavio, Alessandro Panconesi, Prabhakar Ra ghavan, Mauro Sozio,\\nAlessandro Tiberi, and Eli Upfal. 2007. Finding near neighb ors through cluster\\npruning. In Proc. PODS .149,521,528,529,531,532\\nCho, Junghoo, and Hector Garcia-Molina. 2002. Parallel cra wlers. In Proc. WWW , pp.\\n124–135. ACM Press. DOI:doi.acm.org/10.1145/511446.511464 .458,521,523\\nCho, Junghoo, Hector Garcia-Molina, and Lawrence Page. 199 8. Efﬁcient crawling\\nthrough URL ordering. In Proc. WWW , pp. 161–172. 458,521,523,528\\nChu-Carroll, Jennifer, John Prager, Krzysztof Czuba, Davi d Ferrucci, and\\nPablo Duboue. 2006. Semantic search via XML fragments: A hig h-\\nprecision approach to IR. In Proc. SIGIR , pp. 445–452. ACM Press. DOI:\\ndoi.acm.org/10.1145/1148170.1148247 .216,521,522,529\\nClarke, Charles L.A., Gordon V . Cormack, and Elizabeth A. Tu dhope. 2000. Relevance\\nranking for one to three term queries. IP&M 36:291–311. 149,521,532\\nCleverdon, Cyril W. 1991. The signiﬁcance of the Cranﬁeld te sts on index languages.\\nInProc. SIGIR , pp. 3–12. ACM Press. 17,173,521\\nCoden, Anni R., Eric W. Brown, and Savitha Srinivasan (eds.) . 2002. Information\\nRetrieval Techniques for Speech Applications . Springer. xxxiv ,520,521,531\\nCohen, Paul R. 1995. Empirical methods for artiﬁcial intelligence . MIT Press. 286,521\\nCohen, William W. 1998. Integration of heterogeneous datab ases without common\\ndomains using queries based on textual similarity. In Proc. SIGMOD , pp. 201–212.\\nACM Press. 217,521\\nCohen, William W., Robert E. Schapire, and Yoram Singer. 199 8. Learn-\\ning to order things. In Proc. NIPS . The MIT Press. URL:cite-\\nseer.ist.psu.edu/article/cohen98learning.html .150,521,530,531\\nCohen, William W., and Yoram Singer. 1999. Context-sensiti ve learning methods for\\ntext categorization. TOIS 17(2):141–173. 339,521,531\\nComtet, Louis. 1974. Advanced Combinatorics . Reidel. 356,521\\nCooper, William S., Aitao Chen, and Fredric C. Gey. 1994. Ful l text retrieval based on\\nprobabilistic equations with coefﬁcients ﬁtted by logisti c regression. In Proc. TREC ,\\npp. 57–66. 150,521,523', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 527}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP492 Bibliography\\nCormen, Thomas H., Charles Eric Leiserson, and Ronald L. Riv est. 1990. Introduction\\nto Algorithms . MIT Press. 11,79,399,521,526,529\\nCover, Thomas M., and Peter E. Hart. 1967. Nearest neighbor p attern classiﬁcation.\\nIEEE Transactions on Information Theory 13(1):21–27. 315,521,524\\nCover, Thomas M., and Joy A. Thomas. 1991. Elements of Information Theory . Wiley.\\n106,251,521,531\\nCrammer, Koby, and Yoram Singer. 2001. On the algorithmic im plementation of\\nmulticlass kernel-based machines. JMLR 2:265–292. 347,521,531\\nCreecy, Robert H., Brij M. Masand, Stephen J. Smith, and Davi d L. Waltz. 1992.\\nTrading MIPS and memory for knowledge engineering. CACM 35(8):48–64. DOI:\\ndoi.acm.org/10.1145/135226.135228 .314,521,527,531,532\\nCrestani, Fabio, Mounia Lalmas, Cornelis J. Van Rijsbergen , and Iain Campbell.\\n1998. Is this document relevant? . . . probably: A survey of pr obabilistic mod-\\nels in information retrieval. ACM Computing Surveys 30(4):528–552. DOI:\\ndoi.acm.org/10.1145/299917.299920 .235,521,526,529\\nCristianini, Nello, and John Shawe-Taylor. 2000. Introduction to Support Vector Ma-\\nchines and Other Kernel-based Learning Methods . Cambridge University Press. 346,\\n521,530\\nCroft, W. Bruce. 1978. A ﬁle organization for cluster-based retrieval. In Proc. SIGIR ,\\npp. 65–82. ACM Press. 372,521\\nCroft, W. Bruce, and David J. Harper. 1979. Using probabilis tic models of document\\nretrieval without relevance information. Journal of Documentation 35(4):285–295.\\n133,227,521,524\\nCroft, W. Bruce, and John Lafferty (eds.). 2003. Language Modeling for Information\\nRetrieval . Springer. 252,522,526\\nCrouch, Carolyn J. 1988. A cluster-based approach to thesau rus construction. In Proc.\\nSIGIR , pp. 309–320. ACM Press. DOI:doi.acm.org/10.1145/62437.62467 .374,522\\nCucerzan, Silviu, and Eric Brill. 2004. Spelling correctio n as an iterative process that\\nexploits the collective knowledge of web users. In Proc. Empirical Methods in Natural\\nLanguage Processing .65,520,522\\nCutting, Douglas R., David R. Karger, and Jan O. Pedersen. 19 93. Constant\\ninteraction-time Scatter/Gather browsing of very large do cument collections. In\\nProc. SIGIR , pp. 126–134. ACM Press. 399,522,525,528\\nCutting, Douglas R., Jan O. Pedersen, David Karger, and John W. Tukey. 1992. Scat-\\nter/Gather: A cluster-based approach to browsing large doc ument collections. In\\nProc. SIGIR , pp. 318–329. ACM Press. 372,399,522,525,528,532\\nDamerau, Fred J. 1964. A technique for computer detection an d correction of spelling\\nerrors. CACM 7(3):171–176. DOI:doi.acm.org/10.1145/363958.363994 .65,522\\nDavidson, Ian, and Ashwin Satyanarayana. 2003. Speeding up k-means clustering\\nby bootstrap averaging. In ICDM 2003 Workshop on Clustering Large Data Sets .373,\\n522,530', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 528}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 493\\nDay, William H., and Herbert Edelsbrunner. 1984. Efﬁcient a lgorithms for agglomer-\\native hierarchical clustering methods. Journal of Classiﬁcation 1:1–24. 399,522\\nde Moura, Edleno Silva, Gonzalo Navarro, Nivio Ziviani, and Ricardo Baeza-Yates.\\n2000. Fast and ﬂexible word searching on compressed text. TOIS 18(2):113–139.\\nDOI:doi.acm.org/10.1145/348751.348754 .107,519,528,533\\nDean, Jeffrey, and Sanjay Ghemawat. 2004. MapReduce: Simpl iﬁed data processing\\non large clusters. In Proc. Symposium on Operating System Design and Implementat ion.\\nxx,76,83,522,523\\nDeerwester, Scott, Susan T. Dumais, George W. Furnas, Thoma s K. Landauer, and\\nRichard Harshman. 1990. Indexing by latent semantic analys is.JASIS 41(6):391–\\n407. 417,522,523,524,526\\ndel Bimbo, Alberto. 1999. Visual Information Retrieval . Morgan Kaufmann. xxxiv ,533\\nDempster, A.P ., N.M. Laird, and D.B. Rubin. 1977. Maximum li kelihood from incom-\\nplete data via the EM algorithm. Journal of the Royal Statistical Society Series B 39:\\n1–38. 373,522,526,530\\nDhillon, Inderjit S. 2001. Co-clustering documents and wor ds using bipartite spectral\\ngraph partitioning. In Proc. KDD , pp. 269–274. 374,400,522\\nDhillon, Inderjit S., and Dharmendra S. Modha. 2001. Concep t decompositions for\\nlarge sparse text data using clustering. Machine Learning 42(1/2):143–175. DOI:\\ndx.doi.org/10.1023/A:1007612920971 .373,522,527\\nDi Eugenio, Barbara, and Michael Glass. 2004. The kappa stat istic: A second look.\\nComputational Linguistics 30(1):95–101. DOI:dx.doi.org/10.1162/089120104773633402 .\\n174,522,523\\nDietterich, Thomas G. 2002. Ensemble learning. In Michael A . Arbib (ed.), The Hand-\\nbook of Brain Theory and Neural Networks , 2nd edition. MIT Press. 347,522\\nDietterich, Thomas G., and Ghulum Bakiri. 1995. Solving mul ticlass learning prob-\\nlems via error-correcting output codes. Journal of Artiﬁcial Intelligence Research 2:\\n263–286. 315,519,522\\nDom, Byron E. 2002. An information-theoretic external clus ter-validity measure. In\\nProc. UAI .373,522\\nDomingos, Pedro. 2000. A uniﬁed bias-variance decompositi on for zero-one and\\nsquared loss. In Proc. National Conference on Artiﬁcial Intelligence and Pr oc. Conference\\nInnovative Applications of Artiﬁcial Intelligence , pp. 564–569. AAAI Press / The MIT\\nPress. 315,522\\nDomingos, Pedro, and Michael J. Pazzani. 1997. On the optima lity of the simple\\nBayesian classiﬁer under zero-one loss. Machine Learning 29(2-3):103–130. URL:\\nciteseer.ist.psu.edu/domingos97optimality.html .286,522,528\\nDownie, J. Stephen. 2006. The Music Information Retrieval E valuation eXchange\\n(MIREX). D-Lib Magazine 12(12). xxxiv ,522\\nDuda, Richard O., Peter E. Hart, and David G. Stork. 2000. Pattern Classiﬁcation , 2nd\\nedition. Wiley-Interscience. 286,372,522,524,531', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 529}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP494 Bibliography\\nDumais, Susan, John Platt, David Heckerman, and Mehran Saha mi. 1998. Inductive\\nlearning algorithms and representations for text categori zation. In Proc. CIKM , pp.\\n148–155. ACM Press. DOI:doi.acm.org/10.1145/288627.288651 .xvii,282,333,334,\\n347,522,524,529,530\\nDumais, Susan T. 1993. Latent semantic indexing (LSI) and TR EC-2. In Proc. TREC ,\\npp. 105–115. 415,417,522\\nDumais, Susan T. 1995. Latent semantic indexing (LSI): TREC -3 report. In Proc. TREC ,\\npp. 219–230. 416,417,522\\nDumais, Susan T., and Hao Chen. 2000. Hierarchical classiﬁc ation of Web content. In\\nProc. SIGIR , pp. 256–263. ACM Press. 347,521,522\\nDunning, Ted. 1993. Accurate methods for the statistics of s urprise and coincidence.\\nComputational Linguistics 19(1):61–74. 286,522\\nDunning, Ted. 1994. Statistical identiﬁcation of language . Technical Report 94-273,\\nComputing Research Laboratory, New Mexico State Universit y.46,522\\nEckart, Carl, and Gale Young. 1936. The approximation of a ma trix by another of\\nlower rank. Psychometrika 1:211–218. 417,522,533\\nEl-Hamdouchi, Abdelmoula, and Peter Willett. 1986. Hierar chic document classiﬁca-\\ntion using Ward’s clustering method. In Proc. SIGIR , pp. 149–156. ACM Press. DOI:\\ndoi.acm.org/10.1145/253168.253200 .399,522,532\\nElias, Peter. 1975. Universal code word sets and representa tions of the integers. IEEE\\nTransactions on Information Theory 21(2):194–203. 106,522\\nEyheramendy, Susana, David Lewis, and David Madigan. 2003. On the Naive Bayes\\nmodel for text categorization. In International Workshop on Artiﬁcial Intelligence and\\nStatistics . Society for Artiﬁcial Intelligence and Statistics. 286,522,526,527\\nFallows, Deborah, 2004. The internet and daily life. URL:\\nwww.pewinternet.org/pdfs/PIP_Internet_and_Daily_Lif e.pdf . Pew/Internet and American\\nLife Project. xxxi,522\\nFayyad, Usama M., Cory Reina, and Paul S. Bradley. 1998. Init ialization of iterative\\nreﬁnement clustering algorithms. In Proc. KDD , pp. 194–198. 374,520,522,529\\nFellbaum, Christiane D. 1998. WordNet – An Electronic Lexical Database . MIT Press.\\n194,522\\nFerragina, Paolo, and Rossano Venturini. 2007. Compressed permuterm indexes. In\\nProc. SIGIR . ACM Press. 65,522,532\\nForman, George. 2004. A pitfall and solution in multi-class feature selection for text\\nclassiﬁcation. In Proc. ICML .286,523\\nForman, George. 2006. Tackling concept drift by temporal in ductive transfer. In Proc.\\nSIGIR , pp. 252–259. ACM Press. DOI:doi.acm.org/10.1145/1148170.1148216 .286,523\\nForman, George, and Ira Cohen. 2004. Learning from little: C omparison of classiﬁers\\ngiven little training. In Proc. PKDD , pp. 161–172. 336,521,523\\nFowlkes, Edward B., and Colin L. Mallows. 1983. A method for c omparing two\\nhierarchical clusterings. Journal of the American Statistical Association 78(383):553–\\n569. URL:www.jstor.org/view/01621459/di985957/98p0926l/0 .400,523,527', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 530}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 495\\nFox, Edward A., and Whay C. Lee. 1991. FAST-INV: A fast algori thm for building\\nlarge inverted ﬁles. Technical report, Virginia Polytechn ic Institute & State Univer-\\nsity, Blacksburg, VA, USA. 83,523,526\\nFraenkel, Aviezri S., and Shmuel T. Klein. 1985. Novel compr ession of sparse bit-\\nstrings – preliminary report. In Combinatorial Algorithms on Words, NATO ASI Series\\nVol F12 , pp. 169–183. Springer. 106,523,525\\nFrakes, William B., and Ricardo Baeza-Yates (eds.). 1992. Information Retrieval: Data\\nStructures and Algorithms . Prentice Hall. 497,509,519,523\\nFraley, Chris, and Adrian E. Raftery. 1998. How many cluster s? Which clustering\\nmethod? Answers via model-based cluster analysis. Computer Journal 41(8):578–\\n588. 373,523,529\\nFriedl, Jeffrey E. F. 2006. Mastering Regular Expressions , 3rd edition. O’Reilly. 18,523\\nFriedman, Jerome H. 1997. On bias, variance, 0/1–loss, and t he curse-of-\\ndimensionality. Data Mining and Knowledge Discovery 1(1):55–77. 286,315,523\\nFriedman, Nir, and Moises Goldszmidt. 1996. Building class iﬁers using Bayesian\\nnetworks. In Proc. National Conference on Artiﬁcial Intelligence , pp. 1277–1284. 231,\\n523\\nFuhr, Norbert. 1989. Optimum polynomial retrieval functio ns based on the probabil-\\nity ranking principle. TOIS 7(3):183–204. 150,523\\nFuhr, Norbert. 1992. Probabilistic models in information r etrieval. Computer Journal\\n35(3):243–255. 235,348,523\\nFuhr, Norbert, Norbert Gövert, Gabriella Kazai, and Mounia Lalmas (eds.). 2003a.\\nINitiative for the Evaluation of XML Retrieval (INEX). Proc . First INEX Workshop .\\nERCIM. 216,523,525,526\\nFuhr, Norbert, and Kai Großjohann. 2004. XIRQL: An XML query lan-\\nguage based on information retrieval concepts. TOIS 22(2):313–356. URL:\\ndoi.acm.org/10.1145/984321.984326 .216,523\\nFuhr, Norbert, and Mounia Lalmas. 2007. Advances in XML retr ieval: The INEX\\ninitiative. In International Workshop on Research Issues in Digital Libra ries.216,523,\\n526\\nFuhr, Norbert, Mounia Lalmas, Saadia Malik, and Gabriella K azai (eds.). 2006. Ad-\\nvances in XML Information Retrieval and Evaluation, 4th Int ernational Workshop of the\\nInitiative for the Evaluation of XML Retrieval, INEX 2005 . Springer. 216,523,525,526,\\n527\\nFuhr, Norbert, Mounia Lalmas, Saadia Malik, and Zoltán Szlá vik (eds.). 2005. Ad-\\nvances in XML Information Retrieval, Third International W orkshop of the Initiative for\\nthe Evaluation of XML Retrieval, INEX 2004 . Springer. 216,507,515,523,526,527,\\n531\\nFuhr, Norbert, Mounia Lalmas, and Andrew Trotman (eds.). 20 07.Comparative Evalu-\\nation of XML Information Retrieval Systems, 5th Internatio nal Workshop of the Initiative\\nfor the Evaluation of XML Retrieval, INEX 2006 . Springer. 216,502,504,523,526,532', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 531}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP496 Bibliography\\nFuhr, Norbert, Saadia Malik, and Mounia Lalmas (eds.). 2003 b.INEX 2003 Workshop .\\nURL:inex.is.informatik.uni-duisburg.de:2003/proceedings .pdf.216,496,505,523,526,527\\nFuhr, Norbert, and Ulrich Pfeifer. 1994. Probabilistic inf ormation retrieval as a com-\\nbination of abstraction, inductive learning, and probabil istic assumptions. TOIS 12\\n(1):92–115. DOI:doi.acm.org/10.1145/174608.174612 .150,523,529\\nFuhr, Norbert, and Thomas Rölleke. 1997. A probabilistic re lational algebra for the\\nintegration of information retrieval and database systems .TOIS 15(1):32–66. DOI:\\ndoi.acm.org/10.1145/239041.239045 .217,523,530\\nGaertner, Thomas, John W. Lloyd, and Peter A. Flach. 2002. Ke rnels for structured\\ndata. In Proc. International Conference on Inductive Logic Program ming , pp. 66–83. 347,\\n522,523,527\\nGao, Jianfeng, Mu Li, Chang-Ning Huang, and Andi Wu. 2005. Ch inese word seg-\\nmentation and named entity recognition: A pragmatic approa ch. Computational\\nLinguistics 31(4):531–574. 46,523,524,526,533\\nGao, Jianfeng, Jian-Yun Nie, Guangyuan Wu, and Guihong Cao. 2004. Dependence\\nlanguage model for information retrieval. In Proc. SIGIR , pp. 170–177. ACM Press.\\n252,521,523,528,533\\nGarcia, Steven, Hugh E. Williams, and Adam Cannane. 2004. Ac cess-ordered indexes.\\nInProc. Australasian Conference on Computer Science , pp. 7–14. 149,521,523,533\\nGarcia-Molina, Hector, Jennifer Widom, and Jeffrey D. Ullm an. 1999. Database System\\nImplementation . Prentice Hall. 84,523,532\\nGarﬁeld, Eugene. 1955. Citation indexes to science: A new di mension in documenta-\\ntion through association of ideas. Science 122:108–111. 480,523\\nGarﬁeld, Eugene. 1976. The permuterm subject index: An auto biographic review.\\nJASIS 27(5-6):288–291. 65,523\\nGeman, Stuart, Elie Bienenstock, and René Doursat. 1992. Ne ural networks and the\\nbias/variance dilemma. Neural Computation 4(1):1–58. 315,520,522,523\\nGeng, Xiubo, Tie-Yan Liu, Tao Qin, and Hang Li. 2007. Feature selection for ranking.\\nInProc. SIGIR , pp. 407–414. ACM Press. 348,523,526,529\\nGerrand, Peter. 2007. Estimating linguistic diversity on t he internet: A taxonomy\\nto avoid pitfalls and paradoxes. Journal of Computer-Mediated Communication 12(4).\\nURL:jcmc.indiana.edu/vol12/issue4/gerrand.html . article 8. 30,523\\nGey, Fredric C. 1994. Inferring probability of relevance us ing the method of logistic\\nregression. In Proc. SIGIR , pp. 222–231. ACM Press. 348,523\\nGhamrawi, Nadia, and Andrew McCallum. 2005. Collective mul ti-label classiﬁcation.\\nInProc. CIKM , pp. 195–200. ACM Press. DOI:doi.acm.org/10.1145/1099554.1099591 .\\n315,523,527\\nGlover, Eric, David M. Pennock, Steve Lawrence, and Robert K rovetz. 2002a. In-\\nferring hierarchical descriptions. In Proc. CIKM , pp. 507–514. ACM Press. DOI:\\ndoi.acm.org/10.1145/584792.584876 .400,523,526,529', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 532}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 497\\nGlover, Eric J., Kostas Tsioutsiouliklis, Steve Lawrence, David M. Pennock,\\nand Gary W. Flake. 2002b. Using web structure for classifyin g and de-\\nscribing web pages. In Proc. WWW , pp. 562–569. ACM Press. DOI:\\ndoi.acm.org/10.1145/511446.511520 .400,522,523,526,529,532\\nGövert, Norbert, and Gabriella Kazai. 2003. Overview of the INitiative for the\\nEvaluation of XML retrieval (INEX) 2002. In Fuhr et al. (2003b ), pp. 1–17. URL:\\ninex.is.informatik.uni-duisburg.de:2003/proceedings .pdf.216,523,525\\nGrabs, Torsten, and Hans-Jörg Schek. 2002. Generating vect or spaces on-the-ﬂy for\\nﬂexible XML retrieval. In XML and Information Retrieval Workshop at SIGIR 2002 .\\n216,523,530\\nGreiff, Warren R. 1998. A theory of term weighting based on ex ploratory data analy-\\nsis. In Proc. SIGIR , pp. 11–19. ACM Press. 227,523\\nGrinstead, Charles M., and J. Laurie Snell. 1997. Introduction to\\nProbability , 2nd edition. American Mathematical Society. URL:\\nwww.dartmouth.edu/ ~chance/teaching_aids/books_articles/probability_boo k/amsbook.mac.pdf .\\n235,523,531\\nGrossman, David A., and Ophir Frieder. 2004. Information Retrieval: Algorithms and\\nHeuristics , 2nd edition. Springer. xxxiv ,84,217,523\\nGusﬁeld, Dan. 1997. Algorithms on Strings, Trees and Sequences: Computer Scien ce and\\nComputational Biology . Cambridge University Press. 65,523\\nHamerly, Greg, and Charles Elkan. 2003. Learning the kink-means. In Proc. NIPS .\\nURL:books.nips.cc/papers/ﬁles/nips16/NIPS2003_AA36.pdf .373,522,523\\nHan, Eui-Hong, and George Karypis. 2000. Centroid-based do cument classiﬁcation:\\nAnalysis and experimental results. In Proc. PKDD , pp. 424–431. 314,524,525\\nHand, David J. 2006. Classiﬁer technology and the illusion o f progress. Statistical\\nScience 21:1–14. 286,524\\nHand, David J., and Keming Yu. 2001. Idiot’s Bayes: Not so stu pid after all. Interna-\\ntional Statistical Review 69(3):385–398. 286,524,533\\nHarman, Donna. 1991. How effective is sufﬁxing? JASIS 42:7–15. 46,524\\nHarman, Donna. 1992. Relevance feedback revisited. In Proc. SIGIR , pp. 1–10. ACM\\nPress. 185,194,524\\nHarman, Donna, Ricardo Baeza-Yates, Edward Fox, and W. Lee. 1992. Inverted ﬁles.\\nInFrakes and Baeza-Yates (1992 ), pp. 28–43. 83,519,523,524,526\\nHarman, Donna, and Gerald Candela. 1990. Retrieving record s from a gigabyte of\\ntext on a minicomputer using statistical ranking. JASIS 41(8):581–589. 83,521,524\\nHarold, Elliotte Rusty, and Scott W. Means. 2004. XML in a Nutshell , 3rd edition.\\nO’Reilly. 216,524,527\\nHarter, Stephen P . 1998. Variations in relevance assessmen ts and the measurement of\\nretrieval effectiveness. JASIS 47:37–49. 174,524\\nHartigan, J. A., and M. A. Wong. 1979. A K-means clustering al gorithm. Applied\\nStatistics 28:100–108. 373,524,533', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 533}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP498 Bibliography\\nHastie, Trevor, Robert Tibshirani, and Jerome H. Friedman. 2001. The Elements of\\nStatistical Learning: Data Mining, Inference, and Predict ion. Springer. 286,314,315,\\n347,523,524,531\\nHatzivassiloglou, Vasileios, Luis Gravano, and Ankineedu Maganti. 2000.\\nAn investigation of linguistic features and clustering alg orithms for topi-\\ncal document clustering. In Proc. SIGIR , pp. 224–231. ACM Press. DOI:\\ndoi.acm.org/10.1145/345508.345582 .373,523,524,527\\nHaveliwala, Taher. 2003. Topic-sensitive PageRank: A cont ext-sensitive ranking al-\\ngorithm for web search. IEEE Transactions on Knowledge and Data Engineering 15(4):\\n784–796. URL:citeseer.ist.psu.edu/article/haveliwala03topicsensi tive.html .481,524\\nHaveliwala, Taher H. 2002. Topic-sensitive PageRank. In Proc. WWW .URL:cite-\\nseer.ist.psu.edu/haveliwala02topicsensitive.html .481,524\\nHayes, Philip J., and Steven P . Weinstein. 1990. CONSTRUE/T IS: A system for\\ncontent-based indexing of a database of news stories. In Proc. Conference on In-\\nnovative Applications of Artiﬁcial Intelligence , pp. 49–66. 335,524,532\\nHeaps, Harold S. 1978. Information Retrieval: Computational and Theoretical Asp ects.\\nAcademic Press. 105,524\\nHearst, Marti A. 1997. TextTiling: Segmenting text into mul ti-paragraph subtopic\\npassages. Computational Linguistics 23(1):33–64. 217,524\\nHearst, Marti A. 2006. Clustering versus faceted categorie s for information explo-\\nration. CACM 49(4):59–61. DOI:doi.acm.org/10.1145/1121949.1121983 .372,524\\nHearst, Marti A., and Jan O. Pedersen. 1996. Reexamining the cluster hypothesis. In\\nProc. SIGIR , pp. 76–84. ACM Press. 372,524,528\\nHearst, Marti A., and Christian Plaunt. 1993. Subtopic stru cturing for full-\\nlength document access. In Proc. SIGIR , pp. 59–68. ACM Press. DOI:\\ndoi.acm.org/10.1145/160688.160695 .217,524,529\\nHeinz, Steffen, and Justin Zobel. 2003. Efﬁcient single-pa ss index construction for\\ntext databases. JASIST 54(8):713–729. DOI:dx.doi.org/10.1002/asi.10268 .83,524,533\\nHeinz, Steffen, Justin Zobel, and Hugh E. Williams. 2002. Bu rst tries: A\\nfast, efﬁcient data structure for string keys. TOIS 20(2):192–223. DOI:\\ndoi.acm.org/10.1145/506309.506312 .84,524,533\\nHenzinger, Monika R., Allan Heydon, Michael Mitzenmacher, and Marc Najork.\\n2000. On near-uniform URL sampling. In Proc. WWW , pp. 295–308. North-Holland.\\nDOI:dx.doi.org/10.1016/S1389-1286(00)00055-4 .442,524,527,528\\nHerbrich, Ralf, Thore Graepel, and Klaus Obermayer. 2000. L arge margin rank\\nboundaries for ordinal regression. In Advances in Large Margin Classiﬁers , pp. 115–\\n132. MIT Press. 348,523,524,528\\nHersh, William, Chris Buckley, T. J. Leone, and David Hickam . 1994. OHSUMED: An\\ninteractive retrieval evaluation and new large test collec tion for research. In Proc.\\nSIGIR , pp. 192–201. ACM Press. 174,520,524,526\\nHersh, William R., Andrew Turpin, Susan Price, Benjamin Cha n, Dale Kraemer,\\nLynetta Sacherek, and Daniel Olson. 2000a. Do batch and user evaluation give\\nthe same results? In Proc. SIGIR , pp. 17–24. 175,521,524,526,528,529,530,532', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 534}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 499\\nHersh, William R., Andrew Turpin, Susan Price, Dale Kraemer , Daniel Olson, Ben-\\njamin Chan, and Lynetta Sacherek. 2001. Challenging conven tional assumptions\\nof automated information retrieval with real users: Boolea n searching and batch\\nretrieval evaluations. IP&M 37(3):383–402. 175,521,524,526,528,529,530,532\\nHersh, William R., Andrew Turpin, Lynetta Sacherek, Daniel Olson, Susan Price, Ben-\\njamin Chan, and Dale Kraemer. 2000b. Further analysis of whe ther batch and user\\nevaluations give the same results with a question-answerin g task. In Proc. TREC .\\n175,521,524,526,528,529,530,532\\nHiemstra, Djoerd. 1998. A linguistically motivated probab ilistic model of information\\nretrieval. In Proc. ECDL , volume 1513 of LNCS , pp. 569–584. 252,524\\nHiemstra, Djoerd. 2000. A probabilistic justiﬁcation for u sing tf.idf term weighting in\\ninformation retrieval. International Journal on Digital Libraries 3(2):131–139. 246,524\\nHiemstra, Djoerd, and Wessel Kraaij. 2005. A language-mode ling approach to TREC.\\nInVoorhees and Harman (2005 ), pp. 373–395. 252,524,526\\nHirai, Jun, Sriram Raghavan, Hector Garcia-Molina, and And reas Paepcke. 2000.\\nWebBase: A repository of web pages. In Proc. WWW , pp. 277–293. 458,523,524,\\n528,529\\nHofmann, Thomas. 1999a. Probabilistic Latent Semantic Ind exing. In Proc. UAI .URL:\\nciteseer.ist.psu.edu/hofmann99probabilistic.html .417,524\\nHofmann, Thomas. 1999b. Probabilistic Latent Semantic Ind exing. In Proc. SIGIR , pp.\\n50–57. ACM Press. URL:citeseer.ist.psu.edu/article/hofmann99probabilistic .html .417,\\n524\\nHollink, Vera, Jaap Kamps, Christof Monz, and Maarten de Rij ke. 2004. Monolingual\\ndocument retrieval for European languages. IR7(1):33–52. 46,524,525,528,529\\nHopcroft, John E., Rajeev Motwani, and Jeffrey D. Ullman. 20 00.Introduction to Au-\\ntomata Theory, Languages, and Computation , 2nd edition. Addison Wesley. 18,524,\\n528,532\\nHuang, Yifen, and Tom M. Mitchell. 2006. Text clustering wit h ex-\\ntended user feedback. In Proc. SIGIR , pp. 413–420. ACM Press. DOI:\\ndoi.acm.org/10.1145/1148170.1148242 .374,524,527\\nHubert, Lawrence, and Phipps Arabie. 1985. Comparing parti tions. Journal of Classi-\\nﬁcation 2:193–218. 373,519,524\\nHughes, Baden, Timothy Baldwin, Steven Bird, Jeremy Nichol son, and Andrew\\nMacKinlay. 2006. Reconsidering language identiﬁcation fo r written language re-\\nsources. In Proc. International Conference on Language Resources and E valuation , pp.\\n485–488. 46,519,520,524,527,528\\nHull, David. 1993. Using statistical testing in the evaluat ion of retrieval performance.\\nInProc. SIGIR , pp. 329–338. ACM Press. 173,524\\nHull, David. 1996. Stemming algorithms – A case study for det ailed evaluation. JASIS\\n47(1):70–84. 46,524\\nIde, E. 1971. New experiments in relevance feedback. In Salton (1971b ), pp. 337–354.\\n193,524', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 535}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP500 Bibliography\\nIndyk, Piotr. 2004. Nearest neighbors in high-dimensional spaces. In J. E. Good-\\nman and J. O’Rourke (eds.), Handbook of Discrete and Computational Geometry , 2nd\\nedition, pp. 877–892. Chapman and Hall/CRC Press. 314,524\\nIngwersen, Peter, and Kalervo Järvelin. 2005. The Turn: Integration of Information\\nSeeking and Retrieval in Context . Springer. xxxiv ,524\\nIttner, David J., David D. Lewis, and David D. Ahn. 1995. Text categorization of low\\nquality images. In Proc. SDAIR , pp. 301–315. 314,519,524,526\\nIwayama, Makoto, and Takenobu Tokunaga. 1995. Cluster-bas ed text categorization:\\nA comparison of category search strategies. In Proc. SIGIR , pp. 273–280. ACM\\nPress. 314,524,531\\nJackson, Peter, and Isabelle Moulinier. 2002. Natural Language Processing for Online\\nApplications: Text Retrieval, Extraction and Categorizat ion. John Benjamins. 334,335,\\n524,528\\nJacobs, Paul S., and Lisa F. Rau. 1990. SCISOR: Extracting in formation from on-line\\nnews. CACM 33:88–97. 335,524,529\\nJain, Anil, M. Narasimha Murty, and Patrick Flynn. 1999. Dat a clustering: A review.\\nACM Computing Surveys 31(3):264–323. 399,523,524,528\\nJain, Anil K., and Richard C. Dubes. 1988. Algorithms for Clustering Data . Prentice\\nHall. 399,522,524\\nJardine, N., and Cornelis Joost van Rijsbergen. 1971. The us e of hierarchic clustering\\nin information retrieval. Information Storage and Retrieval 7:217–240. 372,525,529\\nJärvelin, Kalervo, and Jaana Kekäläinen. 2002. Cumulated g ain-based evaluation of\\nIR techniques. TOIS 20(4):422–446. 174,525\\nJeh, Glen, and Jennifer Widom. 2003. Scaling personalized w eb search. In Proc.\\nWWW , pp. 271–279. ACM Press. 481,525,532\\nJensen, Finn V ., and Finn B. Jensen. 2001. Bayesian Networks and Decision Graphs .\\nSpringer. 234,525\\nJeong, Byeong-Soo, and Edward Omiecinski. 1995. Inverted ﬁ le partitioning schemes\\nin multiple disk systems. IEEE Transactions on Parallel and Distributed Systems 6(2):\\n142–153. 458,525,528\\nJi, Xiang, and Wei Xu. 2006. Document clustering with prior k nowledge. In Proc.\\nSIGIR , pp. 405–412. ACM Press. DOI:doi.acm.org/10.1145/1148170.1148241 .374,\\n525,533\\nJing, Hongyan. 2000. Sentence reduction for automatic text summarization. In Proc.\\nConference on Applied Natural Language Processing , pp. 310–315. 174,525\\nJoachims, Thorsten. 1997. A probabilistic analysis of the R occhio algorithm with tﬁdf\\nfor text categorization. In Proc. ICML , pp. 143–151. Morgan Kaufmann. 314,525\\nJoachims, Thorsten. 1998. Text categorization with suppor t vector machines: Learn-\\ning with many relevant features. In Proc. ECML , pp. 137–142. Springer. xvii,282,\\n333,334,525', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 536}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 501\\nJoachims, Thorsten. 1999. Making large-scale SVM learning practical. In B. Schölkopf,\\nC. Burges, and A. Smola (eds.), Advances in Kernel Methods - Support Vector Learning .\\nMIT Press. 347,525\\nJoachims, Thorsten. 2002a. Learning to Classify Text Using Support Vector Machines .\\nKluwer. xvii,334,347,525\\nJoachims, Thorsten. 2002b. Optimizing search engines usin g clickthrough data. In\\nProc. KDD , pp. 133–142. 175,185,348,525\\nJoachims, Thorsten. 2006a. Training linear SVMs in linear t ime. In Proc. KDD , pp.\\n217–226. ACM Press. DOI:doi.acm.org/10.1145/1150402.1150429 .286,329,347,525\\nJoachims, Thorsten. 2006b. Transductive support vector ma chines. In Chapelle et al.\\n(2006 ), pp. 105–118. 347,525\\nJoachims, Thorsten, Laura Granka, Bing Pan, Helene Hembroo ke, and Geri Gay. 2005.\\nAccurately interpreting clickthrough data as implicit fee dback. In Proc. SIGIR , pp.\\n154–161. ACM Press. 175,185,523,524,525,528\\nJohnson, David, Vishv Malhotra, and Peter Vamplew. 2006. Mo re effective web search\\nusing bigrams and trigrams. Webology 3(4). URL:www.webology.ir/2006/v3n4/a35.html .\\nArticle 35. 47,525,527,532\\nJurafsky, Dan, and James H. Martin. 2008. Speech and Language Processing: An Introduc-\\ntion to Natural Language Processing, Computational Lingui stics and Speech Recognition ,\\n2nd edition. Prentice Hall. xxxiv ,252,525,527\\nKäki, Mika. 2005. Findex: Search result categories help use rs when doc-\\nument ranking fails. In Proc. SIGCHI , pp. 131–140. ACM Press. DOI:\\ndoi.acm.org/10.1145/1054972.1054991 .372,400,526\\nKammenhuber, Nils, Julia Luxenburger, Anja Feldmann, and G erhard Weikum. 2006.\\nWeb search clickstreams. In Proc. ACM SIGCOMM on Internet Measurement , pp.\\n245–250. ACM Press. 47,522,525,527,532\\nKamps, Jaap, Maarten de Rijke, and Börkur Sigurbjörnsson. 2 004. Length nor-\\nmalization in XML retrieval. In Proc. SIGIR , pp. 80–87. ACM Press. DOI:\\ndoi.acm.org/10.1145/1008992.1009009 .216,525,529,530\\nKamps, Jaap, Maarten Marx, Maarten de Rijke, and Börkur Sigu rbjörnsson. 2006.\\nArticulating information needs in XML query languages. TOIS 24(4):407–436. DOI:\\ndoi.acm.org/10.1145/1185877.1185879 .216,525,527,529,530\\nKamvar, Sepandar D., Dan Klein, and Christopher D. Manning. 2002. Interpreting\\nand extending classical agglomerative clustering algorit hms using a model-based\\napproach. In Proc. ICML , pp. 283–290. Morgan Kaufmann. 400,525,527\\nKannan, Ravi, Santosh Vempala, and Adrian Vetta. 2000. On cl usterings – Good, bad\\nand spectral. In Proc. Symposium on Foundations of Computer Science , pp. 367–377.\\nIEEE Computer Society. 400,525,532\\nKaszkiel, Marcin, and Justin Zobel. 1997. Passage retrieva l revisited. In Proc. SIGIR ,\\npp. 178–185. ACM Press. DOI:doi.acm.org/10.1145/258525.258561 .217,525,533\\nKaufman, Leonard, and Peter J. Rousseeuw. 1990. Finding groups in data . Wiley. 373,\\n525,530', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 537}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP502 Bibliography\\nKazai, Gabriella, and Mounia Lalmas. 2006. eXtended cumula ted gain measures\\nfor the evaluation of content-oriented XML retrieval. TOIS 24(4):503–542. DOI:\\ndoi.acm.org/10.1145/1185883 .217,525,526\\nKekäläinen, Jaana. 2005. Binary and graded relevance in IR e valuations – Comparison\\nof the effects on ranking of IR systems. IP&M 41:1019–1033. 174,525\\nKekäläinen, Jaana, and Kalervo Järvelin. 2002. Using grade d relevance assessments\\nin IR evaluation. JASIST 53(13):1120–1129. 174,525\\nKemeny, John G., and J. Laurie Snell. 1976. Finite Markov Chains . Springer. 480,525,\\n531\\nKent, Allen, Madeline M. Berry, Fred U. Luehrs, Jr., and J. W. Perry. 1955. Machine\\nliterature searching VIII. Operational criteria for desig ning information retrieval\\nsystems. American Documentation 6(2):93–101. 173,520,525,527,529\\nKernighan, Mark D., Kenneth W. Church, and William A. Gale. 1 990. A spelling\\ncorrection program based on a noisy channel model. In Proc. ACL , pp. 205–210. 65,\\n521,523,525\\nKing, Benjamin. 1967. Step-wise clustering procedures. Journal of the American Statis-\\ntical Association 69:86–101. 399,525\\nKishida, Kazuaki, Kuang-Hua Chen, Sukhoon Lee, Kazuko Kuri yama, Noriko\\nKando, Hsin-Hsi Chen, and Sung Hyon Myaeng. 2005. Overview o f CLIR task\\nat the ﬁfth NTCIR workshop. In NTCIR Workshop Meeting on Evaluation of Informa-\\ntion Access Technologies: Information Retrieval, Questio n Answering and Cross-Lingual\\nInformation Access . National Institute of Informatics. 45,521,525,526,528\\nKlein, Dan, and Christopher D. Manning. 2002. Conditional s tructure versus con-\\nditional estimation in NLP models. In Proc. Empirical Methods in Natural Language\\nProcessing , pp. 9–16. 336,525,527\\nKleinberg, Jon M. 1997. Two algorithms for nearest-neighbo r search in high dimen-\\nsions. In Proc. ACM Symposium on Theory of Computing , pp. 599–608. ACM Press.\\nDOI:doi.acm.org/10.1145/258533.258653 .314,525\\nKleinberg, Jon M. 1999. Authoritative sources in a hyperlin ked environment. JACM\\n46(5):604–632. URL:citeseer.ist.psu.edu/article/kleinberg98authoritati ve.html .481,525\\nKleinberg, Jon M. 2002. An impossibility theorem for cluste ring. In Proc. NIPS .373,\\n525\\nKnuth, Donald E. 1997. The Art of Computer Programming, Volume 3: Sorting and\\nSearching , 3rd edition. Addison Wesley. 65,525\\nKo, Youngjoong, Jinwoo Park, and Jungyun Seo. 2004. Improvi ng text categorization\\nusing the importance of sentences. IP&M 40(1):65–79. 340,525,528,530\\nKoenemann, Jürgen, and Nicholas J. Belkin. 1996. A case for i nteraction: A study of\\ninteractive information retrieval behavior and effective ness. In Proc. SIGCHI , pp.\\n205–212. ACM Press. DOI:doi.acm.org/10.1145/238386.238487 .194,520,525\\nKołcz, Aleksander, Vidya Prabakarmurthi, and Jugal Kalita . 2000. Summarization as\\nfeature selection for text categorization. In Proc. CIKM , pp. 365–370. ACM Press.\\n340,525,529', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 538}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 503\\nKołcz, Aleksander, and Wen-Tau Yih. 2007. Raising the basel ine for high-precision\\ntext classiﬁers. In Proc. KDD .286,525,533\\nKoller, Daphne, and Mehran Sahami. 1997. Hierarchically cl assifying documents\\nusing very few words. In Proc. ICML , pp. 170–178. 347,525,530\\nKonheim, Alan G. 1981. Cryptography: A Primer . John Wiley & Sons. 46,525\\nKorfhage, Robert R. 1997. Information Storage and Retrieval . Wiley. xxxiv ,175,525\\nKozlov, M. K., S. P . Tarasov, and L. G. Khachiyan. 1979. Polyn omial solvability of con-\\nvex quadratic programming. Soviet Mathematics Doklady 20:1108–1111. Translated\\nfrom original in Doklady Akademiia Nauk SSR , 228 (1979). 328,525,531\\nKraaij, Wessel, and Martijn Spitters. 2003. Language model s for topic tracking. In\\nW. B. Croft and J. Lafferty (eds.), Language Modeling for Information Retrieval , pp.\\n95–124. Kluwer. 251,526,531\\nKraaij, Wessel, Thijs Westerveld, and Djoerd Hiemstra. 200 2. The importance of prior\\nprobabilities for entry page search. In Proc. SIGIR , pp. 27–34. ACM Press. 252,524,\\n526,532\\nKrippendorff, Klaus. 2003. Content Analysis: An Introduction to its Methodology . Sage.\\n174,526\\nKrovetz, Bob. 1995. Word sense disambiguation for large text databases . PhD thesis,\\nUniversity of Massachusetts Amherst. 46,526\\nKukich, Karen. 1992. Techniques for automatically correct ing words in text. ACM\\nComputing Surveys 24(4):377–439. DOI:doi.acm.org/10.1145/146370.146380 .65,526\\nKumar, Ravi, Prabhakar Raghavan, Sridhar Rajagopalan, and Andrew Tomkins. 1999.\\nTrawling the Web for emerging cyber-communities. Computer Networks 31(11–16):\\n1481–1493. URL:citeseer.ist.psu.edu/kumar99trawling.html .442,526,529,531\\nKumar, S. Ravi, Prabhakar Raghavan, Sridhar Rajagopalan, D andapani Sivakumar,\\nAndrew Tomkins, and Eli Upfal. 2000. The Web as a graph. In Proc. PODS , pp.\\n1–10. ACM Press. URL:citeseer.ist.psu.edu/article/kumar00web.html .441,526,529,531,\\n532\\nKupiec, Julian, Jan Pedersen, and Francine Chen. 1995. A tra inable document sum-\\nmarizer. In Proc. SIGIR , pp. 68–73. ACM Press. 174,521,526,529\\nKurland, Oren, and Lillian Lee. 2004. Corpus structure, lan guage models, and\\nad hoc information retrieval. In Proc. SIGIR , pp. 194–201. ACM Press. DOI:\\ndoi.acm.org/10.1145/1008992.1009027 .372,526\\nLafferty, John, and Chengxiang Zhai. 2001. Document langua ge models, query mod-\\nels, and risk minimization for information retrieval. In Proc. SIGIR , pp. 111–119.\\nACM Press. 250,251,526,533\\nLafferty, John, and Chengxiang Zhai. 2003. Probabilistic r elevance models based\\non document and query generation. In W. Bruce Croft and John L afferty (eds.),\\nLanguage Modeling for Information Retrieval . Kluwer. 252,526,533\\nLalmas, Mounia, Gabriella Kazai, Jaap Kamps, Jovan Pehcevs ki, Benjamin Pi-\\nwowarski, and Stephen E. Robertson. 2007. INEX 2006 evaluat ion measures. In\\nFuhr et al. (2007 ), pp. 20–34. 217,525,526,529', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 539}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP504 Bibliography\\nLalmas, Mounia, and Anastasios Tombros. 2007. Evaluating X ML retrieval effective-\\nness at INEX. SIGIR Forum 41(1):40–57. DOI:doi.acm.org/10.1145/1273221.1273225 .\\n216,526,531\\nLance, G. N., and W. T. Williams. 1967. A general theory of cla ssiﬁcatory sorting\\nstrategies 1. Hierarchical systems. Computer Journal 9(4):373–380. 399,526,533\\nLangville, Amy, and Carl Meyer. 2006. Google’s PageRank and Beyond: The Science of\\nSearch Engine Rankings . Princeton University Press. 481,526,527\\nLarsen, Bjornar, and Chinatsu Aone. 1999. Fast and effectiv e text mining using\\nlinear-time document clustering. In Proc. KDD , pp. 16–22. ACM Press. DOI:\\ndoi.acm.org/10.1145/312129.312186 .399,400,519,526\\nLarson, Ray R. 2005. A fusion approach to XML structured docu ment retrieval. IR8\\n(4):601–629. DOI:dx.doi.org/10.1007/s10791-005-0749-0 .216,526\\nLavrenko, Victor, and W. Bruce Croft. 2001. Relevance-base d language models. In\\nProc. SIGIR , pp. 120–127. ACM Press. 250,522,526\\nLawrence, Steve, and C. Lee Giles. 1998. Searching the World Wide Web. Science 280\\n(5360):98–100. URL:citeseer.ist.psu.edu/lawrence98searching.html .442,523,526\\nLawrence, Steve, and C. Lee Giles. 1999. Accessibility of in formation on the web.\\nNature 500:107–109. 442,523,526\\nLee, Whay C., and Edward A. Fox. 1988. Experimental comparis on of schemes for in-\\nterpreting Boolean queries. Technical Report TR-88-27, Co mputer Science, Virginia\\nPolytechnic Institute and State University. 17,523,526\\nLempel, Ronny, and Shlomo Moran. 2000. The stochastic appro ach for link-structure\\nanalysis (SALSA) and the TKC effect. Computer Networks 33(1–6):387–401. URL:\\nciteseer.ist.psu.edu/lempel00stochastic.html .481,526,528\\nLesk, Michael. 1988. Grab – Inverted indexes with low storag e overhead. Computing\\nSystems 1:207–220. 83,526\\nLesk, Michael. 2004. Understanding Digital Libraries , 2nd edition. Morgan Kaufmann.\\nxxxiv ,526\\nLester, Nicholas, Alistair Moffat, and Justin Zobel. 2005. Fast on-line index con-\\nstruction by geometric partitioning. In Proc. CIKM , pp. 776–783. ACM Press. DOI:\\ndoi.acm.org/10.1145/1099554.1099739 .84,526,527,533\\nLester, Nicholas, Justin Zobel, and Hugh E. Williams. 2006. Efﬁcient online\\nindex maintenance for contiguous inverted lists. IP&M 42(4):916–933. DOI:\\ndx.doi.org/10.1016/j.ipm.2005.09.005 .84,526,533\\nLevenshtein, Vladimir I. 1965. Binary codes capable of corr ecting spurious insertions\\nand deletions of ones. Problems of Information Transmission 1:8–17. 65,526\\nLew, Michael S. 2001. Principles of Visual Information Retrieval . Springer. xxxiv ,526\\nLewis, David D. 1995. Evaluating and optimizing autonomous text classiﬁcation\\nsystems. In Proc. SIGIR . ACM Press. 286,526\\nLewis, David D. 1998. Naive (Bayes) at forty: The independen ce assumption in\\ninformation retrieval. In Proc. ECML , pp. 4–15. Springer. 286,526', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 540}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 505\\nLewis, David D., and Karen Spärck Jones. 1996. Natural langu age processing for\\ninformation retrieval. CACM 39(1):92–101. DOI:doi.acm.org/10.1145/234173.234210 .\\nxxxiv ,525,526\\nLewis, David D., and Marc Ringuette. 1994. A comparison of tw o learning algorithms\\nfor text categorization. In Proc. SDAIR , pp. 81–93. 286,526,529\\nLewis, David D., Robert E. Schapire, James P . Callan, and Ron Papka. 1996. Training\\nalgorithms for linear text classiﬁers. In Proc. SIGIR , pp. 298–306. ACM Press. DOI:\\ndoi.acm.org/10.1145/243199.243277 .315,521,526,528,530\\nLewis, David D., Yiming Yang, Tony G. Rose, and Fan Li. 2004. R CV1: A new bench-\\nmark collection for text categorization research. JMLR 5:361–397. 84,287,526,530,\\n533\\nLi, Fan, and Yiming Yang. 2003. A loss function analysis for c lassiﬁcation methods in\\ntext categorization. In Proc. ICML , pp. 472–479. xvii,282,347,526,533\\nLiddy, Elizabeth D. 2005. Automatic document retrieval. In Encyclopedia of Language\\nand Linguistics , 2nd edition. Elsevier. 17,526\\nList, Johan, Vojkan Mihajlovic, Georgina Ramírez, Arjen P . Vries, Djoerd Hiemstra,\\nand Henk Ernst Blok. 2005. TIJAH: Embracing IR methods in XML databases. IR\\n8(4):547–570. DOI:dx.doi.org/10.1007/s10791-005-0747-2 .216,520,524,526,527,529,\\n532\\nLita, Lucian Vlad, Abe Ittycheriah, Salim Roukos, and Nanda Kambhatla. 2003. tRuE-\\ncasIng. In Proc. ACL , pp. 152–159. 46,524,525,526,530\\nLittman, Michael L., Susan T. Dumais, and Thomas K. Landauer . 1998. Automatic\\ncross-language information retrieval using latent semant ic indexing. In Gregory\\nGrefenstette (ed.), Proc. Cross-Language Information Retrieval . Kluwer. URL:cite-\\nseer.ist.psu.edu/littman98automatic.html .417,522,526\\nLiu, Tie-Yan, Yiming Yang, Hao Wan, Hua-Jun Zeng, Zheng Chen , and Wei-Ying Ma.\\n2005. Support vector machines classiﬁcation with very larg e scale taxonomy. ACM\\nSIGKDD Explorations 7(1):36–43. 347,521,526,527,532,533\\nLiu, Xiaoyong, and W. Bruce Croft. 2004. Cluster-based retr ieval us-\\ning language models. In Proc. SIGIR , pp. 186–193. ACM Press. DOI:\\ndoi.acm.org/10.1145/1008992.1009026 .252,351,372,522,526\\nLloyd, Stuart P . 1982. Least squares quantization in PCM. IEEE Transactions on Infor-\\nmation Theory 28(2):129–136. 373,527\\nLodhi, Huma, Craig Saunders, John Shawe-Taylor, Nello Cris tianini, and Chris\\nWatkins. 2002. Text classiﬁcation using string kernels. JMLR 2:419–444. 347,521,\\n527,530,532\\nLombard, Matthew, Cheryl C. Bracken, and Jennifer Snyder-D uch. 2002. Content\\nanalysis in mass communication: Assessment and reporting o f intercoder reliabil-\\nity.Human Communication Research 28:587–604. 174,520,527,531\\nLong, Xiaohui, and Torsten Suel. 2003. Optimized query exec ution in large\\nsearch engines with global page ordering. In Proc. VLDB . URL:cite-\\nseer.ist.psu.edu/long03optimized.html .149,527,531', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 541}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP506 Bibliography\\nLovins, Julie Beth. 1968. Development of a stemming algorit hm. Translation and\\nComputational Linguistics 11(1):22–31. 33,527\\nLu, Wei, Stephen E. Robertson, and Andrew MacFarlane. 2007. CISR at INEX 2006.\\nInFuhr et al. (2007 ), pp. 57–63. 216,527,529\\nLuhn, Hans Peter. 1957. A statistical approach to mechanize d encoding and searching\\nof literary information. IBM Journal of Research and Development 1(4):309–317. 133,\\n527\\nLuhn, Hans Peter. 1958. The automatic creation of literatur e abstracts. IBM Journal of\\nResearch and Development 2(2):159–165, 317. 133,527\\nLuk, Robert W. P ., and Kui-Lam Kwok. 2002. A comparison of Chi nese document\\nindexing strategies and retrieval models. ACM Transactions on Asian Language In-\\nformation Processing 1(3):225–268. 45,526,527\\nLunde, Ken. 1998. CJKV Information Processing . O’Reilly. 45,527\\nMacFarlane, A., J.A. McCann, and S.E. Robertson. 2000. Para llel search using parti-\\ntioned inverted ﬁles. In Proc. SPIRE , pp. 209–220. 458,527,529\\nMacQueen, James B. 1967. Some methods for classiﬁcation and analysis of mul-\\ntivariate observations. In Proc. Berkeley Symposium on Mathematics, Statistics and\\nProbability , pp. 281–297. University of California Press. 373,527\\nManning, Christopher D., and Hinrich Schütze. 1999. Foundations of Statistical Natural\\nLanguage Processing . MIT Press. xxxiv ,40,105,251,252,286,372,527,530\\nMaron, M. E., and J. L. Kuhns. 1960. On relevance, probabilis tic indexing, and infor-\\nmation retrieval. JACM 7(3):216–244. 235,286,526,527\\nMass, Yosi, Matan Mandelbrod, Einat Amitay, David Carmel, Y oëlle S. Maarek, and\\nAya Soffer. 2003. JuruXML – An XML retrieval system at INEX’0 2. In Fuhr et al.\\n(2003b ), pp. 73–80. URL:inex.is.informatik.uni-duisburg.de:2003/proceedings .pdf.216,\\n519,521,527,531\\nMcBryan, Oliver A. 1994. GENVL and WWWW: Tools for Taming the Web. In Proc.\\nWWW .URL:citeseer.ist.psu.edu/mcbryan94genvl.html .442,480,527\\nMcCallum, Andrew, and Kamal Nigam. 1998. A comparison of eve nt models for\\nNaive Bayes text classiﬁcation. In AAAI/ICML Workshop on Learning for Text Cate-\\ngorization , pp. 41–48. 286,527,528\\nMcCallum, Andrew, Ronald Rosenfeld, Tom M. Mitchell, and An drew Y. Ng. 1998.\\nImproving text classiﬁcation by shrinkage in a hierarchy of classes. In Proc. ICML ,\\npp. 359–367. Morgan Kaufmann. 347,527,528,530\\nMcCallum, Andrew Kachites. 1996. Bow: A toolkit for statist ical language modeling,\\ntext retrieval, classiﬁcation and clustering. www.cs.cmu.edu/ ~mccallum/bow .316,527\\nMcKeown, Kathleen, and Dragomir R. Radev. 1995. Generating summaries\\nof multiple news articles. In Proc. SIGIR , pp. 74–82. ACM Press. DOI:\\ndoi.acm.org/10.1145/215206.215334 .400,527,529\\nMcKeown, Kathleen R., Regina Barzilay, David Evans, Vasile ios Hatzivassiloglou, Ju-\\ndith L. Klavans, Ani Nenkova, Carl Sable, Barry Schiffman, a nd Sergey Sigelman.', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 542}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 507\\n2002. Tracking and summarizing news on a daily basis with Col umbia’s News-\\nblaster. In Proc. Human Language Technology Conference .351,373,520,522,524,525,\\n527,528,530\\nMcLachlan, Geoffrey J., and Thiriyambakam Krishnan. 1996. The EM Algorithm and\\nExtensions . John Wiley & Sons. 373,526,527\\nMeadow, Charles T., Donald H. Kraft, and Bert R. Boyce. 1999. Text Information Re-\\ntrieval Systems . Academic Press. xxxiv ,520,526,527\\nMeil˘ a, Marina. 2005. Comparing clusterings – An axiomatic view. In Proc. ICML .373,\\n527\\nMelnik, Sergey, Sriram Raghavan, Beverly Yang, and Hector G arcia-Molina. 2001.\\nBuilding a distributed full-text index for the web. In Proc. WWW , pp. 396–406.\\nACM Press. DOI:doi.acm.org/10.1145/371920.372095 .83,523,527,529,533\\nMihajlovi´ c, Vojkan, Henk Ernst Blok, Djoerd Hiemstra, and Peter M. G. Apers. 2005.\\nScore region algebra: Building a transparent XML-R databas e. In Proc. CIKM , pp.\\n12–19. DOI:doi.acm.org/10.1145/1099554.1099560 .216,519,520,524,527\\nMiller, David R. H., Tim Leek, and Richard M. Schwartz. 1999. A hidden Markov\\nmodel information retrieval system. In Proc. SIGIR , pp. 214–221. ACM Press. 246,\\n252,526,527,530\\nMinsky, Marvin Lee, and Seymour Papert (eds.). 1988. Perceptrons: An introduction to\\ncomputational geometry . MIT Press. Expanded edition. 315,527,528\\nMitchell, Tom M. 1997. Machine Learning . McGraw Hill. 286,527\\nMoffat, Alistair, and Timothy A. H. Bell. 1995. In situ gener ation of compressed\\ninverted ﬁles. JASIS 46(7):537–550. 83,520,527\\nMoffat, Alistair, and Lang Stuiver. 1996. Exploiting clust ering in inverted ﬁle com-\\npression. In Proc. Conference on Data Compression , pp. 82–91. IEEE Computer Soci-\\nety.106,527,531\\nMoffat, Alistair, and Justin Zobel. 1992. Parameterised co mpression\\nfor sparse bitmaps. In Proc. SIGIR , pp. 274–285. ACM Press. DOI:\\ndoi.acm.org/10.1145/133160.133210 .106,528,533\\nMoffat, Alistair, and Justin Zobel. 1996. Self-indexing in verted ﬁles for fast text re-\\ntrieval. TOIS 14(4):349–379. 46,47,528,533\\nMoffat, Alistair, and Justin Zobel. 1998. Exploring the sim ilarity space. SIGIR Forum\\n32(1). 133,528,533\\nMooers, Calvin. 1961. From a point of view of mathematical et c. techniques. In R. A.\\nFairthorne (ed.), Towards information retrieval , pp. xvii–xxiii. Butterworths. 17,528\\nMooers, Calvin E. 1950. Coding, information retrieval, and the rapid selector. Ameri-\\ncan Documentation 1(4):225–229. 17,528\\nMoschitti, Alessandro. 2003. A study on optimal parameter t uning for Rocchio text\\nclassiﬁer. In Proc. ECIR , pp. 420–435. 315,528\\nMoschitti, Alessandro, and Roberto Basili. 2004. Complex l inguistic features for text\\nclassiﬁcation: A comprehensive study. In Proc. ECIR , pp. 181–196. 347,520,528', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 543}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP508 Bibliography\\nMurata, Masaki, Qing Ma, Kiyotaka Uchimoto, Hiromi Ozaku, M asao Utiyama, and\\nHitoshi Isahara. 2000. Japanese probabilistic informatio n retrieval using location\\nand category information. In International Workshop on Information Retrieval With\\nAsian Languages , pp. 81–88. URL:portal.acm.org/citation.cfm?doid=355214.355226 .340,\\n524,527,528,532\\nMuresan, Gheorghe, and David J. Harper. 2004. Topic modelin g for medi-\\nated access to very large document collections. JASIST 55(10):892–910. DOI:\\ndx.doi.org/10.1002/asi.20034 .372,524,528\\nMurtagh, Fionn. 1983. A survey of recent advances in hierarc hical clustering algo-\\nrithms. Computer Journal 26(4):354–359. 399,528\\nNajork, Marc, and Allan Heydon. 2001. High-performance web crawling. Technical\\nReport 173, Compaq Systems Research Center. 458,524,528\\nNajork, Marc, and Allan Heydon. 2002. High-performance web crawling. In James\\nAbello, Panos Pardalos, and Mauricio Resende (eds.), Handbook of Massive Data\\nSets, chapter 2. Kluwer. 458,524,528\\nNavarro, Gonzalo, and Ricardo Baeza-Yates. 1997. Proximal nodes: A model to\\nquery document databases by content and structure. TOIS 15(4):400–435. DOI:\\ndoi.acm.org/10.1145/263479.263482 .217,519,528\\nNewsam, Shawn, Sitaram Bhagavathy, and B. S. Manjunath. 200 1. Category-based\\nimage retrieval. In Proc. IEEE International Conference on Image Processing, S pecial\\nSession on Multimedia Indexing, Browsing and Retrieval , pp. 596–599. 179,520,527,\\n528\\nNg, Andrew Y., and Michael I. Jordan. 2001. On discriminativ e vs. generative clas-\\nsiﬁers: A comparison of logistic regression and naive Bayes . In Proc. NIPS , pp.\\n841–848. URL:www-2.cs.cmu.edu/Groups/NIPS/NIPS2001/papers/psgz/A A28.ps.gz .286,\\n336,525,528\\nNg, Andrew Y., Michael I. Jordan, and Yair Weiss. 2001a. On sp ectral clustering:\\nAnalysis and an algorithm. In Proc. NIPS , pp. 849–856. 400,525,528,532\\nNg, Andrew Y., Alice X. Zheng, and Michael I. Jordan. 2001b. L ink analysis, eigenvec-\\ntors and stability. In Proc. IJCAI , pp. 903–910. URL:citeseer.ist.psu.edu/ng01link.html .\\n481,525,528,533\\nNigam, Kamal, Andrew McCallum, and Tom Mitchell. 2006. Semi -supervised text\\nclassiﬁcation using EM. In Chapelle et al. (2006 ), pp. 33–56. 347,527,528\\nNtoulas, Alexandros, and Junghoo Cho. 2007. Pruning polici es for two-tiered in-\\nverted index with correctness guarantee. In Proc. SIGIR , pp. 191–198. ACM Press.\\n105,521,528\\nOard, Douglas W., and Bonnie J. Dorr. 1996. A survey of multil ingual text retrieval.\\nTechnical Report UMIACS-TR-96-19, Institute for Advanced Computer Studies,\\nUniversity of Maryland, College Park, MD, USA. xxxiv ,522,528\\nOgilvie, Paul, and Jamie Callan. 2005. Parameter estimatio n for a simple hierar-\\nchical generative model for XML retrieval. In Proc. INEX , pp. 211–224. DOI:\\ndx.doi.org/10.1007/11766278_16 .216,521,528', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 544}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 509\\nO’Keefe, Richard A., and Andrew Trotman. 2004. The simplest query language that\\ncould possibly work. In Fuhr et al. (2005 ), pp. 167–174. 217,528,532\\nOsi´ nski, Stanisław, and Dawid Weiss. 2005. A concept-driv en algorithm for clustering\\nsearch results. IEEE Intelligent Systems 20(3):48–54. 400,528,532\\nPage, Lawrence, Sergey Brin, Rajeev Motwani, and Terry Wino grad. 1998. The Page-\\nRank citation ranking: Bringing order to the web. Technical report, Stanford Digital\\nLibrary Technologies Project. URL:citeseer.ist.psu.edu/page98pagerank.html .480,520,\\n528,533\\nPaice, Chris D. 1990. Another stemmer. SIGIR Forum 24(3):56–61. 33,528\\nPapineni, Kishore. 2001. Why inverse document frequency? I nProc. North American\\nChapter of the Association for Computational Linguistics , pp. 1–8. 133,528\\nPavlov, Dmitry, Ramnath Balasubramanyan, Byron Dom, Shyam Kapur, and Jig-\\nnashu Parikh. 2004. Document preprocessing for naive Bayes classiﬁcation and\\nclustering with mixture of multinomials. In Proc. KDD , pp. 829–834. 286,519,522,\\n525,528\\nPelleg, Dan, and Andrew Moore. 1999. Accelerating exact k-m eans algorithms\\nwith geometric reasoning. In Proc. KDD , pp. 277–281. ACM Press. DOI:\\ndoi.acm.org/10.1145/312129.312248 .373,528,529\\nPelleg, Dan, and Andrew Moore. 2000. X-means: Extending k-m eans with efﬁcient es-\\ntimation of the number of clusters. In Proc. ICML , pp. 727–734. Morgan Kaufmann.\\n373,528,529\\nPerkins, Simon, Kevin Lacker, and James Theiler. 2003. Graf ting: Fast, incremental\\nfeature selection by gradient descent in function space. JMLR 3:1333–1356. 286,\\n526,529,531\\nPersin, Michael. 1994. Document ﬁltering for fast ranking. InProc. SIGIR , pp. 339–348.\\nACM Press. 149,529\\nPersin, Michael, Justin Zobel, and Ron Sacks-Davis. 1996. F iltered document retrieval\\nwith frequency-sorted indexes. JASIS 47(10):749–764. 149,529,530,533\\nPeterson, James L. 1980. Computer programs for detecting an d correcting spelling\\nerrors. CACM 23(12):676–687. DOI:doi.acm.org/10.1145/359038.359041 .65,529\\nPicca, Davide, Benoît Curdy, and François Bavaud. 2006. Non -linear correspondence\\nanalysis in text retrieval: A kernel view. In Proc. JADT .308,520,522,529\\nPinski, Gabriel, and Francis Narin. 1976. Citation inﬂuenc e for journal aggregates of\\nscientiﬁc publications: Theory, with application to the li terature of Physics. IP&M\\n12:297–326. 480,528,529\\nPirolli, Peter L. T. 2007. Information Foraging Theory: Adaptive Interaction With In forma-\\ntion. Oxford University Press. 373,529\\nPlatt, John. 2000. Probabilistic outputs for support vecto r machines and comparisons\\nto regularized likelihood methods. In A.J. Smola, P .L. Bart lett, B. Schölkopf, and\\nD. Schuurmans (eds.), Advances in Large Margin Classiﬁers , pp. 61–74. MIT Press.\\n325,529', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 545}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP510 Bibliography\\nPonte, Jay M., and W. Bruce Croft. 1998. A language modeling a pproach to informa-\\ntion retrieval. In Proc. SIGIR , pp. 275–281. ACM Press. xxii,246,247,249,252,522,\\n529\\nPopescul, Alexandrin, and Lyle H. Ungar. 2000. Automatic la beling of document\\nclusters. Unpublished MS, U. Pennsylvania. URL:http://www.cis.upenn.edu/ popes-\\ncul/Publications/popescul00labeling.pdf .400,529,532\\nPorter, Martin F. 1980. An algorithm for sufﬁx stripping. Program 14(3):130–137. 33,\\n529\\nPugh, William. 1990. Skip lists: A probabilistic alternati ve to balanced trees. CACM\\n33(6):668–676. 46,529\\nQin, Tao, Tie-Yan Liu, Wei Lai, Xu-Dong Zhang, De-Sheng Wang , and Hang Li. 2007.\\nRanking with multiple hyperplanes. In Proc. SIGIR . ACM Press. 348,526,529,532,\\n533\\nQiu, Yonggang, and H.P . Frei. 1993. Concept based query expa nsion. In Proc. SIGIR ,\\npp. 160–169. ACM Press. 194,523,529\\nR Development Core Team. 2005. R: A language and environment for statistical comput-\\ning. R Foundation for Statistical Computing, Vienna. URL:www.R-project.org . ISBN\\n3-900051-07-0. 374,400,529\\nRadev, Dragomir R., Sasha Blair-Goldensohn, Zhu Zhang, and Revathi Sundara\\nRaghavan. 2001. Interactive, domain-independent identiﬁ cation and summariza-\\ntion of topically related news articles. In Proc. European Conference on Research and\\nAdvanced Technology for Digital Libraries , pp. 225–238. 373,520,529,533\\nRahm, Erhard, and Philip A. Bernstein. 2001. A survey of appr oaches\\nto automatic schema matching. VLDB Journal 10(4):334–350. URL:cite-\\nseer.ist.psu.edu/rahm01survey.html .216,520,529\\nRand, William M. 1971. Objective criteria for the evaluatio n of clustering methods.\\nJournal of the American Statistical Association 66(336):846–850. 373,529\\nRasmussen, Edie. 1992. Clustering algorithms. In Frakes and Baeza-Yates (1992 ), pp.\\n419–442. 372,529\\nRennie, Jason D., Lawrence Shih, Jaime Teevan, and David R. K arger. 2003. Tackling\\nthe poor assumptions of naive Bayes text classiﬁers. In Proc. ICML , pp. 616–623.\\n286,525,529,530,531\\nRibeiro-Neto, Berthier, Edleno S. Moura, Marden S. Neubert , and Nivio Ziviani. 1999.\\nEfﬁcient distributed algorithms to build inverted ﬁles. In Proc. SIGIR , pp. 105–112.\\nACM Press. DOI:doi.acm.org/10.1145/312624.312663 .83,528,529,533\\nRibeiro-Neto, Berthier A., and Ramurti A. Barbosa. 1998. Qu ery performance for\\ntightly coupled distributed digital libraries. In Proc. ACM Conference on Digital\\nLibraries , pp. 182–190. 459,519,529\\nRice, John A. 2006. Mathematical Statistics and Data Analysis . Duxbury Press. 99,235,\\n276,529\\nRichardson, M., A. Prakash, and E. Brill. 2006. Beyond PageR ank: machine learning\\nfor static ranking. In Proc. WWW , pp. 707–715. 348,520,529', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 546}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 511\\nRiezler, Stefan, Alexander Vasserman, Ioannis Tsochantar idis, Vibhu Mittal, and\\nYi Liu. 2007. Statistical machine translation for query exp ansion in answer re-\\ntrieval. In Proc. ACL , pp. 464–471. Association for Computational Linguistics. URL:\\nwww.aclweb.org/anthology/P/P07/P07-1059 .194,527,529,532\\nRipley, B. D. 1996. Pattern Recognition and Neural Networks . Cambridge University\\nPress. 222,235,529\\nRobertson, Stephen. 2005. How Okapi came to TREC. In Voorhees and Harman\\n(2005 ), pp. 287–299. 235,529\\nRobertson, Stephen, Hugo Zaragoza, and Michael Taylor. 200 4. Simple BM25\\nextension to multiple weighted ﬁelds. In Proc. CIKM , pp. 42–49. DOI:\\ndoi.acm.org/10.1145/1031171.1031181 .235,529,531,533\\nRobertson, Stephen E., and Karen Spärck Jones. 1976. Releva nce weighting of search\\nterms. JASIS 27:129–146. 133,235,525,529\\nRocchio, J. J. 1971. Relevance feedback in information retr ieval. In Salton (1971b ), pp.\\n313–323. 181,193,314,530\\nRoget, P . M. 1946. Roget’s International Thesaurus . Thomas Y. Crowell. 194,530\\nRosen-Zvi, Michal, Thomas Grifﬁths, Mark Steyvers, and Pad hraic Smyth. 2004. The\\nauthor-topic model for authors and documents. In Proc. UAI , pp. 487–494. 418,\\n523,530,531\\nRoss, Sheldon. 2006. A First Course in Probability . Pearson Prentice Hall. 99,235,530\\nRusmevichientong, Paat, David M. Pennock, Steve Lawrence, and C. Lee Giles. 2001.\\nMethods for sampling pages uniformly from the world wide web . In Proc. AAAI\\nFall Symposium on Using Uncertainty Within Computation , pp. 121–128. URL:cite-\\nseer.ist.psu.edu/rusmevichientong01methods.html .442,523,526,529,530\\nRuthven, Ian, and Mounia Lalmas. 2003. A survey on the use of r elevance feedback\\nfor information access systems. Knowledge Engineering Review 18(1). 194,526,530\\nSahoo, Nachiketa, Jamie Callan, Ramayya Krishnan, George D uncan, and Rema Pad-\\nman. 2006. Incremental hierarchical clustering of text doc uments. In Proc. CIKM ,\\npp. 357–366. DOI:doi.acm.org/10.1145/1183614.1183667 .400,521,522,526,528,530\\nSakai, Tetsuya. 2007. On the reliability of information ret rieval metrics based on\\ngraded relevance. IP&M 43(2):531–548. 174,530\\nSalton, Gerard. 1971a. Cluster search strategies and the op timization of retrieval\\neffectiveness. In The SMART Retrieval System – Experiments in Automatic Docum ent\\nProcessing Salton (1971b ), pp. 223–242. 351,372,530\\nSalton, Gerard (ed.). 1971b. The SMART Retrieval System – Experiments in Automatic\\nDocument Processing . Prentice Hall. 133,173,193,499,509,510,530\\nSalton, Gerard. 1975. Dynamic information and library processing . Prentice Hall. 372,\\n530\\nSalton, Gerard. 1989. Automatic Text Processing: The Transformation, Analysis, and\\nRetrieval of Information by Computer . Addison Wesley. 46,194,530', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 547}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP512 Bibliography\\nSalton, Gerard. 1991. The Smart project in automatic docume nt retrieval. In Proc.\\nSIGIR , pp. 356–358. ACM Press. 173,530\\nSalton, Gerard, James Allan, and Chris Buckley. 1993. Appro aches to passage re-\\ntrieval in full text information systems. In Proc. SIGIR , pp. 49–58. ACM Press. DOI:\\ndoi.acm.org/10.1145/160688.160693 .217,519,520,530\\nSalton, Gerard, and Chris Buckley. 1987. Term weighting app roaches in automatic\\ntext retrieval. Technical report, Cornell University, Ith aca, NY, USA. 133,520,530\\nSalton, Gerard, and Christopher Buckley. 1988. Term-weigh ting approaches in auto-\\nmatic text retrieval. IP&M 24(5):513–523. 133,520,530\\nSalton, Gerard, and Chris Buckley. 1990. Improving retriev al performance by rele-\\nvance feedback. JASIS 41(4):288–297. 194,520,530\\nSaracevic, Tefko, and Paul Kantor. 1988. A study of informat ion seeking and retriev-\\ning. II: Users, questions and effectiveness. JASIS 39:177–196. 173,525,530\\nSaracevic, Tefko, and Paul Kantor. 1996. A study of informat ion seeking and retriev-\\ning. III: Searchers, searches, overlap. JASIS 39(3):197–216. 173,525,530\\nSavaresi, Sergio M., and Daniel Boley. 2004. A comparative a nalysis on the bisecting\\nK-means and the PDDP clustering algorithms. Intelligent Data Analysis 8(4):345–\\n362. 400,520,530\\nSchamber, Linda, Michael Eisenberg, and Michael S. Nilan. 1 990. A re-examination\\nof relevance: toward a dynamic, situational deﬁnition. IP&M 26(6):755–776. 174,\\n522,528,530\\nSchapire, Robert E. 2003. The boosting approach to machine l earning: An overview.\\nIn D. D. Denison, M. H. Hansen, C. Holmes, B. Mallick, and B. Yu (eds.), Nonlinear\\nEstimation and Classiﬁcation . Springer. 347,530\\nSchapire, Robert E., and Yoram Singer. 2000. Boostexter: A b oosting-based system\\nfor text categorization. Machine Learning 39(2/3):135–168. 347,530,531\\nSchapire, Robert E., Yoram Singer, and Amit Singhal. 1998. B oosting and Rocchio\\napplied to text ﬁltering. In Proc. SIGIR , pp. 215–223. ACM Press. 314,315,530,531\\nSchlieder, Torsten, and Holger Meuss. 2002. Querying and ra nking XML documents.\\nJASIST 53(6):489–503. DOI:dx.doi.org/10.1002/asi.10060 .216,527,530\\nScholer, Falk, Hugh E. Williams, John Yiannis, and Justin Zo bel. 2002. Compression\\nof inverted indexes for fast query evaluation. In Proc. SIGIR , pp. 222–229. ACM\\nPress. DOI:doi.acm.org/10.1145/564376.564416 .106,530,533\\nSchölkopf, Bernhard, and Alexander J. Smola. 2001. Learning with Kernels: Support\\nVector Machines, Regularization, Optimization, and Beyon d. MIT Press. 346,530,531\\nSchütze, Hinrich. 1998. Automatic word sense discriminati on.Computational Linguis-\\ntics24(1):97–124. 192,194,530\\nSchütze, Hinrich, David A. Hull, and Jan O. Pedersen. 1995. A comparison of clas-\\nsiﬁers and document representations for the routing proble m. In Proc. SIGIR , pp.\\n229–237. ACM Press. 193,286,315,524,529,530', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 548}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 513\\nSchütze, Hinrich, and Jan O. Pedersen. 1995. Information re trieval based on word\\nsenses. In Proc. SDAIR , pp. 161–175. 374,529,530\\nSchütze, Hinrich, and Craig Silverstein. 1997. Projection s for efﬁcient document\\nclustering. In Proc. SIGIR , pp. 74–81. ACM Press. 373,417,530\\nSchwarz, Gideon. 1978. Estimating the dimension of a model. Annals of Statistics 6\\n(2):461–464. 373,530\\nSebastiani, Fabrizio. 2002. Machine learning in automated text categorization. ACM\\nComputing Surveys 34(1):1–47. 286,530\\nShawe-Taylor, John, and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis .\\nCambridge University Press. 346,521,530\\nShkapenyuk, Vladislav, and Torsten Suel. 2002. Design and i mplementation of a\\nhigh-performance distributed web crawler. In Proc. International Conference on Data\\nEngineering .URL:citeseer.ist.psu.edu/shkapenyuk02design.html .458,530,531\\nSiegel, Sidney, and N. John Castellan, Jr. 1988. Nonparametric Statistics for the Behavioral\\nSciences , 2nd edition. McGraw Hill. 174,521,530\\nSifry, Dave, 2007. The state of the Live Web, April 2007. URL:techno-\\nrati.com/weblog/2007/04/328.html .30,530\\nSigurbjörnsson, Börkur, Jaap Kamps, and Maarten de Rijke. 2 004. Mixture models,\\noverlap, and structural hints in XML element retrieval. In Proc. INEX , pp. 196–210.\\n216,525,529,530\\nSilverstein, Craig, Monika Rauch Henzinger, Hannes Marais , and Michael Moricz.\\n1999. Analysis of a very large web search engine query log. SIGIR Forum 33(1):\\n6–12. 47,524,527,528,530\\nSilvestri, Fabrizio. 2007. Sorting out the document identi ﬁer assignment problem. In\\nProc. ECIR , pp. 101–112. 106,531\\nSilvestri, Fabrizio, Raffaele Perego, and Salvatore Orlan do. 2004. Assigning docu-\\nment identiﬁers to enhance compressibility of web search en gines indexes. In Proc.\\nACM Symposium on Applied Computing , pp. 600–605. 106,528,529,531\\nSindhwani, V ., and S. S. Keerthi. 2006. Large scale semi-sup ervised linear SVMs. In\\nProc. SIGIR , pp. 477–484. 348,525,531\\nSinghal, Amit, Chris Buckley, and Mandar Mitra. 1996a. Pivo ted document\\nlength normalization. In Proc. SIGIR , pp. 21–29. ACM Press. URL:cite-\\nseer.ist.psu.edu/singhal96pivoted.html .133,520,527,531\\nSinghal, Amit, Mandar Mitra, and Chris Buckley. 1997. Learn ing routing queries in a\\nquery zone. In Proc. SIGIR , pp. 25–32. ACM Press. 193,520,527,531\\nSinghal, Amit, Gerard Salton, and Chris Buckley. 1995. Leng th normalization in\\ndegraded text collections. Technical report, Cornell Univ ersity, Ithaca, NY. 133,\\n520,530,531\\nSinghal, Amit, Gerard Salton, and Chris Buckley. 1996b. Len gth normalization in\\ndegraded text collections. In Proc. SDAIR , pp. 149–162. 133,520,530,531', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 549}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP514 Bibliography\\nSingitham, Pavan Kumar C., Mahathi S. Mahabhashyam, and Pra bhakar Raghavan.\\n2004. Efﬁciency-quality tradeoffs for vector score aggreg ation. In Proc. VLDB , pp.\\n624–635. URL:www.vldb.org/conf/2004/RS17P1.PDF .149,372,527,529,531\\nSmeulders, Arnold W. M., Marcel Worring, Simone Santini, Am arnath Gupta,\\nand Ramesh Jain. 2000. Content-based image retrieval at the end of the\\nearly years. IEEE Trans. Pattern Anal. Mach. Intell. 22(12):1349–1380. DOI:\\ndx.doi.org/10.1109/34.895972 .xxxiv ,523,524,530,531,533\\nSneath, Peter H.A., and Robert R. Sokal. 1973. Numerical Taxonomy: The Principles and\\nPractice of Numerical Classiﬁcation . W.H. Freeman. 399,531\\nSnedecor, George Waddel, and William G. Cochran. 1989. Statistical methods . Iowa\\nState University Press. 286,521,531\\nSomogyi, Zoltan. 1990. The Melbourne University bibliogra phy system. Technical\\nReport 90/3, Melbourne University, Parkville, Victoria, A ustralia. 83,531\\nSong, Ruihua, Ji-Rong Wen, and Wei-Ying Ma. 2005. Viewing te rm proximity from a\\ndifferent perspective. Technical Report MSR-TR-2005-69, Microsoft Research. 149,\\n527,531,532\\nSornil, Ohm. 2001. Parallel Inverted Index for Large-Scale, Dynamic Digital L ibraries . PhD\\nthesis, Virginia Tech. URL:scholar.lib.vt.edu/theses/available/etd-02062001-11 4915/ .\\n459,531\\nSpärck Jones, Karen. 1972. A statistical interpretation of term speciﬁcity and its ap-\\nplication in retrieval. Journal of Documentation 28(1):11–21. 133,525\\nSpärck Jones, Karen. 2004. Language modelling’s generativ e model: Is it\\nrational? MS, Computer Laboratory, University of Cambridg e. URL:\\nwww.cl.cam.ac.uk/ ~ksj21/langmodnote4.pdf .252,525\\nSpärck Jones, Karen, S. Walker, and Stephen E. Robertson. 20 00. A probabilistic model\\nof information retrieval: Development and comparative exp eriments. IP&M 36(6):\\n779–808, 809–840. 232,234,235,525,529,532\\nSpink, Amanda, and Charles Cole (eds.). 2005. New Directions in Cognitive Information\\nRetrieval . Springer. 175,521,531\\nSpink, Amanda, Bernard J. Jansen, and H. Cenk Ozmultu. 2000. Use\\nof query reformulation and relevance feedback by Excite use rs. Internet\\nResearch: Electronic Networking Applications and Policy 10(4):317–328. URL:\\nist.psu.edu/faculty_pages/jjansen/academic/pubs/int ernetresearch2000.pdf .185,524,528,\\n531\\nSproat, Richard, and Thomas Emerson. 2003. The ﬁrst interna tional Chinese word\\nsegmentation bakeoff. In SIGHAN Workshop on Chinese Language Processing .46,\\n522,531\\nSproat, Richard, William Gale, Chilin Shih, and Nancy Chang . 1996. A stochastic\\nﬁnite-state word-segmentation algorithm for Chinese. Computational Linguistics 22\\n(3):377–404. 46,521,523,530,531\\nSproat, Richard William. 1992. Morphology and computation . MIT Press. 46,531', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 550}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 515\\nStein, Benno, and Sven Meyer zu Eissen. 2004. Topic identiﬁc ation: Framework and\\napplication. In Proc. International Conference on Knowledge Management .400,522,\\n531\\nStein, Benno, Sven Meyer zu Eissen, and Frank Wißbrock. 2003 . On cluster validity\\nand the information need of users. In Proc. Artiﬁcial Intelligence and Applications .\\n373,522,531,533\\nSteinbach, Michael, George Karypis, and Vipin Kumar. 2000. A comparison of docu-\\nment clustering techniques. In KDD Workshop on Text Mining .400,525,526,531\\nStrang, Gilbert (ed.). 1986. Introduction to Applied Mathematics . Wellesley-Cambridge\\nPress. 417,531\\nStrehl, Alexander. 2002. Relationship-based Clustering and Cluster Ensembles for H igh-\\ndimensional Data Mining . PhD thesis, The University of Texas at Austin. 373,531\\nStrohman, Trevor, and W. Bruce Croft. 2007. Efﬁcient docume nt retrieval in main\\nmemory. In Proc. SIGIR , pp. 175–182. ACM Press. 47,522,531\\nSwanson, Don R. 1988. Historical note: Information retriev al and the future of an\\nillusion. JASIS 39(2):92–98. 173,193,531\\nTague-Sutcliffe, Jean, and James Blustein. 1995. A statist ical analysis of the TREC-3\\ndata. In Proc. TREC , pp. 385–398. 174,520,531\\nTan, Songbo, and Xueqi Cheng. 2007. Using hypothesis margin to boost centroid text\\nclassiﬁer. In Proc. ACM Symposium on Applied Computing , pp. 398–403. ACM Press.\\nDOI:doi.acm.org/10.1145/1244002.1244096 .314,521,531\\nTannier, Xavier, and Shlomo Geva. 2005. XML retrieval with a natural language\\ninterface. In Proc. SPIRE , pp. 29–40. 217,523,531\\nTao, Tao, Xuanhui Wang, Qiaozhu Mei, and ChengXiang Zhai. 20 06. Language\\nmodel information retrieval with document expansion. In Proc. Human Language\\nTechnology Conference / North American Chapter of the Assoc iation for Computational\\nLinguistics , pp. 407–414. 252,527,531,532,533\\nTaube, Mortimer, and Harold Wooster (eds.). 1958. Information storage and retrieval:\\nTheory, systems, and devices . Columbia University Press. 17,531,533\\nTaylor, Michael, Hugo Zaragoza, Nick Craswell, Stephen Rob ertson, and Chris\\nBurges. 2006. Optimisation methods for ranking functions w ith multiple parame-\\nters. In Proc. CIKM . ACM Press. 348,520,521,529,531,533\\nTeh, Yee Whye, Michael I. Jordan, Matthew J. Beal, and David M . Blei. 2006. Hier-\\narchical Dirichlet processes. Journal of the American Statistical Association 101(476):\\n1566–1581. 418,520,525,531\\nTheobald, Martin, Holger Bast, Debapriyo Majumdar, Ralf Sc henkel, and Gerhard\\nWeikum. 2008. TopX: Efﬁcient and versatile top- kquery processing for semistruc-\\ntured data. VLDB Journal 17(1):81–115. 216,520,527,530,531,532\\nTheobald, Martin, Ralf Schenkel, and Gerhard Weikum. 2005. An efﬁcient and versa-\\ntile query engine for TopX search. In Proc. VLDB , pp. 625–636. VLDB Endowment.\\n216,530,531,532', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 551}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP516 Bibliography\\nTibshirani, Robert, Guenther Walther, and Trevor Hastie. 2 001. Estimating the num-\\nber of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society\\nSeries B 63:411–423. 374,524,531,532\\nTishby, Naftali, and Noam Slonim. 2000. Data clustering by M arkovian relaxation\\nand the information bottleneck method. In Proc. NIPS , pp. 640–646. 374,531\\nToda, Hiroyuki, and Ryoji Kataoka. 2005. A search result clu stering method using\\ninformatively named entities. In International Workshop on Web Information and Data\\nManagement , pp. 81–86. ACM Press. DOI:doi.acm.org/10.1145/1097047.1097063 .372,\\n525,531\\nTomasic, Anthony, and Hector Garcia-Molina. 1993. Query pr ocessing and inverted\\nindices in shared-nothing document information retrieval systems. VLDB Journal\\n2(3):243–275. 458,523,531\\nTombros, Anastasios, and Mark Sanderson. 1998. Advantages of query biased\\nsummaries in information retrieval. In Proc. SIGIR , pp. 2–10. ACM Press. DOI:\\ndoi.acm.org/10.1145/290941.290947 .174,530,531\\nTombros, Anastasios, Robert Villa, and Cornelis Joost van R ijsbergen. 2002. The\\neffectiveness of query-speciﬁc hierarchic clustering in i nformation retrieval. IP&M\\n38(4):559–582. DOI:dx.doi.org/10.1016/S0306-4573(01)00048-6 .372,529,531,532\\nTomlinson, Stephen. 2003. Lexical and algorithmic stemmin g compared for 9 Eu-\\nropean languages with Hummingbird Searchserver at CLEF 200 3. In Proc. Cross-\\nLanguage Evaluation Forum , pp. 286–300. 46,531\\nTong, Simon, and Daphne Koller. 2001. Support vector machin e active learning with\\napplications to text classiﬁcation. JMLR 2:45–66. 348,525,531\\nToutanova, Kristina, and Robert C. Moore. 2002. Pronunciat ion modeling for im-\\nproved spelling correction. In Proc. ACL , pp. 144–151. 65,528,531\\nTreeratpituk, Pucktada, and Jamie Callan. 2006. An experim ental study on automat-\\nically labeling hierarchical clusters using statistical f eatures. In Proc. SIGIR , pp.\\n707–708. ACM Press. DOI:doi.acm.org/10.1145/1148170.1148328 .400,521,532\\nTrotman, Andrew. 2003. Compressing inverted ﬁles. IR6(1):5–19. DOI:\\ndx.doi.org/10.1023/A:1022949613039 .106,532\\nTrotman, Andrew, and Shlomo Geva. 2006. Passage retrieval a nd other XML-retrieval\\ntasks. In SIGIR 2006 Workshop on XML Element Retrieval Methodology , pp. 43–50. 217,\\n523,532\\nTrotman, Andrew, Shlomo Geva, and Jaap Kamps (eds.). 2007. SIGIR Workshop on\\nFocused Retrieval . University of Otago. 217,523,525,532\\nTrotman, Andrew, Nils Pharo, and Miro Lehtonen. 2006. XML-I R users and use cases.\\nInProc. INEX , pp. 400–412. 216,526,529,532\\nTrotman, Andrew, and Börkur Sigurbjörnsson. 2004. Narrowe d Extended XPath I\\n(NEXI). In Fuhr et al. (2005 ), pp. 16–40. DOI:dx.doi.org/10.1007/11424550_2 .217,\\n530,532\\nTseng, Huihsin, Pichuan Chang, Galen Andrew, Daniel Jurafs ky, and Christopher\\nManning. 2005. A conditional random ﬁeld word segmenter. In SIGHAN Workshop\\non Chinese Language Processing .46,519,521,525,527,532', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 552}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 517\\nTsochantaridis, Ioannis, Thorsten Joachims, Thomas Hofma nn, and Yasemin Altun.\\n2005. Large margin methods for structured and interdepende nt output variables.\\nJMLR 6:1453–1484. 347,519,524,525,532\\nTurpin, Andrew, and William R. Hersh. 2001. Why batch and use r evaluations do not\\ngive the same results. In Proc. SIGIR , pp. 225–231. 175,524,532\\nTurpin, Andrew, and William R. Hersh. 2002. User interface e ffects in past batch\\nversus user experiments. In Proc. SIGIR , pp. 431–432. 175,524,532\\nTurpin, Andrew, Yohannes Tsegay, David Hawking, and Hugh E. Williams. 2007.\\nFast generation of result snippets in web search. In Proc. SIGIR , pp. 127–134. ACM\\nPress. 174,524,532,533\\nTurtle, Howard. 1994. Natural language vs. Boolean query ev aluation: A comparison\\nof retrieval performance. In Proc. SIGIR , pp. 212–220. ACM Press. 15,532\\nTurtle, Howard, and W. Bruce Croft. 1989. Inference network s for document retrieval.\\nInProc. SIGIR , pp. 1–24. ACM Press. 234,522,532\\nTurtle, Howard, and W. Bruce Croft. 1991. Evaluation of an in ference network-based\\nretrieval model. TOIS 9(3):187–222. 234,522,532\\nTurtle, Howard, and James Flood. 1995. Query evaluation: st rategies and optimiza-\\ntions. IP&M 31(6):831–850. DOI:dx.doi.org/10.1016/0306-4573(95)00020-H .133,522,\\n532\\nVaithyanathan, Shivakumar, and Byron Dom. 2000. Model-bas ed hierarchical clus-\\ntering. In Proc. UAI , pp. 599–608. Morgan Kaufmann. 400,522,532\\nvan Rijsbergen, Cornelis Joost. 1979. Information Retrieval , 2nd edition. Butterworths.\\n173,216,221,231,235,529\\nvan Rijsbergen, Cornelis Joost. 1989. Towards an informati on logic. In Proc. SIGIR ,\\npp. 77–86. ACM Press. DOI:doi.acm.org/10.1145/75334.75344 .xxxiv ,529\\nvan Zwol, Roelof, Jeroen Baas, Herre van Oostendorp, and Fra ns Wiering. 2006.\\nBricks: The building blocks to tackle query formulation in s tructured document\\nretrieval. In Proc. ECIR , pp. 314–325. 217,519,528,532,533\\nVapnik, Vladimir N. 1998. Statistical Learning Theory . Wiley-Interscience. 346,532\\nVittaut, Jean-Noël, and Patrick Gallinari. 2006. Machine l earning ranking for struc-\\ntured information retrieval. In Proc. ECIR , pp. 338–349. 216,523,532\\nVoorhees, Ellen M. 1985a. The cluster hypothesis revisited . In Proc. SIGIR , pp. 188–\\n196. ACM Press. 372,532\\nVoorhees, Ellen M. 1985b. The effectiveness and efﬁciency o f agglomerative hierar-\\nchic clustering in document retrieval. Technical Report TR 85-705, Cornell. 399,\\n532\\nVoorhees, Ellen M. 2000. Variations in relevance judgments and the measurement of\\nretrieval effectiveness. IP&M 36:697–716. 174,532\\nVoorhees, Ellen M., and Donna Harman (eds.). 2005. TREC: Experiment and Evaluation\\nin Information Retrieval . MIT Press. 173,314,498,509,524,532', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 553}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP518 Bibliography\\nWagner, Robert A., and Michael J. Fischer. 1974. The string- to-string correction prob-\\nlem. JACM 21(1):168–173. DOI:doi.acm.org/10.1145/321796.321811 .65,522,532\\nWard Jr., J. H. 1963. Hierarchical grouping to optimize an ob jective function. Journal\\nof the American Statistical Association 58:236–244. 399,532\\nWei, Xing, and W. Bruce Croft. 2006. LDA-based document mode ls\\nfor ad-hoc retrieval. In Proc. SIGIR , pp. 178–185. ACM Press. DOI:\\ndoi.acm.org/10.1145/1148170.1148204 .418,522,532\\nWeigend, Andreas S., Erik D. Wiener, and Jan O. Pedersen. 199 9. Exploiting hierarchy\\nin text categorization. IR1(3):193–216. 347,529,532\\nWeston, Jason, and Chris Watkins. 1999. Support vector mach ines for multi-class\\npattern recognition. In Proc. European Symposium on Artiﬁcial Neural Networks , pp.\\n219–224. 347,532\\nWilliams, Hugh E., and Justin Zobel. 2005. Searchable words on the web. International\\nJournal on Digital Libraries 5(2):99–105. DOI:dx.doi.org/10.1007/s00799-003-0050-z .\\n105,533\\nWilliams, Hugh E., Justin Zobel, and Dirk Bahle. 2004. Fast p hrase querying with\\ncombined indexes. TOIS 22(4):573–594. 43,519,533\\nWitten, Ian H., and Timothy C. Bell. 1990. Source models for n atural language text.\\nInternational Journal Man-Machine Studies 32(5):545–579. 105,520,533\\nWitten, Ian H., and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools\\nand Techniques , 2nd edition. Morgan Kaufmann. 374,523,533\\nWitten, Ian H., Alistair Moffat, and Timothy C. Bell. 1999. Managing Gigabytes: Com-\\npressing and Indexing Documents and Images , 2nd edition. Morgan Kaufmann. 18,\\n83,105,106,520,528,533\\nWong, S. K. Michael, Yiyu Yao, and Peter Bollmann. 1988. Line ar structure in infor-\\nmation retrieval. In Proc. SIGIR , pp. 219–232. ACM Press. 348,520,533\\nWoodley, Alan, and Shlomo Geva. 2006. NLPX at INEX 2006. In Proc. INEX , pp.\\n302–311. 217,523,533\\nXu, Jinxi, and W. Bruce Croft. 1996. Query expansion using lo cal and global document\\nanalysis. In Proc. SIGIR , pp. 4–11. ACM Press. 194,522,533\\nXu, Jinxi, and W. Bruce Croft. 1999. Cluster-based language models for\\ndistributed retrieval. In Proc. SIGIR , pp. 254–261. ACM Press. DOI:\\ndoi.acm.org/10.1145/312624.312687 .372,522,533\\nYang, Hui, and Jamie Callan. 2006. Near-duplicate detectio n by instance-\\nlevel constrained clustering. In Proc. SIGIR , pp. 421–428. ACM Press. DOI:\\ndoi.acm.org/10.1145/1148170.1148243 .373,521,533\\nYang, Yiming. 1994. Expert network: Effective and efﬁcient learning from human\\ndecisions in text categorization and retrieval. In Proc. SIGIR , pp. 13–22. ACM Press.\\n314,533\\nYang, Yiming. 1999. An evaluation of statistical approache s to text categorization. IR\\n1:69–90. 347,533', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 554}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPBibliography 519\\nYang, Yiming. 2001. A study of thresholding strategies for t ext categorization. In\\nProc. SIGIR , pp. 137–145. ACM Press. DOI:doi.acm.org/10.1145/383952.383975 .315,\\n533\\nYang, Yiming, and Bryan Kisiel. 2003. Margin-based local re gression for adaptive\\nﬁltering. In Proc. CIKM , pp. 191–198. DOI:doi.acm.org/10.1145/956863.956902 .315,\\n525,533\\nYang, Yiming, and Xin Liu. 1999. A re-examination of text cat egorization methods.\\nInProc. SIGIR , pp. 42–49. ACM Press. 287,347,527,533\\nYang, Yiming, and Jan Pedersen. 1997. Feature selection in s tatistical learning of text\\ncategorization. In Proc. ICML .286,529,533\\nYue, Yisong, Thomas Finley, Filip Radlinski, and Thorsten J oachims. 2007. A support\\nvector method for optimizing average precision. In Proc. SIGIR . ACM Press. 348,\\n522,525,529,533\\nZamir, Oren, and Oren Etzioni. 1999. Grouper: A dynamic clus tering interface to\\nweb search results. In Proc. WWW , pp. 1361–1374. Elsevier North-Holland. DOI:\\ndx.doi.org/10.1016/S1389-1286(99)00054-7 .372,400,522,533\\nZaragoza, Hugo, Djoerd Hiemstra, Michael Tipping, and Step hen Robertson. 2003.\\nBayesian extension to the language model for ad hoc informat ion retrieval. In Proc.\\nSIGIR , pp. 4–9. ACM Press. 252,524,529,531,533\\nZavrel, Jakub, Peter Berck, and Willem Lavrijssen. 2000. In formation extraction by\\ntext classiﬁcation: Corpus mining for features. In Workshop Information Extraction\\nMeets Corpus Linguistics .URL:www.cnts.ua.ac.be/Publications/2000/ZBL00 . Held in\\nconjunction with LREC-2000. 315,520,526,533\\nZha, Hongyuan, Xiaofeng He, Chris H. Q. Ding, Ming Gu, and Hor st D. Simon. 2001.\\nBipartite graph partitioning and data clustering. In Proc. CIKM , pp. 25–32. 374,\\n400,522,523,524,531,533\\nZhai, Chengxiang, and John Lafferty. 2001a. Model-based fe edback in the language\\nmodeling approach to information retrieval. In Proc. CIKM . ACM Press. 250,526,\\n533\\nZhai, Chengxiang, and John Lafferty. 2001b. A study of smoot hing methods for\\nlanguage models applied to ad hoc information retrieval. In Proc. SIGIR , pp. 334–\\n342. ACM Press. 252,526,533\\nZhai, ChengXiang, and John Lafferty. 2002. Two-stage langu age models\\nfor information retrieval. In Proc. SIGIR , pp. 49–56. ACM Press. DOI:\\ndoi.acm.org/10.1145/564376.564387 .252,526,533\\nZhang, Jiangong, Xiaohui Long, and Torsten Suel. 2007. Perf ormance of compressed\\ninverted list caching in search engines. In Proc. CIKM .106,527,531,533\\nZhang, Tong, and Frank J. Oles. 2001. Text categorization ba sed on regularized linear\\nclassiﬁcation methods. IR4(1):5–31. URL:citeseer.ist.psu.edu/zhang00text.html .347,\\n528,533\\nZhao, Ying, and George Karypis. 2002. Evaluation of hierarc hical clustering al-\\ngorithms for document datasets. In Proc. CIKM , pp. 515–524. ACM Press. DOI:\\ndoi.acm.org/10.1145/584792.584877 .399,525,533', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 555}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP520 Bibliography\\nZipf, George Kingsley. 1949. Human Behavior and the Principle of Least Effort . Addison\\nWesley. 105,533\\nZobel, Justin. 1998. How reliable are the results of large-s cale information retrieval\\nexperiments? In Proc. SIGIR , pp. 307–314. 174,533\\nZobel, Justin, and Philip Dart. 1995. Finding approximate m atches in\\nlarge lexicons. Software Practice and Experience 25(3):331–345. URL:cite-\\nseer.iﬁ.unizh.ch/zobel95ﬁnding.html .65,522,533\\nZobel, Justin, and Philip Dart. 1996. Phonetic string match ing: Lessons from infor-\\nmation retrieval. In Proc. SIGIR , pp. 166–173. ACM Press. 65,522,533\\nZobel, Justin, and Alistair Moffat. 2006. Inverted ﬁles for text search engines. ACM\\nComputing Surveys 38(2). 18,83,106,133,528,533\\nZobel, Justin, Alistair Moffat, Ross Wilkinson, and Ron Sac ks-Davis. 1995. Efﬁcient\\nretrieval of partial documents. IP&M 31(3):361–377. DOI:dx.doi.org/10.1016/0306-\\n4573(94)00052-5 .217,528,530,532,533\\nZukowski, Marcin, Sandor Heman, Niels Nes, and Peter Boncz. 2006. Super-scalar\\nRAM-CPU cache compression. In Proc. International Conference on Data Engineering ,\\np. 59. IEEE Computer Society. DOI:dx.doi.org/10.1109/ICDE.2006.150 .106,520,524,\\n528,533', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 556}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPAuthor Index\\nAberer: Aberer (2001 )\\nAhn: Ittner et al. (1995 )\\nAizerman: Aizerman et al. (1964 )\\nAkaike: Akaike (1974 )\\nAllan: Allan (2005 ),Allan et al. (1998 ),\\nBuckley et al. (1994a ),Buckley\\net al. (1994b ),Salton et al. (1993 )\\nAllwein: Allwein et al. (2000 )\\nAlonso: Alonso et al. (2006 )\\nAltingövde: Can et al. (2004 )\\nAltingövde: Altingövde et al. (2007 )\\nAltun: Tsochantaridis et al. (2005 )\\nAmer-Yahia: Amer-Yahia et al. (2006 ),\\nAmer-Yahia et al. (2005 ),\\nAmer-Yahia and Lalmas (2006 )\\nAmitay: Mass et al. (2003 )\\nAnagnostopoulos: Anagnostopoulos\\net al. (2006 )\\nAnderberg: Anderberg (1973 )\\nAnderson: Burnham and Anderson\\n(2002 )\\nAndoni: Andoni et al. (2006 )\\nAndrew: Tseng et al. (2005 )\\nAnh: Anh et al. (2001 ),Anh and\\nMoffat (2005 ),Anh and Moffat\\n(2006a ),Anh and Moffat (2006b ),\\nAnh and Moffat (2006c )\\nAone: Larsen and Aone (1999 )\\nApers: Mihajlovi´ c et al. (2005 )\\nApté: Apté et al. (1994 )\\nArabie: Hubert and Arabie (1985 )\\nArthur: Arthur and Vassilvitskii\\n(2006 )\\nArvola: Arvola et al. (2005 )Aslam: Aslam and Yilmaz (2005 )\\nAult: Ault and Yang (2002 )\\nBaas: van Zwol et al. (2006 )\\nBadue: Badue et al. (2001 )\\nBaeza-Yates: Badue et al. (2001 ),\\nBaeza-Yates et al. (2005 ),\\nBaeza-Yates and Ribeiro-Neto\\n(1999 ),de Moura et al. (2000 ),\\nFrakes and Baeza-Yates (1992 ),\\nHarman et al. (1992 ),Navarro\\nand Baeza-Yates (1997 )\\nBahle: Bahle et al. (2002 ),Williams\\net al. (2004 )\\nBai: Cao et al. (2005 )\\nBakiri: Dietterich and Bakiri (1995 )\\nBalasubramanyan: Pavlov et al.\\n(2004 )\\nBaldridge: Baldridge and Osborne\\n(2004 )\\nBaldwin: Hughes et al. (2006 )\\nBall: Ball (1965 )\\nBanerjee: Alonso et al. (2006 ),Basu\\net al. (2004 )\\nBanko: Banko and Brill (2001 )\\nBar-Ilan: Bar-Ilan and Gutman (2005 )\\nBar-Yossef: Bar-Yossef and Gurevich\\n(2006 )\\nBarbosa: Ribeiro-Neto and Barbosa\\n(1998 )\\nBarreiro: Blanco and Barreiro (2006 ),\\nBlanco and Barreiro (2007 )\\nBarroso: Barroso et al. (2003 )\\nBartell: Bartell (1994 ),Bartell et al.\\n(1998 )', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 557}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP522 Author Index\\nBarzilay: Barzilay and Elhadad\\n(1997 ),McKeown et al. (2002 )\\nBasili: Moschitti and Basili (2004 )\\nBast: Bast and Majumdar (2005 ),\\nTheobald et al. (2008 )\\nBasu: Basu et al. (2004 )\\nBavaud: Picca et al. (2006 )\\nBeal: Teh et al. (2006 )\\nBeesley: Beesley (1998 ),Beesley and\\nKarttunen (2003 )\\nBelew: Bartell et al. (1998 )\\nBelkin: Koenemann and Belkin (1996 )\\nBell: Moffat and Bell (1995 ),Witten\\nand Bell (1990 ),Witten et al.\\n(1999 )\\nBennett: Bennett (2000 )\\nBerck: Zavrel et al. (2000 )\\nBerger: Berger and Lafferty (1999 )\\nBerkhin: Berkhin (2005 ),Berkhin\\n(2006a ),Berkhin (2006b )\\nBerners-Lee: Berners-Lee et al. (1992 )\\nBernstein: Rahm and Bernstein (2001 )\\nBerry: Berry and Young (1995 ),Berry\\net al. (1995 ),Kent et al. (1955 )\\nBetsi: Betsi et al. (2006 )\\nBhagavathy: Newsam et al. (2001 )\\nBharat: Bharat and Broder (1998 ),\\nBharat et al. (1998 ),Bharat et al.\\n(2000 ),Bharat and Henzinger\\n(1998 )\\nBienenstock: Geman et al. (1992 )\\nBird: Hughes et al. (2006 )\\nBishop: Bishop (2006 )\\nBlair: Blair and Maron (1985 )\\nBlair-Goldensohn: Radev et al. (2001 )\\nBlanco: Blanco and Barreiro (2006 ),\\nBlanco and Barreiro (2007 )\\nBlandford: Blandford and Blelloch\\n(2002 )\\nBlei: Blei et al. (2003 ),Teh et al. (2006 )\\nBlelloch: Blandford and Blelloch\\n(2002 )\\nBlok: List et al. (2005 ),Mihajlovi´ c\\net al. (2005 )\\nBlustein: Tague-Sutcliffe and Blustein\\n(1995 )Boldi: Baeza-Yates et al. (2005 ),Boldi\\net al. (2002 ),Boldi et al. (2005 ),\\nBoldi and Vigna (2004a ),Boldi\\nand Vigna (2004b ),Boldi and\\nVigna (2005 )\\nBoley: Boley (1998 ),Savaresi and\\nBoley (2004 )\\nBollmann: Wong et al. (1988 )\\nBoncz: Zukowski et al. (2006 )\\nBorodin: Borodin et al. (2001 )\\nBotev: Amer-Yahia et al. (2006 )\\nBourne: Bourne and Ford (1961 )\\nBoyce: Meadow et al. (1999 )\\nBracken: Lombard et al. (2002 )\\nBradley: Bradley and Fayyad (1998 ),\\nBradley et al. (1998 ),Fayyad\\net al. (1998 )\\nBraverman: Aizerman et al. (1964 )\\nBrill: Banko and Brill (2001 ),Brill and\\nMoore (2000 ),Cucerzan and Brill\\n(2004 ),Richardson et al. (2006 )\\nBrin: Brin and Page (1998 ),Page et al.\\n(1998 )\\nBrisaboa: Brisaboa et al. (2007 )\\nBroder: Anagnostopoulos et al.\\n(2006 ),Bharat and Broder (1998 ),\\nBharat et al. (1998 ),Bharat et al.\\n(2000 ),Broder (2002 ),Broder\\net al. (2000 ),Broder et al. (1997 )\\nBrown: Brown (1995 ),Coden et al.\\n(2002 )\\nBuckley: Buckley et al. (1994a ),\\nBuckley and Salton (1995 ),\\nBuckley et al. (1994b ),Buckley\\net al. (1995 ),Buckley and\\nVoorhees (2000 ),Hersh et al.\\n(1994 ),Salton et al. (1993 ),Salton\\nand Buckley (1987 ),Salton and\\nBuckley (1988 ),Salton and\\nBuckley (1990 ),Singhal et al.\\n(1996a ),Singhal et al. (1997 ),\\nSinghal et al. (1995 ),Singhal et al.\\n(1996b )\\nBurges: Burges et al. (2005 ),Burges\\n(1998 ),Taylor et al. (2006 )\\nBurner: Burner (1997 )', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 558}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPAuthor Index 523\\nBurnham: Burnham and Anderson\\n(2002 )\\nBush: Bush (1945 )\\nBüttcher: Büttcher and Clarke\\n(2005a ),Büttcher and Clarke\\n(2005b ),Büttcher and Clarke\\n(2006 ),Büttcher et al. (2006 )\\nCacheda: Cacheda et al. (2003 )\\nCailliau: Berners-Lee et al. (1992 )\\nCallan: Callan (2000 ),Lewis et al.\\n(1996 ),Ogilvie and Callan (2005 ),\\nSahoo et al. (2006 ),Treeratpituk\\nand Callan (2006 ),Yang and\\nCallan (2006 )\\nCampbell: Crestani et al. (1998 )\\nCan: Altingövde et al. (2007 ),Can\\net al. (2004 ),Can and Ozkarahan\\n(1990 )\\nCandela: Harman and Candela (1990 )\\nCannane: Garcia et al. (2004 )\\nCao: Cao et al. (2005 ),Cao et al.\\n(2006 ),Gao et al. (2004 )\\nCarbonell: Carbonell and Goldstein\\n(1998 )\\nCarletta: Carletta (1996 )\\nCarmel: Carmel et al. (2001 ),Carmel\\net al. (2003 ),Mass et al. (2003 )\\nCarneiro: Cacheda et al. (2003 )\\nCaruana: Caruana and\\nNiculescu-Mizil (2006 )\\nCase: Amer-Yahia et al. (2005 )\\nCastellan: Siegel and Castellan (1988 )\\nCastillo: Baeza-Yates et al. (2005 )\\nCastro: Castro et al. (2004 )\\nCavnar: Cavnar and Trenkle (1994 )\\nChakrabarti: Chakrabarti (2002 ),\\nChakrabarti et al. (1998 )\\nChan: Hersh et al. (2000a ),Hersh\\net al. (2001 ),Hersh et al. (2000b )\\nChang: Sproat et al. (1996 ),Tseng\\net al. (2005 )\\nChapelle: Chapelle et al. (2006 )\\nChaudhuri: Chaudhuri et al. (2006 )\\nCheeseman: Cheeseman and Stutz\\n(1996 )\\nChen: Chen and Lin (2000 ),Chenet al. (2005 ),Cooper et al. (1994 ),\\nDumais and Chen (2000 ),\\nKishida et al. (2005 ),Kishida\\net al. (2005 ),Kupiec et al. (1995 ),\\nLiu et al. (2005 )\\nCheng: Tan and Cheng (2007 )\\nChiaramella: Chiaramella et al. (1996 )\\nChierichetti: Chierichetti et al. (2007 )\\nCho: Cho and Garcia-Molina (2002 ),\\nCho et al. (1998 ),Ntoulas and\\nCho (2007 )\\nChu-Carroll: Chu-Carroll et al. (2006 )\\nChurch: Kernighan et al. (1990 )\\nClarke: Büttcher and Clarke (2005a ),\\nBüttcher and Clarke (2005b ),\\nBüttcher and Clarke (2006 ),\\nBüttcher et al. (2006 ),Clarke\\net al. (2000 )\\nCleverdon: Cleverdon (1991 )\\nCoates: Castro et al. (2004 )\\nCochran: Snedecor and Cochran\\n(1989 )\\nCoden: Coden et al. (2002 )\\nCodenotti: Boldi et al. (2002 )\\nCohen: Carmel et al. (2001 ),Cohen\\n(1995 ),Cohen (1998 ),Cohen et al.\\n(1998 ),Cohen and Singer (1999 ),\\nForman and Cohen (2004 )\\nCole: Spink and Cole (2005 )\\nComtet: Comtet (1974 )\\nCooper: Cooper et al. (1994 )\\nCormack: Clarke et al. (2000 )\\nCormen: Cormen et al. (1990 )\\nCottrell: Bartell et al. (1998 )\\nCover: Cover and Hart (1967 ),Cover\\nand Thomas (1991 )\\nCrammer: Crammer and Singer\\n(2001 )\\nCraswell: Taylor et al. (2006 )\\nCreecy: Creecy et al. (1992 )\\nCrestani: Crestani et al. (1998 )\\nCristianini: Cristianini and\\nShawe-Taylor (2000 ),Lodhi et al.\\n(2002 ),Shawe-Taylor and\\nCristianini (2004 )\\nCroft: Croft (1978 ),Croft and Harper', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 559}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP524 Author Index\\n(1979 ),Croft and Lafferty (2003 ),\\nLavrenko and Croft (2001 ),Liu\\nand Croft (2004 ),Ponte and Croft\\n(1998 ),Strohman and Croft\\n(2007 ),Turtle and Croft (1989 ),\\nTurtle and Croft (1991 ),Wei and\\nCroft (2006 ),Xu and Croft (1996 ),\\nXu and Croft (1999 )\\nCrouch: Crouch (1988 )\\nCucerzan: Cucerzan and Brill (2004 )\\nCurdy: Picca et al. (2006 )\\nCutting: Cutting et al. (1993 ),Cutting\\net al. (1992 )\\nCzuba: Chu-Carroll et al. (2006 )\\nDamerau: Apté et al. (1994 ),\\nDamerau (1964 )\\nDart: Zobel and Dart (1995 ),Zobel\\nand Dart (1996 )\\nDas: Chaudhuri et al. (2006 )\\nDatar: Andoni et al. (2006 )\\nDavidson: Davidson and\\nSatyanarayana (2003 )\\nDay: Day and Edelsbrunner (1984 )\\nDean: Barroso et al. (2003 ),Bharat\\net al. (2000 ),Dean and\\nGhemawat (2004 )\\nDeeds: Burges et al. (2005 )\\nDeerwester: Deerwester et al. (1990 )\\nDemir: Can et al. (2004 )\\nDempster: Dempster et al. (1977 )\\nDhillon: Dhillon (2001 ),Dhillon and\\nModha (2001 )\\nDi Eugenio: Di Eugenio and Glass\\n(2004 )\\nDietterich: Dietterich (2002 ),\\nDietterich and Bakiri (1995 )\\nDing: Zha et al. (2001 )\\nDom: Chakrabarti et al. (1998 ),Dom\\n(2002 ),Pavlov et al. (2004 ),\\nVaithyanathan and Dom (2000 )\\nDomingos: Domingos (2000 ),\\nDomingos and Pazzani (1997 )\\nDorr: Oard and Dorr (1996 )\\nDoursat: Geman et al. (1992 )\\nDownie: Downie (2006 )\\nDrake: Alonso et al. (2006 )Dubes: Jain and Dubes (1988 )\\nDuboue: Chu-Carroll et al. (2006 )\\nDuda: Duda et al. (2000 )\\nDumais: Berry et al. (1995 ),\\nDeerwester et al. (1990 ),Dumais\\net al. (1998 ),Dumais (1993 ),\\nDumais (1995 ),Dumais and\\nChen (2000 ),Littman et al. (1998 )\\nDuncan: Sahoo et al. (2006 )\\nDunning: Dunning (1993 ),Dunning\\n(1994 )\\nDörre: Amer-Yahia et al. (2006 )\\nEckart: Eckart and Young (1936 )\\nEdelsbrunner: Day and Edelsbrunner\\n(1984 )\\nEisenberg: Schamber et al. (1990 )\\nEissen: Stein and zu Eissen (2004 ),\\nStein et al. (2003 )\\nEl-Hamdouchi: El-Hamdouchi and\\nWillett (1986 )\\nElhadad: Barzilay and Elhadad (1997 )\\nElias: Elias (1975 )\\nElkan: Hamerly and Elkan (2003 )\\nEmerson: Sproat and Emerson (2003 )\\nEtzioni: Zamir and Etzioni (1999 )\\nEvans: McKeown et al. (2002 )\\nEyheramendy: Eyheramendy et al.\\n(2003 )\\nFagin: Carmel et al. (2001 )\\nFallows: Fallows (2004 )\\nFarchi: Carmel et al. (2001 )\\nFariña: Brisaboa et al. (2007 )\\nFayyad: Bradley and Fayyad (1998 ),\\nBradley et al. (1998 ),Fayyad\\net al. (1998 )\\nFeldmann: Kammenhuber et al.\\n(2006 )\\nFellbaum: Fellbaum (1998 )\\nFerragina: Ferragina and Venturini\\n(2007 )\\nFerrucci: Chu-Carroll et al. (2006 )\\nFinley: Yue et al. (2007 )\\nFischer: Wagner and Fischer (1974 )\\nFlach: Gaertner et al. (2002 )\\nFlake: Glover et al. (2002b )\\nFlood: Turtle and Flood (1995 )', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 560}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPAuthor Index 525\\nFlynn: Jain et al. (1999 )\\nFord: Bourne and Ford (1961 )\\nForman: Forman (2004 ),Forman\\n(2006 ),Forman and Cohen (2004 )\\nFourel: Chiaramella et al. (1996 )\\nFowlkes: Fowlkes and Mallows\\n(1983 )\\nFox: Fox and Lee (1991 ),Harman\\net al. (1992 ),Lee and Fox (1988 )\\nFraenkel: Fraenkel and Klein (1985 )\\nFrakes: Frakes and Baeza-Yates (1992 )\\nFraley: Fraley and Raftery (1998 )\\nFrank: Witten and Frank (2005 )\\nFrei: Qiu and Frei (1993 )\\nFrieder: Grossman and Frieder (2004 )\\nFriedl: Friedl (2006 )\\nFriedman: Friedman (1997 ),\\nFriedman and Goldszmidt\\n(1996 ),Hastie et al. (2001 )\\nFuhr: Fuhr (1989 ),Fuhr (1992 ),Fuhr\\net al. (2003a ),Fuhr and\\nGroßjohann (2004 ),Fuhr and\\nLalmas (2007 ),Fuhr et al. (2006 ),\\nFuhr et al. (2005 ),Fuhr et al.\\n(2007 ),Fuhr et al. (2003b ),Fuhr\\nand Pfeifer (1994 ),Fuhr and\\nRölleke (1997 )\\nFurnas: Deerwester et al. (1990 )\\nGaertner: Gaertner et al. (2002 )\\nGale: Kernighan et al. (1990 ),Sproat\\net al. (1996 )\\nGallinari: Vittaut and Gallinari (2006 )\\nGao: Gao et al. (2005 ),Gao et al.\\n(2004 )\\nGarcia: Garcia et al. (2004 )\\nGarcia-Molina: Cho and\\nGarcia-Molina (2002 ),Cho et al.\\n(1998 ),Garcia-Molina et al.\\n(1999 ),Hirai et al. (2000 ),Melnik\\net al. (2001 ),Tomasic and\\nGarcia-Molina (1993 )\\nGarﬁeld: Garﬁeld (1955 ),Garﬁeld\\n(1976 )\\nGay: Joachims et al. (2005 )\\nGeman: Geman et al. (1992 )\\nGeng: Geng et al. (2007 )Gerrand: Gerrand (2007 )\\nGeva: Tannier and Geva (2005 ),\\nTrotman and Geva (2006 ),\\nTrotman et al. (2007 ),Woodley\\nand Geva (2006 )\\nGey: Cooper et al. (1994 ),Gey (1994 )\\nGhamrawi: Ghamrawi and\\nMcCallum (2005 )\\nGhemawat: Dean and Ghemawat\\n(2004 )\\nGibson: Chakrabarti et al. (1998 )\\nGiles: Lawrence and Giles (1998 ),\\nLawrence and Giles (1999 ),\\nRusmevichientong et al. (2001 )\\nGlass: Di Eugenio and Glass (2004 )\\nGlassman: Broder et al. (1997 )\\nGlover: Glover et al. (2002a ),Glover\\net al. (2002b )\\nGoldstein: Carbonell and Goldstein\\n(1998 )\\nGoldszmidt: Friedman and\\nGoldszmidt (1996 )\\nGrabs: Grabs and Schek (2002 )\\nGraepel: Herbrich et al. (2000 )\\nGranka: Joachims et al. (2005 )\\nGravano: Hatzivassiloglou et al.\\n(2000 )\\nGreiff: Greiff (1998 )\\nGrifﬁths: Rosen-Zvi et al. (2004 )\\nGrinstead: Grinstead and Snell (1997 )\\nGroff: Berners-Lee et al. (1992 )\\nGrossman: Grossman and Frieder\\n(2004 )\\nGroßjohann: Fuhr and Großjohann\\n(2004 )\\nGu: Zha et al. (2001 )\\nGuerrero: Cacheda et al. (2003 )\\nGupta: Smeulders et al. (2000 )\\nGurevich: Bar-Yossef and Gurevich\\n(2006 )\\nGusﬁeld: Gusﬁeld (1997 )\\nGutman: Bar-Ilan and Gutman (2005 )\\nGövert: Fuhr et al. (2003a ),Gövert\\nand Kazai (2003 )\\nHamerly: Hamerly and Elkan (2003 )\\nHamilton: Burges et al. (2005 )', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 561}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP526 Author Index\\nHan: Han and Karypis (2000 )\\nHand: Hand (2006 ),Hand and Yu\\n(2001 )\\nHarman: Harman (1991 ),Harman\\n(1992 ),Harman et al. (1992 ),\\nHarman and Candela (1990 ),\\nVoorhees and Harman (2005 )\\nHarold: Harold and Means (2004 )\\nHarper: Croft and Harper (1979 ),\\nMuresan and Harper (2004 )\\nHarshman: Deerwester et al. (1990 )\\nHart: Cover and Hart (1967 ),Duda\\net al. (2000 )\\nHarter: Harter (1998 )\\nHartigan: Hartigan and Wong (1979 )\\nHastie: Hastie et al. (2001 ),Tibshirani\\net al. (2001 )\\nHatzivassiloglou: Hatzivassiloglou\\net al. (2000 ),McKeown et al.\\n(2002 )\\nHaveliwala: Haveliwala (2003 ),\\nHaveliwala (2002 )\\nHawking: Turpin et al. (2007 )\\nHayes: Hayes and Weinstein (1990 )\\nHe:Zha et al. (2001 )\\nHeaps: Heaps (1978 )\\nHearst: Hearst (1997 ),Hearst (2006 ),\\nHearst and Pedersen (1996 ),\\nHearst and Plaunt (1993 )\\nHeckerman: Dumais et al. (1998 )\\nHeinz: Heinz and Zobel (2003 ),Heinz\\net al. (2002 )\\nHeman: Zukowski et al. (2006 )\\nHembrooke: Joachims et al. (2005 )\\nHenzinger: Bharat et al. (1998 ),\\nBharat et al. (2000 ),Bharat and\\nHenzinger (1998 ),Henzinger\\net al. (2000 ),Silverstein et al.\\n(1999 )\\nHerbrich: Herbrich et al. (2000 )\\nHerscovici: Carmel et al. (2001 )\\nHersh: Hersh et al. (1994 ),Hersh\\net al. (2000a ),Hersh et al. (2001 ),\\nHersh et al. (2000b ),Turpin and\\nHersh (2001 ),Turpin and Hersh\\n(2002 )Heydon: Henzinger et al. (2000 ),\\nNajork and Heydon (2001 ),\\nNajork and Heydon (2002 )\\nHickam: Hersh et al. (1994 )\\nHiemstra: Hiemstra (1998 ),Hiemstra\\n(2000 ),Hiemstra and Kraaij\\n(2005 ),Kraaij et al. (2002 ),List\\net al. (2005 ),Mihajlovi´ c et al.\\n(2005 ),Zaragoza et al. (2003 )\\nHirai: Hirai et al. (2000 )\\nHofmann: Hofmann (1999a ),\\nHofmann (1999b ),Tsochantaridis\\net al. (2005 )\\nHollink: Hollink et al. (2004 )\\nHon: Cao et al. (2006 )\\nHopcroft: Hopcroft et al. (2000 )\\nHristidis: Chaudhuri et al. (2006 )\\nHuang: Cao et al. (2006 ),Gao et al.\\n(2005 ),Huang and Mitchell\\n(2006 )\\nHubert: Hubert and Arabie (1985 )\\nHughes: Hughes et al. (2006 )\\nHull: Hull (1993 ),Hull (1996 ),\\nSchütze et al. (1995 )\\nHullender: Burges et al. (2005 )\\nHölzle: Barroso et al. (2003 )\\nIde: Ide(1971 )\\nImmorlica: Andoni et al. (2006 )\\nIndyk: Andoni et al. (2006 ),Indyk\\n(2004 )\\nIngwersen: Ingwersen and Järvelin\\n(2005 )\\nIsahara: Murata et al. (2000 )\\nIttner: Ittner et al. (1995 )\\nIttycheriah: Lita et al. (2003 )\\nIwayama: Iwayama and Tokunaga\\n(1995 )\\nJärvelin: Ingwersen and Järvelin\\n(2005 )\\nJackson: Jackson and Moulinier\\n(2002 )\\nJacobs: Jacobs and Rau (1990 )\\nJain: Jain et al. (1999 ),Jain and Dubes\\n(1988 ),Smeulders et al. (2000 )\\nJansen: Spink et al. (2000 )', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 562}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPAuthor Index 527\\nJardine: Jardine and van Rijsbergen\\n(1971 )\\nJeh: Jeh and Widom (2003 )\\nJensen: Jensen and Jensen (2001 ),\\nJensen and Jensen (2001 )\\nJeong: Jeong and Omiecinski (1995 )\\nJi:Ji and Xu (2006 )\\nJing: Jing (2000 )\\nJoachims: Joachims (1997 ),Joachims\\n(1998 ),Joachims (1999 ),Joachims\\n(2002a ),Joachims (2002b ),\\nJoachims (2006a ),Joachims\\n(2006b ),Joachims et al. (2005 ),\\nTsochantaridis et al. (2005 ),Yue\\net al. (2007 )\\nJohnson: Johnson et al. (2006 )\\nJones: Lewis and Jones (1996 ),\\nRobertson and Jones (1976 ),\\nSpärck Jones (1972 ),Spärck Jones\\n(2004 ),Spärck Jones et al. (2000 )\\nJordan: Blei et al. (2003 ),Ng and\\nJordan (2001 ),Ng et al. (2001a ),\\nNg et al. (2001b ),Teh et al. (2006 )\\nJr:Kent et al. (1955 )\\nJunkkari: Arvola et al. (2005 )\\nJurafsky: Jurafsky and Martin (2008 ),\\nTseng et al. (2005 )\\nJärvelin: Järvelin and Kekäläinen\\n(2002 ),Kekäläinen and Järvelin\\n(2002 )\\nKalita: Kołcz et al. (2000 )\\nKambhatla: Lita et al. (2003 )\\nKammenhuber: Kammenhuber et al.\\n(2006 )\\nKamps: Hollink et al. (2004 ),Kamps\\net al. (2004 ),Kamps et al. (2006 ),\\nLalmas et al. (2007 ),\\nSigurbjörnsson et al. (2004 ),\\nTrotman et al. (2007 )\\nKamvar: Kamvar et al. (2002 )\\nKando: Kishida et al. (2005 )\\nKannan: Kannan et al. (2000 )\\nKantor: Saracevic and Kantor (1988 ),\\nSaracevic and Kantor (1996 )\\nKapur: Pavlov et al. (2004 )Karger: Cutting et al. (1993 ),Cutting\\net al. (1992 ),Rennie et al. (2003 )\\nKarttunen: Beesley and Karttunen\\n(2003 )\\nKarypis: Han and Karypis (2000 ),\\nSteinbach et al. (2000 ),Zhao and\\nKarypis (2002 )\\nKaszkiel: Kaszkiel and Zobel (1997 )\\nKataoka: Toda and Kataoka (2005 )\\nKaufman: Kaufman and Rousseeuw\\n(1990 )\\nKazai: Fuhr et al. (2003a ),Fuhr et al.\\n(2006 ),Gövert and Kazai (2003 ),\\nKazai and Lalmas (2006 ),Lalmas\\net al. (2007 )\\nKeerthi: Sindhwani and Keerthi\\n(2006 )\\nKekäläinen: Arvola et al. (2005 ),\\nJärvelin and Kekäläinen (2002 ),\\nKekäläinen (2005 ),Kekäläinen\\nand Järvelin (2002 )\\nKemeny: Kemeny and Snell (1976 )\\nKent: Kent et al. (1955 )\\nKernighan: Kernighan et al. (1990 )\\nKhachiyan: Kozlov et al. (1979 )\\nKing: King (1967 )\\nKishida: Kishida et al. (2005 )\\nKisiel: Yang and Kisiel (2003 )\\nKlavans: McKeown et al. (2002 )\\nKlein: Fraenkel and Klein (1985 ),\\nKamvar et al. (2002 ),Klein and\\nManning (2002 )\\nKleinberg: Chakrabarti et al. (1998 ),\\nKleinberg (1997 ),Kleinberg\\n(1999 ),Kleinberg (2002 )\\nKnuth: Knuth (1997 )\\nKo:Ko et al. (2004 )\\nKoenemann: Koenemann and Belkin\\n(1996 )\\nKoller: Koller and Sahami (1997 ),\\nTong and Koller (2001 )\\nKonheim: Konheim (1981 )\\nKorfhage: Korfhage (1997 )\\nKozlov: Kozlov et al. (1979 )\\nKołcz: Kołcz et al. (2000 ),Kołcz and\\nYih(2007 )', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 563}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP528 Author Index\\nKraaij: Hiemstra and Kraaij (2005 ),\\nKraaij and Spitters (2003 ),Kraaij\\net al. (2002 )\\nKraemer: Hersh et al. (2000a ),Hersh\\net al. (2001 ),Hersh et al. (2000b )\\nKraft: Meadow et al. (1999 )\\nKretser: Anh et al. (2001 )\\nKrippendorff: Krippendorff (2003 )\\nKrishnan: McLachlan and Krishnan\\n(1996 ),Sahoo et al. (2006 )\\nKrovetz: Glover et al. (2002a ),\\nKrovetz (1995 )\\nKuhns: Maron and Kuhns (1960 )\\nKukich: Kukich (1992 )\\nKumar: Bharat et al. (1998 ),Broder\\net al. (2000 ),Kumar et al. (1999 ),\\nKumar et al. (2000 ),Steinbach\\net al. (2000 )\\nKupiec: Kupiec et al. (1995 )\\nKuriyama: Kishida et al. (2005 )\\nKurland: Kurland and Lee (2004 )\\nKwok: Luk and Kwok (2002 )\\nKäki: Käki (2005 )\\nLacker: Perkins et al. (2003 )\\nLafferty: Berger and Lafferty (1999 ),\\nCroft and Lafferty (2003 ),\\nLafferty and Zhai (2001 ),Lafferty\\nand Zhai (2003 ),Zhai and\\nLafferty (2001a ),Zhai and\\nLafferty (2001b ),Zhai and\\nLafferty (2002 )\\nLai: Qin et al. (2007 )\\nLaird: Dempster et al. (1977 )\\nLalmas: Amer-Yahia and Lalmas\\n(2006 ),Betsi et al. (2006 ),Crestani\\net al. (1998 ),Fuhr et al. (2003a ),\\nFuhr and Lalmas (2007 ),Fuhr\\net al. (2006 ),Fuhr et al. (2005 ),\\nFuhr et al. (2007 ),Fuhr et al.\\n(2003b ),Kazai and Lalmas\\n(2006 ),Lalmas et al. (2007 ),\\nLalmas and Tombros (2007 ),\\nRuthven and Lalmas (2003 )\\nLance: Lance and Williams (1967 )\\nLandauer: Deerwester et al. (1990 ),\\nLittman et al. (1998 )Langville: Langville and Meyer\\n(2006 )\\nLarsen: Larsen and Aone (1999 )\\nLarson: Larson (2005 )\\nLavrenko: Allan et al. (1998 ),\\nLavrenko and Croft (2001 )\\nLavrijssen: Zavrel et al. (2000 )\\nLawrence: Glover et al. (2002a ),\\nGlover et al. (2002b ),Lawrence\\nand Giles (1998 ),Lawrence and\\nGiles (1999 ),Rusmevichientong\\net al. (2001 )\\nLazier: Burges et al. (2005 )\\nLee: Fox and Lee (1991 ),Harman\\net al. (1992 ),Kishida et al. (2005 ),\\nKurland and Lee (2004 ),Lee and\\nFox(1988 )\\nLeek: Miller et al. (1999 )\\nLehtonen: Trotman et al. (2006 )\\nLeiserson: Cormen et al. (1990 )\\nLempel: Lempel and Moran (2000 )\\nLeone: Hersh et al. (1994 )\\nLesk: Lesk (1988 ),Lesk (2004 )\\nLester: Lester et al. (2005 ),Lester\\net al. (2006 )\\nLevenshtein: Levenshtein (1965 )\\nLew: Lew (2001 )\\nLewis: Eyheramendy et al. (2003 ),\\nIttner et al. (1995 ),Lewis (1995 ),\\nLewis (1998 ),Lewis and Jones\\n(1996 ),Lewis and Ringuette\\n(1994 ),Lewis et al. (1996 ),Lewis\\net al. (2004 )\\nLi:Cao et al. (2006 ),Gao et al. (2005 ),\\nGeng et al. (2007 ),Lewis et al.\\n(2004 ),Li and Yang (2003 ),Qin\\net al. (2007 )\\nLiddy: Liddy (2005 )\\nLin: Chen and Lin (2000 ),Chen et al.\\n(2005 )\\nList: List et al. (2005 )\\nLita: Lita et al. (2003 )\\nLittman: Littman et al. (1998 )\\nLiu: Cao et al. (2006 ),Geng et al.\\n(2007 ),Liu et al. (2005 ),Liu and\\nCroft (2004 ),Qin et al. (2007 ),', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 564}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPAuthor Index 529\\nRiezler et al. (2007 ),Yang and Liu\\n(1999 )\\nLloyd: Gaertner et al. (2002 ),Lloyd\\n(1982 )\\nLodhi: Lodhi et al. (2002 )\\nLombard: Lombard et al. (2002 )\\nLong: Long and Suel (2003 ),Zhang\\net al. (2007 )\\nLovins: Lovins (1968 )\\nLu:Lu et al. (2007 )\\nLuehrs: Kent et al. (1955 )\\nLuhn: Luhn (1957 ),Luhn (1958 )\\nLuk: Luk and Kwok (2002 )\\nLunde: Lunde (1998 )\\nLushman: Büttcher et al. (2006 )\\nLuxenburger: Kammenhuber et al.\\n(2006 )\\nMa: Liu et al. (2005 ),Murata et al.\\n(2000 ),Song et al. (2005 )\\nMaarek: Carmel et al. (2001 ),Carmel\\net al. (2003 ),Mass et al. (2003 )\\nMacFarlane: Lu et al. (2007 ),\\nMacFarlane et al. (2000 )\\nMacKinlay: Hughes et al. (2006 )\\nMacQueen: MacQueen (1967 )\\nMadigan: Eyheramendy et al. (2003 )\\nMaganti: Hatzivassiloglou et al.\\n(2000 )\\nMaghoul: Broder et al. (2000 )\\nMahabhashyam: Singitham et al.\\n(2004 )\\nMajumdar: Bast and Majumdar\\n(2005 ),Theobald et al. (2008 )\\nMalhotra: Johnson et al. (2006 )\\nMalik: Fuhr et al. (2006 ),Fuhr et al.\\n(2005 ),Fuhr et al. (2003b )\\nMallows: Fowlkes and Mallows\\n(1983 )\\nManasse: Broder et al. (1997 )\\nMandelbrod: Carmel et al. (2003 ),\\nMass et al. (2003 )\\nManjunath: Newsam et al. (2001 )\\nManning: Kamvar et al. (2002 ),Klein\\nand Manning (2002 ),Manning\\nand Schütze (1999 ),Tseng et al.\\n(2005 )Marais: Silverstein et al. (1999 )\\nMaron: Blair and Maron (1985 ),\\nMaron and Kuhns (1960 )\\nMartin: Jurafsky and Martin (2008 )\\nMarx: Kamps et al. (2006 )\\nMasand: Creecy et al. (1992 )\\nMass: Carmel et al. (2003 ),Mass et al.\\n(2003 )\\nMcBryan: McBryan (1994 )\\nMcCallum: Ghamrawi and\\nMcCallum (2005 ),McCallum and\\nNigam (1998 ),McCallum et al.\\n(1998 ),McCallum (1996 ),Nigam\\net al. (2006 )\\nMcCann: MacFarlane et al. (2000 )\\nMcKeown: McKeown and Radev\\n(1995 ),McKeown et al. (2002 )\\nMcLachlan: McLachlan and Krishnan\\n(1996 )\\nMeadow: Meadow et al. (1999 )\\nMeans: Harold and Means (2004 )\\nMei: Tao et al. (2006 )\\nMeil˘ a: Meil˘ a (2005 )\\nMelnik: Melnik et al. (2001 )\\nMeuss: Schlieder and Meuss (2002 )\\nMeyer: Langville and Meyer (2006 )\\nMihajlovi´ c: Mihajlovi´ c et al. (2005 )\\nMihajlovic: List et al. (2005 )\\nMiller: Miller et al. (1999 )\\nMinsky: Minsky and Papert (1988 )\\nMirrokni: Andoni et al. (2006 )\\nMitchell: Huang and Mitchell (2006 ),\\nMcCallum et al. (1998 ),Mitchell\\n(1997 ),Nigam et al. (2006 )\\nMitra: Buckley et al. (1995 ),Singhal\\net al. (1996a ),Singhal et al. (1997 )\\nMittal: Riezler et al. (2007 )\\nMitzenmacher: Henzinger et al.\\n(2000 )\\nModha: Dhillon and Modha (2001 )\\nMoffat: Anh et al. (2001 ),Anh and\\nMoffat (2005 ),Anh and Moffat\\n(2006a ),Anh and Moffat (2006b ),\\nAnh and Moffat (2006c ),Lester\\net al. (2005 ),Moffat and Bell\\n(1995 ),Moffat and Stuiver (1996 ),', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 565}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP530 Author Index\\nMoffat and Zobel (1992 ),Moffat\\nand Zobel (1996 ),Moffat and\\nZobel (1998 ),Witten et al. (1999 ),\\nZobel and Moffat (2006 ),Zobel\\net al. (1995 )\\nMonz: Hollink et al. (2004 )\\nMooers: Mooers (1961 ),Mooers\\n(1950 )\\nMooney: Basu et al. (2004 )\\nMoore: Brill and Moore (2000 ),Pelleg\\nand Moore (1999 ),Pelleg and\\nMoore (2000 ),Toutanova and\\nMoore (2002 )\\nMoran: Lempel and Moran (2000 )\\nMoricz: Silverstein et al. (1999 )\\nMoschitti: Moschitti (2003 ),Moschitti\\nand Basili (2004 )\\nMotwani: Hopcroft et al. (2000 ),Page\\net al. (1998 )\\nMoulinier: Jackson and Moulinier\\n(2002 )\\nMoura: de Moura et al. (2000 ),\\nRibeiro-Neto et al. (1999 )\\nMulhem: Chiaramella et al. (1996 )\\nMurata: Murata et al. (2000 )\\nMuresan: Muresan and Harper (2004 )\\nMurtagh: Murtagh (1983 )\\nMurty: Jain et al. (1999 )\\nMyaeng: Kishida et al. (2005 )\\nNajork: Henzinger et al. (2000 ),\\nNajork and Heydon (2001 ),\\nNajork and Heydon (2002 )\\nNarin: Pinski and Narin (1976 )\\nNavarro: Brisaboa et al. (2007 ),\\nde Moura et al. (2000 ),Navarro\\nand Baeza-Yates (1997 )\\nNenkova: McKeown et al. (2002 )\\nNes: Zukowski et al. (2006 )\\nNeubert: Ribeiro-Neto et al. (1999 )\\nNewsam: Newsam et al. (2001 )\\nNg: Blei et al. (2003 ),McCallum et al.\\n(1998 ),Ng and Jordan (2001 ),Ng\\net al. (2001a ),Ng et al. (2001b )\\nNicholson: Hughes et al. (2006 )\\nNiculescu-Mizil: Caruana and\\nNiculescu-Mizil (2006 )Nie: Cao et al. (2005 ),Gao et al. (2004 )\\nNigam: McCallum and Nigam (1998 ),\\nNigam et al. (2006 )\\nNilan: Schamber et al. (1990 )\\nNowak: Castro et al. (2004 )\\nNtoulas: Ntoulas and Cho (2007 )\\nO’Brien: Berry et al. (1995 )\\nO’Keefe: O’Keefe and Trotman (2004 )\\nOard: Oard and Dorr (1996 )\\nObermayer: Herbrich et al. (2000 )\\nOcalan: Altingövde et al. (2007 )\\nOgilvie: Ogilvie and Callan (2005 )\\nOles: Zhang and Oles (2001 )\\nOlson: Hersh et al. (2000a ),Hersh\\net al. (2001 ),Hersh et al. (2000b )\\nOmiecinski: Jeong and Omiecinski\\n(1995 )\\nOostendorp: van Zwol et al. (2006 )\\nOrlando: Silvestri et al. (2004 )\\nOsborne: Baldridge and Osborne\\n(2004 )\\nOsi´ nski: Osi´ nski and Weiss (2005 )\\nOzaku: Murata et al. (2000 )\\nOzcan: Altingövde et al. (2007 )\\nOzkarahan: Can and Ozkarahan\\n(1990 )\\nOzmultu: Spink et al. (2000 )\\nPadman: Sahoo et al. (2006 )\\nPaepcke: Hirai et al. (2000 )\\nPage: Brin and Page (1998 ),Cho et al.\\n(1998 ),Page et al. (1998 )\\nPaice: Paice (1990 )\\nPan: Joachims et al. (2005 )\\nPanconesi: Chierichetti et al. (2007 )\\nPapert: Minsky and Papert (1988 )\\nPapineni: Papineni (2001 )\\nPapka: Allan et al. (1998 ),Lewis et al.\\n(1996 )\\nParamá: Brisaboa et al. (2007 )\\nParikh: Pavlov et al. (2004 )\\nPark: Ko et al. (2004 )\\nPavlov: Pavlov et al. (2004 )\\nPazzani: Domingos and Pazzani\\n(1997 )\\nPedersen: Cutting et al. (1993 ),\\nCutting et al. (1992 ),Hearst and', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 566}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPAuthor Index 531\\nPedersen (1996 ),Kupiec et al.\\n(1995 ),Schütze et al. (1995 ),\\nSchütze and Pedersen (1995 ),\\nWeigend et al. (1999 ),Yang and\\nPedersen (1997 )\\nPehcevski: Lalmas et al. (2007 )\\nPelleg: Pelleg and Moore (1999 ),\\nPelleg and Moore (2000 )\\nPennock: Glover et al. (2002a ),Glover\\net al. (2002b ),Rusmevichientong\\net al. (2001 )\\nPerego: Silvestri et al. (2004 )\\nPerkins: Perkins et al. (2003 )\\nPerry: Kent et al. (1955 )\\nPersin: Persin (1994 ),Persin et al.\\n(1996 )\\nPeterson: Peterson (1980 )\\nPfeifer: Fuhr and Pfeifer (1994 )\\nPharo: Trotman et al. (2006 )\\nPicca: Picca et al. (2006 )\\nPinski: Pinski and Narin (1976 )\\nPirolli: Pirolli (2007 )\\nPiwowarski: Lalmas et al. (2007 )\\nPlatt: Dumais et al. (1998 ),Platt (2000 )\\nPlaunt: Hearst and Plaunt (1993 )\\nPollermann: Berners-Lee et al. (1992 )\\nPonte: Ponte and Croft (1998 )\\nPopescul: Popescul and Ungar (2000 )\\nPorter: Porter (1980 )\\nPrabakarmurthi: Kołcz et al. (2000 )\\nPrager: Chu-Carroll et al. (2006 )\\nPrakash: Richardson et al. (2006 )\\nPrice: Hersh et al. (2000a ),Hersh\\net al. (2001 ),Hersh et al. (2000b )\\nPugh: Pugh (1990 )\\nPunera: Anagnostopoulos et al.\\n(2006 )\\nQin: Geng et al. (2007 ),Qin et al.\\n(2007 )\\nQiu: Qiu and Frei (1993 )\\nR Development Core Team: R\\nDevelopment Core Team (2005 )\\nRadev: McKeown and Radev (1995 ),\\nRadev et al. (2001 )\\nRadlinski: Yue et al. (2007 )\\nRaftery: Fraley and Raftery (1998 )Raghavan: Broder et al. (2000 ),\\nChakrabarti et al. (1998 ),\\nChierichetti et al. (2007 ),Hirai\\net al. (2000 ),Kumar et al. (1999 ),\\nKumar et al. (2000 ),Melnik et al.\\n(2001 ),Radev et al. (2001 ),\\nSingitham et al. (2004 )\\nRahm: Rahm and Bernstein (2001 )\\nRajagopalan: Broder et al. (2000 ),\\nChakrabarti et al. (1998 ),Kumar\\net al. (1999 ),Kumar et al. (2000 )\\nRamírez: List et al. (2005 )\\nRand: Rand (1971 )\\nRasmussen: Rasmussen (1992 )\\nRau: Jacobs and Rau (1990 )\\nReina: Bradley et al. (1998 ),Fayyad\\net al. (1998 )\\nRennie: Rennie et al. (2003 )\\nRenshaw: Burges et al. (2005 )\\nRibeiro-Neto: Badue et al. (2001 ),\\nBaeza-Yates and Ribeiro-Neto\\n(1999 ),Ribeiro-Neto et al. (1999 ),\\nRibeiro-Neto and Barbosa (1998 )\\nRice: Rice (2006 )\\nRichardson: Richardson et al. (2006 )\\nRiezler: Riezler et al. (2007 )\\nRijke: Hollink et al. (2004 ),Kamps\\net al. (2004 ),Kamps et al. (2006 ),\\nSigurbjörnsson et al. (2004 )\\nRijsbergen: Crestani et al. (1998 ),\\nJardine and van Rijsbergen\\n(1971 ),Tombros et al. (2002 ),\\nvan Rijsbergen (1979 ),\\nvan Rijsbergen (1989 )\\nRinguette: Lewis and Ringuette\\n(1994 )\\nRipley: Ripley (1996 )\\nRivest: Cormen et al. (1990 )\\nRoberts: Borodin et al. (2001 )\\nRobertson: Lalmas et al. (2007 ),Lu\\net al. (2007 ),MacFarlane et al.\\n(2000 ),Robertson (2005 ),\\nRobertson et al. (2004 ),Robertson\\nand Jones (1976 ),Spärck Jones\\net al. (2000 ),Taylor et al. (2006 ),\\nZaragoza et al. (2003 )', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 567}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP532 Author Index\\nRocchio: Rocchio (1971 )\\nRoget: Roget (1946 )\\nRose: Lewis et al. (2004 )\\nRosen-Zvi: Rosen-Zvi et al. (2004 )\\nRosenfeld: McCallum et al. (1998 )\\nRosenthal: Borodin et al. (2001 )\\nRoss: Ross (2006 )\\nRoukos: Lita et al. (2003 )\\nRousseeuw: Kaufman and\\nRousseeuw (1990 )\\nRozonoér: Aizerman et al. (1964 )\\nRubin: Dempster et al. (1977 )\\nRusmevichientong:\\nRusmevichientong et al. (2001 )\\nRuthven: Ruthven and Lalmas (2003 )\\nRölleke: Amer-Yahia et al. (2005 ),\\nFuhr and Rölleke (1997 )\\nSable: McKeown et al. (2002 )\\nSacherek: Hersh et al. (2000a ),Hersh\\net al. (2001 ),Hersh et al. (2000b )\\nSacks-Davis: Persin et al. (1996 ),\\nZobel et al. (1995 )\\nSahami: Dumais et al. (1998 ),Koller\\nand Sahami (1997 )\\nSahoo: Sahoo et al. (2006 )\\nSakai: Sakai (2007 )\\nSalton: Buckley et al. (1994a ),Buckley\\nand Salton (1995 ),Buckley et al.\\n(1994b ),Salton (1971a ),Salton\\n(1971b ),Salton (1975 ),Salton\\n(1989 ),Salton (1991 ),Salton et al.\\n(1993 ),Salton and Buckley\\n(1987 ),Salton and Buckley\\n(1988 ),Salton and Buckley\\n(1990 ),Singhal et al. (1995 ),\\nSinghal et al. (1996b )\\nSanderson: Tombros and Sanderson\\n(1998 )\\nSantini: Boldi et al. (2002 ),Boldi et al.\\n(2005 ),Smeulders et al. (2000 )\\nSaracevic: Saracevic and Kantor\\n(1988 ),Saracevic and Kantor\\n(1996 )\\nSatyanarayana: Davidson and\\nSatyanarayana (2003 )\\nSaunders: Lodhi et al. (2002 )Savaresi: Savaresi and Boley (2004 )\\nSchamber: Schamber et al. (1990 )\\nSchapire: Allwein et al. (2000 ),Cohen\\net al. (1998 ),Lewis et al. (1996 ),\\nSchapire (2003 ),Schapire and\\nSinger (2000 ),Schapire et al.\\n(1998 )\\nSchek: Grabs and Schek (2002 )\\nSchenkel: Theobald et al. (2008 ),\\nTheobald et al. (2005 )\\nSchiffman: McKeown et al. (2002 )\\nSchlieder: Schlieder and Meuss (2002 )\\nScholer: Scholer et al. (2002 )\\nSchwartz: Miller et al. (1999 )\\nSchwarz: Schwarz (1978 )\\nSchölkopf: Chen et al. (2005 ),\\nSchölkopf and Smola (2001 )\\nSchütze: Manning and Schütze\\n(1999 ),Schütze (1998 ),Schütze\\net al. (1995 ),Schütze and\\nPedersen (1995 ),Schütze and\\nSilverstein (1997 )\\nSebastiani: Sebastiani (2002 )\\nSeo: Ko et al. (2004 )\\nShaked: Burges et al. (2005 )\\nShanmugasundaram: Amer-Yahia\\net al. (2006 ),Amer-Yahia et al.\\n(2005 )\\nShawe-Taylor: Cristianini and\\nShawe-Taylor (2000 ),Lodhi et al.\\n(2002 ),Shawe-Taylor and\\nCristianini (2004 )\\nShih: Rennie et al. (2003 ),Sproat et al.\\n(1996 )\\nShkapenyuk: Shkapenyuk and Suel\\n(2002 )\\nSiegel: Siegel and Castellan (1988 )\\nSifry: Sifry (2007 )\\nSigelman: McKeown et al. (2002 )\\nSigurbjörnsson: Kamps et al. (2004 ),\\nKamps et al. (2006 ),\\nSigurbjörnsson et al. (2004 ),\\nTrotman and Sigurbjörnsson\\n(2004 )\\nSilverstein: Schütze and Silverstein\\n(1997 ),Silverstein et al. (1999 )', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 568}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPAuthor Index 533\\nSilvestri: Silvestri (2007 ),Silvestri\\net al. (2004 )\\nSimon: Zha et al. (2001 )\\nSindhwani: Sindhwani and Keerthi\\n(2006 )\\nSinger: Allwein et al. (2000 ),Cohen\\net al. (1998 ),Cohen and Singer\\n(1999 ),Crammer and Singer\\n(2001 ),Schapire and Singer\\n(2000 ),Schapire et al. (1998 )\\nSinghal: Buckley et al. (1995 ),\\nSchapire et al. (1998 ),Singhal\\net al. (1996a ),Singhal et al.\\n(1997 ),Singhal et al. (1995 ),\\nSinghal et al. (1996b )\\nSingitham: Singitham et al. (2004 )\\nSivakumar: Kumar et al. (2000 )\\nSlonim: Tishby and Slonim (2000 )\\nSmeulders: Smeulders et al. (2000 )\\nSmith: Creecy et al. (1992 )\\nSmola: Schölkopf and Smola (2001 )\\nSmyth: Rosen-Zvi et al. (2004 )\\nSneath: Sneath and Sokal (1973 )\\nSnedecor: Snedecor and Cochran\\n(1989 )\\nSnell: Grinstead and Snell (1997 ),\\nKemeny and Snell (1976 )\\nSnyder-Duch: Lombard et al. (2002 )\\nSoffer: Carmel et al. (2001 ),Carmel\\net al. (2003 ),Mass et al. (2003 )\\nSokal: Sneath and Sokal (1973 )\\nSomogyi: Somogyi (1990 )\\nSong: Song et al. (2005 )\\nSornil: Sornil (2001 )\\nSozio: Chierichetti et al. (2007 )\\nSpink: Spink and Cole (2005 ),Spink\\net al. (2000 )\\nSpitters: Kraaij and Spitters (2003 )\\nSproat: Sproat and Emerson (2003 ),\\nSproat et al. (1996 ),Sproat (1992 )\\nSrinivasan: Coden et al. (2002 )\\nStata: Broder et al. (2000 )\\nStein: Stein and zu Eissen (2004 ),\\nStein et al. (2003 )\\nSteinbach: Steinbach et al. (2000 )\\nSteyvers: Rosen-Zvi et al. (2004 )Stork: Duda et al. (2000 )\\nStrang: Strang (1986 )\\nStrehl: Strehl (2002 )\\nStrohman: Strohman and Croft (2007 )\\nStuiver: Moffat and Stuiver (1996 )\\nStutz: Cheeseman and Stutz (1996 )\\nSuel: Long and Suel (2003 ),\\nShkapenyuk and Suel (2002 ),\\nZhang et al. (2007 )\\nSwanson: Swanson (1988 )\\nSzlávik: Fuhr et al. (2005 )\\nTague-Sutcliffe: Tague-Sutcliffe and\\nBlustein (1995 )\\nTan: Tan and Cheng (2007 )\\nTannier: Tannier and Geva (2005 )\\nTao: Tao et al. (2006 )\\nTarasov: Kozlov et al. (1979 )\\nTaube: Taube and Wooster (1958 )\\nTaylor: Robertson et al. (2004 ),Taylor\\net al. (2006 )\\nTeevan: Rennie et al. (2003 )\\nTeh: Teh et al. (2006 )\\nTheiler: Perkins et al. (2003 )\\nTheobald: Theobald et al. (2008 ),\\nTheobald et al. (2005 )\\nThomas: Cover and Thomas (1991 )\\nTiberi: Chierichetti et al. (2007 )\\nTibshirani: Hastie et al. (2001 ),\\nTibshirani et al. (2001 )\\nTipping: Zaragoza et al. (2003 )\\nTishby: Tishby and Slonim (2000 )\\nToda: Toda and Kataoka (2005 )\\nTokunaga: Iwayama and Tokunaga\\n(1995 )\\nTomasic: Tomasic and Garcia-Molina\\n(1993 )\\nTombros: Betsi et al. (2006 ),Lalmas\\nand Tombros (2007 ),Tombros\\nand Sanderson (1998 ),Tombros\\net al. (2002 )\\nTomkins: Broder et al. (2000 ),Kumar\\net al. (1999 ),Kumar et al. (2000 )\\nTomlinson: Tomlinson (2003 )\\nTong: Tong and Koller (2001 )\\nToutanova: Toutanova and Moore\\n(2002 )', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 569}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP534 Author Index\\nTreeratpituk: Treeratpituk and Callan\\n(2006 )\\nTrenkle: Cavnar and Trenkle (1994 )\\nTrotman: Fuhr et al. (2007 ),O’Keefe\\nand Trotman (2004 ),Trotman\\n(2003 ),Trotman and Geva (2006 ),\\nTrotman et al. (2007 ),Trotman\\net al. (2006 ),Trotman and\\nSigurbjörnsson (2004 )\\nTsaparas: Borodin et al. (2001 )\\nTsegay: Turpin et al. (2007 )\\nTseng: Tseng et al. (2005 )\\nTsikrika: Betsi et al. (2006 )\\nTsioutsiouliklis: Glover et al. (2002b )\\nTsochantaridis: Riezler et al. (2007 ),\\nTsochantaridis et al. (2005 )\\nTudhope: Clarke et al. (2000 )\\nTukey: Cutting et al. (1992 )\\nTurpin: Hersh et al. (2000a ),Hersh\\net al. (2001 ),Hersh et al. (2000b ),\\nTurpin and Hersh (2001 ),Turpin\\nand Hersh (2002 ),Turpin et al.\\n(2007 )\\nTurtle: Turtle (1994 ),Turtle and Croft\\n(1989 ),Turtle and Croft (1991 ),\\nTurtle and Flood (1995 )\\nUchimoto: Murata et al. (2000 )\\nUllman: Garcia-Molina et al. (1999 ),\\nHopcroft et al. (2000 )\\nUlusoy: Altingövde et al. (2007 )\\nUngar: Popescul and Ungar (2000 )\\nUpfal: Chierichetti et al. (2007 ),\\nKumar et al. (2000 )\\nUtiyama: Murata et al. (2000 )\\nVaithyanathan: Vaithyanathan and\\nDom (2000 )\\nVamplew: Johnson et al. (2006 )\\nVapnik: Vapnik (1998 )\\nVasserman: Riezler et al. (2007 )\\nVassilvitskii: Arthur and Vassilvitskii\\n(2006 )\\nVempala: Kannan et al. (2000 )\\nVenkatasubramanian: Bharat et al.\\n(1998 )\\nVenturini: Ferragina and Venturini\\n(2007 )Veta: Kannan et al. (2000 )\\nVigna: Boldi et al. (2002 ),Boldi et al.\\n(2005 ),Boldi and Vigna (2004a ),\\nBoldi and Vigna (2004b ),Boldi\\nand Vigna (2005 )\\nVilla: Tombros et al. (2002 )\\nVittaut: Vittaut and Gallinari (2006 )\\nViña: Cacheda et al. (2003 )\\nVoorhees: Buckley and Voorhees\\n(2000 ),Voorhees (1985a ),\\nVoorhees (1985b ),Voorhees\\n(2000 ),Voorhees and Harman\\n(2005 )\\nVries: List et al. (2005 )\\nWagner: Wagner and Fischer (1974 )\\nWalker: Spärck Jones et al. (2000 )\\nWalther: Tibshirani et al. (2001 )\\nWaltz: Creecy et al. (1992 )\\nWan: Liu et al. (2005 )\\nWang: Qin et al. (2007 ),Tao et al.\\n(2006 )\\nWard Jr.: Ward Jr. (1963 )\\nWatkins: Lodhi et al. (2002 ),Weston\\nand Watkins (1999 )\\nWei: Wei and Croft (2006 )\\nWeigend: Weigend et al. (1999 )\\nWeikum: Amer-Yahia et al. (2005 ),\\nChaudhuri et al. (2006 ),\\nKammenhuber et al. (2006 ),\\nTheobald et al. (2008 ),Theobald\\net al. (2005 )\\nWeinstein: Hayes and Weinstein\\n(1990 )\\nWeiss: Apté et al. (1994 ),Ng et al.\\n(2001a ),Osi´ nski and Weiss (2005 )\\nWen: Song et al. (2005 )\\nWesterveld: Kraaij et al. (2002 )\\nWeston: Weston and Watkins (1999 )\\nWidom: Garcia-Molina et al. (1999 ),\\nJeh and Widom (2003 )\\nWiener: Broder et al. (2000 ),Weigend\\net al. (1999 )\\nWiering: van Zwol et al. (2006 )\\nWilkinson: Zobel et al. (1995 )\\nWillett: El-Hamdouchi and Willett\\n(1986 )', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 570}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPAuthor Index 535\\nWilliams: Bahle et al. (2002 ),Garcia\\net al. (2004 ),Heinz et al. (2002 ),\\nLance and Williams (1967 ),\\nLester et al. (2006 ),Scholer et al.\\n(2002 ),Turpin et al. (2007 ),\\nWilliams and Zobel (2005 ),\\nWilliams et al. (2004 )\\nWinograd: Page et al. (1998 )\\nWitten: Witten and Bell (1990 ),Witten\\nand Frank (2005 ),Witten et al.\\n(1999 )\\nWißbrock: Stein et al. (2003 )\\nWong: Hartigan and Wong (1979 ),\\nWong et al. (1988 )\\nWoodley: Woodley and Geva (2006 )\\nWooster: Taube and Wooster (1958 )\\nWorring: Smeulders et al. (2000 )\\nWu: Gao et al. (2005 ),Gao et al. (2004 )\\nXu:Cao et al. (2006 ),Ji and Xu (2006 ),\\nXu and Croft (1996 ),Xu and\\nCroft (1999 )\\nYang: Ault and Yang (2002 ),Lewis\\net al. (2004 ),Li and Yang (2003 ),\\nLiu et al. (2005 ),Melnik et al.\\n(2001 ),Yang and Callan (2006 ),\\nYang (1994 ),Yang (1999 ),Yang\\n(2001 ),Yang and Kisiel (2003 ),\\nYang and Liu (1999 ),Yang and\\nPedersen (1997 )\\nYao: Wong et al. (1988 )\\nYiannis: Scholer et al. (2002 )\\nYih: Kołcz and Yih (2007 )\\nYilmaz: Aslam and Yilmaz (2005 )\\nYoung: Berry and Young (1995 ),\\nEckart and Young (1936 )\\nYu:Hand and Yu (2001 )\\nYue: Yue et al. (2007 )\\nZamir: Zamir and Etzioni (1999 )\\nZaragoza: Robertson et al. (2004 ),\\nTaylor et al. (2006 ),Zaragoza\\net al. (2003 )\\nZavrel: Zavrel et al. (2000 )\\nZeng: Liu et al. (2005 )\\nZha: Zha et al. (2001 )\\nZhai: Lafferty and Zhai (2001 ),\\nLafferty and Zhai (2003 ),Taoet al. (2006 ),Zhai and Lafferty\\n(2001a ),Zhai and Lafferty\\n(2001b ),Zhai and Lafferty (2002 )\\nZhang: Qin et al. (2007 ),Radev et al.\\n(2001 ),Zhang et al. (2007 ),Zhang\\nand Oles (2001 )\\nZhao: Zhao and Karypis (2002 )\\nZheng: Ng et al. (2001b )\\nZien: Chapelle et al. (2006 )\\nZipf: Zipf (1949 )\\nZiviani: Badue et al. (2001 ),de Moura\\net al. (2000 ),Ribeiro-Neto et al.\\n(1999 )\\nZobel: Bahle et al. (2002 ),Heinz and\\nZobel (2003 ),Heinz et al. (2002 ),\\nKaszkiel and Zobel (1997 ),Lester\\net al. (2005 ),Lester et al. (2006 ),\\nMoffat and Zobel (1992 ),Moffat\\nand Zobel (1996 ),Moffat and\\nZobel (1998 ),Persin et al. (1996 ),\\nScholer et al. (2002 ),Williams\\nand Zobel (2005 ),Williams et al.\\n(2004 ),Zobel (1998 ),Zobel and\\nDart (1995 ),Zobel and Dart\\n(1996 ),Zobel and Moffat (2006 ),\\nZobel et al. (1995 )\\nZukowski: Zukowski et al. (2006 )\\nZweig: Broder et al. (1997 )\\nZwol: van Zwol et al. (2006 )\\ndel Bimbo: del Bimbo (1999 )', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 571}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 572}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPIndex\\nL2distance, 131\\nχ2feature selection, 275\\nδcodes, 104\\nγencoding, 99\\nknearest neighbor classiﬁcation, 297\\nk-gram index, 54,60\\n1/0 loss, 221\\n11-point interpolated average\\nprecision, 159\\n20 Newsgroups, 154\\nA/B test, 170\\naccess control lists, 81\\naccumulator, 113,125\\naccuracy, 155\\nactive learning, 336\\nad hoc retrieval, 5,253\\nadd-one smoothing, 260\\nadjacency table, 455\\nadversarial information retrieval, 429\\nAkaike Information Criterion, 367\\nalgorithmic search, 430\\nanchor text, 425\\nany-of classiﬁcation, 257,306\\nauthority score, 474\\nauxiliary index, 78\\naverage-link clustering, 389\\nB-tree, 50\\nbag of words, 117,267\\nbag-of-words, 269\\nbalanced F measure, 156\\nBayes error rate, 300\\nBayes Optimal Decision Rule, 222\\nBayes risk, 222Bayes’ Rule, 220\\nBayesian networks, 234\\nBayesian prior, 226\\nBernoulli model, 263\\nbest-merge persistence, 388\\nbias, 311\\nbias-variance tradeoff, 241,312,321\\nbiclustering, 374\\nbigram language model, 240\\nBinary Independence Model, 222\\nbinary tree, 50,377\\nbiword index, 39,43\\nblind relevance feedback, seepseudo\\nrelevance feedback\\nblocked sort-based indexing\\nalgorithm, 71\\nblocked storage, 92\\nblog, 195\\nBM25 weights, 232\\nboosting, 286\\nbottom-up clustering, seehierarchical\\nagglomerative clustering\\nbowtie, 426\\nbreak-even, 334\\nbreak-even point, 161\\nBSBI, 71\\nBuckshot algorithm, 399\\nbuffer, 69\\ncaching, 9,68,146,447,450\\ncapture-recapture method, 435\\ncardinality\\nin clustering, 355\\nCAS topics, 211\\ncase-folding, 30', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 573}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP538 Index\\ncategory, 256\\ncentroid, 292,360\\nin relevance feedback, 181\\ncentroid-based classiﬁcation, 314\\nchain rule, 220\\nchaining\\nin clustering, 385\\nchampion lists, 143\\nclass boundary, 303\\nclassiﬁcation, 253,344\\nclassiﬁcation function, 256\\nclassiﬁer, 183\\nCLEF, 154\\nclick spam, 431\\nclickstream mining, 170,188\\nclickthrough log analysis, 170\\nclique, 384\\ncluster, 74,349\\nin relevance feedback, 184\\ncluster hypothesis, 350\\ncluster-based classiﬁcation, 314\\ncluster-internal labeling, 396\\nCO topics, 211\\nco-clustering, 374\\ncollection, 4\\ncollection frequency, 27\\ncombination similarity, 378,384,393\\ncomplete-link clustering, 382\\ncomplete-linkage clustering, see\\ncomplete-link clustering\\ncomponent coverage, 212\\ncompound-splitter, 25\\ncompounds, 25\\nconcept drift, 269,283,286,336\\nconditional independence\\nassumption, 224,266\\nconfusion matrix, 307\\nconnected component, 384\\nconnectivity queries, 455\\nconnectivity server, 455\\ncontent management system, 84\\ncontext\\nXML, 199\\ncontext resemblance, 208\\ncontiguity hypothesis, 289\\ncontinuation bit, 96corpus, 4\\ncosine similarity, 121,372\\nCPC, 430\\nCPM, 430\\nCranﬁeld, 153\\ncross-entropy, 251\\ncross-language information retrieval,\\n154,417\\ncumulative gain, 162\\ndata-centric XML, 196,214\\ndatabase\\nrelational, 1,195,214\\ndecision boundary, 292,303\\ndecision hyperplane, 290,302\\ndecision trees, 282,286\\ndendrogram, 378\\ndevelopment set, 283\\ndevelopment test collection, 153\\nDice coefﬁcient, 163\\ndictionary, 6,7\\ndifferential cluster labeling, 396\\ndigital libraries, 195\\ndistortion, 366\\ndistributed index, 74,458\\ndistributed indexing, 74\\ndistributed information retrieval, see\\ndistributed crawling, 458\\ndivisive clustering, 395\\nDNS resolution, 450\\nDNS server, 450\\ndocID, 7\\ndocument, 4,20\\ndocument collection, seecollection\\ndocument frequency, 7,118\\ndocument likelihood model, 250\\ndocument partitioning, 454\\ndocument space, 256\\ndocument vector, 119,120\\ndocument-at-a-time, 126,140\\ndocument-partitioned index, 75\\ndot product, 121\\nEast Asian languages, 45\\nedit distance, 58\\neffectiveness, 5,280\\neigen decomposition, 406', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 574}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPIndex 539\\neigenvalue, 404\\nEM algorithm, 369\\nemail sorting, 254\\nenterprise resource planning, 84\\nenterprise search, 67\\nentropy, 99,106,358\\nequivalence classes, 28\\nErgodic Markov Chain, 467\\nEuclidean distance, 131,372\\nEuclidean length, 121\\nevidence accumulation, 146\\nexclusive clustering, 355\\nexhaustive clustering, 355\\nexpectation step, 370\\nExpectation-Maximization algorithm,\\n336,369\\nexpected edge density, 373\\nextended query, 205\\nExtensible Markup Language, 196\\nexternal criterion of quality, 356\\nexternal sorting algorithm, 70\\nF measure, 156,173\\nas an evaluation measure in\\nclustering, 359\\nfalse negative, 359\\nfalse positive, 359\\nfeature engineering, 338\\nfeature selection, 271\\nﬁeld, 110\\nﬁltering, 253,314\\nﬁrst story detection, 395,399\\nﬂat clustering, 350\\nfocused retrieval, 217\\nfree text, 109,148\\nfree text query, seequery, free text,\\n124,145,196\\nfrequency-based feature selection, 277\\nFrobenius norm, 410\\nfront coding, 93\\nfunctional margin, 322\\nGAAC, 388\\ngenerative model, 237,309,311\\ngeometric margin, 323\\ngold standard, 152\\nGolomb codes, 106GOV2, 154\\ngreedy feature selection, 279\\ngrep, 3\\nground truth, 152\\ngroup-average agglomerative\\nclustering, 388\\ngroup-average clustering, 389\\nHAC, 378\\nhard assignment, 350\\nhard clustering, 350,355\\nharmonic number, 101\\nHeaps’ law, 88\\nheld-out, 298\\nheld-out data, 283\\nhierarchic clustering, 377\\nhierarchical agglomerative clustering,\\n378\\nhierarchical classiﬁcation, 337,347\\nhierarchical clustering, 350,377\\nHierarchical Dirichlet Processes, 418\\nhierarchy\\nin clustering, 377\\nhighlighting, 203\\nHITS, 477\\nHTML, 421\\nhttp, 421\\nhub score, 474\\nhyphens, 24\\ni.i.d., 283,seeindependent and\\nidentically distributed\\nIde dec-hi, 183\\nidf,83,204,227,232\\niid,seeindependent and identically\\ndistributed\\nimpact, 81\\nimplicit relevance feedback, 187\\nin-links, 425,461\\nincidence matrix, 3,408\\nindependence, 275\\nindependent and identically\\ndistributed, 283\\nin clustering, 367\\nindex, 3,seepermuterm index, see also\\nparametric index, zone index\\nindex construction, 67', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 575}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP540 Index\\nindexer, 67\\nindexing, 67\\nsort-based, 7\\nindexing granularity, 21\\nindexing unit, 201\\nINEX, 210\\ninformation gain, 285\\ninformation need, 5,152\\ninformation retrieval, 1\\ninformational queries, 432\\ninner product, 121\\ninstance-based learning, 300\\ninter-similarity, 381\\ninternal criterion of quality, 356\\ninterpolated precision, 158\\nintersection\\npostings list, 10\\ninverse document frequency, 118,125\\ninversion, 71,378,391\\ninverted ﬁle, seeinverted index\\ninverted index, 6\\ninverted list, seepostings list\\ninverter, 76\\nIP address, 449\\nJaccard coefﬁcient, 61,438\\nK-medoids, 365\\nkappa statistic, 165,174,373\\nkernel, 332\\nkernel function, 332\\nkernel trick, 331\\nkey-value pairs, 75\\nkeyword-in-context, 171\\nkNN classiﬁcation, 297\\nKruskal’s algorithm, 399\\nKullback-Leibler divergence, 251,\\n317,372\\nKWIC, seekeyword-in-context\\nlabel, 256\\nlabeling, 255\\nlanguage, 237\\nlanguage identiﬁcation, 24,46\\nlanguage model, 238\\nLaplace smoothing, 260\\nLatent Dirichlet Allocation, 418latent semantic indexing, 192,413\\nLDA, 418\\nlearning algorithm, 256\\nlearning error, 310\\nlearning method, 256\\nlemma, 32\\nlemmatization, 32\\nlemmatizer, 33\\nlength-normalization, 121\\nLevenshtein distance, 58\\nlexicalized subtree, 206\\nlexicon, 6\\nlikelihood, 221\\nlikelihood ratio, 239\\nlinear classiﬁer, 301,343\\nlinear problem, 303\\nlinear separability, 304\\nlink farms, 481\\nlink spam, 429,461\\nLM, 243\\nlogarithmic merging, 79\\nlossless, 87\\nlossy compression, 87\\nlow-rank approximation, 410\\nLSA, 413\\nLSI as soft clustering, 417\\nmachine translation, 240,243,251\\nmachine-learned relevance, 113,342\\nmacroaveraging, 280\\nMAP , 159,227,258\\nmap phase, 75\\nMapReduce, 75\\nmargin, 320\\nmarginal relevance, 167\\nmarginal statistic, 165\\nmaster node, 75\\nmatrix decomposition, 406\\nmaximization step, 370\\nmaximum a posteriori, 227,265\\nmaximum a posteriori class, 258\\nmaximum likelihood estimate, 226,\\n259\\nmaximum likelihood estimation, 244\\nMean Average Precision, seeMAP\\nmedoid, 365\\nmemory capacity, 312', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 576}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPIndex 541\\nmemory-based learning, 300\\nMercator, 445\\nMercer kernel, 332\\nmerge\\npostings, 10\\nmerge algorithm, 10\\nmetadata, 24,110,171,197,373,428\\nmicroaveraging, 280\\nminimum spanning tree, 399,401\\nminimum variance clustering, 399\\nMLE, seemaximum likelihood\\nestimate\\nModApte split, 279,286\\nmodel complexity, 312,366\\nmodel-based clustering, 368\\nmonotonicity, 378\\nmulticlass classiﬁcation, 306\\nmulticlass SVM, 347\\nmultilabel classiﬁcation, 306\\nmultimodal class, 296\\nmultinomial classiﬁcation, 306\\nmultinomial distribution, 241\\nmultinomial model, 263,270\\nmultinomial Naive Bayes, 258\\nmultinomial NB, seemultinomial\\nNaive Bayes\\nmultivalue classiﬁcation, 306\\nmultivariate Bernoulli model, 263\\nmutual information, 272,358\\nNaive Bayes assumption, 224\\nnamed entity tagging, 195,339\\nNational Institute of Standards and\\nTechnology, 153\\nnatural language processing, xxxiv ,\\n33,171,217,249,372\\nnavigational queries, 432\\nNDCG, 163\\nnested elements, 203\\nNEXI, 200\\nnext word index, 44\\nnibble, 98\\nNLP , seenatural language processing\\nNMI, 358\\nnoise document, 303\\nnoise feature, 271\\nnonlinear classiﬁer, 305nonlinear problem, 305\\nnormal vector, 293\\nnormalized discounted cumulative\\ngain, 163\\nnormalized mutual information, 358\\nnovelty detection, 395\\nNTCIR, 154,174\\nobjective function, 354,360\\nodds, 221\\nodds ratio, 225\\nOkapi weighting, 232\\none-of classiﬁcation, 257,284,306\\noptimal classiﬁer, 270,310\\noptimal clustering, 393\\noptimal learning method, 310\\nordinal regression, 344\\nout-links, 425\\noutlier, 363\\noverﬁtting, 271,312\\nPageRank, 464\\npaid inclusion, 428\\nparameter tuning, 153,314,315,348\\nparameter tying, 340\\nparameter-free compression, 100\\nparameterized compression, 106\\nparametric index, 110\\nparametric search, 197\\nparser, 75\\npartition rule, 220\\npartitional clustering, 355\\npassage retrieval, 217\\npatent databases, 195\\nperceptron algorithm, 286,315\\nperformance, 280\\npermuterm index, 53\\npersonalized PageRank, 471\\nphrase index, 40\\nphrase queries, 39,47\\nphrase search, 15\\npivoted document length\\nnormalization, 129\\npointwise mutual information, 286\\npolychotomous, 306\\npolytomous classiﬁcation, 306\\npolytope, 298', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 577}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP542 Index\\npooling, 164,174\\npornography ﬁltering, 338\\nPorter stemmer, 33\\npositional independence, 267\\npositional index, 41\\nposterior probability, 220\\nposting, 6,7,71,86\\npostings list, 6\\npower law, 89,426\\nprecision, 5,155\\nprecision at k,161\\nprecision-recall curve, 158\\npreﬁx-free code, 100\\nprincipal direction divisive\\npartitioning, 400\\nprincipal left eigenvector, 465\\nprior probability, 220\\nProbability Ranking Principle, 221\\nprobability vector, 466\\nprototype, 290\\nproximity operator, 14\\nproximity weighting, 145\\npseudo relevance feedback, 187\\npseudocounts, 226\\npull model, 314\\npurity, 356\\npush model, 314\\nQuadratic Programming, 324\\nquery, 5\\nfree text, 14,16,117\\nsimple conjunctive, 10\\nquery expansion, 189\\nquery likelihood model, 242\\nquery optimization, 11\\nquery-by-example, 201,249\\nR-precision, 161,174\\nRand index, 359\\nadjusted, 373\\nrandom variable, 220\\nrandom variable C,268\\nrandom variable U,266\\nrandom variable X,266\\nrank, 403\\nRanked Boolean retrieval, 112\\nranked retrieval, 81,107model, 14\\nranking SVM, 345\\nrecall, 5,155\\nreduce phase, 75\\nreduced SVD, 409,412\\nregression, 344\\nregular expressions, 3,18\\nregularization, 328\\nrelational database, 195,214\\nrelative frequency, 226\\nrelevance, 5,152\\nrelevance feedback, 178\\nresidual sum of squares, 360\\nresults snippets, 146\\nretrieval model\\nBoolean, 4\\nRetrieval Status Value, 225\\nretrieval systems, 81\\nReuters-21578, 154\\nReuters-RCV1, 69,154\\nRF,178\\nRobots Exclusion Protocol, 447\\nROC curve, 162\\nRocchio algorithm, 181\\nRocchio classiﬁcation, 292\\nrouting, 253,314\\nRSS, 360\\nrule of 30, 86\\nrules in text classiﬁcation, 255\\nScatter-Gather, 351\\nschema, 199\\nschema diversity, 204\\nschema heterogeneity, 204\\nsearch advertising, 430\\nsearch engine marketing, 431\\nSearch Engine Optimizers, 429\\nsearch result clustering, 351\\nsearch results, 351\\nsecurity, 81\\nseed, 361\\nseek time, 68\\nsegment ﬁle, 75\\nsemi-supervised learning, 336\\nsemistructured query, 197\\nsemistructured retrieval, 2,197\\nsensitivity, 162', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 578}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPIndex 543\\nsentiment detection, 254\\nsequence model, 267\\nshingling, 438\\nsingle-label classiﬁcation, 306\\nsingle-link clustering, 382\\nsingle-linkage clustering, see\\nsingle-link clustering\\nsingle-pass in-memory indexing, 73\\nsingleton, 378\\nsingleton cluster, 363\\nsingular value decomposition, 407\\nskip list, 36,46\\nslack variables, 327\\nSMART, 182\\nsmoothing, 127,226\\nadd α,226\\nadd1\\n2,232\\nadd1\\n2,226–229,262\\nBayesian prior, 226,228,245\\nlinear interpolation, 245\\nsnippet, 170\\nsoft assignment, 350\\nsoft clustering, 350,355,377\\nsorting\\nin index construction, 7\\nsoundex, 63\\nspam, 338,427\\nemail, 254\\nweb, 254\\nsparseness, 241,244,260\\nspeciﬁcity, 162\\nspectral clustering, 400\\nspeech recognition, 240\\nspelling correction, 147,240,242\\nspider, 443\\nspider traps, 433\\nSPIMI, 73\\nsplits, 75\\nsponsored search, 430\\nstanding query, 253\\nstatic quality scores, 138\\nstatic web pages, 424\\nstatistical signiﬁcance, 276\\nstatistical text classiﬁcation, 255\\nsteady-state, 467,468\\nstemming, 32,46stochastic matrix, 465\\nstop words, 117\\nstop list, 27\\nstop words, 117\\nstop words, 23,27,45,127\\nstructural SVM, 345\\nstructural SVMs, 330\\nstructural term, 207\\nstructured document retrieval\\nprinciple, 201\\nstructured query, 197\\nstructured retrieval, 195,197\\nsummarization, 400\\nsummary\\ndynamic, 171\\nstatic, 171\\nsupervised learning, 256\\nsupport vector, 320\\nsupport vector machine, 319,346\\nmulticlass, 330\\nSVD, 373,400,408\\nSVM, seesupport vector machine\\nsymmetric diagonal decomposition,\\n407,408\\nsynonymy, 177\\nteleport, 464\\nterm, 3,19,22\\nterm frequency, 16,117\\nterm normalization, 28\\nterm partitioning, 454\\nterm-at-a-time, 125,140\\nterm-document matrix, 123\\nterm-partitioned index, 74\\ntermID, 69\\ntest data, 256\\ntest set, 256,283\\ntext categorization, 253\\ntext classiﬁcation, 253\\ntext summarization, 171\\ntext-centric XML, 214\\ntf,seeterm frequency\\ntf-idf, 119\\ntiered indexes, 143\\ntoken, 19,22\\ntoken normalization, 28\\ntop docs, 149', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 579}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP544 Index\\ntop-down clustering, 395\\ntopic, 153,253\\nin XML retrieval, 211\\ntopic classiﬁcation, 253\\ntopic spotting, 253\\ntopic-speciﬁc PageRank, 471\\ntopical relevance, 212\\ntraining set, 256,283\\ntransactional query, 433\\ntransductive SVMs, 336\\ntranslation model, 251\\nTREC, 153,314\\ntrec_eval, 174\\ntruecasing, 30,46\\ntruncated SVD, 409,412,415\\ntwo-class classiﬁer, 279\\ntype, 22\\nunary code, 99\\nunigram language model, 240\\nunion-ﬁnd algorithm, 395,440\\nuniversal code, 100\\nunsupervised learning, 349\\nURL, 422\\nURL normalization, 447\\nutility measure, 286\\nvariable byte encoding, 96\\nvariance, 311\\nvector space model, 120\\nvertical search engine, 254\\nvocabulary, 6\\nVoronoi tessellation, 297\\nWard’s method, 399\\nweb crawler, 443\\nweight vector, 322\\nweighted zone scoring, 110\\nWikipedia, 211\\nwildcard query, 3,49,52\\nwithin-point scatter, 375\\nword segmentation, 25\\nXML, 20,196\\nXML attribute, 197\\nXML DOM, 197\\nXML DTD, 199XML element, 197\\nXML fragment, 216\\nXML Schema, 199\\nXML tag, 197\\nXPath, 199\\nZipf’s law, 89\\nzone, 110,337,339,340\\nzone index, 110\\nzone search, 197', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 580})]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3FWfg9kcy1zZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "6yWIktVTQ-mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But what occurs when you present these models with a document that exceeds their context window? This is where a clever strategy known as \"chunking\" comes into play. Chunking involves dividing the document into smaller, more manageable sections that fit comfortably within the context window of the large language model.\n",
        "\n",
        "Langchain provides users with a range of chunking techniques to choose from. However, among these options, the RecursiveCharacterTextSplitter emerges as the favored and strongly recommended method.\n",
        "\n",
        "The RecursiveCharacterTextSplitter takes a large text and splits it based on a specified chunk size. It does this by using a set of characters. The default characters provided to it are [\"\\n\\n\", \"\\n\", \" \", \"\"].\n",
        "\n",
        "\n",
        "`How the text is split: by list of characters`\n",
        "____________________\n",
        "`How the chunk size is measured: by number of characters`\n",
        "______\n",
        "`trying to keep paragraphs, then sentences,then words`\n",
        "\n",
        "________\n",
        "\n",
        "Important parameters to know here are chunkSize and chunkOverlap. chunkSize controls the max size (in terms of number of characters) of the final documents. chunkOverlap specifies how much overlap there should be between chunks. This is often helpful to make sure that the text isn't split weirdly."
      ],
      "metadata": {
        "id": "Ys7Aj0oVTpja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "qPTURn64y11G"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text = \"\"\"Hi.\\n\\nI'm Harrison.\\n\\nHow? Are? You?\\nOkay then f f f f.\n",
        "This is a weird text to write, but gotta test the splittingggg some how.\\n\\n\n",
        "Bye!\\n\\n-H.\"\"\"\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 10,\n",
        "    chunk_overlap = 1\n",
        "\n",
        "\n",
        ")\n",
        "texts = text_splitter.split_text(text)\n",
        "\n",
        "print(len(texts))\n",
        "print(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlgo1KR7y124",
        "outputId": "e4c2c9af-4759-4252-8391-172699e03440"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18\n",
            "['Hi.', \"I'm\", 'Harrison.', 'How? Are?', 'You?', 'Okay then', 'f f f f.', 'This is a', 'weird', 'text to', 'write,', 'but gotta', 'test the', 'splitting', 'gggg', 'some how.', 'Bye!', '-H.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vJCbEESuy14n"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}
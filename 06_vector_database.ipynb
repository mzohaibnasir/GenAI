{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQuSVcLyDc0n0SDIu2JtQr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzohaibnasir/GenAI/blob/main/06_vector_database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "eCC232gMrD9q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# VECTOR DATABASE\n",
        "\n",
        "is a database for storing high dimensional vector such as word embeddings and image embeddings.\n",
        "A vector database stores pieces of information as vectors. Vector databases cluster related items together, enabling similarity searches and the construction of powerful AI models.\n",
        "\n",
        "# How do vector databases work?\n",
        "Each vector in a vector database corresponds to an object or item, whether that is a word, an image, a video, a movie, a document, or any other piece of data. These vectors are likely to be lengthy and complex, expressing the location of each object along dozens or even hundreds of dimensions.\n",
        "\n",
        "For example, a vector database of movies may locate movies along dimensions like running time, genre, year released, parental guidance rating, number of actors in common, number of viewers in common, and so on. If these vectors are created accurately, then similar movies are likely to end up clustered together in the vector database.\n",
        "\n",
        "# How are vector databases used?\n",
        "Similarity and semantic searches: Vector databases allow applications to connect pertinent items together. Vectors that are clustered together are similar and likely relevant to each other. This can help users search for relevant information (e.g. an image search), but it also helps applications:\n",
        "Recommend similar products\n",
        "Suggest songs, movies, or shows\n",
        "Suggest images or video\n",
        "Machine learning and deep learning: The ability to connect relevant items of information makes it possible to construct machine learning (and deep learning) models that can do complex cognitive tasks.\n",
        "Large language models (LLMs) and generative AI: LLMs, like that on which ChatGPT and Bard are built, rely on the contextual analysis of text made possible by vector databases. By associating words, sentences, and ideas with each other, LLMs can understand natural human language and even generate text.\n",
        "To summarize: Vector databases work at scale, work quickly, and are more cost-effective than querying machine learning models without them.\n",
        "\n",
        "\n",
        "\n",
        "# Embedding generation\n",
        "\n",
        "## non dl (frequency based)\n",
        "\n",
        "1. BOW(docmat)\n",
        "2. TF-IDF\n",
        "3. n-gram\n",
        "4. One hot encoding\n",
        "5. integer encoding\n",
        "\n",
        "## issues with non-dl\n",
        "\n",
        "### for One hot encoding & integer encoding\n",
        "\n",
        "1. sparse matrix(too many zeroes)\n",
        "2. no context\n",
        "\n",
        "### for BOW(docmat), TF-IDF & n-gram\n",
        "\n",
        "1. we create encoding using vocabularly\n",
        "2. still no context\n",
        "3. frequency based\n",
        "\n",
        "## with dl\n",
        "\n",
        "1. word2vec\n",
        "2. fast text\n",
        "3. ELMO\n",
        "4. BERT\n",
        "5. Glove(matrix factorization)\n",
        "\n",
        "### benefits\n",
        "\n",
        "1. creating dense vector\n",
        "2. context-full\n",
        "\n",
        "## WORD2VEC\n",
        "\n",
        "# `based on features i.e. king has features`\n",
        "\n",
        "we pass features into NN and we get embedding vector\n",
        "\n",
        "# Vector databases store embeddings. it indexes and store embeddings for faster retrieval and similarity search.\n",
        "\n",
        "1. are used in searching\n",
        "2. clustering where text strings are grouped by similarity\n",
        "3. Recommendation: related items are recommended\n",
        "4. classification\n"
      ],
      "metadata": {
        "id": "OivSdGFkxbXE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SzzRrQeGyqkb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ib_07YNvyuMR"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Pinecone Vector DB"
      ],
      "metadata": {
        "id": "LrLVm4Q2yuUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain\n",
        "! pip install pinecone-client\n",
        "! pip install openai\n",
        "! pip install tiktoken\n",
        "! pip install pypdf\n",
        "! pip install -U langchain_pinecone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJaxwnhyy0tD",
        "outputId": "457db79a-14dd-4eba-d282-798cc8f1058d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.16)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.33)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.45)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.49)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (3.2.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.2.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.11.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.23.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Requirement already satisfied: langchain_pinecone in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.40 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (0.1.45)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (1.25.2)\n",
            "Requirement already satisfied: pinecone-client<4.0.0,>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (3.2.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain_pinecone) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain_pinecone) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain_pinecone) (0.1.49)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain_pinecone) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain_pinecone) (2.7.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain_pinecone) (8.2.3)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (2024.2.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (4.11.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (2.0.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.40->langchain_pinecone) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain_pinecone) (3.10.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain_pinecone) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.40->langchain_pinecone) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.40->langchain_pinecone) (2.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain_pinecone) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain_pinecone) (3.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "# from langchain.vectorstores import Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os"
      ],
      "metadata": {
        "id": "Bz9GWKLJy1ss"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "PINECONE_API_KEY = userdata.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "\n",
        "\n",
        "# OPENAIAPIKEY,PINECONEAPIKEY"
      ],
      "metadata": {
        "id": "56ywTtO1zfWv"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we'll collect data from pdfs and convert it into embeddings"
      ],
      "metadata": {
        "id": "AxWpZ7HsI2bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## perpare data"
      ],
      "metadata": {
        "id": "UQHd4PgRQ6uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aNVPAW9y1u3",
        "outputId": "58c82218-2d9b-4d7c-fdb9-bb5510d97b8d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘pdfs’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
        "loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMwcCYHUIxAP",
        "outputId": "e832cd88-d5e9-497b-8efc-29523706d58e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.document_loaders.pdf.PyPDFDirectoryLoader at 0x7c57b7a2bee0>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()\n",
        "len(data), data[0], data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOeeDxu7y1xH",
        "outputId": "60f039e5-fc32-49da-c02b-3d83be22b257"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 3293 0 (offset 2397)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(581,\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPAn\\nIntroduction\\nto\\nInformation\\nRetrieval\\nDraft of April 1, 2009', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 0}),\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UP', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 1}))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3FWfg9kcy1zZ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization : dividing data into chunks"
      ],
      "metadata": {
        "id": "6yWIktVTQ-mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But what occurs when you present these models with a document that exceeds their context window? This is where a clever strategy known as \"chunking\" comes into play. Chunking involves dividing the document into smaller, more manageable sections that fit comfortably within the context window of the large language model.\n",
        "\n",
        "Langchain provides users with a range of chunking techniques to choose from. However, among these options, the RecursiveCharacterTextSplitter emerges as the favored and strongly recommended method.\n",
        "\n",
        "The RecursiveCharacterTextSplitter takes a large text and splits it based on a specified chunk size. It does this by using a set of characters. The default characters provided to it are [\"\\n\\n\", \"\\n\", \" \", \"\"].\n",
        "\n",
        "\n",
        "`How the text is split: by list of characters`\n",
        "____________________\n",
        "`How the chunk size is measured: by number of characters`\n",
        "______\n",
        "`trying to keep paragraphs, then sentences,then words`\n",
        "\n",
        "________\n",
        "\n",
        "Important parameters to know here are chunkSize and chunkOverlap. chunkSize controls the max size (in terms of number of characters) of the final documents. chunkOverlap specifies how much overlap there should be between chunks. This is often helpful to make sure that the text isn't split weirdly."
      ],
      "metadata": {
        "id": "Ys7Aj0oVTpja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_demo = \"\"\"Hi.\\n\\nI'm Harrison.\\n\\nHow? Are? You?\\nOkay then f f f f.\n",
        "This is a weird text to write, but gotta test the splittingggg some how.\\n\\n\n",
        "Bye!\\n\\n-H.\"\"\"\n",
        "\n",
        "\n",
        "text_splitter_demo = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 10,\n",
        "    chunk_overlap = 1\n",
        "\n",
        "\n",
        ")\n",
        "texts_demo = text_splitter_demo.split_text(text_demo)\n",
        "\n",
        "print(len(texts_demo))\n",
        "print(texts_demo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlgo1KR7y124",
        "outputId": "18d3b4cf-145f-45a5-8cb0-603001c58d2a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18\n",
            "['Hi.', \"I'm\", 'Harrison.', 'How? Are?', 'You?', 'Okay then', 'f f f f.', 'This is a', 'weird', 'text to', 'write,', 'but gotta', 'test the', 'splitting', 'gggg', 'some how.', 'Bye!', '-H.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 20\n",
        "\n",
        ")\n",
        "text_splitter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPTURn64y11G",
        "outputId": "c9eae4a0-c24a-4af5-e2d7-5018ca3414da"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_text_splitters.character.RecursiveCharacterTextSplitter at 0x7c57b7c21a50>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks = text_splitter.split_documents(data)\n",
        "len(text_chunks), text_chunks[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmxfts6vsWaf",
        "outputId": "034494c0-0dae-4494-e54d-3c5db3d779c4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3038,\n",
              " Document(page_content='Online edition (c)\\n2009 Cambridge UPAn\\nIntroduction\\nto\\nInformation\\nRetrieval\\nDraft of April 1, 2009', metadata={'source': 'pdfs/irbookonlinereading.pdf', 'page': 0}))"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_chunks[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJCbEESuy14n",
        "outputId": "ed02bed4-8844-452e-85e1-a11135e16736"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Online edition (c)\n",
            "2009 Cambridge UPAn\n",
            "Introduction\n",
            "to\n",
            "Information\n",
            "Retrieval\n",
            "Draft of April 1, 2009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_chunks[30].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0aJB9xJuVoz",
        "outputId": "dd48ef0b-d85f-4934-c526-1aeb74fc401d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20.1.2 Features a crawler should provide 444\n",
            "20.2 Crawling 444\n",
            "20.2.1 Crawler architecture 445\n",
            "20.2.2 DNS resolution 449\n",
            "20.2.3 The URL frontier 451\n",
            "20.3 Distributing indexes 454\n",
            "20.4 Connectivity servers 455\n",
            "20.5 References and further reading 458\n",
            "21Link analysis 461\n",
            "21.1 The Web as a graph 462\n",
            "21.1.1 Anchor text and the web graph 462\n",
            "21.2 PageRank 464\n",
            "21.2.1 Markov chains 465\n",
            "21.2.2 The PageRank computation 468\n",
            "21.2.3 Topic-speciﬁc PageRank 471\n",
            "21.3 Hubs and Authorities 474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create openai embdding class's objects"
      ],
      "metadata": {
        "id": "MoUuM4VKzqeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from openai import OpenAI\n",
        "# client = OpenAI(\n",
        "#     api_key=OPENAIAPIKEY\n",
        "# )\n",
        "\n",
        "# client"
      ],
      "metadata": {
        "id": "FPTQKXQIxTi1"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_openai import OpenAIEmbeddings\n",
        "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "# embedding"
      ],
      "metadata": {
        "id": "vFXnQpVXxUEI"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embedding.embed_query(\n",
        "    \"how are you\"\n",
        ")) # len of embedding vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVUhFkNcxUGg",
        "outputId": "d683ece0-0f82-4464-f6bd-a8b2de92002a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import pinecone"
      ],
      "metadata": {
        "id": "HDmfkLx92gjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### we'll create embedding for each text chunk\n"
      ],
      "metadata": {
        "id": "J19xSictRpnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "pinecone is running on cloud so\n",
        "\n",
        "\n",
        "\n",
        "The PineconeVectorStore class provided by LangChain can be used to interact\n",
        "with Pinecone indexes. It’s important to remember that you must have an\n",
        "existing Pinecone index before you can create a PineconeVectorStore object.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "# vectorstore = PineconeVectorStore(\n",
        "#     # pinecone_api_key= PINECONE_API_KEY,\n",
        "#     index_name=\"testing\",\n",
        "#     embedding=embedding)\n",
        "\n",
        "# vectorstore"
      ],
      "metadata": {
        "id": "vaEHYmkR1A3l"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "The from_documents and from_texts methods of LangChain’s PineconeVectorStore class\n",
        " add records to a Pinecone index and return a PineconeVectorStore object.\n",
        "\n",
        "The from_documents method accepts a list of LangChain’s Document class objects,\n",
        "which can be created using LangChain’s CharacterTextSplitter class. The\n",
        "from_texts method accepts a list of strings. Similarly to above, you must\n",
        "provide the name of an existing Pinecone index and an Embeddings object.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "index_name = 'testing'\n",
        "\n",
        "# vectorstore_from_docs = PineconeVectorStore.from_documents(\n",
        "#         text_chunks,\n",
        "#         index_name=index_name,\n",
        "#         embedding=embedding,\n",
        "#     )\n",
        "# vectorstore_from_docs"
      ],
      "metadata": {
        "id": "cQgScyH41A6A"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorstore_from_texts = PineconeVectorStore.from_texts(\n",
        "#         [t.page_content for t in text_chunks],\n",
        "#         index_name=index_name,\n",
        "#         embedding=embedding,\n",
        "#     )\n",
        "# vectorstore_from_texts"
      ],
      "metadata": {
        "id": "Q5xszqaCamZ_"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = PineconeVectorStore.from_texts(\n",
        "        [t.page_content for t in text_chunks],\n",
        "        index_name=index_name,\n",
        "        embedding=embedding,\n",
        "    )\n",
        "docsearch # you can see all embedings on 'testing' index on pinecone.io too"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea_zL6sSouE4",
        "outputId": "85c043c2-925b-49b9-f28f-509b37bf9413"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x7c579587d810>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### doing similarity search"
      ],
      "metadata": {
        "id": "NPr2oVv5quR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query =\" what's best ML model?\""
      ],
      "metadata": {
        "id": "h9CNFusq1A79"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = docsearch.similarity_search(query)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDHjfQUJxULP",
        "outputId": "5e282bc7-9758-4830-8c73-fab146ea45bc"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='ear classiﬁcation that we have already looked at in Chapters 13–15provide\\nmethods for choosing this line. Provided we can build a sufﬁc iently rich col-\\nlection of training samples, we can thus altogether avoid ha nd-tuning score\\nfunctions as in Section 7.2.3 (page 145). The bottleneck of course is the ability\\nto maintain a suitably representative set of training examp les, whose rele-\\nvance assessments must be made by experts.\\n15.4.2 Result ranking by machine learning'),\n",
              " Document(page_content='ear classiﬁcation that we have already looked at in Chapters 13–15provide\\nmethods for choosing this line. Provided we can build a sufﬁc iently rich col-\\nlection of training samples, we can thus altogether avoid ha nd-tuning score\\nfunctions as in Section 7.2.3 (page 145). The bottleneck of course is the ability\\nto maintain a suitably representative set of training examp les, whose rele-\\nvance assessments must be made by experts.\\n15.4.2 Result ranking by machine learning'),\n",
              " Document(page_content='13.2 Naive Bayes algorithm (multinomial model): Training a nd\\ntesting. 260\\n13.3 NB algorithm (Bernoulli model): Training and testing. 263\\n13.4 The multinomial NB model. 266\\n13.5 The Bernoulli NB model. 267\\n13.6 Basic feature selection algorithm for selecting the kbest features. 271\\n13.7 Features with high mutual information scores for six\\nReuters-RCV1 classes. 274\\n13.8 Effect of feature set size on accuracy for multinomial a nd\\nBernoulli models. 275'),\n",
              " Document(page_content='13.2 Naive Bayes algorithm (multinomial model): Training a nd\\ntesting. 260\\n13.3 NB algorithm (Bernoulli model): Training and testing. 263\\n13.4 The multinomial NB model. 266\\n13.5 The Bernoulli NB model. 267\\n13.6 Basic feature selection algorithm for selecting the kbest features. 271\\n13.7 Features with high mutual information scores for six\\nReuters-RCV1 classes. 274\\n13.8 Effect of feature set size on accuracy for multinomial a nd\\nBernoulli models. 275')]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm=OpenAI()"
      ],
      "metadata": {
        "id": "DL4whYPkxUNg"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM9eGF8AtVHQ",
        "outputId": "c478b3ed-f80a-41b3-f496-228162590c79"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x7c579587d810>)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(  # responsible for question answer\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=docsearch.as_retriever()   # docsearch is where all embeddings are stored\n",
        ")\n",
        "# qa"
      ],
      "metadata": {
        "id": "cYLe4JlQrdpE"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "mulyWqRUrdsK",
        "outputId": "fdf83017-fbe1-4bed-99a3-b5e1ca69e194"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBased on the context provided, it seems that the best machine learning model in this scenario would be the multinomial NB model. However, it is important to note that this may vary depending on the specific data and task at hand.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### to make a small QnA system(PDF based)\n",
        "\n",
        "we'll ask from PDF"
      ],
      "metadata": {
        "id": "XZ9_hsFutlnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "while True:\n",
        "  user_input = input(f\"Input Prompt: \")\n",
        "  if user_input == 'exit':\n",
        "    print(';Exiting')\n",
        "    sys.exit()\n",
        "  if user_input == \"\":\n",
        "    continue\n",
        "  result = qa.invoke({\n",
        "      'query': user_input\n",
        "  })\n",
        "  print(f\"Answer:{result['result']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "hSwsuKSxrduM",
        "outputId": "1c893977-68d7-45d9-f861-99c310283de1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Prompt: exit\n",
            ";Exiting\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MAdDItUbrdwA"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KqxwSLY9rd0C"
      },
      "execution_count": 67,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0QDtEAzd7vFO0tWQA3rIO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzohaibnasir/GenAI/blob/main/09_EndtoEndMedicalChatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-End Medical Chatbot\n",
        "\n",
        "* will be based on out custom data. (PDF files)corpus would be a book\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Process:\n",
        "\n",
        "### Backend Component\n",
        "*** [ Data ingestion ] ***  --->  *** [ extract data/content ] ***  --->  *** [ split into text chunks(multiple chunks to facilitate input context window. context window token limit: 4096 )] ***  --->  *** [embeddings of chunks] ***  --->  *** [build semantic index based on all embeddings (chunk overlap would be beneficial here. Due to this chunk overlap, model would be able to create a relation between different chunks) ] ***  --->  *** [ building Knowledge base using pinecone vector store ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "-------------------------------------------------------------- ---\n",
        "**RAG is used to fix hellucinationsiu. RAG is base don our own populated vector database.it also gives context based on metadata we input through input docs.**\n",
        "**In vector db indexes are being updated instantaneously**\n",
        "**In vector db, neighbors are semantically close**\n",
        "\n",
        "In vector db, we have two concepts:\n",
        "\n",
        "### knowledge base\n",
        "### semantic index\n",
        "will build clusters based on semantic index(king and queen will be in single cluster based on distance between vectors\n",
        "\n",
        "\n",
        "-------------------------------------------------\n",
        "### chunk overlap : is to get some context from previous text.\n",
        "Chunk overlap is used in LLM (Large Language Models) to address the issue of context windows. LLMs process input text in fixed-size chunks, but sometimes, the context required to understand the text exceeds the chunk size. By using chunk overlap, the model can consider the surrounding context, even if it's in a different chunk.\n",
        "Here's how it works:\n",
        "The input text is divided into overlapping chunks, typically with a overlap of 50-100 tokens.\n",
        "Each chunk is processed separately, but the model has access to the context from the previous chunk (due to the overlap).\n",
        "This allows the model to capture long-range dependencies and relationships between different parts of the input text.\n",
        "Chunk overlap helps LLMs to:\n",
        "Improve context awareness\n",
        "Enhance understanding of complex texts\n",
        "Reduce the impact of chunk boundaries on model performance\n",
        "It's a technique to help LLMs better understand the input text and generate more accurate and coherent responses."
      ],
      "metadata": {
        "id": "Y0IvJsKEWFVJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VK9CdkZUVMYJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3jheGI7VWGh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KcrfL_isWGkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_BAL9j7aWGnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4qjL2ONuWGqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tW2rLupoWGs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C8eB8R1PWGvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q9cKZmW2WGxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jd-LbPwKWGzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RYkBQq-kWG3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jcywVuRSWG5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IMmNWgu6WG7c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}